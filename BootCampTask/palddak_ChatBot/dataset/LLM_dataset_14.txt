텍스트 처리의 핵심 기법과 임베딩 활용하기📕 LLM & RAG를 활용한 AI 서비스 만들기/📘 LLM & RAG를 활용한 AI 서비스 만들기 - 5주차/📕텍스트 처리의 핵심 기법과 임베딩 활용하기📕텍스트 처리의 핵심 기법과 임베딩 활용하기수업 목표주요 텍스트 처리 기법인 토큰화, 정규화 등을 학습합니다.임베딩의 개념과 중요성에 대해 학습합니다.목차텍스트 처리가 중요한 이유텍스트 처리의 목표주요 텍스트 처리 기법토큰화 (Tokenization)정규화 (Normalization)불용어 제거 (Stopword Removal)형태소 분석 (Morphological Analysis)어간 추출과 표제어 추출 (Stemming and Lemmatization)문장 분리 및 길이 조정임베딩의 개념과 중요성임베딩(Embedding)란?대표적인 임베딩 기법Bag of Words (BoW)TF-IDF (Term Frequency-Inverse Document Frequency)Word2Vec, GloVeTransformer 기반 임베딩 (BERT, GPT)배운 내용 정리하기!❗LLM(대규모 언어 모델)을 효율적으로 활용하려면 텍스트 처리가 기본이 되어야 합니다. 텍스트 처리는 모델이 자연어를 이해하고 분석할 수 있게 돕는 첫 단계이죠. 또한, 텍스트를 벡터로 변환하는 임베딩(embedding) 기법은 문장의 의미를 숫자로 표현해 모델이 문서의 유사성을 파악하거나 정확한 추론을 할 수 있게 해줘요.이번 강의에서는 텍스트 처리의 주요 기법과 임베딩의 개념을 살펴보며, 이 두 가지가 LLM에서 왜 중요한지, 그리고 실제로 어떻게 활용되는지 알아보겠습니다.텍스트 처리가 중요한 이유💡텍스트 처리(Text Preprocessing)는 데이터의 품질을 높이고 모델의 성능을 향상시키기 위한 필수 작업이에요. 자연어는 매우 복잡하고 다양하기 때문에, LLM이 텍스트를 정확하게 이해하고 처리하기 위해서는 데이터가 구조화되고 정제될 필요가 있어요. 잘못된 텍스트 처리 과정은 모델이 혼동하거나 잘못된 추론을 하게 할 수 있습니다.텍스트 처리의 목표노이즈 제거: 텍스트 내 불필요한 정보나 오류를 제거해 정확한 분석을 할 수 있도록 합니다.일관성 확보: 문장의 구조나 형태를 일관되게 유지하여 모델이 더 쉽게 패턴을 학습하게 돕습니다.효율적인 처리: 불필요한 단어를 제거하고 중요한 정보만 남겨, 모델이 더 빠르게 계산할 수 있도록 해줍니다.주요 텍스트 처리 기법1️⃣토큰화 (Tokenization)토큰화는 텍스트를 단어 또는 서브워드 단위로 나누는 작업입니다. 이 과정은 텍스트를 숫자로 변환하기 전의 가장 중요한 단계에 해당해요.단어 단위 토큰화: 텍스트를 단어 단위로 나누는 기본 방법입니다.예: "나는 오늘 책을 읽었다." → ["나는", "오늘", "책을", "읽었다"]서브워드 토큰화: 단어를 더 작은 단위로 분리해 새로운 단어를 처리할 수 있도록 합니다. BPE나 WordPiece 같은 방법이 있어요.예: "읽었다" → ["읽", "었다"]이렇게 나눠진 토큰은 모델이 이해할 수 있는 형태로 변환됩니다.2️⃣정규화 (Normalization)정규화는 텍스트를 표준화된 형식으로 변환하는 작업이에요. 텍스트에 포함된 대소문자, 특수문자 등을 일관되게 변환하여, 모델이 불필요한 변동에 혼란을 겪지 않도록 합니다.소문자 변환: 대문자와 소문자를 통일하여 같은 단어로 인식하게 합니다.예: "OpenAI" → "openai"불필요한 기호 제거: 분석에 필요 없는 특수문자나 기호를 제거합니다.예: "서울, 2023년!" → "서울 2023년"정규화를 통해 모델이 텍스트의 의미에 집중하게 할 수 있어요.3️⃣불용어 제거 (Stopword Removal)불용어란 자주 등장하지만 정보가 없는 단어를 말해요. 예를 들어, "그리고", "이", "는" 같은 단어들은 문맥에 큰 영향을 미치지 않기 때문에 불용어로 처리해요. 이를 제거하면 모델이 중요한 단어에만 집중할 수 있습니다.예: "나는 책을 읽었다." → ["책", "읽었다"]4️⃣형태소 분석 (Morphological Analysis)한국어와 같은 교착어에서는 형태소 분석이 필수적이에요. 형태소는 단어의 최소 의미 단위로, 한국어에서 특히 조사나 어미와 같은 부분을 정확하게 분리해내는 데 유용합니다.예: "책을 읽었다." → [책(Noun), 을(Postposition), 읽었다(Verb)]5️⃣어간 추출과 표제어 추출 (Stemming and Lemmatization)텍스트에서 동사나 형용사의 변형을 기본 형태로 돌리는 작업입니다. 이를 통해 동일한 단어를 일관되게 처리할 수 있어요.어간 추출은 단어에서 어미를 제거하고, 기본 어간만 남깁니다.예: "studying", "studied", "study" → "study"표제어 추출은 단어를 사전적 기본형으로 변환합니다.예: "am", "is", "are" → "be"이 과정은 동사나 형용사처럼 변형이 많은 단어를 처리할 때 매우 유용해요.6️⃣문장 분리 및 길이 조정텍스트가 너무 길거나 복잡할 경우, 이를 적절하게 나누거나 길이를 조정해야 해요. 긴 문장을 처리할 때, 모델의 메모리 제한이나 성능 저하를 방지할 수 있습니다.임베딩의 개념과 중요성임베딩(Embedding)란?💡임베딩은 텍스트 데이터를 **벡터(숫자 배열)**로 변환하는 과정이에요. 
LLM이 텍스트의 의미를 이해하려면 텍스트가 숫자로 변환되어야 하는데, 그 과정에서 각 문장이나 단어를 벡터 공간에 매핑하게 됩니다. 이를 통해 모델이 의미적 유사성을 파악하고, 효율적인 검색이나 문서 분류를 할 수 있어요.임베딩 벡터는 단어의 의미나 문장의 문맥을 반영하여, 
유사한 의미를 가진 텍스트는 유사한 벡터 값을 가집니다.대표적인 임베딩 기법1️⃣Bag of Words (BoW)Bag of Words는 단어의 빈도만을 기반으로 텍스트를 벡터화하는 가장 단순한 방법입니다. 단어의 순서나 문맥을 고려하지 않기 때문에 의미 파악에 한계가 있지만, 간단한 문서 분류나 텍스트 분석에 유용해요.문장: "고양이가 야옹했다"벡터 표현: [1, 1, 1, 0, 0] (각 단어의 빈도수)2️⃣TF-IDF (Term Frequency-Inverse Document Frequency)TF-IDF는 단순한 단어 빈도 외에도 단어의 중요도를 반영한 임베딩 기법이에요. 특정 단어가 문서 내에서 자주 등장하지만 전체 문서에서 드물게 등장한다면, 그 단어는 해당 문서에서 중요한 단어로 간주됩니다.TF: 단어의 빈도IDF: 단어의 전체 문서에서의 등장 빈도 반비례값이를 통해 문서 내에서 의미 있는 단어를 강조할 수 있어요.3️⃣Word2Vec, GloVeWord2Vec과 GloVe는 단어 간의 의미적 유사성을 반영하는 임베딩 기법이에요. 단어를 고차원 벡터로 변환하여, 단어 간의 관계를 학습합니다.Word2Vec: 주위 단어들에 기반해 단어의 의미를 학습GloVe: 전체 문맥을 기반으로 단어 간의 공통 패턴을 학습이러한 임베딩 기법을 사용하면, 단어의 의미를 벡터로 비교해 문맥 유사성을 파악할 수 있어요.4️⃣Transformer 기반 임베딩 (BERT, GPT)BERT나 GPT 같은 **Transformer* 모델들은 문장의 문맥을 고려하여 더 깊이 있는 의미를 반영한 임베딩을 생성해요. 특히, 이들은 문장 단위로 텍스트를 벡터화할 수 있어 문장 간의 유사도를 정확하게 파악합니다.BERT: 양방향으로 문맥을 고려한 임베딩 생성GPT: 자동 완성 및 생성에 강점을 둔 임베딩 생성임베딩의 활용임베딩을 활용하면 텍스트 검색, 문서 분류, 대화형 AI 등 다양한 응용 분야에서 의미 기반 검색과 유사성 분석을 수행할 수 있습니다.배운 내용 정리하기!텍스트 처리와 임베딩은 필수적인 기초!텍스트 처리와 임베딩은 LLM 시스템 구축의 핵심 기초입니다. 정확한 텍스트 처리와 적절한 임베딩 기법을 활용하면, LLM의 성능을 최대한 끌어올리고 더 나은 결과를 얻을 수 있어요.텍스트 처리는 모델의 입력을 정제하고, 임베딩은 모델이 추론할 수 있도록 텍스트를 벡터화합니다. 이 둘을 잘 이해하고 적용하는 것이 LLM 시스템 구축의 첫걸음이에요!