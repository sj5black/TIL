Python LangChainê³¼ FAISSğŸ“• LLM & RAGë¥¼ í™œìš©í•œ AI ì„œë¹„ìŠ¤ ë§Œë“¤ê¸°/ğŸ“˜ LLM & RAGë¥¼ í™œìš©í•œ AI ì„œë¹„ìŠ¤ ë§Œë“¤ê¸° - 5ì£¼ì°¨/ğŸ“•Python LangChainê³¼ FAISSMade withğŸ“•Python LangChainê³¼ FAISSìˆ˜ì—… ëª©í‘œLangChain ì‚¬ìš©ì„ ìœ„í•œ í™˜ê²½ ì„¤ì •ì„ ì§„í–‰í•©ë‹ˆë‹¤.FAISSë¥¼ í™œìš©í•œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ êµ¬ì„±í•˜ëŠ” ì‹¤ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤.ëª©ì°¨ì„¤ì¹˜ ë° ê¸°ë³¸ ì„¤ì •LangChain ê¸°ë³¸ ê°œë…ì–¸ì–´ ëª¨ë¸ ì´ˆê¸°í™”í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‚¬ìš©í•˜ê¸°LangChain Expression Language (LCEL)ë¡œ ì²´ì¸ ì—°ê²°FAISSë¥¼ í™œìš©í•œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì„± ë° ì¿¼ë¦¬Step 1: OpenAI ì„ë² ë”© ëª¨ë¸ë¡œ ë²¡í„° ì„ë² ë”© ìƒì„±Step 2: FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™”Step 3: ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ë¬¸ì„œ ì¶”ê°€Step 4: ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬RAG ì²´ì¸ì— FAISS í†µí•©Step 1: Retrieverë¡œ ë³€í™˜Step 2: RAG ì²´ì¸ ìƒì„±FAISS ì¸ë±ìŠ¤ì˜ ì €ì¥ ë° ë¡œë“œFAISS ë°ì´í„°ë² ì´ìŠ¤ ë³‘í•©â—ì´ë²ˆì—ëŠ” LangChainê³¼ FAISSë¥¼ ì´ìš©í•œ ì‹¤ìŠµì„ ì§„í–‰í•  ì˜ˆì •ì¸ë°ìš”,
ì‹¤ìŠµì€ ì•„ë˜ì™€ ê°™ì€ ìˆœì„œë¡œ ì§„í–‰ë©ë‹ˆë‹¤!

ì„¤ì¹˜ ë° ê¸°ë³¸ ì„¤ì •LangChain ê¸°ë³¸ ê°œë… - ì–¸ì–´ ëª¨ë¸, í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿LangChain Expression Language (LCEL)ë¡œ ì²´ì¸ ì—°ê²°FAISSë¥¼ í™œìš©í•œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì„± ë° ì¿¼ë¦¬RAG ì²´ì¸ì— FAISS í†µí•©FAISS ì¸ë±ìŠ¤ì˜ ì €ì¥ ë° ë¡œë“œ, ë³‘í•©ì„¤ì¹˜ ë° ê¸°ë³¸ ì„¤ì •ğŸ’¡LangChain, OpenAI, ê·¸ë¦¬ê³  FAISS íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤. GPUë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš° faiss-gpuë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ShellCopypip install langchain langchain-openai faiss-cpu

â€‹
ì„¤ì¹˜ í›„, OpenAI API í‚¤ë¥¼ ì„¤ì •í•´ ì‚¬ìš© í™˜ê²½ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.PythonCopyimport os
from getpass import getpass

os.environ["OPENAI_API_KEY"] = getpass("OpenAI API key ì…ë ¥: ")

â€‹LangChain ê¸°ë³¸ ê°œë…ì–¸ì–´ ëª¨ë¸ ì´ˆê¸°í™”OpenAIì˜ GPT-4 ëª¨ë¸ì„ LangChainì„ í†µí•´ ì‚¬ìš©í•´ ë´…ë‹ˆë‹¤. ChatOpenAIë¥¼ ì´ìš©í•´ ì´ˆê¸°í™”í•˜ê³  invoke ë©”ì„œë“œë¥¼ í†µí•´ ë©”ì‹œì§€ë¥¼ ì „ë‹¬í•˜ì—¬ ì‘ë‹µì„ ë°›ì•„ì˜µë‹ˆë‹¤.PythonCopyfrom langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

# ëª¨ë¸ ì´ˆê¸°í™”
model = ChatOpenAI(model="gpt-4")
# ëª¨ë¸ì— ë©”ì‹œì§€ ì „ë‹¬
response = model.invoke([HumanMessage(content="ì•ˆë…•í•˜ì„¸ìš”, ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")])
print(response.content)

â€‹í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‚¬ìš©í•˜ê¸°í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì€ ë‹¤ì–‘í•œ ì…ë ¥ì„ ë°›ì•„ ë©”ì‹œì§€ë¥¼ ìƒì„±í•˜ëŠ”ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì˜ì–´ ë¬¸ì¥ì„ ë‹¤ë¥¸ ì–¸ì–´ë¡œ ë²ˆì—­í•˜ëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ ì •ì˜í•´ ë´…ì‹œë‹¤.PythonCopyfrom langchain_core.prompts import ChatPromptTemplate

# ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì„¤ì •
system_template = "Translate the following sentence from English to {language}:"
# ì‚¬ìš©ì í…ìŠ¤íŠ¸ ì…ë ¥
prompt_template = ChatPromptTemplate.from_messages([
("system", system_template),
("user", "{text}")
])
# í”„ë¡¬í”„íŠ¸ ìƒì„±
result = prompt_template.invoke({"language": "French", "text": "How are you?"})
print(result.to_messages())

â€‹LangChain Expression Language (LCEL)ë¡œ ì²´ì¸ ì—°ê²°ì—¬ëŸ¬ ì»´í¬ë„ŒíŠ¸ë¥¼ ì²´ì¸ìœ¼ë¡œ ì—°ê²°í•˜ì—¬ ë°ì´í„° íë¦„ì„ í†µì œí•˜ëŠ” LCELì„ ì‚¬ìš©í•©ë‹ˆë‹¤.PythonCopyfrom langchain_core.output_parsers import StrOutputParser

# ì‘ë‹µì„ íŒŒì‹±í•˜ëŠ” íŒŒì„œ ì´ˆê¸°í™”
parser = StrOutputParser()
# í…œí”Œë¦¿, ëª¨ë¸, íŒŒì„œë¥¼ ì²´ì¸ìœ¼ë¡œ ì—°ê²°
chain = prompt_template | model | parser

# ì²´ì¸ ì‹¤í–‰
response = chain.invoke({"language": "Spanish", "text": "Where is the library?"})
print(response)

â€‹FAISSë¥¼ í™œìš©í•œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì„± ë° ì¿¼ë¦¬FAISSëŠ” ë²¡í„° ìœ ì‚¬ì„± ê²€ìƒ‰ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤. OpenAIEmbeddingsë¡œ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•´ FAISS ì¸ë±ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤.Step 1: OpenAI ì„ë² ë”© ëª¨ë¸ë¡œ ë²¡í„° ì„ë² ë”© ìƒì„±PythonCopyfrom langchain_openai import OpenAIEmbeddings

# OpenAI ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
embeddings = OpenAIEmbeddings(model="text-embedding-ada-002")

â€‹Step 2: FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™”PythonCopyimport faiss
from langchain_community.vectorstores import FAISS
from langchain_community.docstore.in_memory import InMemoryDocstore

# FAISS ì¸ë±ìŠ¤ ìƒì„±
index = faiss.IndexFlatL2(len(embeddings.embed_query("hello world")))
vector_store = FAISS(
    embedding_function=embeddings,
    index=index,
    docstore=InMemoryDocstore(),
    index_to_docstore_id={}
)

â€‹Step 3: ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ë¬¸ì„œ ì¶”ê°€PythonCopyfrom langchain_core.documents import Document
from uuid import uuid4

# ë¬¸ì„œ ìƒì„±
documents = [
    Document(page_content="LangChainì„ ì‚¬ìš©í•´ í”„ë¡œì íŠ¸ë¥¼ êµ¬ì¶•í•˜ê³  ìˆìŠµë‹ˆë‹¤!", metadata={"source": "tweet"}),
    Document(page_content="ë‚´ì¼ ë‚ ì”¨ëŠ” ë§‘ê³  ë”°ëœ»í•  ì˜ˆì •ì…ë‹ˆë‹¤.", metadata={"source": "news"}),
    Document(page_content="ì˜¤ëŠ˜ ì•„ì¹¨ì—ëŠ” íŒ¬ì¼€ì´í¬ì™€ ê³„ë€ì„ ë¨¹ì—ˆì–´ìš”.", metadata={"source": "personal"}),
    Document(page_content="ì£¼ì‹ ì‹œì¥ì´ ê²½ê¸° ì¹¨ì²´ ìš°ë ¤ë¡œ í•˜ë½ ì¤‘ì…ë‹ˆë‹¤.", metadata={"source": "news"}),
]
# ê³ ìœ  ID ìƒì„± ë° ë¬¸ì„œ ì¶”ê°€
uuids = [str(uuid4()) for _ in range(len(documents))]
vector_store.add_documents(documents=documents, ids=uuids)

â€‹Step 4: ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ìœ ì‚¬ì„± ê²€ìƒ‰ì„ í†µí•´ íŠ¹ì • ì¿¼ë¦¬ì™€ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•´ë³´ê² ìŠµë‹ˆë‹¤.PythonCopy# ê¸°ë³¸ ìœ ì‚¬ì„± ê²€ìƒ‰
results = vector_store.similarity_search("ë‚´ì¼ ë‚ ì”¨ëŠ” ì–´ë–¨ê¹Œìš”?", k=2, filter={"source": "news"})
for res in results:
print(f"* {res.page_content} [{res.metadata}]")
# ì ìˆ˜ì™€ í•¨ê»˜ ìœ ì‚¬ì„± ê²€ìƒ‰
results_with_scores = vector_store.similarity_search_with_score("LangChainì— ëŒ€í•´ ì´ì•¼ê¸°í•´ì£¼ì„¸ìš”.", k=2, filter={"source": "tweet"})
for res, score in results_with_scores:
print(f"* [SIM={score:.3f}] {res.page_content} [{res.metadata}]")

â€‹RAG ì²´ì¸ì— FAISS í†µí•©ğŸ’¡RAG (Retrieval-Augmented Generation) ì²´ì¸ì„ êµ¬ì„±í•˜ì—¬ ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ì‘ë‹µí•  ìˆ˜ ìˆë„ë¡ êµ¬ì„±í•©ë‹ˆë‹¤.Step 1: Retrieverë¡œ ë³€í™˜FAISSë¥¼ retrieverë¡œ ë³€í™˜í•´ RAG ì²´ì¸ì—ì„œ ì‚¬ìš©í•©ë‹ˆë‹¤.PythonCopyretriever = vector_store.as_retriever(search_type="similarity", search_kwargs={"k": 1})

â€‹Step 2: RAG ì²´ì¸ ìƒì„±LangChainì˜ ëª¨ë¸ê³¼ í”„ë¡¬í”„íŠ¸ë¥¼ ì—°ê²°í•˜ì—¬ RAG ì²´ì¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤.PythonCopy
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
contextual_prompt = ChatPromptTemplate.from_messages([
("system", "Answer the question using only the following context."),
("user", "Context: {context}\\: {question}")
])
class DebugPassThrough(RunnablePassthrough):
def invoke(self, *args, **kwargs):
        output = super().invoke(*args, **kwargs)
print("Debug Output:", output)
return output
# ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” ë‹¨ê³„ ì¶”ê°€
class ContextToText(RunnablePassthrough):
def invoke(self, inputs, config=None, **kwargs): # config ì¸ìˆ˜ ì¶”ê°€
# contextì˜ ê° ë¬¸ì„œë¥¼ ë¬¸ìì—´ë¡œ ê²°í•©
        context_text = "".join([doc.page_content for doc in inputs["context"]])
return {"context": context_text, "question": inputs["question"]}
# RAG ì²´ì¸ì—ì„œ ê° ë‹¨ê³„ë§ˆë‹¤ DebugPassThrough ì¶”ê°€
rag_chain_debug = {
"context": retriever, # ì»¨í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜¤ëŠ” retriever
"question": DebugPassThrough() # ì‚¬ìš©ì ì§ˆë¬¸ì´ ê·¸ëŒ€ë¡œ ì „ë‹¬ë˜ëŠ”ì§€ í™•ì¸í•˜ëŠ” passthrough
} | DebugPassThrough() | ContextToText()|   contextual_prompt | model

# ì§ˆë¬¸ ì‹¤í–‰ ë° ê° ë‹¨ê³„ ì¶œë ¥ í™•ì¸
response = rag_chain_debug.invoke("ê°•ì‚¬ì´ë¦„ì€?")
print("Final Response:")
print(response.content)
â€‹FAISS ì¸ë±ìŠ¤ì˜ ì €ì¥ ë° ë¡œë“œFAISS ì¸ë±ìŠ¤ë¥¼ ì €ì¥í•´ ë‹¤ì‹œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.PythonCopy# ì¸ë±ìŠ¤ ì €ì¥
vector_store.save_local("faiss_index")
# ì €ì¥ëœ ì¸ë±ìŠ¤ ë¡œë“œ
new_vector_store = FAISS.load_local("faiss_index", embeddings)

â€‹FAISS ë°ì´í„°ë² ì´ìŠ¤ ë³‘í•©ë‘ ê°œì˜ FAISS ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë³‘í•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.PythonCopydb1 = FAISS.from_texts(["ë¬¸ì„œ 1 ë‚´ìš©"], embeddings)
db2 = FAISS.from_texts(["ë¬¸ì„œ 2 ë‚´ìš©"], embeddings)
# ë³‘í•©
db1.merge_from(db2)

â€‹