[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 7. ì–´í…ì…˜ (Attention) ë©”ì»¤ë‹ˆì¦˜ğŸ“˜[SCC] ê¸°ì´ˆê°€ íƒ„íƒ„í•œ ë”¥ëŸ¬ë‹/ğŸ“š[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] ê¸°ì´ˆê°€ íƒ„íƒ„í•œ ë”¥ëŸ¬ë‹ - 3ì£¼ì°¨/ğŸ“•[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 7. ì–´í…ì…˜ (Attention) ë©”ì»¤ë‹ˆì¦˜Made withğŸ“•[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 7. ì–´í…ì…˜ (Attention) ë©”ì»¤ë‹ˆì¦˜[ìˆ˜ì—… ëª©í‘œ]ìµœê·¼ ê°€ì¥ ì„±ëŠ¥ ì¢‹ì€ ë§¤ì»¤ë‹ˆì¦˜! ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì— ëŒ€í•´ ì•Œì•„ë´…ì‹œë‹¤Pytorchì˜ êµ¬í˜„ ì˜ˆì‹œë¥¼ ì‚´í´ë´…ì‹œë‹¤[ëª©ì°¨]01. ê°œë…02. ì‹¤ìŠµ:  Attention ë©”ì»¤ë‹ˆì¦˜ì˜ êµ¬í˜„í•œë²ˆ í›‘ëŠ” ì •ë„ë¡œ ë„˜ì–´ê°‘ì‹œë‹¤!1)  Attentionâ˜‘ï¸ Scaled Dot-Product AttentionScaled Dot-Product attention ë©”ì»¤ë‹ˆì¦˜ êµ¬í˜„{5px}Scaled Dot-Product attention ë©”ì»¤ë‹ˆì¦˜ êµ¬í˜„ï»¿â€‹PythonCopyimport torch
import torch.nn.functional as F

def scaled_dot_product_attention(Q, K, V):
    d_k = Q.size(-1) # Keyì˜ ì°¨ì› ìˆ˜
    scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32)) # ìœ ì‚¬ë„ ê³„ì‚° ë° ìŠ¤ì¼€ì¼ë§
    attn_weights = F.softmax(scores, dim=-1) # Softmaxë¥¼ í†µí•œ ê°€ì¤‘ì¹˜ ê³„ì‚°
    output = torch.matmul(attn_weights, V) # ê°€ì¤‘í•©ì„ í†µí•œ ìµœì¢… ì¶œë ¥ ê³„ì‚°
return output, attn_weights

â€‹â˜‘ï¸ Multi-Head Attention Multi-Head Attention ë©”ì»¤ë‹ˆì¦˜ êµ¬í˜„{5px} Multi-Head Attention ë©”ì»¤ë‹ˆì¦˜ êµ¬í˜„ï»¿â€‹PythonCopyclass MultiHeadAttention(nn.Module):
def __init__(self, embed_size, heads):
super(MultiHeadAttention, self).__init__()
        self.embed_size = embed_size
        self.heads = heads
        self.head_dim = embed_size // heads

        assert (
            self.head_dim * heads == embed_size
        ), "Embedding size needs to be divisible by heads"

        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)
        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)
        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)
        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)
def forward(self, values, keys, query, mask=None):
        N = query.shape[0]
        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]
# Linear transformations
        values = self.values(values).view(N, value_len, self.heads, self.head_dim)
        keys = self.keys(keys).view(N, key_len, self.heads, self.head_dim)
        queries = self.queries(query).view(N, query_len, self.heads, self.head_dim)
# Scaled dot-product attention
        out, _ = scaled_dot_product_attention(queries, keys, values)

        out = out.view(N, query_len, self.heads * self.head_dim)
        out = self.fc_out(out)
return out

â€‹Copyright â“’ TeamSparta All rights reserved.