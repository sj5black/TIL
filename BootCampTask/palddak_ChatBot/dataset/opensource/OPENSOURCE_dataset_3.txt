import pandas as pdimport numpy as npimport matplotlib.pyplot as pltimport seaborn as snsimport warningswarnings.simplefilter(action='ignore', category=futurewarning)!sudo apt-get install -y fonts-nanum!sudo fc-cache -fv!rm ~/.cache/matplotlib -rfimport matplotlib as mplimport matplotlib.font_manager as fmimport matplotlib.pyplot as plt#plt.rc('font', family='nanumsquareround') import matplotlib.pyplot as pltplt.rc('font', family='nanumsquareround') #목차### 분석 배경 및 분석 목적### 1.데이터셋 준비#### 1-1) 결측치, 이상치 확인###2. eda####2-1) 그래프####2-2) 분석 ###3. 시계열 전처리#### 3-1) 정상성 확인#### 3-2) 정상성을 갖는 데이터로 바꾸기#### 3-3) 특성 공학###4. 모델 생성####4-1) 모델 1: ols####4-2) 모델 2: xgboost regressor#### 4-3) 모델 3: arima#### 4-4) 모델 4: xgboost classifier###5. 모델 선택####5-1) 하이퍼파라미터 튜닝을 통한 모델 최적화####5-2) 최종모델###6. 특성중요도####6-1) 모델 예측에 중요한 특성 설명####6-2) 개별 샘플을 예시로 예측 설명###7. 번외) 다른 etf 분석 및 포트폴리오 구성###6. 주요 특성 설명###7. 묶어서 포트폴리오 만들고 평가## 분석 배경저는 데이터 분석회사에서 일하고 있는 직원입니다. 2016년부터 시작된 로보 어드바이저 사업이 이제는 많이 상용화가 되어 여러 증권사/은행/핀테크 기업에서 이미 로보 어드바이저 서비스를 시작하였습니다. 모든 기업이 각자 알고리즘을 개발해서 서비스를 하는 것이 아니라, 다른 데이터 기업과 협업해서 로보 어드바이저 사업을 하는 경우가 있어, 여기에 사용할 알고리즘을 개발하는 프로젝트를 제안하려고 합니다.## 분석 목표1. 종가가 20일 지수이동평균를 넘어서 수익을 낼 etf를 예측한다.2. 투자 성향별로 etf를 분배한 후 이들의 평균수익률을 예측한다.분석 모델은 시계열 회귀분석과 분류 모델을 설계한 후 더 적절한 모델을 선택할 예정입니다.타겟은 시계열 회귀분석의 경우 etf의 차분이며, 분류 모델의 경우 타겟은 종가가 20일 지수평균을 초과하는 것입니다.타겟을 20일 지수이동평균 초과하는 것으로 정한 것은, 누적 수익률을 이용한 타겟인 것이 가장 적절하겠으나 분석의 편의성 및 로보 어드바이저의 평균 매매회전율이 높기 때문에(200%이상) 20일을 기준으로 설정하였습니다.*매매회전율 = (매매대금 * 100) / (운용자금 * 2)#1. 데이터셋 준비여러 로보 어드바이저의 알고리즘 설명서를 분석한 결과 대부분의 로보 어드바이저는 etf 위주의 투자를 하는 것을 알 수 있었습니다. 따라서 이번 분석에서는 etf만을 사용하며, 위험도에 따라서 분류할 예정입니다.제가 이번 분석에서 사용할 etf의 선정기준은 시가총액 1000억 원 이상인 것들 중에 분야별(국내 시장지수, 국내 업종테마/국내 파생/해외 주식/원자재/채권)로 고르게 고르며, 생긴지 5년 이상된 지수들입니다.위험도 분류는 * 위험도 1: 채권* 위험도 2: 국내 시장지수, 국내 업종테마, 국내 파생* 위험도 3: 해외 주식, 원자재3단계로 하며, 분류기준 역시 다른 알고리즘 설명서를 참고하였습니다. etf 종목 선택은 분야별로 생긴지 10년 이상이 지난 etf 중 거래량이 제일 많은 하나의 etf 데이터를 사용할 예정입니다.* 국내 지수: kodex200* 국내 업종 테마: kodex 삼성그룹* 국내 파생 지수: kodex 레버리지* 해외 지수: tiger 미국나스닥 100* 원자재: kodex 골드선물(h)* 채권: kodex 단기채권[벤치마크] kospi200앞으로 불러올 etf들과 추세를 비교하기 위해서 kospi200 지수를 불러오겠습니다.import pandas_datareader.data as webfrom datetime import datetimekospi200 = web.datareader('kpi200', 'naver', start='2011-02-01', end='2021-02-23')kospi200!pip install --upgrade mplfinancekospi200daily = kospi200[['open','high','low','close','volume']].iloc[-250:]kospi200daily = kospi200daily.astype('float')import mplfinance as mpfmpf.plot(kospi200daily,type='candle',mav=(5,20, 60),         style='charles',          title='kospi200, feb 2020 to feb 2021',         figratio=(29,14),volume=true)kospi200 그래프를 보면 지난 1년 간 kospi200지수의 등락을 볼 수 있습니다. kospi200 지수는 3월달에 가파르게 하락했다가 계속 완만하게 상승하는 모습을 확인할 수 있으며, 거래량은 증가와 감소를 반복하고 있습니다.이제 이번 분석에서 사용할 etf 중 하나인 kodex200을 예시로 어떤 분석방법이 가장 알맞을지 살펴보겠습니다.## kodex 200kodex200은 국내 kospi 200 지수를 그대로 추종하는 etf입니다. 분석의 편의와 여러 etf 간의 시간 간격을 맞추기 위해 10년 간의 데이터를 불러옵니다.import pandas_datareader.data as webfrom datetime import datetimekodex200  = web.datareader('069500', 'naver', start='2011-02-01', end='2021-02-23')kodex200### 결측치 확인kodex200.info()제가 사용한 api 특성 상 결측값은 불러와지지 않았습니다.### 이상치 확인kodex200 = kodex200.astype('float')fig = plt.figure(figsize=(10,5))ax1 = fig.add_subplot(1, 2, 1)ax1 = sns.boxplot(kodex200['close']);ax2 = fig.add_subplot(1, 2, 2)#ax1.plot(x, y)#ax2.bar(x, y)ax2=sns.boxplot(kodex200['volume']);plt.show()그래프 상으로는 종가와 거래량에 이상치가 있는 것 같아 보입니다. 이를 아래 캔들차트를 통해서 자세히 알아보겠습니다.kodex200daily = kodex200[-250:]kodex200daily = kodex200daily.astype('float')mpf.plot(kodex200daily,type='candle',mav=(5,20, 60),         style='charles',          title='kodex200, feb 2020 to feb 2021',         figratio=(15,8),volume=true)kodex 200 지수의 캔들차트를 보면 kodex200은 kospi200를 그대로 추종하기 때문에 kospi200과 비슷한 추세를 보이는 것을 알 수 있습니다. 지수가 폭락하던 3월에 거래량이 급증한 것만 kospi200과 다른 점입니다. 종가는 계속 증가하는 추세이고, 거래량이 40,000,000이상인 값들을 이상치로 처리하기는 어려울 것 같아 그대로 사용하겠습니다.이제 이번 분석에서 사용할 kodex200의 10년치 추세를 보겠습니다.sns.set_style('white')sns.set_palette('cividis')kodex200 = kodex200.astype('float')plt.title("kodex200, feb 2011 to feb 2021")kodex200['close'].plot();10년 간의 kodex200지수를 보면 전반적으로는 증가하면서도 뚜렷한 계절성 특징을 보인다거나 선형으로 나타내기 어려워 보입니다.이렇게 시계열 그래프가 긴 주기를 갖는 추세가 있지만 갑작스럽고 예측할 수 없는 방향의 변화가 있을 때는 데이터의 정상성을 확인해야 합니다. 정상성을 따르지 않는 시계열 데이터의 예측은 정확도가 많이 떨어지기 때문에 kodex200 데이터가 정상성을 띠는지 확인해보겠습니다.#시계열 데이터 전처리###정상성(stationarity) 확인from statsmodels.tsa.stattools import kpssprint(kpss(kodex200['close'], regression='ct') )정상성을 확인하는 방법으로 kpss 테스트를 시행했는데, kpss 값은 0.52, p값은 0.01로 kodex200지수 종가가 정상성을 띤다는 귀무가설을 기각합니다. 즉, kodex200의 종가는 정상성을 따르지 않기 때문에 시계열분석에 적합하지 않습니다. (정상성 확인 전에 kodex 200 종가로 모델을 돌렸더니 r2 score가 음수가 나왔습니다...) #### 정상성 갖는 데이터로 바꾸기(차분)차분은 t기간의 데이터에서 t-1기간의 데이터를 뺀 값으로 여기서는 하루 간의 kdoex200 종가 차이를 구합니다.#차분 구하기kodex200 = kodex200.astype('float')kodex200['diff'] =  np.r_[0, np.diff(kodex200['close'])]kodex200import seaborn as snsimport matplotlibmatplotlib.rcparams['axes.unicode_minus'] = falsesns.set_palette('cividis')kodex200['diff'].plot();t기간의 kodex200 종가에서 t-1기간의 kodex200 종가를 뺀 차분을 구한 후 이를 그래프로 나타낸 것입니다. kodex200지수 종가와는 다르게 일정한 분산을 보이는 것을 알 수 있습니다.from statsmodels.tsa.stattools import kpssprint(kpss(kodex200['diff'], regression='ct') )kpss 테스트를 해보면 kpss값 0.084, p값 0.1로, p값이 0.05이상이기 때문에 데이터가 정상성을 따른다는 귀무가설을 기각하지 못합니다. 따라서 차분 데이터를 사용해서 분석을 진행할 수 있습니다.이어서 차분한 종가의 자기상관을 알아보겠습니다. 많은 시계열 데이터는 고전적 회귀모형의 기본 가정인 오차항들끼리 독립이며, 등분산일 것을 만족하지 못하며 이를 자기상관이라고 부릅니다.자기상관(autocorrelation)은 시계열의 시차 값(lagged values) 사이의 선형 관계를 측정합니다. 정상성을 만족하지 못하는 kodex200 종가는 자기상관현상을 보이기 때문에 바로 분석에 이용하기는 어렵습니다. import matplotlib.pyplot as pltfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacfplot_acf(kodex200['close'])plot_pacf(kodex200['close'])plt.show()kodex200 종가를 이용해서 그린 acf, pacf 그래프입니다. acf 그래프를 보면 잔차가 일정한 패턴을 보이며 자기상관 현상을 보이고 있습니다.이와 비교하기 위해 차분한 데이터의 자기상관을 알아보기 위한 acf, pacf 그래프를 그려보겠습니다. import matplotlib.pyplot as pltfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacfplot_acf(kodex200['diff'])plot_pacf(kodex200['diff'])plt.show()kodex200종가의 차분 데이터는 잔차가 일정하기 때문에 자기상관현상을 보이지 않습니다. 이는 시계열 데이터가 정상성을 띠면 자기상관함수는 시간에 따라 일정해지기 때문입니다.그외에도 수익률 데이터로 정상성 테스트를 진행할 수 있습니다. #### 정상성을 갖는 데이터로 바꾸기(일일수익률)kodex200['return'] = kodex200['close'].pct_change() * 100  #pct_change(5) 5일간 수익률kodex200 = kodex200.dropna()print(kpss(kodex200['return'], regression='ct') )일일수익률 역시 kpss 테스트 결과 p값이 0.05이상이기 때문에 귀무가설을 기각하지 못해서 kodex200의 일일수익률 데이터는 정상성을 보입니다.import matplotlib.pyplot as pltfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacfplot_acf(kodex200['return'])plot_pacf(kodex200['return'])plt.show()수익률의 acf, pacf 그래프를 살펴보면 차분 데이터와 마찬가지로 자기상관현상을 보이지 않는 것을 알 수 있습니다.#특성 공학랜덤 워크 모델이기 때문에 지수 종가를 사용해서 만든 특성을 이용해서 분석합니다. 새롭게 만들 특성은 5일 단순이동평균, 5일 지수이동평균, 이격도, stochastic %k(slow), %d(slow), macd, rsi입니다. 위의 특성들은 지수 종가와 고가, 저가를 이용한 특성들로 논문을 참고하였습니다. 참고 논문: 하대우(연세대학교) 외. (2019.05).xgboost 모형을 활용한 코스피 200 주가지수 등락 예측에 관한 연구.한국데이터정보과학회def engineer(df):  df = df.astype('float')    #5일 단순이동평균(sma)  df['sma5'] = df['close'].rolling(window=5, min_periods=1).mean()    #5일 지수이동평균(ema)  df['ema5'] = df['close'].ewm(5).mean()  #20일 지수이동평균  df['ema20'] = df['close'].ewm(20).mean()  #이격도  df['disparity'] = df['close'] - df['sma5']    #stochastic k, d : fast %k의 m기간 이동평균(sma)  df['fast_k'] = ((df['close'] - df['low'].rolling(5).min()) / (df['high'].rolling(5).max() - df['low'].rolling(5).min())) * 100  df['slow_k'] = df['fast_k'].rolling(3).mean()  df['slow_d'] = df['slow_k'].rolling(3).mean()  #macd  df['emafast'] = df['close'].ewm( span = 5, min_periods = 4).mean()  df['emaslow'] = df['close'].ewm( span = 20, min_periods = 19).mean()  df['macd'] = df['emafast'] - df['emaslow']  #df['macdsignal'] = df['macd'].ewm( span = 9, min_periods = 8).mean()  #df['macddiff'] = df['macd'] - df['macdsignal']  #rsi  delta = df['close'].diff(5)  delta = delta.dropna()  up = delta.copy()  down = delta.copy()  up[up<0] = 0  down[down>0] = 0  df['up'] = up  df['down'] = down  avg_gain = df['up'].rolling(window = 5).mean()  avg_loss = abs(df['down'].rolling(window = 5).mean())  rs = avg_gain/avg_loss  rsi = 100.0 - (100.0/(1+rs))  df['rsi'] = rsi   df['diff'] =  np.r_[0, np.diff(df['close'])]  df = df.drop(columns=['open', 'close', 'high','low', 'fast_k', 'emafast', 'emaslow', 'up', 'down'])  df = df.dropna()  df = df.reset_index() #time_series_split할 때 필요  return df# 모델 생성지난 과제를 통해서 주가 예측을 회귀 모델로 할 경우 r2 score가 높더라도 예측 가격이 실제 가격이랑 많이 다르게 나와서 분석이 어렵다고 느꼈습니다. 그렇지만 일단 분류문제로 풀기 전에 회귀분석으로 예측을 해서 수익률 예측을 해보겠습니다.##회귀분석 모델* 모델 1: 다중선형회귀모델* 모델 2: xgboost 모델* 모델 3: arima 모델회귀분석 모델 3가지를 만들 것인데, 타겟은 일일 종가의 차분으로 하고, 평가지표는 mse, r2 score 두 가지를 사용하겠습니다.## 테스트 데이터 분리cv 이후 사용할 테스트 데이터를 분리합니다. 전체 데이터 중에서 최근 20%를 테스트 데이터로 사용하겠습니다.len(kodex200)*0.2kodex200test = kodex200[-495:]kodex200train = kodex200.drop(index=kodex200test.index)kodex200train.shape, kodex200test.shape#특성 공학kodex200train = engineer(kodex200train)kodex200test = engineer(kodex200test)kodex200train = kodex200train.drop(columns=['return'])kodex200test = kodex200test.drop(columns=['return'])## 모델 1 다중선형회귀분석타겟을 차분으로 정했기 때문에 선형회귀 모델의 가정인 오차의 정규성, 등분산성, 독립성을 만족합니다.from sklearn.linear_model import linearregressionfrom sklearn.model_selection import timeseriessplitfrom sklearn.metrics import mean_squared_errortscv = timeseriessplit(n_splits=10)target = 'diff'y =kodex200train[target]x =kodex200train.drop(columns = target)x = x.dropna()y= y.dropna()x = x.drop(columns='date')pred1 = []scores = []mses = []for train_index, val_index in tscv.split(kodex200train):    x_train   = x.iloc[train_index] #drop('date', axis=1)    y_train = y.iloc[train_index]    x_val    = x.iloc[val_index]     y_val  = y.iloc[val_index]    ols = linearregression()    ols.fit(x_train,y_train)    preds = ols.predict(x_val)    pred1.append(preds)    mse = mean_squared_error(y_val, preds)    r2score = ols.score(x_val,y_val)    mses.append(mse)    scores.append(r2score)average_r2score = np.mean(scores)print(average_r2score)print(np.mean(mses))print("평균 r2 점수: ", average_r2score)print("평균 mse:",np.mean(mses))scorescv = 10일 때 검증 데이터의 평균 r2 score는 0.634이고 mse는 15767입니다. 재밌는 것은 모델에 사용한 데이터가 늘어날수록 평가지표가 안 좋아진다는 점입니다. 2016년 이후 데이터만 사용했을 때 r2 score와 mse 모두 더 좋은 점수였는데, 아무래도 시계열 데이터이다 보니 훈련 데이터가 너무 예전 것인 경우 예측력이 떨어지는 것 같습니다. 이제 예측값을 그래프에 나타내 보겠습니다.y_val.plot();차분 그래프는 예측이 잘 맞는지 알기 어렵기 때문에 다시 종가로 바꿔서 비교해보겠습니다.new_df1 = kodex200[(kodex200.index >= '2018-05-29')&(kodex200.index <= '2019-02-20')]new_df1new_df2 = new_df1.drop(columns=['open','high','low','volume','diff','return'])val_pred = pd.dataframe(pred1).tval_predval_pred.iloc[0,:] = val_pred.iloc[0,:] + 30130.0val_predval_pred2 = val_pred.cumsum()val_pred2 = round(val_pred2, 1)val_pred2val_pred2['avg'] = val_pred2.mean(axis=1)val_pred2val_pred2.iloc[:,0].plot()val_pred2.iloc[:,1].plot()val_pred2.iloc[:,2].plot()val_pred2.iloc[:,3].plot()val_pred2.iloc[:,4].plot()val_pred2.iloc[:,5].plot()val_pred2.iloc[:,6].plot()val_pred2.iloc[:,7].plot()val_pred2.iloc[:,8].plot()val_pred2.iloc[:,9].plot()val_pred2.iloc[:, 10].plot()new_df2.plot()예측값과 실제값을 비교해보니 선형모델은 급락을 전혀 예측하지 못하는 것을 알 수 있습니다.두 그래프를 같이 놓고 보겠습니다.val_pred2.index = new_df2.indexplot_df = pd.concat([new_df2['close'], val_pred2['avg']], axis=1)plot_dfsns.set_palette('purd')plot_df.plot()cv로 예측한 값들의 평균과 실제 종가의 그래프입니다. 실제 종가가 2018년 7월부터 급격히 떨어지기 시작하는데 선형모델은 이를 전혀 예측하지 못합니다. 또한, 차분를 예측한 후 이를 누적합해서 종가를 만든 것이기 때문에 차분의 오차가 점점 쌓여서 실제 값과 더욱더 괴리되는 모습을 확인할 수 있습니다. 이런 급락이 있는 경우 모델 성능이 더 떨어지기 때문에 검증 데이터의 값에 따라 모델의 성능이 많이 차이날 것 같습니다.모델 1의 예측에 중요한 영향을 미친 특성중요도를 살펴보겠습니다.!pip install eli5import warningswarnings.simplefilter(action='ignore', category=futurewarning)import eli5from eli5.sklearn import permutationimportance# permuter 정의permuter = permutationimportance(    ols,    scoring='r2', # metric    n_iter=5, # 다른 random seed를 사용하여 5번 반복    random_state=2)permuter.fit(x_val, y_val);feature_names = x_val.columns.tolist()eli5.show_weights(    permuter,     top=none, # top n 지정 가능, none 일 경우 모든 특성     feature_names=feature_names # list 형식으로 넣어야 합니다)모델 1에서 차분(difference)를 예측하는 데 가장 중요한 특성으로는 단순이동평균(5일), 지수이동평균(5일), 지수이동평균(20일), macd, 이격도가 있습니다. 이동평균이 높은 순위를 차지하고 있는데, 차분이 종가의 하루 차이인 것을 생각해보면, 단순이동평균은 5일 간의 평균이기 때문에 약간의 leakeage가 있다고 볼 수도 있습니다. 그런데 대부분의 랜덤워크 모형은 가격에서 파생된 특성으로 예측을 해서 모형 특성상 발생하는 애매한 부분인 것 같습니다.## 모델 2 xgboost regressorfrom xgboost import xgbregressor'''target = 'diff'y =kodex200train[target]x =kodex200train.drop(columns = target)x = x.dropna()y= y.dropna()x = x.drop(columns='date')'''scores_xgb = []mse_xgb = []preds_xgb = []for train_index, val_index in tscv.split(kodex200train):    x_train2   = x.iloc[train_index] #drop('date', axis=1)    y_train2 = y.iloc[train_index]    x_val2  = x.iloc[val_index] #.drop('record_date', axis=1)    y_val2  = y.iloc[val_index]    # if needed, do preprocessing here    xgb_model = xgbregressor(n_estimators=100, learning_rate=0.08, gamma=0,                            max_depth=7)    xgb_model.fit(x_train2, y_train2)    pred_xgb = xgb_model.predict(x_val2)    preds_xgb.append(pred_xgb)    r2score_xgb = xgb_model.score(x_val2,y_val2)    scores_xgb.append(r2score_xgb)    mse_xg = mean_squared_error(y_val2, pred_xgb)    mse_xgb.append(mse_xg)average_r2score_xgb = np.mean(scores_xgb)print("평균 r2 score: ",average_r2score_xgb)print("평균 mse: ",np.mean(mse_xgb))모델 2의 r2 score는 0.574이고 mse는 17490으로 모델 1보다 r2 score는 낮고 mse는 더 큽니다. 따라서 모델 1이 오류가 더 적은 모델이라고 할 수 있습니다. import eli5from eli5.sklearn import permutationimportance# permuter 정의permuter2 = permutationimportance(    xgb_model,    scoring='r2', # metric    n_iter=5,     random_state=2)permuter2.fit(x_val2, y_val2);feature_names2 = x_val2.columns.tolist()eli5.show_weights(    permuter2,     top=none,     feature_names=feature_names2 )모델 2에서 차분(difference)을 예측하는 데 가장 중요한 특성으로는 이격도, stochastic %k, macd, stochastic %d가 있습니다. 이격도가 가장 중요한 특성으로 나왔는데, 이격도를 구하는데 종가와 이동평균이 필요한 것을 생각해보면 순위가 낮게 나온 sma5는 특성에서 제외해도 될 것 같습니다. ##모델 3 arima시계열 예측에 많이 사용하는 arima 모델을 사용해서 분석을 해보겠습니다. arima 모델은 ar모델과 ma모델을 합친 것으로 p,d,q 세가지의 모수를 가집니다. p는 ar모형의 lag, d는 차분 횟수, q는 ma모형의 lag를 의미합니다. 정상성 확인하는 과정에서의 acf, pacf 그래프를 보고 p = 1, d=1, q=0으로 유추하였습니다.#모델1,2에서 사용된 훈련 데이터 + 검증 데이터arima_df = kodex200[(kodex200.index<='2019-02-21') & (kodex200.index>'2011-03-15')] from statsmodels.tsa.arima_model import arimamod = arima(arima_df['diff'], order=(1, 1, 0)) res = mod.fit(trend='c',full_output=true, disp=0)print(res.summary())residuals = pd.dataframe(res.resid)fig = plt.figure(figsize=(10,5))#ax1 = fig.add_subplot(1, 2, 1)ax1 = residuals.plot(title="residuals")#ax2 = fig.add_subplot(1, 2, 2)ax2 = residuals.plot(kind='kde', title='density')plt.show()잔차그림을 그려봤을 때 어떤 경향도 보이지 않고 있습니다.plt.style.use('ggplot')res.plot_predict(dynamic=false)plt.show()arima 모델로 예측한 값과 실제 차분의 그래프입니다. 이대로는 분석이 어렵고, 투명도 조절을 하거나 종가로 바꿔서 봐야할 것 같습니다.이렇게 회귀분석 모델 세가지를 이용한 분석을 했는데, 회귀분석로는 의사결정 시그널을 주기 어려운 것 같습니다.물론 차분을 예측해서 누적합을 하면 지수의 종가 예측값이 나오는데, 오차범위도 정해야 하고 모델이 너무 복잡해집니다.이번엔 의사결정에 직접적으로 도움을 줄 수 있는 분류 모델로 다시 분석해 볼 것입니다. ## 분류모델## 모델 4 xgboost classifier타겟은 종가가 ema20선보다 큰 값은 1, 아닌 값은 0으로 하겠습니다.def engineer2(df):  df = df.astype('float')    #5일 단순이동평균(sma)  df['sma5'] = df['close'].rolling(window=5, min_periods=1).mean()    #5일 지수이동평균(ema)  df['ema5'] = df['close'].ewm(5).mean()  #20일 지수이동평균  df['ema20'] = df['close'].ewm(20).mean()  #이격도  df['disparity'] = df['close'] - df['sma5']    #stochastic k, d : fast %k의 m기간 이동평균(sma)  df['fast_k'] = ((df['close'] - df['low'].rolling(5).min()) / (df['high'].rolling(5).max() - df['low'].rolling(5).min())) * 100  df['slow_k'] = df['fast_k'].rolling(3).mean()  df['slow_d'] = df['slow_k'].rolling(3).mean()  #macd  df['emafast'] = df['close'].ewm( span = 5, min_periods = 4).mean()  df['emaslow'] = df['close'].ewm( span = 20, min_periods = 19).mean()  df['macd'] = df['emafast'] - df['emaslow']  #df['macdsignal'] = df['macd'].ewm( span = 9, min_periods = 8).mean()  #df['macddiff'] = df['macd'] - df['macdsignal']  #rsi  delta = df['close'].diff(5)  delta = delta.dropna()  up = delta.copy()  down = delta.copy()  up[up<0] = 0  down[down>0] = 0  df['up'] = up  df['down'] = down  avg_gain = df['up'].rolling(window = 5).mean()  avg_loss = abs(df['down'].rolling(window = 5).mean())  rs = avg_gain/avg_loss  rsi = 100.0 - (100.0/(1+rs))  df['rsi'] = rsi   df['diff'] =  np.r_[0, np.diff(df['close'])]  df = df.drop(columns=['open', 'high','low', 'fast_k', 'emafast', 'emaslow', 'up', 'down'])  df = df.dropna()  df = df.reset_index() #time_series_split할 때 필요  return df#타겟값 만들기def define_target_condition(df):     # price above trend multiple days later    df['target_cls'] = np.where(df['close'].shift(-20) > df.ema20.shift(-20), 1, 0)    # important, remove nan values    df=df.fillna(0).copy()        df.tail()        return dfkodex200test2 = kodex200[-495:]kodex200train2 = kodex200.drop(index=kodex200test2.index)kodex200train2.shape, kodex200test2.shapekodex200train2 = engineer2(kodex200train2)kodex200test2 = engineer2(kodex200test2)kodex200train2 =define_target_condition(kodex200train2)kodex200test2 = define_target_condition(kodex200test2)kodex200train2베이스라인kodex200train2['target_cls'].value_counts(normalize=true)print('베이스라인 모델 검증 정확도: ', 0.543556)타겟의 분포는 고른 편이기 때문에 모델 생성시 타겟 클래스 비율을 고려할 필요는 없을 것 같습니다.eval_set = [(x_train2, y_train2),             (x_val2, y_val2)]model.fit(x_train2, y_train2,           eval_set=eval_set,          eval_metric='error', # #(wrong cases)/#(all cases)          early_stopping_rounds=50         ) # 50 rounds 동안 스코어의 개선이 없으면 멈춤kodex200train2 = kodex200train2.drop(columns = ['return','diff'])from xgboost import xgbclassifier#from sklearn.metrics import accuracy_scorefrom sklearn.metrics import classification_report, f1_score, precision_score, recall_scoretarget2 = 'target_cls'y2 =kodex200train2[target2]x2 =kodex200train2.drop(columns = target2)x2 = x2.dropna()y2= y2.dropna()x2 = x2.drop(columns='date')scores_xgbc = []preds_xgbc = []f1_score1 = []prec_score = []rec_score = []for train_index, val_index in tscv.split(kodex200train2):    x2_train   = x2.iloc[train_index] #drop('date', axis=1)    y2_train = y2.iloc[train_index]    x2_val  = x2.iloc[val_index] #.drop('record_date', axis=1)    y2_val  = y2.iloc[val_index]    # if needed, do preprocessing here    xgb = xgbclassifier(n_estimators=1000, learning_rate=0.08, gamma=0,                            max_depth=7)    #xgb.fit(x2_train, y2_train, eval_metric='error', early_stopping_rounds=50)    eval_set = [(x2_train, y2_train),             (x2_val, y2_val)]    xgb.fit(x2_train, y2_train,           eval_set=eval_set,          eval_metric='error', # #(wrong cases)/#(all cases)          early_stopping_rounds=50         )     pred_xgbc = xgb.predict(x2_val)    preds_xgbc.append(pred_xgbc)    score_xgbc = xgb.score(x2_val,y2_val)    scores_xgbc.append(score_xgbc)   # preci = precision_score(x2_val,pred_xgbc)    #prec_score.append(preci)    #recall_s = recall_score(x2_val, pred_xgbc)    #rec_score.append(recall_s)    #f1 = f1_score(x2_val, pred_xgbc)    #f1_score1.append(f1)average_score_xgbc = np.mean(scores_xgbc)#average_f1_score = np.mean(f1_score1)print("평균 accuracy score: ",average_score_xgbc)#print("평균 f1: ",np.mean(average_f1_score))모델 4의 검증 정확도는 0.53으로 베이스라인 모델의 0.54보다 낮은 편입니다. 이를 gridsearchcv로 개선할 수 있는지 확인해 보겠습니다.# 모델 최적화gridsearchcv를 이용하여 모델 4의 하이퍼 파라미터 튜닝을 진행하겠습니다.from xgboost import xgbclassifierfrom sklearn.model_selection import timeseriessplit, gridsearchcvimport numpy as npxgb_f = xgb.xgbclassifier()param_search = {'booster' :['gbtree'],                'max_depth' : [3, 5, 10],                'gamma':[0,1,2,3],                'n_estimators':[50, 100, 1000],                'objective':['binary:logistic'],                'random_state':[2]}#tscv = timeseriessplit(n_splits=10)gsearch = gridsearchcv(estimator=xgb_f, cv=tscv,                        param_grid=param_search, scoring ='f1', verbose=1, n_jobs=-1)gsearch.fit(x2_train, y2_train)            print('최적 하이퍼파라미터: ', gsearch.best_params_)print('f1: ', gsearch.best_score_)위에서 나온 최적 하이퍼 파라미터 근처의 값으로 다시 gridsearchcv를 시행해보겠습니다.xgb_f = xgb.xgbclassifier()param_search = {'booster' :['gbtree'],                'max_depth' : [5, 7],                'gamma':[3, 4,5],                'n_estimators':[100, 150, 200],                'objective':['binary:logistic'],                'random_state':[2]}gsearch = gridsearchcv(estimator=xgb_f, cv=tscv,      #모델 1부터 쓰던 timeseries cv를 사용                        param_grid=param_search, scoring ='f1', verbose=1, n_jobs=-1)gsearch.fit(x2_train, y2_train)print('최적 하이퍼파라미터: ', gsearch.best_params_)print('f1: ', gsearch.best_score_)처음에 시행한 gridsearchcv와 같은 하이퍼파라미터들이 선택되었습니다. 이를 사용하여 최종 모델을 만들겠습니다.## 최종 모델모델 4를 최적화한 모델을 최종 모델로 선정하여 검증세트 정확도와 f1 score를 확인하겠습니다.pipe = gsearch.best_estimator_y_pred_v = pipe.predict(x2_val)accu_grid = accuracy_score(y2_val, y_pred_v)f1_grid = f1_score(y2_val, y_pred_v)print(f'검증세트 정확도: {accu_grid:,.3f}')print(f'검증세트 f1 score: {f1_grid:,.3f}')최종 모델은 점수가 많이 낮은 편이지만, 비교적 최근인 검증세트까지 넣어서 훈련하면 성능이 더 좋아질 수도 있기 때문에,일단 검증세트까지 다시 넣어서 gridsearch를 다시 시행하겠습니다.x2_train.columnsx_train3 = kodex200train2.drop(columns=['date','target_cls'])y_train3 = kodex200train2['target_cls']x_train3.shape, y_train3.shapefrom xgboost import xgbclassifierfrom sklearn.model_selection import timeseriessplit, gridsearchcvimport numpy as npxgb_f = xgb.xgbclassifier()param_search = {'booster' :['gbtree'],                'max_depth' : [3, 5, 10],                'gamma':[0,1,2,3],                'n_estimators':[50, 100, 1000],                'objective':['binary:logistic'],                'random_state':[2]}#tscv = timeseriessplit(n_splits=10)gsearch = gridsearchcv(estimator=xgb_f, cv=tscv,                        param_grid=param_search, scoring ='f1', verbose=1, n_jobs=-1)gsearch.fit(x_train3, y_train3)kodex200test2x_test = kodex200test2.drop(columns=['date','target_cls'])y_test = kodex200test2['target_cls']x_test.shape, y_test.shapex_test = x_test.drop(columns=['diff','return'])pipe2 = gsearch.best_estimator_y_pred = pipe2.predict(x_test)accu_grid2 = accuracy_score(y_test, y_pred)f1_grid2 = f1_score(y_test, y_pred)print(f'테스트세트 정확도: {accu_grid2:,.3f}')print(f'테스트세트 f1 score: {f1_grid2:,.3f}')검증세트까지 넣어서 만든 모델의 테스트세트 정확도와 f1 score 모두 오른 것을 확인할 수 있습니다. 시계열 분석을 할 때는 무조건 데이터가 많다고 좋은 것이 아니 때문에 훈련데이터, 검증데이터, 테스트데이터를 나누는 기준이 중요할 것 같습니다. 이번 분석에서는 총 10년 간의 데이터 중에서 테스트데이터를 최근 20%로 잡고 모델링했는데, 데이터 크기를 더 줄이면(최신 데이터로 학습) 성능이 더 좋아지지 않을까 생각됩니다.roc curve와 auc 점수 확인#roc curvefrom sklearn.metrics import roc_curveimport matplotlib.pyplot as pltfpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)plt.scatter(fpr, tpr, color='#413c69')plt.plot(fpr, tpr, color='#bd2000')plt.title('roc curve')plt.xlabel('fpr')plt.ylabel('tpr')from sklearn.metrics import roc_auc_scorey_pred_proba = pipe2.predict_proba(x_test)[:, -1]print('auc score: ', roc_auc_score(y_test, y_pred_proba))roc curve를 보면 대각선보다는 그래프가 오목한 것(우측 하단 기준)을 알 수 있습니다. 하지만 auc 점수가 0.63인 것을 보아 성능이 아주 좋아보이지는 않습니다.# 특성중요도# permuter 정의permuter_f = permutationimportance(    pipe2, # model    scoring='f1', # metric    n_iter=5, # 다른 random seed를 사용하여 5번 반복    random_state=2)permuter_f.fit(x_test, y_test);feature_names_f = x_test.columns.tolist()pd.series(permuter_f.feature_importances_, feature_names_f).sort_values()# 특성별 score 확인  #여러 번 섞은 결과임eli5.show_weights(    permuter_f,     # top n 지정 가능, none 일 경우 모든 특성     feature_names=feature_names_f # list 형식으로 넣어야 합니다)모델 4의 경우 타겟을 예측하는 데 가장 중요한 특성으로는 20일 지수이동평균, macd, 종가, 이격도, stochastic %k 등이 있습니다. 이때 20일 지수이동평균은 타겟을 만드는 데 사용하였기 때문에 leakage 문제가 있습니다. 하지만, 타겟을 미래의 종가가 20일 지수이동평균 보다 높은 지를 예측하는 것으로 했기 때문에 모델 설계상 필연적으로 발생할 수밖에 없었던 문제 같습니다. 거래량이 낮은 상관관계를 보이는 것이 특이했고, 모델 1,2,3과 다르게 5일 단순이동평균(sma5)이 가장 덜 중요한 특성으로 나온 것이 특이점입니다.이제 개별 샘플을 예시로 특성 설명을 하겠습니다.x_test.tail(5)!pip install shaprow = x_test.iloc[[-1]]pipe2.predict(row)마지막 샘플(2021년 2월 23일)의 타겟 예측값은 0(종가가 20일 지수이동평균 보다 낮다)입니다. 이러한 예측에 영향을 준 특성들을 살펴보겠습니다. import shapexplainer = shap.treeexplainer(pipe2)shap_values = explainer.shap_values(row)shap.initjs()shap.force_plot(    base_value=explainer.expected_value,     shap_values=shap_values,           features=row)예측하는 데 긍정적 영향을 준 것은 5일 지수이동평균, 종가이고 부정적 영향을 준 것은 5일 단순이동평균과 20일 지수이동평균입니다. 이번 분석을 하면서 느꼈던 아쉬운 점과 개선점을 생각해봤습니다.아쉬운 점 및 개선방법1. 시계열 데이터 분석이 미숙하여 회귀 분석 모델을 활용하지 못한 것모델 1, 2, 3은 회귀 분석 모델인데 시계열 데이터의 cv 방법이 익숙하지 않고, 데이터를 정상화해서 타겟 설정이 어려운 것과 arima 모델은 처음 사용해 보는 등 과제를 수행하는 데 있어 여러 어려움이 있었습니다. 다음에 시계열 분석을 시도한다면 타겟을 오차범위 내 수익률 등으로 바꾸면 해석이 더 용이할 것 같습니다.2. 시계열 분석에 머신러닝이 적합한지에 대한 의문. 많은 블로그를 보면 시계열 예측에 트리기반 모델 등 머신러닝 모델보다는 lstm 같은 딥러닝 모델을 많이 사용하는 것 같습니다. 지난 과제에서 집값 예측 등 시계열 분석에 사용한 회귀분석 등이 있었지만, 랜덤워크 모형처럼 가격을 기반으로 예측할 때는 다른 모델이 더 적절할 것 같습니다. 머신러닝을 이용한 분석은 여기까지입니다. 아래의 분석은 제가 설정한 시나리오 상 20일 간 지수이동평균을 넘을 것이라고 분류된 etf들을 어떻게 포트폴리오로 구성할 지에 대한 것입니다.#다른 etf 분석국내 지수 tiger200  = web.datareader('102110', 'naver', start='2011-02-01', end='2021-02-23')tiger200국내 업종/테마 kodex_samsung = web.datareader('102780', 'naver', start='2016-12-27', end='2021-02-23')kodex_samsung국내 파생kodex200_lev2 = web.datareader('122630', 'naver', start='2011-02-01', end='2021-02-23')kodex200_lev2해외 주식tiger_nasdaq100 = web.datareader('133690', 'naver', start='2011-02-01', end='2021-02-23')tiger_nasdaq100원자재kodex_gold = web.datareader('132030', 'naver', start='2011-02-01', end='2021-02-23')kodex_gold채권kodex_bond_short = web.datareader('153130', 'naver', start='2011-02-01', end='2021-02-23')kodex_bond_short#포트폴리오 구성assets = [ 'kodex200', 'kodex_samsung', 'kodex_wti', 'kodex_bond_short', 'tiger_nasdaq', 'kodex_lev']weights = np.array([0.2, 0.2, 0.2, 0.2, 0.2]) #여기 일단 다섯개#각 etf별 종가 df로 만들기df = pd.concat([kodex_gold['close'], tiger_nasdaq100['close'], kodex200_lev2['close'], kodex_samsung['close'], tiger200['close']],               keys=['kodex_gold','tiger_nasdaq','kodex200_lev2','kodex_samsung','tiger200'],axis =1)dfdf = df.dropna()dfdf = df.astype('float')#시각화title = 'portfolio'my_etf = dfplt.figure(figsize=(12,5))for c in my_etf.columns.values:  plt.plot(my_etf[c], label=c)plt.title(title)plt.xlabel('date', fontsize=15)plt.ylabel('close price')plt.legend(my_etf.columns.values, loc = 'upper left')plt.show()#일일수익률returns = df.pct_change()returnsreturns = returns.dropna()#연간 공분산 행렬cov_matrix_annual = returns.cov() * 251cov_matrix_annual#포트폴리오 분산의 기댓값 = weight.t dot (공분산행렬) dot weightport_variance = np.dot(weights.t, np.dot(cov_matrix_annual, weights))port_variance#포트폴리오 변동성의 기댓값 = 분산의 기댓값의 제곱근(표준편차)port_volatility = np.sqrt(port_variance)port_volatility#포트폴리오 연간 단순 수익률port_annual_return = np.sum(returns.mean()*weights)*251#예상 연간 수익, 변동성 또는 위험 및 분산percent_var = str(round(port_variance, 2) * 100)percent_vols = str(round(port_volatility, 2) * 100)percent_ret = str(round(port_annual_return, 2)* 100)print("예상 연간수익률: ", percent_ret, '%')print("예상 변동성: ", percent_vols, '%')print("예상 분산: ", percent_var, '%')!pip install pyportfolioopt#포트폴리오 최적화from pypfopt.efficient_frontier import efficientfrontier from pypfopt import risk_modelsfrom pypfopt import expected_returnsmu = expected_returns.mean_historical_return(df)s = risk_models.sample_cov(df)ef = efficientfrontier(mu,s)weights = ef.max_sharpe()cleaned_weights = ef.clean_weights()print(cleaned_weights)ef.portfolio_performance(verbose=true)펀드의 위험성을 평가하기 위해 샤프 비율을 사용합니다. 샤프 비율은 투자자가 부담하는 위험을 자산 수익률이 얼마나 잘 보상하는지를 규정합니다. 두 자산을 공동의 기준지표와 비교할 경우, 더 높은 샤프 비율을 나타내는 자산이 동일한 위험에 대해 더 높은 수익률을 제공합니다.샤프 비율은 기간에 따라 달라질 수 있습니다. 일일수익률, 주간수익률, 1개월수익률 모두 샤프비율을 구할 수 있습니다.#개별 종목 할당!pip install pulpfrom pypfopt.discrete_allocation import discreteallocation, get_latest_priceslatest_prices = get_latest_prices(df)weights = cleaned_weightsda = discreteallocation(weights, latest_prices, total_portfolio_value=1000000)allocation, leftover = da.lp_portfolio()print("자산 분배: ", allocation)print("남은 계좌 잔액: \{:.2f}".format(leftover))다섯 가지 분야에 골고루 투자하는 위험중립형 투자자의 경우 최적화된 포트폴리오의 추천은 kodex골드, tiger나스닥100, kodex 삼성그룹을 각각 26, 8, 14개 매수할 것을 추천하고 있습니다.