Python LangChain과 FAISS📕 LLM & RAG를 활용한 AI 서비스 만들기/📘 LLM & RAG를 활용한 AI 서비스 만들기 - 5주차/📕Python LangChain과 FAISSMade with📕Python LangChain과 FAISS수업 목표LangChain 사용을 위한 환경 설정을 진행합니다.FAISS를 활용한 벡터 데이터베이스를 구성하는 실습을 진행합니다.목차설치 및 기본 설정LangChain 기본 개념언어 모델 초기화프롬프트 템플릿 사용하기LangChain Expression Language (LCEL)로 체인 연결FAISS를 활용한 벡터 데이터베이스 구성 및 쿼리Step 1: OpenAI 임베딩 모델로 벡터 임베딩 생성Step 2: FAISS 인덱스 초기화Step 3: 벡터 데이터베이스에 문서 추가Step 4: 벡터 데이터베이스 쿼리RAG 체인에 FAISS 통합Step 1: Retriever로 변환Step 2: RAG 체인 생성FAISS 인덱스의 저장 및 로드FAISS 데이터베이스 병합❗이번에는 LangChain과 FAISS를 이용한 실습을 진행할 예정인데요,
실습은 아래와 같은 순서로 진행됩니다!

설치 및 기본 설정LangChain 기본 개념 - 언어 모델, 프롬프트 템플릿LangChain Expression Language (LCEL)로 체인 연결FAISS를 활용한 벡터 데이터베이스 구성 및 쿼리RAG 체인에 FAISS 통합FAISS 인덱스의 저장 및 로드, 병합설치 및 기본 설정💡LangChain, OpenAI, 그리고 FAISS 패키지를 설치합니다. GPU를 사용하는 경우 faiss-gpu를 사용할 수 있습니다.ShellCopypip install langchain langchain-openai faiss-cpu

​
설치 후, OpenAI API 키를 설정해 사용 환경을 준비합니다.PythonCopyimport os
from getpass import getpass

os.environ["OPENAI_API_KEY"] = getpass("OpenAI API key 입력: ")

​LangChain 기본 개념언어 모델 초기화OpenAI의 GPT-4 모델을 LangChain을 통해 사용해 봅니다. ChatOpenAI를 이용해 초기화하고 invoke 메서드를 통해 메시지를 전달하여 응답을 받아옵니다.PythonCopyfrom langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

# 모델 초기화
model = ChatOpenAI(model="gpt-4")
# 모델에 메시지 전달
response = model.invoke([HumanMessage(content="안녕하세요, 무엇을 도와드릴까요?")])
print(response.content)

​프롬프트 템플릿 사용하기프롬프트 템플릿은 다양한 입력을 받아 메시지를 생성하는데 도움을 줍니다. 예를 들어, 영어 문장을 다른 언어로 번역하는 프롬프트를 정의해 봅시다.PythonCopyfrom langchain_core.prompts import ChatPromptTemplate

# 시스템 메시지 설정
system_template = "Translate the following sentence from English to {language}:"
# 사용자 텍스트 입력
prompt_template = ChatPromptTemplate.from_messages([
("system", system_template),
("user", "{text}")
])
# 프롬프트 생성
result = prompt_template.invoke({"language": "French", "text": "How are you?"})
print(result.to_messages())

​LangChain Expression Language (LCEL)로 체인 연결여러 컴포넌트를 체인으로 연결하여 데이터 흐름을 통제하는 LCEL을 사용합니다.PythonCopyfrom langchain_core.output_parsers import StrOutputParser

# 응답을 파싱하는 파서 초기화
parser = StrOutputParser()
# 템플릿, 모델, 파서를 체인으로 연결
chain = prompt_template | model | parser

# 체인 실행
response = chain.invoke({"language": "Spanish", "text": "Where is the library?"})
print(response)

​FAISS를 활용한 벡터 데이터베이스 구성 및 쿼리FAISS는 벡터 유사성 검색을 위한 라이브러리입니다. OpenAIEmbeddings로 텍스트를 벡터로 변환해 FAISS 인덱스에 저장합니다.Step 1: OpenAI 임베딩 모델로 벡터 임베딩 생성PythonCopyfrom langchain_openai import OpenAIEmbeddings

# OpenAI 임베딩 모델 초기화
embeddings = OpenAIEmbeddings(model="text-embedding-ada-002")

​Step 2: FAISS 인덱스 초기화PythonCopyimport faiss
from langchain_community.vectorstores import FAISS
from langchain_community.docstore.in_memory import InMemoryDocstore

# FAISS 인덱스 생성
index = faiss.IndexFlatL2(len(embeddings.embed_query("hello world")))
vector_store = FAISS(
    embedding_function=embeddings,
    index=index,
    docstore=InMemoryDocstore(),
    index_to_docstore_id={}
)

​Step 3: 벡터 데이터베이스에 문서 추가PythonCopyfrom langchain_core.documents import Document
from uuid import uuid4

# 문서 생성
documents = [
    Document(page_content="LangChain을 사용해 프로젝트를 구축하고 있습니다!", metadata={"source": "tweet"}),
    Document(page_content="내일 날씨는 맑고 따뜻할 예정입니다.", metadata={"source": "news"}),
    Document(page_content="오늘 아침에는 팬케이크와 계란을 먹었어요.", metadata={"source": "personal"}),
    Document(page_content="주식 시장이 경기 침체 우려로 하락 중입니다.", metadata={"source": "news"}),
]
# 고유 ID 생성 및 문서 추가
uuids = [str(uuid4()) for _ in range(len(documents))]
vector_store.add_documents(documents=documents, ids=uuids)

​Step 4: 벡터 데이터베이스 쿼리유사성 검색을 통해 특정 쿼리와 유사한 문서를 검색해보겠습니다.PythonCopy# 기본 유사성 검색
results = vector_store.similarity_search("내일 날씨는 어떨까요?", k=2, filter={"source": "news"})
for res in results:
print(f"* {res.page_content} [{res.metadata}]")
# 점수와 함께 유사성 검색
results_with_scores = vector_store.similarity_search_with_score("LangChain에 대해 이야기해주세요.", k=2, filter={"source": "tweet"})
for res, score in results_with_scores:
print(f"* [SIM={score:.3f}] {res.page_content} [{res.metadata}]")

​RAG 체인에 FAISS 통합💡RAG (Retrieval-Augmented Generation) 체인을 구성하여 검색된 문서를 바탕으로 질문에 응답할 수 있도록 구성합니다.Step 1: Retriever로 변환FAISS를 retriever로 변환해 RAG 체인에서 사용합니다.PythonCopyretriever = vector_store.as_retriever(search_type="similarity", search_kwargs={"k": 1})

​Step 2: RAG 체인 생성LangChain의 모델과 프롬프트를 연결하여 RAG 체인을 구성합니다.PythonCopy
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough

# 프롬프트 템플릿 정의
contextual_prompt = ChatPromptTemplate.from_messages([
("system", "Answer the question using only the following context."),
("user", "Context: {context}\\: {question}")
])
class DebugPassThrough(RunnablePassthrough):
def invoke(self, *args, **kwargs):
        output = super().invoke(*args, **kwargs)
print("Debug Output:", output)
return output
# 문서 리스트를 텍스트로 변환하는 단계 추가
class ContextToText(RunnablePassthrough):
def invoke(self, inputs, config=None, **kwargs): # config 인수 추가
# context의 각 문서를 문자열로 결합
        context_text = "".join([doc.page_content for doc in inputs["context"]])
return {"context": context_text, "question": inputs["question"]}
# RAG 체인에서 각 단계마다 DebugPassThrough 추가
rag_chain_debug = {
"context": retriever, # 컨텍스트를 가져오는 retriever
"question": DebugPassThrough() # 사용자 질문이 그대로 전달되는지 확인하는 passthrough
} | DebugPassThrough() | ContextToText()|   contextual_prompt | model

# 질문 실행 및 각 단계 출력 확인
response = rag_chain_debug.invoke("강사이름은?")
print("Final Response:")
print(response.content)
​FAISS 인덱스의 저장 및 로드FAISS 인덱스를 저장해 다시 사용할 수 있습니다.PythonCopy# 인덱스 저장
vector_store.save_local("faiss_index")
# 저장된 인덱스 로드
new_vector_store = FAISS.load_local("faiss_index", embeddings)

​FAISS 데이터베이스 병합두 개의 FAISS 데이터베이스를 병합할 수 있습니다.PythonCopydb1 = FAISS.from_texts(["문서 1 내용"], embeddings)
db2 = FAISS.from_texts(["문서 2 내용"], embeddings)
# 병합
db1.merge_from(db2)

​