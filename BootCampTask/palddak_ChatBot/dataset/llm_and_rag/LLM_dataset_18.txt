문서 임베딩 실습하기📕 LLM & RAG를 활용한 AI 서비스 만들기/📘 LLM & RAG를 활용한 AI 서비스 만들기 - 5주차/📕문서 임베딩 실습하기📕문서 임베딩 실습하기수업 목표문서 임베딩의 기본 개념과 Faiss로 유사도 검색 실습을 진행합니다.목차실습 준비물 문서 임베딩의 기본 개념문서 단위 임베딩 전체 문서 임베딩하기문장 단위 임베딩하기문단 단위 임베딩하기Faiss로 유사도 검색하기Faiss 초기화 및 인덱싱유사 문장 검색하기유사도에 대한 실험길이 단위 선택 가이드추가 실험: 임베딩 벡터 시각화하기추가 실험: t-SNE를 이용한 고차원 임베딩 시각화💡여러분 안녕하세요! 오늘은 input.txt라는 txt 파일을 가지고 문서 임베딩을 실습해보려고 합니다. 🤓 특히 Faiss를 활용해 유사 단어 검색까지 해볼 거예요! 문서를 임베딩하는 여러 방법에 대해서도 실험해볼 테니 기대해 주세요. 🚀실습 준비물 Python 코드 실행 환경 (Jupyter Notebook 또는 IDE 추천)input.txt 파일 (이 파일에는 우리가 분석할 텍스트 데이터가 들어있어요!)sentence-transformers 라이브러리 설치faiss 라이브러리 설치Plain TextCopypip install sentence-transformers faiss-cpu​문서 임베딩의 기본 개념❗문서를 임베딩할 때, 텍스트를 벡터 형태로 변환하여 컴퓨터가 이해하기 쉽게 만들어요. 변환된 벡터들은 문서 간의 유사도를 계산하거나, 검색을 위해 활용됩니다. 이번 실습에서는 최신 한국어 모델을 사용해 문서를 임베딩할 거예요! 최신 모델은 sentence-transformers를 통해 불러올 예정입니다. ✨참고: 임베딩이란 텍스트, 이미지 등 다양한 데이터를 고차원의 벡터 공간에 표현하는 것을 말합니다. 
이렇게 표현된 벡터들은 기계 학습 모델들이 데이터를 더 쉽게 이해하고 분석할 수 있게 도와줘요!문서 단위 임베딩 💡문서를 임베딩할 때 중요한 결정 중 하나는 임베딩할 텍스트의 길이를 어떻게 설정할지입니다. 한 번 전체 문서를 통째로 임베딩할 수도 있고, 문장을 단위로 하거나 문단 단위로 쪼개서 임베딩할 수도 있어요. 각 방법의 장단점을 비교해 보죠!전체 문서 임베딩하기먼저 전체 문서를 하나의 벡터로 임베딩하는 방법입니다.
이 방법은 문서 전체의 맥락을 이해하는 데 유리하지만, 길이가 너무 길 경우 중요한 정보가 손실될 수 있습니다.PythonCopyfrom sentence_transformers import SentenceTransformer
import numpy as np

# 모델 불러오기
model = SentenceTransformer('sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens')
# input.txt 파일 불러오기
with open('input.txt', 'r', encoding='utf-8') as file:
    document = file.read()
# 전체 문서 임베딩
doc_embedding = model.encode(document)
​실습 실험: 전체 문서 임베딩의 성능을 확인하기 위해 doc_embedding을 사용해 문서 전체의 요약본을 다른 문서와 비교해 보세요. 전체 문서를 하나의 벡터로 표현하는 것이 실제 문서의 의미를 얼마나 잘 반영하는지 확인할 수 있어요.문장 단위 임베딩하기문서를 문장 단위로 나누어 임베딩해 볼까요? 이렇게 하면 문장별 유사도를 계산하기에 좋습니다. 특히 검색 시 특정 문장과 유사한 문장을 찾는 데 유용해요.PythonCopyfrom nltk.tokenize import sent_tokenize

# 문장으로 나누기
sentences = sent_tokenize(document)
# 각 문장을 임베딩
sentence_embeddings = model.encode(sentences)
​실습 실험: 문장 단위로 임베딩한 후, 특정 문장을 입력해 유사 문장을 찾아보세요. 예를 들어, "이 문서의 핵심은 무엇인가요?" 같은 질문을 던지고 그와 유사한 문장을 검색해 봅시다.문단 단위 임베딩하기문서를 문단 단위로 쪼개 임베딩하는 방법도 있어요. 이 방법은 문서 내 주제별 정보를 분석할 때 유리합니다.PythonCopyparagraphs = document.split('') # 두 줄 개행 기준으로 문단 나누기
# 각 문단을 임베딩
paragraph_embeddings = model.encode(paragraphs)
​실습 실험: 문단 단위로 나눈 후 각 문단의 벡터를 시각화해서 문단별 유사도를 비교해 보세요. 이 과정에서 PCA 또는 t-SNE 같은 차원 축소 방법을 사용하면 벡터 간의 관계를 쉽게 확인할 수 있어요!Faiss로 유사도 검색하기💡임베딩이 끝났다면, 이제 Faiss를 사용해 유사도를 계산해 봅시다. Faiss는 벡터 간 유사도를 빠르게 계산할 수 있도록 도와주는 라이브러리입니다.Faiss 초기화 및 인덱싱먼저, 임베딩된 벡터들을 인덱싱해서 검색할 준비를 해볼게요!PythonCopyimport faiss

# 문장 임베딩을 사용해 인덱스 생성
dimension = sentence_embeddings.shape[1]
index = faiss.IndexFlatL2(dimension) # L2 거리 기반 인덱스
# 인덱스에 벡터 추가
index.add(np.array(sentence_embeddings))
​유사 문장 검색하기특정 문장과 유사한 문장을 찾아봅시다! 검색하고 싶은 문장을 입력하고, 인덱스를 활용해 유사도를 계산합니다.PythonCopyquery = "검색할 문장을 입력하세요."
query_vec = model.encode(query)
# 유사한 문장 3개 찾기
D, I = index.search(np.array([query_vec]), k=3)
# 결과 출력
for idx in I[0]:
print(f"유사 문장: {sentences[idx]}")
​실습 실험: 다른 문장들을 입력해서 어떤 문장이 가장 유사하게 검색되는지 실험해 보세요. 예를 들어, "오늘 날씨는 어때?"와 같은 문장을 넣어보면 유사한 주제를 가진 문장들이 잘 검색되는지 확인할 수 있습니다.유사도에 대한 실험유사도 검색 결과의 정확도를 평가해봅시다. 다음과 같은 실험을 통해 유사도 결과를 분석해 볼 수 있어요

1️⃣다양한 쿼리 테스트
다양한 쿼리를 사용하여 유사 문장 검색 결과를 비교해 보세요. 예를 들어, 간단한 문장과 복잡한 문장을 각각 입력해 보고 결과가 어떻게 달라지는지 관찰합니다.2️⃣유사도 점수 분석
검색 결과로 반환된 유사도 점수(D)를 분석하여, 높은 유사도와 낮은 유사도 간의 차이를 확인해 보세요. 예를 들어, D 값을 출력하여 얼마나 유사한지 정량적으로 평가해 볼 수 있습니다.PythonCopy# 유사도 점수와 함께 결과 출력
for idx, score in zip(I[0], D[0]):
print(f"유사 문장: {sentences[idx]}, 유사도 점수: {score}")
​실습 실험: 유사도 점수가 높은 문장과 낮은 문장을 비교해 보고, 그 차이가 문장의 의미나 표현 방식에서 어떻게 드러나는지 분석해 보세요. 이렇게 하면 모델이 유사도를 어떻게 판단하는지 더 깊이 이해할 수 있습니다.길이 단위 선택 가이드전체 문서 임베딩: 문서 전체 맥락을 분석하고 싶을 때.문장 단위 임베딩: 특정 문장과의 유사도 계산이나 세밀한 검색이 필요할 때.문단 단위 임베딩: 주제별 유사도를 분석하고 싶을 때.Tip: 각 단위로 임베딩했을 때의 차이를 비교해 보는 것도 좋은 실습이 될 수 있어요. 예를 들어, 문장 단위로 했을 때와 문단 단위로 했을 때의 검색 결과가 어떻게 달라지는지 확인해 보세요. 😊추가 실험: 임베딩 벡터 시각화하기❗임베딩된 벡터들은 고차원 공간에 위치하게 됩니다. 이를 시각화하면 벡터 간의 관계를 더 직관적으로 이해할 수 있어요. PCA나 t-SNE 같은 차원 축소 기법을 사용해 2D나 3D로 시각화해 봅시다!PythonCopyfrom sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# 문장 임베딩 벡터의 차원을 축소하여 시각화
pca = PCA(n_components=2)
reduced_embeddings = pca.fit_transform(sentence_embeddings)
# 시각화
plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1])
plt.title('Sentence Embeddings Visualization')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.show()
​실습 실험: 각 색깔로 다른 문단이나 문서의 벡터를 시각화해서 벡터들이 어떻게 군집되는지 확인해 보세요. 이는 유사한 문장들이 벡터 공간에서 얼마나 가깝게 위치하는지 직관적으로 이해하는 데 도움이 됩니다. 🎨추가 실험: t-SNE를 이용한 고차원 임베딩 시각화❗t-SNE는 고차원 데이터를 저차원으로 시각화하는 데 유용한 기법이에요. 임베딩된 벡터들을 t-SNE를 사용해 시각화해 보면, 벡터 간의 관계를 더 명확하게 이해할 수 있습니다.PythonCopyfrom sklearn.manifold import TSNE

# t-SNE로 차원 축소
tsne = TSNE(n_components=2, perplexity=30, n_iter=300)
tsne_results = tsne.fit_transform(sentence_embeddings)
# 시각화
plt.scatter(tsne_results[:, 0], tsne_results[:, 1])
plt.title('t-SNE Sentence Embeddings Visualization')
plt.xlabel('t-SNE Component 1')
plt.ylabel('t-SNE Component 2')
plt.show()
​Tip: t-SNE는 계산 비용이 많이 들기 때문에, 데이터의 크기가 클 경우 일부만 샘플링해서 시각화해 보세요. t-SNE는 특히 데이터 간의 지역적 구조를 잘 드러내는 데 유리해요.