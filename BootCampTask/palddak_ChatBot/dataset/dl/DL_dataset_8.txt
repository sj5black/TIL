[스파르타코딩클럽] 8. 자연어 처리(NLP) 모델📘[SCC] 기초가 탄탄한 딥러닝/📚[스파르타코딩클럽] 기초가 탄탄한 딥러닝 - 3주차/📕[스파르타코딩클럽] 8. 자연어 처리(NLP) 모델Made with📕[스파르타코딩클럽] 8. 자연어 처리(NLP) 모델[수업 목표]자연어 처리 모델에 대해서 알아보고 동작 원리에 대해서 학습해 봅시다Pytorch로 간단한 텍스트 분류 및 생성 모델 구현 실습을 진행해 봅시다[목차]01. 워드 임베딩과 시퀀스 모델링02. Transformer와 BERT💡모든 토글을 열고 닫는 단축키
Windows : Ctrl + alt + t 
Mac : ⌘ + ⌥ + t 01. 워드 임베딩과 시퀀스 모델링✔️워드임베딩 기법이 무엇인지 알아보고 시퀀스 모델링이 무엇인지 학습해 봅시다1) 워드 임베딩 기법☑️ 워드 임베딩 기법워드 임베딩(Word Embedding)은 단어를 고정된 크기의 벡터로 변환하는 기법으로, 단어 간의 의미적 유사성을 반영합니다.대표적인 워드 임베딩 기법으로는 Word2Vec과 GloVe가 있습니다.ALT☑️ Word2VecWord2Vec은 단어를 벡터로 변환하는 두 가지 모델(CBOW와 Skip-gram)을 제공합니다.CBOW (Continuous Bag of Words): 주변 단어(context)로 중심 단어(target)를 예측합니다.Skip-gram: 중심 단어(target)로 주변 단어(context)를 예측합니다.☑️ GloVe (Global Vectors for Word Representation)GloVe는 단어-단어 공기행렬(word-word co-occurrence matrix)을 사용, 단어 벡터를 학습합니다.전역적인 통계 정보를 활용하여 단어 간의 의미적 유사성을 반영합니다.2) 시퀀스 모델링☑️ 시퀀스 모델링의 기본 개념시퀀스 모델링(Sequence Modeling)은 순차적인 데이터를 처리하고 예측하는 모델링 기법입니다. 시퀀스 모델링은 주로 RNN, LSTM, GRU와 같은 순환 신경망을 사용합니다.ALT☑️ 입력 시퀀스시퀀스 모델링에서는 입력 데이터가 순차적인 형태로 제공됩니다.예를 들어, 텍스트 데이터는 단어의 시퀀스로 표현됩니다.☑️ 은닉 상태순환 신경망은 이전 시간 단계의 은닉 상태를 현재 시간 단계로 전달하여, 시퀀스의 패턴을 학습합니다.☑️ 출력 시퀀스시퀀스 모델링의 출력은 입력 시퀀스와 동일한 길이의 시퀀스일 수도 있고, 단일 값일 수도 있습니다.02. Transformer와 BERT✔️Transformer의 구조에 대해 알아보고 이를 이용한 BERT 모델에 대해서 배워봅시다1) Transformer의 구조와 원리☑️ Transformer의 구조와 원리Transformer는 순차적인 데이터를 병렬로 처리할 수 있는 모델로, 자연어 처리에서 뛰어난 성능을 보입니다.Transformer는 인코더-디코더(Encoder-Decoder) 구조로 구성됩니다.ALT☑️ 인코더 (Encoder)입력 시퀀스를 처리하여 인코딩된 표현을 생성합니다.각 인코더 층은 셀프 어텐션(Self-Attention)과 피드포워드 신경망(Feed-Forward Neural Network)으로 구성됩니다.☑️ 디코더 (Decoder)인코딩된 표현을 바탕으로 출력 시퀀스를 생성합니다.각 디코더 층은 셀프 어텐션, 인코더-디코더 어텐션, 피드포워드 신경망으로 구성됩니다.☑️ 어텐션 메커니즘 (Attention Mechanism)어텐션 메커니즘은 입력 시퀀스의 각 위치에 가중치를 부여하여, 중요한 정보를 강조합니다.셀프 어텐션은 입력 시퀀스 내의 단어 간의 관계를 학습합니다.2) BERT의 개념과 응용☑️ BERT란?BERT(Bidirectional Encoder Representations from Transformers)는 Transformer 인코더를 기반으로 한 사전 학습된 언어 모델입니다.BERT는 양방향으로 문맥을 이해할 수 있어, 다양한 자연어 처리 작업에서 뛰어난 성능을 보입니다.☑️ 사전 학습(Pre-training)BERT는 대규모 텍스트 코퍼스를 사용하여 사전 학습됩니다.마스킹 언어 모델(Masked Language Model)과 다음 문장 예측(Next Sentence Prediction) 작업을 통해 학습됩니다.☑️ 파인튜닝 (Fine-tuning)사전 학습된 BERT 모델을 특정 작업에 맞게 파인튜닝합니다.텍스트 분류, 질의 응답, 텍스트 생성 등 다양한 자연어 처리 작업에 적용할 수 있습니다.Copyright ⓒ TeamSparta All rights reserved.