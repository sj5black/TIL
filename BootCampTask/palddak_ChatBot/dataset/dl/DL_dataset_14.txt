[스파르타코딩클럽] 14. 과적합 방지 기법📘[SCC] 기초가 탄탄한 딥러닝/📚[스파르타코딩클럽] 기초가 탄탄한 딥러닝 - 6주차/📕[스파르타코딩클럽] 14. 과적합 방지 기법Made with📕[스파르타코딩클럽] 14. 과적합 방지 기법[수업 목표]여러 과적합 방지 기법에 대해서 알아봅시다.Pytorch로  과적합 방지 기법에 대한 실습 예시![목차]01. 과적화 방지 기법02. 과적합 방지기법 실습(Pytorch)import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
​☑️데이터셋 로드 및 전처리데이터셋 로드 및 전처리 {5px}데이터셋 로드 및 전처리 ﻿​PythonCopy# 데이터셋 전처리
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])
# CIFAR-10 데이터셋 로드
trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)

testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)
​☑️  드롭아웃과 배치 정규화를 적용한 모델 정의 드롭아웃과 배치 정규화를 적용한 모델 정의 {5px} 드롭아웃과 배치 정규화를 적용한 모델 정의 ﻿​PythonCopyclass CNNWithDropoutAndBatchNorm(nn.Module):
def __init__(self):
super(CNNWithDropoutAndBatchNorm, self).__init__()
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(64)
        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(128)
        self.fc1 = nn.Linear(128 * 56 * 56, 256)
        self.dropout = nn.Dropout(0.5)
        self.fc2 = nn.Linear(256, 10)
def forward(self, x):
        x = torch.relu(self.bn1(self.conv1(x)))
        x = torch.max_pool2d(x, 2)
        x = torch.relu(self.bn2(self.conv2(x)))
        x = torch.max_pool2d(x, 2)
        x = x.view(x.size(0), -1)
        x = torch.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
return x

model = CNNWithDropoutAndBatchNorm()
​nn.Conv2d: 2차원 합성곱 층을 정의합니다. nn.Conv2d(in_channels, out_channels, kernel_size, padding)은 입력 채널 수, 출력 채널 수, 커널 크기, 패딩을 지정.Conv2d(in_channels, out_channels, kernel_size, padding)은 입력 채널 수, 출력 채널 수, 커널 크기, 패딩을 지정﻿​nn.BatchNorm2d: 2차원 배치 정규화 층을 정의합니다.nn.Dropout: 드롭아웃 층을 정의합니다. nn.Dropout(p)은 드롭아웃 확률을 지정합니다..Dropout(p)은 드롭아웃 확률을 지정합니다.﻿​torch.max_pool2d: 2차원 최대 풀링을 수행합니다.☑️ 손실 함수와 최적화 알고리즘 정의손실 함수와 최적화 알고리즘 정의 {5px}손실 함수와 최적화 알고리즘 정의 ﻿​PythonCopycriterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
​nn.CrossEntropyLoss: 교차 엔트로피 손실 함수를 정의합니다.optim.Adam: Adam 최적화 알고리즘을 정의합니다. lr은 학습률을 지정합니다.은 학습률을 지정합니다.﻿​☑️ 모델 학습모델 학습 {5px}모델 학습 ﻿​PythonCopynum_epochs = 10
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)
for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
for i, (inputs, labels) in enumerate(trainloader):
        inputs, labels = inputs.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
if i % 100 == 99: # 매 100 미니배치마다 출력
print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 100:.3f}')
            running_loss = 0.0
print('Finished Training')
​model.train(): 모델을 학습 모드로 전환합니다.optimizer.zero_grad(): 이전 단계에서 계산된 기울기를 초기화합니다.loss.backward(): 역전파를 통해 기울기를 계산합니다.optimizer.step(): 계산된 기울기를 바탕으로 가중치를 업데이트합니다.☑️ 모델 평가모델 평가 {5px}모델 평가 ﻿​PythonCopymodel.eval()
correct = 0
total = 0
with torch.no_grad():
for inputs, labels in testloader:
        inputs, labels = inputs.to(device), labels.to(device)
        outputs = model(inputs)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')
​model.eval(): 모델을 평가 모드로 전환합니다.torch.no_grad(): 평가 단계에서는 기울기를 계산할 필요가 없으므로, 이를 비활성화하여 메모리 사용을 줄입니다.torch.max: 텐서의 최대 값을 찾습니다. torch.max(outputs.data, 1)은 각 샘플에 대해 가장 높은 확률을 가진 클래스를 반환합니다..max(outputs.data, 1)은 각 샘플에 대해 가장 높은 확률을 가진 클래스를 반환합니다.﻿​labels.size(0): 배치 크기를 반환합니다.(predicted == labels).sum().item(): 예측 값과 실제 값이 일치하는 샘플의 수를 계산합니다.2)  데이터 증강을 통한 모델 성능 향상 실습☑️데이터셋 로드 및 전처리데이터셋 로드 및 전처리 {5px}데이터셋 로드 및 전처리 ﻿​PythonCopy# 데이터 증강 적용
transform_train = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomCrop(32, padding=4),
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

transform_test = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])
# CIFAR-10 데이터셋 로드
trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)

testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)
testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)
​transforms.RandomHorizontalFlip(): 이미지를 무작위로 수평 반전합니다.transforms.RandomCrop(size, padding): 이미지를 무작위로 자르고, 패딩을 추가합니다.☑️  드롭아웃과 배치 정규화를 적용한 모델 정의첫번째 실습에서 정의한 모델을 그대로 사용합니다. 드롭아웃과 배치 정규화를 적용한 모델 정의 {5px} 드롭아웃과 배치 정규화를 적용한 모델 정의 ﻿​PythonCopyclass CNNWithDropoutAndBatchNorm(nn.Module):
def __init__(self):
super(CNNWithDropoutAndBatchNorm, self).__init__()
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(64)
        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(128)
        self.fc1 = nn.Linear(128 * 56 * 56, 256)
        self.dropout = nn.Dropout(0.5)
        self.fc2 = nn.Linear(256, 10)
def forward(self, x):
        x = torch.relu(self.bn1(self.conv1(x)))
        x = torch.max_pool2d(x, 2)
        x = torch.relu(self.bn2(self.conv2(x)))
        x = torch.max_pool2d(x, 2)
        x = x.view(x.size(0), -1)
        x = torch.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
return x

model = CNNWithDropoutAndBatchNorm()
​☑️ 손실 함수와 최적화 알고리즘 정의첫번째 실습에서 정의한 손실함수와 최적화 알고리즘을 그대로 사용합니다.손실 함수와 최적화 알고리즘 정의 {5px}손실 함수와 최적화 알고리즘 정의 ﻿​PythonCopycriterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
​☑️ 모델 학습첫번째 실습에서 정의한 모델 학습 코드를 그대로 사용합니다.모델 학습 {5px}모델 학습 ﻿​PythonCopynum_epochs = 10
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)
for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
for i, (inputs, labels) in enumerate(trainloader):
        inputs, labels = inputs.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
if i % 100 == 99: # 매 100 미니배치마다 출력
print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 100:.3f}')
            running_loss = 0.0
print('Finished Training')
​☑️ 모델 평가첫번째 실습에서 정의한 모델 평가 코드를 그대로 사용합니다.모델 평가 {5px}모델 평가 ﻿​PythonCopymodel.eval()
correct = 0
total = 0
with torch.no_grad():
for inputs, labels in testloader:
        inputs, labels = inputs.to(device), labels.to(device)
        outputs = model(inputs)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')
​Copyright ⓒ TeamSparta All rights reserved.