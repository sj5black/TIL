[스파르타코딩클럽] 2. 신경망의 기본 원리📘[SCC] 기초가 탄탄한 딥러닝/📚[스파르타코딩클럽] 기초가 탄탄한 딥러닝 - 1주차/📕[스파르타코딩클럽] 2. 신경망의 기본 원리Made with📕[스파르타코딩클럽] 2. 신경망의 기본 원리[수업 목표]퍼셉트론의 개념과 다층 퍼셉트론에 대해 배워봅시다.신경망을 강화하기 위한 활성화 함수/손실 함수/역전파/최적화 알고리즘에 대해 배워봅시다[목차]01. 퍼셉트론과 다층 퍼셉트론(XOR 문제 포함)02. 다층 퍼셉트론(MLP)03. 활성화 함수04. 손실 함수와 최적화 알고리즘05. 역전파에 대해 알아볼까요?💡모든 토글을 열고 닫는 단축키
Windows : Ctrl + alt + t 
Mac : ⌘ + ⌥ + t 01. 퍼셉트론과 다층 퍼셉트론(XOR 문제 포함)✔️인공신공망의 가장 기본 단위인 퍼셉트론에 대해 배워봅시다1) 단일 퍼셉트론의 원리☑️ 단일 퍼셉트론의 개념퍼셉트론(Perceptron)은 인공 신경망의 가장 기본적인 단위로, 하나의 뉴런을 모델링한 것입니다.퍼셉트론은 입력 값을 받아 가중치(weight)를 곱하고, 이를 모두 더한 후 활성화 함수(activation function)를 통해 출력 값을 결정합니다.ALT☑️ 퍼셉트론의 수학적 표현y=f(∑i=1nwixi+b)y = f(_{i=1}^{n} w_i x_i + b) y=f(i=1∑n​wi​xi​+b)여기서 xi는 입력 값, wi는 가중치, b는 바이어스(bias), f는 활성화 함수입니다.여기서 xi​는 입력 값, wi​는 가중치, b는 바이어스(bias), f는 활성화 함수입니다.﻿​02. 다층 퍼셉트론(MLP)✔️다층 퍼셉트론에 대해 배워봅시다1) 다층 퍼셉트론(MLP)과 XOR 문제 해결☑️ 다층 퍼셉트론(MLP)의 개념다층 퍼셉트론(Multi-Layer Perceptron, MLP)은 여러 층의 퍼셉트론을 쌓아 올린 신경망 구조입니다.MLP는 입력층(input layer), 은닉층(hidden layer), 출력층(output layer)으로 구성되며, 각 층의 뉴런들이 서로 연결되어 있습니다.ALT☑️ 입력, 은닉, 출력 레이어의 개념입력 레이어(Input Layer) : 외부 데이터가 신경망에 입력되는 부분입니다. 입력 레이어의 뉴런 수는 입력 데이터의 특징 수와 동일합니다.은닉 레이어(Hidden Layer) : 은닉 레이어는 입력 레이어와 출력 레이어 사이에 위치한 층으로, 입력 데이터를 처리하고 특징을 추출하는 역할을 합니다. 은닉 레이어의 뉴런 수와 층 수는 모델의 복잡성과 성능에 영향을 미칩니다.출력 레이어(Output Layer) : 출력 레이어는 신경망의 마지막 층으로, 최종 예측 값을 출력합니다. 출력 레이어의 뉴런 수는 예측하려는 클래스 수 또는 회귀 문제의 출력 차원과 동일합니다.☑️ XOR 문제와 MLP단일 퍼셉트론은 선형 분류기이기 때문에 XOR 문제와 같은 비선형 문제를 해결할 수 없습니다.XOR 문제는 두 입력 값이 다를 때만 1을 출력하는 문제로, 단일 퍼셉트론으로는 해결할 수 없습니다. 그러나 MLP는 은닉층을 통해 비선형성을 학습할 수 있어 XOR 문제를 해결할 수 있습니다.03. 활성화 함수✔️활성화 함수라는게 무엇인지, 신경망에서 어떤 역할을 하는지 알아보고, 어떤 종류의 활성화 함수가 있는지 배워봅시다1) 활성화 함수의 필요성과 종류☑️ 활성화 함수의 필요성활성화 함수는 신경망의 각 뉴런에서 입력값을 출력값으로 변환하는 역할을 합니다.활성화 함수가 없다면 신경망은 단순 선형변환만 수행하게 되어 복잡한 패턴을 학습할 수 없습니다.활성화 함수는 비 선형성을 도입하여 신경망이 복잡한 패턴을 학습할 수 있게합니다.☑️ 활성화 함수의 종류ReLU (Rectified Linear Unit)f(x)=max⁡(0,x)f(x) = (0, x)f(x)=max(0,x)장점: 계산이 간단하고, 기울기 소실 문제(vanishing gradient problem)를 완화합니다.단점: 음수 입력에 대해 기울기가 0이 되는 '죽은 ReLU' 문제가 발생할 수 있습니다.Sigmoidf(x)=11+e−xf(x) = {1 + e^{-x}} f(x)=1+e−x1​장점: 출력 값이 0과 1 사이로 제한되어 확률을 표현하기에 적합합니다.단점: 기울기 소실 문제와 출력 값이 0 또는 1에 가까워질 때 학습이 느려지는 문제가 있습니다.Tanh (Hyperbolic Tangent)f(x)=tanh⁡(x)=ex−e−xex+e−xf(x) = (x) = }{e^x + e^{-x}}f(x)=tanh(x)=ex+e−xex−e−x​장점: 출력 값이 -1과 1 사이로 제한되어 중심이 0에 가까워집니다.단점: 기울기 소실 문제가 발생할 수 있습니다.04. 손실 함수와 최적화 알고리즘✔️손실함수와 최적화 알고리즘이 무엇인지 배우고 주요 사용하는 함수의 종류를 배워봅시다.1) 손실 함수의 역할과 주요 종류☑️ 손실함수의 역할손실 함수(Loss Function)는 모델의 예측 값과 실제 값 사이의 차이를 측정하는 함수입니다.손실 함수는 모델의 성능을 평가하고, 최적화 알고리즘을 통해 모델을 학습시키는 데 사용됩니다.☑️ 주요 손실 함수의 종류MSE (Mean Squared Error)MSE=1n∑i=1n(yi−y^i)2 = {n} _{i=1}^{n} (y_i - _i)^2 MSE=n1​i=1∑n​(yi​−y^​i​)2사용 분야: 회귀 문제에서 주로 사용됩니다.특징: 예측 값과 실제 값의 차이를 제곱하여 평균을 구합니다.Cross-EntropyCross-Entropy=−∑i=1nyilog⁡(y^i) = -_{i=1}^{n} y_i (_i)Cross-Entropy=−i=1∑n​yi​log(y^​i​)사용 분야: 분류 문제에서 주로 사용됩니다.특징: 예측 확률과 실제 클래스 간의 차이를 측정합니다.2) 최적화 알고리즘의 개념과 종류☑️ 최적화 알고리즘의 개념최적화 알고리즘(Optimization Algorithm)은 손실 함수를 최소화하기 위해 모델의 가중치를 조정하는 방법입니다.최적화 알고리즘은 손실 함수의 기울기를 계산하고, 이를 바탕으로 가중치를 업데이트합니다.☑️ 주요 최적화 알고리즘의 종류SGD (Stochastic Gradient Descent)개념: 전체 데이터셋이 아닌 무작위로 선택된 일부 데이터(미니배치)를 사용하여 기울기를 계산하고 가중치를 업데이트합니다.장점: 계산이 빠르고, 큰 데이터셋에서도 효율적으로 동작합니다.단점: 최적점에 도달하기까지 진동이 발생할 수 있습니다.Adam (Adaptive Moment Estimation)개념: 모멘텀과 RMSProp을 결합한 알고리즘으로, 학습률을 적응적으로 조정합니다.장점: 빠른 수렴 속도와 안정적인 학습을 제공합니다.단점: 하이퍼파라미터 설정이 복잡할 수 있습니다.05. 역전파에 대해 알아볼까요?✔️역전파에 대해 배워봅시다1) 역전파 알고리즘의 개념과 수학적 원리☑️ 역전파 알고리즘의 개념역전파(Backpropagation)는 신경망의 가중치를 학습시키기 위해 사용되는 알고리즘입니다. 출력에서 입력 방향으로 손실 함수의 기울기를 계산하고, 이를 바탕으로 가중치를 업데이트합니다.☑️ 역전파의 수학적 원리연쇄 법칙(Chain Rule)을 사용해 손실함수의 기울기를 계산합니다.각 층의 기울기는 이전 층의 기울기와 현재 층의 기울기를 곱하여 계산합니다.이를 통해 신경망의 모든 가중치가 업데이트 됩니다ALTCopyright ⓒ TeamSparta All rights reserved.