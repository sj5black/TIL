[스파르타코딩클럽] 4. 인공 신경망(ANN)📘[SCC] 기초가 탄탄한 딥러닝/📚[스파르타코딩클럽] 기초가 탄탄한 딥러닝 - 2주차/📕[스파르타코딩클럽] 4. 인공 신경망(ANN)Made with📕[스파르타코딩클럽] 4. 인공 신경망(ANN)[수업 목표]인공신경망의 개념에 대해서 배워보고 어떤 원리로 동작하는지 알아봅시다Pytorch로 간단한 인공신경망 모델 구현 실습을 진행해 봅시다[목차]01. 기본 구조와 동작원리02. 실습: 간단한 인공 신경망 모델 구현 (PyTorch) - 입력데이터를 받아들이는 층, 입력층의 뉴런수는 입력데이터 피쳐수와 동일 - 입력데이터를 받아들이는 층, 입력층의 뉴런수는 입력데이터 피쳐수와 동일﻿
은닉층
 - 입력데이터를 처리하고 특징을 추출하는 층, 은닉층의 뉴런수와 층수는 모델의 복잡성과 성능에 영향 - 입력데이터를 처리하고 특징을 추출하는 층, 은닉층의 뉴런수와 층수는 모델의 복잡성과 성능에 영향﻿
출력층
 - 최종 예측값을 출력하는 층, 출력층의 뉴런 수는 예측하려는 클래스 수 또는 회귀문제 출력차원과 동일 - 최종 예측값을 출력하는 층, 출력층의 뉴런 수는 예측하려는 클래스 수 또는 회귀문제 출력차원과 동일﻿​☑️ 동작 방식순전파 (Forward Propagation)입력 데이터를 통해 각 층의 뉴런이 활성화되고, 최종 출력 값을 계산합니다.각 뉴런은 입력 값에 가중치(weight)를 곱하고, 바이어스(bias)를 더한 후 활성화 함수(activation function)를 통해 출력 값을 결정합니다.손실 계산 (Loss Calculation)예측 값과 실제 값의 차이를 손실 함수(Loss Function)로 계산합니다.역전파 (Backpropagation)손실 함수의 기울기를 출력층에서 입력층 방향으로 계산하고, 이를 바탕으로 가중치를 업데이트합니다.ALT2) 출력 레이어의 구성☑️ 출력레이어의 유형과 활용출력 레이어는 신경망의 최종 예측 값을 출력하는 층으로, 문제의 유형에 따라 다양한 형태로 구성될 수 있습니다.회귀 문제 (Regression):출력 레이어의 뉴런 수는 예측하려는 연속적인 값의 차원과 동일합니다.활성화 함수로는 주로 선형 함수(linear function)를 사용합니다.이진 분류 문제 (Binary Classification):출력 레이어의 뉴런 수는 1입니다.활성화 함수로는 시그모이드 함수(Sigmoid Function)를 사용하여 출력 값을 0과 1 사이의 확률로 변환합니다.다중 클래스 분류 문제 (Multi-Class Classification):출력 레이어의 뉴런 수는 예측하려는 클래스 수와 동일합니다.활성화 함수로는 소프트맥스 함수(Softmax Function)를 사용하여 각 클래스에 대한 확률을 출력합니다.02. 실습: 간단한 인공 신경망 모델 구현 (PyTorch)✔️ PyTorch를 사용하여 간단한 인공 신경망 모델을 구축하고 학습해보겠습니다. 예제로는 MNIST 데이터셋을 사용하여 숫자 이미지를 분류하는 모델을 구현하겠습니다.1)  간단한 ANN 모델 구축 및 학습☑️ PyTorch 및 필요한 라이브러리 임포트PyTorch 및 필요한 라이브러리 임포트 {5px}PyTorch 및 필요한 라이브러리 임포트 ﻿​PythonCopyimport torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
​☑️데이터셋 로드 및 전처리데이터셋 로드 및 전처리 {5px}데이터셋 로드 및 전처리 ﻿​PythonCopy# 데이터셋 전처리
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])
# MNIST 데이터셋 로드
trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)

testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)
​☑️ 간단한 ANN 모델 정의간단한 ANN 모델 정의 {5px}간단한 ANN 모델 정의 ﻿​PythonCopyclass SimpleANN(nn.Module):
def __init__(self):
super(SimpleANN, self).__init__()
        self.fc1 = nn.Linear(28 * 28, 128) # 입력층에서 은닉층으로
        self.fc2 = nn.Linear(128, 64) # 은닉층에서 은닉층으로
        self.fc3 = nn.Linear(64, 10) # 은닉층에서 출력층으로
def forward(self, x):
        x = x.view(-1, 28 * 28) # 입력 이미지를 1차원 벡터로 변환
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
return x
​torch.nn.Module: 모든 신경망 모듈의 기본 클래스입니다.  사용자 정의 신경망은 이 클래스를 상속받아야 합니다.nn.Linear: 선형 변환을 적용하는 완전 연결(fully connected) 레이어를 정의합니다.nn.Linear(in_features, out_features)는 입력 특징의 수와 출력 특징의 수를 지정합니다..Linear(in_features, out_features)는 입력 특징의 수와 출력 특징의 수를 지정합니다.﻿​torch.relu: ReLU 활성화 함수를 적용합니다.view: 텐서의 크기를 변경합니다.x.view(-1, 28 * 28)은 입력 이미지를 1차원 벡터로 변환합니다..view(-1, 28 * 28)은 입력 이미지를 1차원 벡터로 변환합니다.﻿​☑️ 모델 학습모델 학습 {5px}모델 학습 ﻿​PythonCopy# 모델 초기화
model = SimpleANN()
# 손실 함수와 최적화 알고리즘 정의
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
# 모델 학습
for epoch in range(10): # 10 에포크 동안 학습
    running_loss = 0.0
for i, data in enumerate(trainloader, 0):
        inputs, labels = data

        # 기울기 초기화
        optimizer.zero_grad()
# 순전파 + 역전파 + 최적화
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
# 손실 출력
        running_loss += loss.item()
if i % 100 == 99: # 매 100 미니배치마다 출력
print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 100:.3f}')
            running_loss = 0.0
print('Finished Training')
​nn.CrossEntropyLoss: 다중 클래스 분류 문제에서 주로 사용되는 손실 함수입니다. 예측 값과 실제 값 사이의 교차 엔트로피 손실을 계산합니다.optim.SGD: 확률적 경사 하강법(Stochastic Gradient Descent) 최적화 알고리즘을 정의합니다.  lr은 학습률, momentum은 모멘텀 값을 지정합니다.은 학습률, momentum은 모멘텀 값을 지정합니다.﻿​optimizer.zero_grad(): 이전 단계에서 계산된 기울기를 초기화합니다.loss.backward(): 역전파를 통해 기울기를 계산합니다.optimizer.step(): 계산된 기울기를 바탕으로 가중치를 업데이트합니다.☑️ 모델  평가모델 평가 {5px}모델 평가 ﻿​PythonCopycorrect = 0
total = 0
with torch.no_grad():
for data in testloader:
        images, labels = data
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')

​torch.no_grad(): 평가 단계에서는 기울기를 계산할 필요가 없으므로, 이를 비활성화하여 메모리 사용을 줄입니다.torch.max: 텐서의 최대 값을 찾습니다. torch.max(outputs.data, 1)은 각 샘플에 대해 가장 높은 확률을 가진 클래스를 반환합니다..max(outputs.data, 1)은 각 샘플에 대해 가장 높은 확률을 가진 클래스를 반환합니다.﻿​labels.size(0): 배치 크기를 반환합니다.(predicted == labels).sum().item(): 예측 값과 실제 값이 일치하는 샘플의 수를 계산합니다.Copyright ⓒ TeamSparta All rights reserved.