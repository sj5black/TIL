[스파르타코딩클럽] 7. 어텐션 (Attention) 메커니즘📘[SCC] 기초가 탄탄한 딥러닝/📚[스파르타코딩클럽] 기초가 탄탄한 딥러닝 - 3주차/📕[스파르타코딩클럽] 7. 어텐션 (Attention) 메커니즘Made with📕[스파르타코딩클럽] 7. 어텐션 (Attention) 메커니즘[수업 목표]최근 가장 성능 좋은 매커니즘! 어텐션 메커니즘에 대해 알아봅시다Pytorch의 구현 예시를 살펴봅시다[목차]01. 개념02. 실습:  Attention 메커니즘의 구현한번 훑는 정도로 넘어갑시다!1)  Attention☑️ Scaled Dot-Product AttentionScaled Dot-Product attention 메커니즘 구현{5px}Scaled Dot-Product attention 메커니즘 구현﻿​PythonCopyimport torch
import torch.nn.functional as F

def scaled_dot_product_attention(Q, K, V):
    d_k = Q.size(-1) # Key의 차원 수
    scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32)) # 유사도 계산 및 스케일링
    attn_weights = F.softmax(scores, dim=-1) # Softmax를 통한 가중치 계산
    output = torch.matmul(attn_weights, V) # 가중합을 통한 최종 출력 계산
return output, attn_weights

​☑️ Multi-Head Attention Multi-Head Attention 메커니즘 구현{5px} Multi-Head Attention 메커니즘 구현﻿​PythonCopyclass MultiHeadAttention(nn.Module):
def __init__(self, embed_size, heads):
super(MultiHeadAttention, self).__init__()
        self.embed_size = embed_size
        self.heads = heads
        self.head_dim = embed_size // heads

        assert (
            self.head_dim * heads == embed_size
        ), "Embedding size needs to be divisible by heads"

        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)
        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)
        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)
        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)
def forward(self, values, keys, query, mask=None):
        N = query.shape[0]
        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]
# Linear transformations
        values = self.values(values).view(N, value_len, self.heads, self.head_dim)
        keys = self.keys(keys).view(N, key_len, self.heads, self.head_dim)
        queries = self.queries(query).view(N, query_len, self.heads, self.head_dim)
# Scaled dot-product attention
        out, _ = scaled_dot_product_attention(queries, keys, values)

        out = out.view(N, query_len, self.heads * self.head_dim)
        out = self.fc_out(out)
return out

​Copyright ⓒ TeamSparta All rights reserved.