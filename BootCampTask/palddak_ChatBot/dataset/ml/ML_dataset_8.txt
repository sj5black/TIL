[스파르타코딩클럽] 8강. 지도학습 : 회귀모델 📘[SCC] 바닥부터 시작하는 머신러닝/📚[스파르타코딩클럽] 바닥부터 시작하는 머신러닝 - 3주차/📕[스파르타코딩클럽] 8강. 지도학습 : 회귀모델 Made with📕[스파르타코딩클럽] 8강. 지도학습 : 회귀모델 [수업 목표]다양한 회귀 모델에 대해서 배워봅시다회귀(Regression)모델은 연속적인 값을 예측하는 문제입니다.회귀(Regression)모델은 연속적인 값을 예측하는 문제입니다.﻿
오늘은 선형회귀/다항회귀/리지회귀/라쏘회귀 를 다뤄볼 예정입니다오늘은 선형회귀/다항회귀/리지회귀/라쏘회귀 를 다뤄볼 예정입니다﻿​[목차]01. 회귀모델y=β0​+β1​x1​+β2​x2​+⋯+βn​xn​+ϵy=β0​+β1​x1​+β2​x2​+⋯+βn​xn​+ϵy=β0​+β1​x1​+β2​x2​+⋯+βn​xn​+ϵ﻿
여기서 y는 종속변수, x1,x2,…,xn 은 독립변수, b0 는 절편, b1,b2,…,bn은 회귀계수, e는 오차입니다여기서 y는 종속변수, x1,x2,…,xn 은 독립변수, b0 는 절편, b1,b2,…,bn은 회귀계수, e는 오차입니다﻿
⋄ 단순 선형 회귀일경우 ⋄ 단순 선형 회귀일경우 ﻿
y=β0​+β1​x+ϵy=β0​+β1​x+ϵ﻿
☑️ Scikit-learn 을 사용한 선형 회귀 모델 구현 및 평가선형 회귀 모델 구현 및 평가 {5px}선형 회귀 모델 구현 및 평가 ﻿​PythonCopyimport numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# 데이터 생성
X = np.array([[1, 1], [2, 2], [3, 3], [4, 4], [5, 5],[6,6]])
y = np.array([1, 2, 3, 4, 5, 6])
# 데이터 분할 (훈련 데이터와 테스트 데이터)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# 선형 회귀 모델 생성 및 학습
model = LinearRegression()
model.fit(X_train, y_train)
# 예측
y_pred = model.predict(X_test)
# 모델 평가
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f'Mean Squared Error: {mse}')
print(f'R^2 Score: {r2}')
​2) 다항 회귀☑️ 다항 회귀다항 회귀(Polynomial Regression)는 종속 변수와 독립 변수 간의 비선형 관계를 모델링하는 방법독립변수의 다항식을 사용하여 관계를 모델링 합니다.다항 회귀의 기본 수식은 다음과 같습니다다항 회귀의 기본 수식은 다음과 같습니다﻿
y=β0​+β1​x+β2​x2+⋯+βn​xn+ϵy=β0​+β1​x+β2​x^2+⋯+βn​x^n+ϵy=β0​+β1​x+β2​x2+⋯+βn​xn+ϵ﻿
여기서 y는 종속변수, x1,x2,…,xn 은 독립변수, b0 는 절편, b1,b2,…,bn은 회귀계수, e는 오차입니다여기서 y는 종속변수, x1,x2,…,xn 은 독립변수, b0 는 절편, b1,b2,…,bn은 회귀계수, e는 오차입니다﻿​☑️ 다항 회귀 차수 선택다항회귀 차수(degree) : 독립 변수의 최대 차수차수가 높을수록 모델이 더 복잡해지며 과적합(overfitting)의 위험 존재 → 적절한 차수 선택 필요과적합이란 학습데이터에 모델이 과도하게 적합(fitting)되는 현상입니다과적합이란 학습데이터에 모델이 과도하게 적합(fitting)되는 현상입니다﻿​☑️ Scikit-learn을 사용한 다항 회귀 모델 구현 및 평가다항 회귀 모델 구현 및 평가 {5px}다항 회귀 모델 구현 및 평가 ﻿​PythonCopyimport numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# 데이터 생성
X = np.array([[1], [2], [3], [4], [5], [6]])
y = np.array([1, 4, 9, 16, 25, 36])
# 다항 특징 생성 (차수 2)
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)
# 데이터 분할 (훈련 데이터와 테스트 데이터)
X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=42)
# 다항 회귀 모델 생성 및 학습
model = LinearRegression()
model.fit(X_train, y_train)
# 예측
y_pred = model.predict(X_test)
# 모델 평가
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f'Mean Squared Error: {mse}')
print(f'R^2 Score: {r2}')
​3) 리지 회귀☑️ 리지 회귀리지 회귀(Ridge Regression)는 선형 회귀의 일종회귀 계수의 크기를 제어하여 과적합을 방지하는 정규화 기법L2 정규화(regularization)를 사용하여 회귀 계수의 제곱합을 최소화 합니다리지 회귀의 기본 수식은 다음과 같습니다리지 회귀의 기본 수식은 다음과 같습니다﻿
J(β)=∑i=1n(yi−y^i)2+λ∑j=1pβj2J() = _{i=1}^{n} (y_i - _i)^2 +  _{j=1}^{p} _j^2J(β)=∑i=1n​(yi​−y^​i​)2+λ∑j=1p​βj2​﻿
여기서λ는 정규화 강도를 조절하는 하이퍼파라미터 입니다. 여기서λ는 정규화 강도를 조절하는 하이퍼파라미터 입니다.﻿​☑️ L2 정규화 L2 정규화는 모든 가중치를 작게 만들어 모델의 복잡도를 줄입니다.손실 함수에 제곱항을 추가하여 매끄러운 최적화가 가능합니다.정규화는 모델의 복잡도를 제어하여 과적합을 방지하는 데 필요합니다.☑️ Scikit-learn을 사용한 리지 회귀 모델 구현 및 평가리지 회귀 모델 구현 및 평가 {5px}리지 회귀 모델 구현 및 평가 ﻿​PythonCopyimport numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error, r2_score

# 데이터 생성
X = np.array([[1, 1], [2, 2], [3, 3], [4, 4], [5, 5], [6, 6]])
y = np.array([1, 2, 3, 4, 5, 6])
# 데이터 분할 (훈련 데이터와 테스트 데이터)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# 리지 회귀 모델 생성 및 학습
model = Ridge(alpha=1.0)
model.fit(X_train, y_train)
# 예측
y_pred = model.predict(X_test)
# 모델 평가
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f'Mean Squared Error: {mse}')
print(f'R^2 Score: {r2}')
​4) 라쏘 회귀☑️ 라쏘 회귀라쏘 회귀(Lasso Regression)는 선형 회귀의 일종회귀 계수의 크기를 제어하여 과적합을 방지하는 정규화 기법L1 정규화(regularization)를 사용하여 회귀 계수의 절대값 합을 최소화 합니다라쏘 회귀의 기본 수식은 다음과 같습니다라쏘 회귀의 기본 수식은 다음과 같습니다﻿
J(β)=∑i=1n(yi−y^i)2+λ∑j=1p∣βj∣J() = _{i=1}^{n} (y_i - _i)^2 +  _{j=1}^{p} |_j|J(β)=∑i=1n​(yi​−y^​i​)2+λ∑j=1p​∣βj​∣﻿
여기서λ는 정규화 강도를 조절하는 하이퍼파라미터 입니다. 여기서λ는 정규화 강도를 조절하는 하이퍼파라미터 입니다.﻿​☑️ L1 정규화와 특징 선택L1 정규화는 일부 회귀 계수를 0으로 만들어 특징 선택(feature selection)을 수행모델의 해석 가능성을 높이고, 불필요한 특징을 제거하는 데 유용합니다☑️ Scikit-learn을 사용한 라쏘 회귀 모델 구현 및 평가라쏘 회귀 모델 구현 및 평가 {5px}라쏘 회귀 모델 구현 및 평가 ﻿​PythonCopyimport numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Lasso
from sklearn.metrics import mean_squared_error, r2_score

# 데이터 생성
X = np.array([[1, 1], [2, 2], [3, 3], [4, 4], [5, 5], [6,6]])
y = np.array([1, 2, 3, 4, 5, 6])
# 데이터 분할 (훈련 데이터와 테스트 데이터)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# 라쏘 회귀 모델 생성 및 학습
model = Lasso(alpha=1.0)
model.fit(X_train, y_train)
# 예측
y_pred = model.predict(X_test)
# 모델 평가
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f'Mean Squared Error: {mse}')
print(f'R^2 Score: {r2}')
​Copyright ⓒ TeamSparta All rights reserved.