{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "\n",
    "- langchain>=0.0.200\n",
    "- openai>=0.27.8\n",
    "- chromadb>=0.4.7\n",
    "- beautifulsoup4>=4.12.2\n",
    "- selenium>=4.11.2\n",
    "- webdriver-manager>=3.8.6\n",
    "- python-dotenv>=1.0.0\n",
    "- pprintpp>=0.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 트러블 슈팅\n",
    "\n",
    "> 2024/11/22 (성진)\n",
    "---\n",
    "**데이터 전처리 과정**\n",
    "1. WebBaseLoader 로 레퍼런스가 정상적으로 인식되지 않는 문제  \n",
    " - 원인 : WebBaseLoader의 한계로 인한 실패 가능성. WebBaseLoader는 간단한 텍스트 추출에 적합하며, 복잡한 HTML 구조나 JavaScript 기반의 동적 콘텐츠를 처리하지 못할 수 있다. \n",
    "\n",
    "    1_1. WebBaseLoader 대신 request를 사용한 HTML 크롤링 시도 -> 실패  \n",
    "     - 원인 : Notion 페이지는 JavaScript를 활용한 동적 렌더링을 사용하는 구조여서 requests만 사용해서는 JavaScript로 렌더링된 내용을 가져올 수 없었다.\n",
    "\n",
    "    1_2. Selenium 사용 --> 부분 성공  \n",
    "     - 원인 : implicitly_wait 으로 대기 후 로드 시 데이터를 가져오는 경우가 있고, 못가져오는 경우가 있었음..\n",
    "\n",
    "    1_3. 대기 방식 변경 (WebDriverWait.until() 사용) --> 성공  \n",
    "     - 로드하려는 웹페이지의 html 중 특정 요소가 로드될 때까지 wait하는 방식 사용\n",
    "\n",
    "2. soup.get_text() 에서 노션 문서의 헤드라인만 추출되는 문제  \n",
    " - 원인 : 노션 페이지에 진입했을 때, 토글(Ctrl+Alt+T)이 닫힌 상태로 텍스트가 추출되고 있었음\n",
    "\n",
    "    2_1. 단축키 입력을 선언하는 방법 (헤드리스 상태에서 사용 불가)  \n",
    "\n",
    "    2_2. html 내에 토글 버튼을 찾아 여는 방법 (헤드리스 상태에서 사용 가능) --> 선택 (성공)  \n",
    "     - drive.find_elements 로 버튼 탐색 후 클릭 (지연시간 1초)\n",
    "\n",
    "3. 추출된 텍스트 파일에 일부 문법 구문이 남아있는 문제  \n",
    " - re (regular expression) 을 활용해 삭제/전처리 (성공)\n",
    "\n",
    "**유사한 질문을 계속해서 질문하는 경우**\n",
    "1. cosine 유사도 비교로 유사한 질문을 생성해내는 경우에는 다른 질문을 생성하도록 했음\n",
    "   - cosine 유사도는 단어의 유사도를 비교하는 것에 약점을 보였기 때문에 성능이 크게 좋아지지 않음 (실패)\n",
    "   - 프롬프팅으로 고도화 시도 (성공))\n",
    "\n",
    "\n",
    "---\n",
    "**AI 챗봇**\n",
    "\n",
    "1. AI 가 질문에 대한 사용자의 답변을 인식하지 못하는 문제  \n",
    " - 사용자의 답변을 질문과 함께 feedback_prompt 형식으로 묶어서 다시 AI에게 전달하는 방식으로 문제 해결\n",
    "\n",
    "2. AI 가 동일한 주제에 대해서 반복적으로 비슷한 질문만을 하는 문제 --> (해결중..)\n",
    "\n",
    "---\n",
    "\n",
    "> 2024/11/26 (수연)\n",
    "\n",
    "2. AI 가 동일한 주제에 대해서 반복적으로 비슷한 질문만을 하는 문제 -> 해결\n",
    "\n",
    "   - 해결 방법 : \n",
    "   - 수동적으로 참고할 docs와 갯수를 할당 (코드에서는 한 교재당 2개의 질문)\n",
    "   - 유사한 질문을 rag에 넘겨주어 겹치지 않도록 프롬프팅 (직전 5개의 문장 기준, 이미 한 docs당 2개의 문장 질문 제한 고려)\n",
    "   - 프롬프팅 고도화\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pprint import pprint\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import re\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(txt:str, file_name:str):\n",
    "\n",
    "    with open(file_name, 'w', encoding='utf-8') as content_file:\n",
    "        content_file.write(txt)\n",
    "\n",
    "    print(f\"TEXT 파일 저장 완료: {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# .env 파일에서 환경변수 로드\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPEN_AI_KEY\")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 교재 저장하는 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "TEXT 파일 저장 완료: DL_dataset_1.txt\n",
      "6\n",
      "TEXT 파일 저장 완료: DL_dataset_2.txt\n",
      "5\n",
      "TEXT 파일 저장 완료: DL_dataset_3.txt\n",
      "3\n",
      "TEXT 파일 저장 완료: DL_dataset_4.txt\n",
      "5\n",
      "TEXT 파일 저장 완료: DL_dataset_5.txt\n",
      "4\n",
      "TEXT 파일 저장 완료: DL_dataset_6.txt\n",
      "3\n",
      "TEXT 파일 저장 완료: DL_dataset_7.txt\n",
      "4\n",
      "TEXT 파일 저장 완료: DL_dataset_8.txt\n",
      "3\n",
      "TEXT 파일 저장 완료: DL_dataset_9.txt\n",
      "3\n",
      "TEXT 파일 저장 완료: DL_dataset_10.txt\n",
      "3\n",
      "TEXT 파일 저장 완료: DL_dataset_11.txt\n",
      "4\n",
      "TEXT 파일 저장 완료: DL_dataset_12.txt\n",
      "2\n",
      "TEXT 파일 저장 완료: DL_dataset_13.txt\n",
      "4\n",
      "TEXT 파일 저장 완료: DL_dataset_14.txt\n",
      "2\n",
      "TEXT 파일 저장 완료: DL_dataset_15.txt\n",
      "3\n",
      "TEXT 파일 저장 완료: DL_dataset_16.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Selenium 옵션 설정 (헤드리스 모드로 실행)\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # 브라우저 창을 띄우지 않음\n",
    "chrome_options.add_argument(\"--disable-gpu\")  # GPU 비활성화 (일부 환경에서 필요)\n",
    "\n",
    "# WebDriver 경로 설정 (자동 설치)\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "\n",
    "url_list=[]\n",
    "txt_list=[]\n",
    "\n",
    "# 환경변수에 저장된 URL 로드\n",
    "for i in range(1, 17):  # URL_1 ~ URL_16\n",
    "    url = os.getenv(f\"DL_URL_{i}\")\n",
    "    if url:  # 환경변수가 존재하면 추가\n",
    "        url_list.append(url)\n",
    "\n",
    "j = 0\n",
    "# 웹페이지 요청\n",
    "for url in url_list:\n",
    "    j+= 1\n",
    "    txt = \"\"\n",
    "    driver.get(url)  # 페이지 로드\n",
    "\n",
    "    # 특정 요소가 로드될 때까지 기다림 (예: Notion 페이지에서 주요 콘텐츠가 담길 요소)\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \".notion-page-content\"))\n",
    "        )\n",
    "    except TimeoutException:\n",
    "        print(f\"페이지 로딩 실패: {url}\")\n",
    "        continue\n",
    "    \n",
    "    # 토글이 닫혀 있으면 토글을 열기\n",
    "    try:\n",
    "        # 모든 토글 버튼을 찾음 (Ctrl+Alt+T에 해당하는 토글을 찾아서 열기)\n",
    "        toggle_buttons = driver.find_elements(By.XPATH, \"//div[@role='button' and (@aria-expanded='false')]\")\n",
    "        print(len(toggle_buttons))\n",
    "        \n",
    "        # 각 토글을 클릭하여 열기\n",
    "        for button in toggle_buttons:\n",
    "            driver.execute_script(\"arguments[0].click();\", button)\n",
    "            time.sleep(1)  # 토글이 열리기 전에 잠깐 대기\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"토글을 여는 데 실패했습니다: {e}\")\n",
    "\n",
    "    # 페이지의 HTML 가져오기\n",
    "    html_code = driver.page_source\n",
    "\n",
    "    # BeautifulSoup으로 HTML 파싱\n",
    "    soup = BeautifulSoup(html_code, 'html.parser')\n",
    "\n",
    "    txt = soup.get_text()\n",
    "\n",
    "    # 1. \\xa0를 공백으로 변환\n",
    "    txt = txt.replace('\\xa0', ' ')\n",
    "\n",
    "    # 2. 정규식을 사용해 \\\\로 시작하는 LaTeX 명령어 제거\n",
    "    txt = re.sub(r'\\\\[a-zA-Z]+\\{.*?\\}', '', txt)  # \\command{...} 형식 제거\n",
    "    txt = re.sub(r'\\\\[a-zA-Z]+', '', txt)        # \\command 형식 제거\n",
    "    txt = re.sub(r'💡모든 토글을 열고 닫는 단축키\\nWindows : [^\\n]*\\nMac : [^\\n]*\\n', '', txt)\n",
    "\n",
    "    # 3. 불필요한 공백 제거 (코드 개행 유지를 위해 주석처리)\n",
    "    # txt = re.sub(r'\\s+', ' ', txt).strip()\n",
    "\n",
    "    # 텍스트만 가져오기\n",
    "    txt_list.append(txt)\n",
    "    save_file(''.join(txt), f\"DL_dataset_{j}.txt\")\n",
    "    \n",
    "\n",
    "\n",
    "driver.quit()  # 브라우저 종료\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_retriever_list = []\n",
    "dl_retriever_list = []\n",
    "llm_retriever_list = []\n",
    "python_retriever_list = []\n",
    "open_source_retrievver_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 저장한 교재 (txt) 불러오는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ML_dataset_1.txt', 'ML_dataset_2.txt', 'ML_dataset_3.txt', 'ML_dataset_4.txt', 'ML_dataset_5.txt', 'ML_dataset_6.txt', 'ML_dataset_7.txt', 'ML_dataset_8.txt', 'ML_dataset_9.txt', 'ML_dataset_10.txt', 'ML_dataset_11.txt', 'ML_dataset_12.txt', 'ML_dataset_13.txt', 'ML_dataset_14.txt', 'ML_dataset_15.txt', 'ML_dataset_16.txt']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['[스파르타코딩클럽] 1강. 강의 소개📘[SCC] 바닥부터 시작하는 머신러닝/📚[스파르타코딩클럽] 바닥부터 시작하는 머신러닝 - 1주차/📕[스파르타코딩클럽] 1강. 강의 소개Made with📕[스파르타코딩클럽] 1강. 강의 소개[수업 목표]강의 소개 및 머신러닝 기본 개념을 알아봅시다[목차]01. 강좌 소개🤔 딥러닝이 대세라던데, 왜 머신러닝부터 시작할까요?02. 머신러닝 소개강의 뒤쪽에서 나오는 어려운 문법들은 강좌 내에서 설명 후에 실습을 진행할 예정이니 걱정하지 않으셔도 됩니다!강의 뒤쪽에서 나오는 어려운 문법들은 강좌 내에서 설명 후에 실습을 진행할 예정이니 걱정하지 않으셔도 됩니다!\\ufeff\\u200b🤔 딥러닝이 대세라던데, 왜 머신러닝부터 시작할까요?딥러닝의 기초는 탄탄한 머신러닝 지식입니다. 기초를 튼튼히 쌓아야 딥러닝도 쉽게 이해할 수 있죠! 차근차근 기초부터 다지면, 딥러닝 정복도 문제없어요! 💪02. 머신러닝 소개✔️머신러닝에 대한 기본적인 개념 및 역사등을 소개합니다1)머신러닝 소개☑️ 머신러닝의 정의컴퓨터가 명시적으로 프로그래밍 되지 않아도 데이터를 통해 학습하고, 예측할 수 있도록 하는 기능대량의 데이터를 알고리즘에 입력하여 학습과정을 통해 모델을 생성하고 예측을 수행기존 프로그램은 명시적으로 작성된 규칙과 논리에 따라 동작한다는것에서 머신러닝과의 차이가 있습니다!기존 프로그램은 명시적으로 작성된 규칙과 논리에 따라 동작한다는것에서 머신러닝과의 차이가 있습니다!\\ufeff\\u200b전통적인 프로그래밍:규칙과 논리를 프로그래머가 직접 정의 🖥️명시적 명령과 조건문을 통해 문제 해결 📝머신러닝:데이터를 이용해 패턴과 규칙을 스스로 학습 📊예측 모델을 통해 새로운 데이터에 대한 결과 도출 🔮프로그램이 아닌 모델이 중심 ⚙️☑️ 머신러닝의 구성요소데이터셋 : 모델을 학습시키기 위한 데이터 모음특징(Feature) : 데이터셋에서 모델이 학습할 수 있는 개별 속성레이블(label) : 예측하고자 하는 목표 변수훈련 : 모델이 데이터를 통해 학습하는 과정테스트 : 학습된 모델의 성능을 평가하는 과정2)머신러닝의 역사 및 발전 이유 ☑️ 머신러닝의 역사1950s : 앨런 튜링의 “튜링테스트”와 퍼셉트론의 개발1980s -90s : 백프로파게이션 알고리즘의 등장2000s :  대규모 데이터와 고성능 컴퓨팅 자원의 발전2010s - : 딥러닝의 부상과 다양한 산업에 적용☑️ 최근 머신러닝의 발전 이유데이터의 폭발적 증가컴퓨팅 파워의 향상알고리즘의 발전오픈소스 커뮤니티와 생태계의 발전🥳이 강좌에서는 파이썬을 활용한 머신러닝의 기본 개념과 실습을 다룰 예정입니다!\\n머신러닝의 기본적인 원리부터 실습을 통해 직접 모델 구축/평가하는 방법까지 모두 함께 배워봅시다!Copyright ⓒ TeamSparta All rights reserved.',\n",
       " '[스파르타코딩클럽] 2강. 머신러닝 개요와 구성요소📘[SCC] 바닥부터 시작하는 머신러닝/📚[스파르타코딩클럽] 바닥부터 시작하는 머신러닝 - 1주차/📕[스파르타코딩클럽] 2강. 머신러닝 개요와 구성요소Made with📕[스파르타코딩클럽] 2강. 머신러닝 개요와 구성요소[수업 목표]머신러닝의 기본적인 구성요소, 학습과정을 알려 드립니다.다양한 머신러닝의 학습방법을 소개합니다.[목차]01. 머신러닝 구성 요소02. 머신러닝의 학습💡모든 토글을 열고 닫는 단축키\\nWindows : Ctrl + alt + t \\nMac : ⌘ + ⌥ + t 01. 머신러닝 구성 요소✔️머신러닝의 필수 구성요소를 학습합니다1) 머신러닝의 구성요소☑️ 데이터셋머신러닝은 데이터셋을 통해서 학습하며, 일반적으로 데이터셋은 입력/출력 데이터로 구성됩니다.입력 데이터 : 모델이 학습할 수 있는 정보출력 데이터(레이블) : 모델이 예측해야 하는 목표 값☑️ Feature(특징)데이터에서 모델이 학습할 수 있는 개별 속성주택가격예측을 예시로 들 경우 주택의 크기, 위치, 방의 개수 등이 Feature에 해당합니다주택가격예측을 예시로 들 경우 주택의 크기, 위치, 방의 개수 등이 Feature에 해당합니다\\ufeff\\u200b☑️ 레이블예측하고자 하는 목표 변수지도학습 모델에서는 레이블이 있는 데이터셋을 이용하여 모델을 학습 시킵니다☑️ 모델데이터의 특징으로 부터 정답(레이블)을 예측할 수 있는 지식을 학습할 수 있는 프로그램/함수입력데이터와 출력 데이터간의 관계를 학습하여 새로운 데이터에 대한 예측 수행☑️ 학습모델이 데이터를 통해서 패턴을 인식하고, 이를 기반으로 예측을 수행 할 수 있도록 함수 내의 가중치를 조정하는 과정02. 머신러닝의 학습✔️머신러닝의 학습과정과 다양한 학습 종류에 대해서 학습합니다1) 머신러닝의 학습 과정머신러닝 학습 과정ALT데이터 수집 : 모델을 학습시키기 위한 필요 데이터 수집데이터 전처리 : 결측값 처리, 이상치 제거, 정규화 등등Feature 선택 : 중요 feature(특징)을 선택하고 불필요한 피쳐를 제거하여 학습효율 높임모델 선택 : 문제에 적합한 머신러닝 알고리즘을 선택모델 훈련 : 트레이닝 데이터셋을 사용해서 모델을 학습시킴모델 평가 : 테스트 데이터셋을 사용하여 모델 성능을 평가모델 배포 : 학습된 모델을 실제 환경에 배포하여 예측 수행2) 학습 방법☑️ 지도 학습 (Supervised Learning)레이블이 있는 데이터셋을 이용하여 모델을 학습시키는 방법회귀(Regression) : 연속적인 값을 예측하는 문제ex : 주택 가격 예측, 주식 가격예측 : 주택 가격 예측, 주식 가격예측\\ufeff\\u200b분류(Classification) : 이산적인 값을 예측하는 문제ex : 이메일 스팸 필터링, 이미지 분류 : 이메일 스팸 필터링, 이미지 분류\\ufeff\\u200b☑️ 비지도 학습 (Unsupervised Learning)레이블이 없는 데이터셋을 이용하려 모델을 학습시키는 방법군집화(Clustering) : 데이터를 유사한 그룹으로 묶는 문제 ex : 고객 세분화, 이미지 세그멘테이션 : 고객 세분화, 이미지 세그멘테이션\\ufeff\\u200b차원축소 (Dimensionality Reduction) : 고차원 데이터를 저차원으로 변환ex : PCA, t-SNE : PCA, t-SNE\\ufeff\\u200b☑️ 앙상블 학습 (Ensemble Learning)여러개의 머신러닝 모델을 결합하여 더 나은 성능을 얻는 방법배깅(Bagging) : 여러 모델을 독립적으로 학습시키고, 예측을 평균내거나 다수결 투표로 최종 예측ex : 랜덤포레스트 : 랜덤포레스트\\ufeff\\u200b부스팅(Boosting) : 여러 모델을 순차적으로 학습시키고, 이전 모델의 오차를 보완하여 최종 예측을 수행ex : 그래디언트 부스팅, XGboost : 그래디언트 부스팅, XGboost\\ufeff\\u200b스태킹(Stacking) : 여러 모델을 학습시키고 예측결과를 새로운 데이터로 사용하여 메타 모델을 학습⚠️ 과적합이란?과적합(Overfitting):모델이 훈련 데이터에 지나치게 적응하여 새로운 데이터에 대한 일반화 성능이 떨어지는 현상입니다. 모델이 너무 복잡하여 훈련 데이터의 노이즈까지 학습해버리는 경우 발생합니다. 📉방지 방법:더 많은 데이터 수집 📊교차 검증(Cross-validation) 사용 🔄정규화(Regularization) 기법 적용 🔧간단한 모델 사용 🚀🚫 머신러닝에서는 \"절대로 좋다\"라는 개념이 없다!모델의 성능:모든 데이터셋에 대해 완벽한 성능을 보이는 모델은 없습니다. 각 모델은 특정 데이터와 상황에서만 최적의 성능을 발휘합니다. 🎯트레이드오프:모델의 복잡성과 일반화 성능 사이에는 항상 균형이 필요합니다. 너무 복잡한 모델은 과적합의 위험이 있고, 너무 단순한 모델은 충분히 학습하지 못할 수 있습니다. ⚖️Copyright ⓒ TeamSparta All rights reserved.',\n",
       " \"[스파르타코딩클럽] 3강. Anaconda 설치 및 라이브러리 소개📘[SCC] 바닥부터 시작하는 머신러닝/📚[스파르타코딩클럽] 바닥부터 시작하는 머신러닝 - 1주차/📕[스파르타코딩클럽] 3강. Anaconda 설치 및 라이브러리 소개Made with📕[스파르타코딩클럽] 3강. Anaconda 설치 및 라이브러리 소개[수업 목표]Anaconda 를 설치합니다머신러닝에서 사용하는 주요 라이브러리 기능을 배우고 실습합니다[목차]01. Anaconda 소개 및 설치02. 주요 라이브러리 소개conda --version\\n\\u200b☑️ Anaconda 주요 기능패키지 관리: conda 명령어를 사용하여 패키지를 설치, 업데이트, 제거할 수 있습니다.환경 관리: 가상 환경을 생성하고 관리할 수 있습니다.Jupyter Notebook: 웹 기반의 대화형 개발 환경을 제공합니다.Spyder: 통합 개발 환경(IDE)으로, 코드 편집기와 디버거를 포함합니다.가상환경 생성 및 관리 {5px}가상환경 생성 및 관리 \\ufeff\\u200bPythonCopy# 새로운 가상 환경 생성\\nconda create --name myenv\\n\\n# 가상 환경 활성화\\nconda activate myenv\\n\\n# 가상 환경 비활성화\\nconda deactivate\\n\\n# 가상 환경 제거\\nconda remove --name myenv --all\\n\\u200b02. 주요 라이브러리 소개✔️머신러닝 강의에서 사용하는 주요 라이브러리를 소개드리고 간단한 실습을 진행해 봅시다1) Numpy 라이브러리 소개 ☑️ Numpy 라이브러리란?수치 계산을 위한 Python 라이브러리Numpy는 다차원 배열 객체인 ndarray를 제공벡터화 연산을 통해 빠르고 효율적인 수치 계산을 수행 가능☑️ Numpy 설치Numpy 설치 {5px}Numpy 설치 \\ufeff\\u200bPythonCopy# conda를 사용하여 Numpy 설치\\nconda install numpy\\n\\n# pip를 사용하여 Numpy 설치\\npip install numpy\\n\\u200b라이브러리는 conda 또는pip를 사용하여 설치할 수 있습니다. { conda }라이브러리는 conda 또는pip를 사용하여 설치할 수 있습니다.\\ufeff\\u200b☑️ Numpy 주요 기능다차원 배열 객체(ndarray): 다차원 배열을 생성하고 조작할 수 있습니다.벡터화 연산: 배열 간의 연산을 빠르게 수행할 수 있습니다.수학 함수: 다양한 수학 함수를 제공합니다.☑️ Numpy 예제다차원 배열을 생성하고 기본 연산을 수행하는 예제를 실습해 봅시다Numpy 예제 {5px}Numpy 예제 \\ufeff\\u200bPythonCopyimport numpy as np\\n\\n# 1차원 배열 생성\\narr1 = np.array([1, 2, 3, 4, 5])\\n# 2차원 배열 생성\\narr2 = np.array([[1, 2, 3], [4, 5, 6]])\\n# 배열의 크기 확인\\nprint(arr1.shape)\\nprint(arr2.shape)\\n# 배열의 데이터 타입 확인\\nprint(arr1.dtype)\\nprint(arr2.dtype)\\n# 배열 간의 연산\\narr_sum = arr1 + arr1\\narr_product = arr1 * arr1\\n\\nprint(arr_sum)\\nprint(arr_product)\\n\\u200b2) Pandas 라이브러리 소개☑️ Pandas 라이브러리란?데이터 조작 및 분석을 위한 Python 라이브러리데이터 프레임(DataFrame)이라는 구조를 사용하여 데이터를 효율적으로 관리하고 조작 가능이 데이터 프레임이라는 구조 덕에 테이블 형식의 데이터를 다루는 데 매우 유용 합니다이 데이터 프레임이라는 구조 덕에 테이블 형식의 데이터를 다루는 데 매우 유용 합니다\\ufeff\\u200b☑️ Pandas 설치Pandas 설치 {5px}Pandas 설치 \\ufeff\\u200bPythonCopy# conda를 사용하여 Pandas 설치\\nconda install pandas\\n\\n# pip를 사용하여 Pandas 설치\\npip install pandas\\n\\u200b☑️ Pandas 주요 기능데이터 프레임(DataFrame): 테이블 형식의 데이터를 생성하고 조작할 수 있습니다.데이터 불러오기 및 저장: CSV, 엑셀, SQL 등 다양한 형식의 데이터를 불러오고 저장할 수 있습니다.데이터 조작: 필터링, 그룹화, 병합 등 다양한 데이터 조작 기능을 제공합니다.☑️ Pandas 예제데이터프레임을 생성하고 기본 조작을 수행하는 예제를 실습해 봅시다Pandas 예제 {5px}Pandas 예제 \\ufeff\\u200bPythonCopyimport pandas as pd\\n\\n# 데이터 프레임 생성\\ndata = {'Name': ['Alice', 'Bob', 'Charlie'],\\n'Age': [25, 30, 35],\\n'City': ['New York', 'Los Angeles', 'Chicago']}\\ndf = pd.DataFrame(data)\\n# 데이터 프레임의 첫 5행 출력\\nprint(df.head())\\n# 데이터 프레임의 크기 확인\\nprint(df.shape)\\n# 데이터 프레임의 컬럼명 확인\\nprint(df.columns)\\n# 데이터 프레임의 데이터 타입 확인\\nprint(df.dtypes)\\n# 특정 컬럼 선택\\nprint(df['Name'])\\n# 조건에 맞는 행 선택\\nprint(df[df['Age'] > 30])\\n\\u200b3) Scikit-learn 라이브러리 소개☑️ Scikit-learn 라이브러리란?머신러닝을 위한 Python 라이브러리다양한 머신러닝 알고리즘을 제공하며, 데이터 전처리, 모델 학습, 평가 및 예측을 위한 도구를 포함☑️ Scikit-learn 설치Scikit-learn 설치{5px}Scikit-learn 설치\\ufeff\\u200bPythonCopy# conda를 사용하여 Scikit-learn 설치\\nconda install scikit-learn\\n\\n# pip를 사용하여 Scikit-learn 설치\\npip install scikit-learn\\n\\u200b☑️ Scikit-learn  주요 기능데이터 전처리: 스케일링, 인코딩, 결측값 처리 등 다양한 데이터 전처리 기능을 제공합니다.모델 학습: 회귀, 분류, 군집화 등 다양한 머신러닝 알고리즘을 제공합니다.모델 평가: 교차 검증, 성능 평가 지표 등 모델 평가를 위한 도구를 제공합니다.모델 예측: 학습된 모델을 사용하여 새로운 데이터에 대한 예측을 수행할 수 있습니다.☑️ Scikit-learn  예제Scikit-learn 예제{5px}Scikit-learn 예제\\ufeff\\u200bPythonCopyfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.metrics import mean_squared_error\\n\\n# 데이터 생성\\nX = [[1], [2], [3], [4], [5]]\\ny = [1, 4, 9, 16, 25]\\n# 데이터 분할 (훈련 데이터와 테스트 데이터)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n# 회귀 모델 생성 및 학습\\nmodel = LinearRegression()\\nmodel.fit(X_train, y_train)\\n# 예측\\ny_pred = model.predict(X_test)\\n# 모델 평가\\nmse = mean_squared_error(y_test, y_pred)\\nprint(f'Mean Squared Error: {mse}')\\n\\u200bCopyright ⓒ TeamSparta All rights reserved.\",\n",
       " '[스파르타코딩클럽] 4강. Jupyter Notebook 사용해보기📘[SCC] 바닥부터 시작하는 머신러닝/📚[스파르타코딩클럽] 바닥부터 시작하는 머신러닝 - 1주차/📕[스파르타코딩클럽] 4강. Jupyter Notebook 사용해보기📕[스파르타코딩클럽] 4강. Jupyter Notebook 사용해보기[수업 목표]Jupyter Notebook 사용해보기![목차]Jupyter Notebook이란?💡모든 토글을 열고 닫는 단축키\\nWindows : Ctrl + alt + t \\nMac : ⌘ + ⌥ + t Jupyter Notebook이란?✔️Jupyter Notebook이 무엇인지 알아봅시다.1) Jupyter Notebook 소개☑️ Jupyter Notebook이란Jupyter Notebook은 데이터 과학자, 연구자, 교육자들이 널리 사용하는 오픈 소스 웹 애플리케이션입니다. 이를 통해 사용자는 실시간으로 코드를 작성하고 실행하며, 그 결과를 시각적으로 확인하고, 문서화할 수 있습니다.Anaconda를 설치하면 자동으로 설치됩니다!2) Jupyter Notebook 사용하기☑️ Jupyter Notebook 사용법환경 설정!가상환경 만들기conda create --name myenv가상환경 활성화conda activate myenv필요 패키지 설치conda install jupyter numpy pandas가상환경과 Jupyter 연결하기python -m ipykernel install --user --name=myenv --display-name \"Python (myenv)\"사용하기!ALTALTALT☑️ Jupyter 사용확인!세팅 확인{5px}세팅 확인\\ufeff\\u200bALTCopyright ⓒ TeamSparta All rights reserved.',\n",
       " \"[스파르타코딩클럽] 5강. 데이터셋 불러오기📘[SCC] 바닥부터 시작하는 머신러닝/📚[스파르타코딩클럽] 바닥부터 시작하는 머신러닝 - 2주차/📕[스파르타코딩클럽] 5강. 데이터셋 불러오기Made with📕[스파르타코딩클럽] 5강. 데이터셋 불러오기[수업 목표]pandas 라이브러리를 이용하여 데이터를 불러오는 법을 배웁니다.캐글(Kaggle)에 대해 알아보고, 캐글의 데이터셋을 다운받아 불러오는 실습을 해봅니다.[목차]01. 데이터 불러오기 및 탐색02. 캐글(Kaggle) 소개 및 데이터셋 다운 받기01. 데이터 불러오기 및 탐색✔️Pandas를 이용하여 데이터를 불러오고 기본 정보를 확인해 봅니다1) 데이터 불러오기 (CSV 및 엑셀파일)☑️ Pandas 라이브러리 소개데이터 조작 및 분석을 위한 Python 라이브러리pandas는 데이터 프레임(DataFrame) 구조를 사용해 데이터를 효율적으로 관리/조작 할 수있습니다☑️ CSV 파일 불러오기CSV : 콤마로 구분된 값들을 저장하는 텍스트 파일Pandas의 read_csv 함수를 사용하여 CSV 파일을 불러올 수 있습니다.CSV 불러오기 {5px}CSV 불러오기 \\ufeff\\u200bPythonCopyimport pandas as pd\\n\\n# CSV 파일 불러오기\\ndf = pd.read_csv('data.csv')\\n# 데이터 프레임의 첫 5행 출력\\nprint(df.head())\\n\\u200b☑️ 엑셀 파일 불러오기Pandas의 read_excel 함수를 사용하여 엑셀 파일을 불러올 수 있습니다.엑셀 불러오기 {5px}엑셀 불러오기 \\ufeff\\u200bPythonCopyimport pandas as pd\\n\\n# 엑셀 파일 불러오기\\ndf = pd.read_excel('data.xlsx', sheet_name='Sheet1')\\n# 데이터 프레임의 첫 5행 출력\\nprint(df.head())\\n\\u200b2) 데이터 구조 확인해 보기☑️ 데이터 프레임의 기본 정보 확인Pandas에서 제공하는 다양한 메서드들을 이용하여 데이터 프레임의 구조와 기본 정보 확인 가능데이터프레임 정보확인 {5px}데이터프레임 정보확인 \\ufeff\\u200bPythonCopy# 데이터 프레임의 크기 (행, 열) 확인\\nprint(df.shape)\\n# 데이터 프레임의 컬럼명 확인\\nprint(df.columns)\\n# 데이터 프레임의 데이터 타입 확인\\nprint(df.dtypes)\\n# 데이터 프레임의 요약 통계량 확인\\nprint(df.describe())\\n# 데이터 프레임의 정보 확인 (null 값, 데이터 타입 등)\\nprint(df.info())\\n\\u200b02. 캐글(Kaggle) 소개 및 데이터셋 다운 받기No accessYou don't have access to this synced blockRequest access1) 캐글(Kaggle) 소개☑️ 캐글이란?데이터 과학 및 머신러닝 경진대회 플랫폼데이터 사이언티스트 / 머신러닝 엔지니어들이 다양한 문제를 해결하고 데이터를 분석모델을 개발하는데 필요한 데이터셋과 도구를 제공☑️ 캐글의 주요 기능경진대회: 다양한 데이터 과학 및 머신러닝 문제를 해결하는 경진대회가 열립니다.데이터셋: 다양한 주제의 데이터셋을 무료로 다운로드할 수 있습니다.커뮤니티: 데이터 과학자와 머신러닝 엔지니어들이 지식을 공유하고 협업할 수 있는 커뮤니티입니다.커널: 웹 기반의 코드 실행 환경으로, Jupyter 노트북과 유사한 기능을 제공합니다.2) 데이터셋 다운받기☑️ 캐글 데이터셋 다운로드 방법캐글에서 데이터셋을 다운로드 하기 위해서는 먼저 캐글 계정이 필요합니다캐글에서 데이터셋을 다운로드 하기 위해서는 먼저 캐글 계정이 필요합니다\\ufeff\\u200b캐글 API 설치PythonCopypip install kaggle\\n\\u200b캐글 API 키 설정캐글 계정에서 API키 생성하고 로컬 환경에 저장API 키는 ~/.kaggle/kaggle.json 파일에 저장됩니다.데이터셋 다운로드PythonCopykaggle datasets download -d <dataset-identifier>\\n\\u200b☑️ 예시 : 타이타닉 데이터셋 다운로드타이타닉 생존자 예측 경진대회의 데이터셋 다운로드 예시타이타닉 데이터셋 다운로드 {5px}타이타닉 데이터셋 다운로드 \\ufeff\\u200bPythonCopy# 타이타닉 데이터셋 다운로드\\nkaggle competitions download -c titanic\\n\\n# 다운로드된 파일 압축 해제\\nunzip titanic.zip\\n\\u200b다운로드한 데이터 import {5px}다운로드한 데이터 import \\ufeff\\u200bPythonCopyimport pandas as pd\\n\\n# 타이타닉 데이터셋 불러오기\\ntrain_df = pd.read_csv('train.csv')\\ntest_df = pd.read_csv('test.csv')\\n# 데이터 프레임의 첫 5행 출력\\nprint(train_df.head())\\nprint(test_df.head())\\n\\u200bCopyright ⓒ TeamSparta All rights reserved.\",\n",
       " '[스파르타코딩클럽] 6강. 데이터 전처리📘[SCC] 바닥부터 시작하는 머신러닝/📚[스파르타코딩클럽] 바닥부터 시작하는 머신러닝 - 2주차/📕[스파르타코딩클럽] 6강. 데이터 전처리Made with📕[스파르타코딩클럽] 6강. 데이터 전처리[수업 목표]데이터 전처리 개념을 알아봅시다데이터 전처리 방법을 알아 봅시다[목차]01. 데이터 전처리 개념 및 API 소개Xnorm=Xmax−XminX−XminXnorm=Xmax−XminX−XminXnorm=Xmax−XminX−Xmin\\ufeff\\u200bXnorm=X−XminXmax−XminXnorm=X−XminXmax−XminXnorm=X−XminXmax−XminX_{norm} = }{X_{max} - X_{min}}Xnorm=X−XminXmax−XminXnorm\\u200b=Xmax\\u200b−Xmin\\u200bX−Xmin\\u200b\\u200b\\ufeff\\u200b☑️ 데이터 표준화 (Standardization)표준화는 데이터를 평균 0, 분산 1로 변환하는 과정입니다.Z-점수 표준화: Xstd=σX−μXstd=σX−μXstd=σX−μ\\ufeff\\n여기서 μμ\\ufeff는 평균, σσ\\ufeff는 표준편차입니다.Xstd=X−μσXstd=X−μσXstd=X−μσX_{std} = {}Xstd=X−μσXstd\\u200b=σX−μ\\u200b\\ufeff\\u200b☑️  특성 공학 (Feature Engineering)특성 공학은 데이터로부터 새로운 유용한 특성을 생성하는 과정입니다.특성 생성: 기존 데이터를 기반으로 새로운 특성을 생성합니다 (예: 날짜 데이터를 사용하여 요일 특성 생성).특성 선택: 모델 성능에 중요한 특성을 선택하고, 중요하지 않은 특성을 제거합니다.☑️  데이터 인코딩 (Data Encoding)비정형 데이터를 모델이 이해할 수 있는 형태로 변환합니다.레이블 인코딩 (Label Encoding): 범주형 데이터를 숫자로 변환합니다.원-핫 인코딩 (One-Hot Encoding): 범주형 데이터를 이진 벡터로 변환합니다.☑️  데이터 분할 (Data Splitting)데이터를 학습용(train), 검증용(validation), 테스트용(test)으로 분할합니다. 이를 통해 모델의 일반화 성능을 평가할 수 있습니다.학습 데이터 (Training Data): 모델 학습에 사용되는 데이터.검증 데이터 (Validation Data): 모델 튜닝 및 성능 검증에 사용되는 데이터.테스트 데이터 (Test Data): 최종 모델 평가에 사용되는 데이터.Copyright ⓒ TeamSparta All rights reserved.',\n",
       " \"[스파르타코딩클럽] 7강. 데이터 전처리 실습 📘[SCC] 바닥부터 시작하는 머신러닝/📚[스파르타코딩클럽] 바닥부터 시작하는 머신러닝 - 2주차/📕[스파르타코딩클럽] 7강. 데이터 전처리 실습 Made with📕[스파르타코딩클럽] 7강. 데이터 전처리 실습 [수업 목표]데이터 전처리 개념 및 Pandas에서 사용하는 API를 알아봅시다데이터 전처리 방법을 알아 봅시다[목차]01. 데이터 전처리Data'A': [1, 2, np.nan, 4, 5, 100, 1, 2, 2, 4, '1', '2', '3', '4', '5', 10, 20, 30, 40, 50],\\n'B': [5, np.nan, np.nan, 8, 10, 60, 10, 20, 20, 40, '10', '20', '30', '40', '50', 5, 4, 3, 2, 1],\\n'C': [1, 2, 3, 4, 5, 5, 100, 200, 200, 400, 100, 200, 300, 400, 500, 1, 2, 3, 4, 5],\\n'D': [np.nan, np.nan, 3, 3, 3, 5, 5, 5, 5, 5, np.nan, np.nan, np.nan, np.nan, np.nan, 2, 3, 4, 5, 6],\\n'category_column': [np.nan]*10 + ['A', 'B', 'A', 'C', 'B'] + [np.nan]*5,\\n'value_column': [np.nan]*10 + [1, 2, 3, 4, 5] + [np.nan]*5,\\n'target': [np.nan]*15 + [1, 0, 1, 0, 1]\\n}\\n\\u200b1)결측값 처리☑️ 결측값 처리 방법제거: 결측값이 포함된 행 또는 열을 제거합니다.대체: 결측값을 특정 값으로 대체합니다.예측: 머신러닝 모델을 사용하여 결측값을 예측합니다.결측값 제거 {5px}결측값 제거 \\ufeff\\u200bPythonCopy# 결측값이 포함된 행 제거\\ndf_dropped_rows = df.dropna()\\n# 결측값이 포함된 열 제거\\ndf_dropped_cols = df.dropna(axis=1)\\n\\u200bdropna()를 사용하여 결측값 제거()를 사용하여 결측값 제거\\ufeff\\u200b결측값 대체 {5px}결측값 대체 \\ufeff\\u200bPythonCopy# 결측값을 0으로 대체\\ndf_filled = df.fillna(0)\\n# 결측값을 각 열의 평균값으로 대체\\ndf_filled_mean = df.fillna(df.mean())\\n# 결측값을 각 열의 중간값으로 대체\\ndf_filled_median = df.fillna(df.median())\\n# 결측값을 각 열의 최빈값으로 대체\\ndf_filled_mode = df.fillna(df.mode().iloc[0])\\n\\u200bfillna()를 사용하여 결측값 대체()를 사용하여 결측값 대체\\ufeff\\u200b결측값 예측 {5px}결측값 예측 \\ufeff\\u200bPythonCopyfrom sklearn.linear_model import LinearRegression\\n\\n# 결측값이 있는 열과 없는 열 분리\\ndf_with_na = df[df['column_with_na'].isnull()]\\ndf_without_na = df[df['column_with_na'].notnull()]\\n# 회귀 모델 학습\\nmodel = LinearRegression()\\nmodel.fit(df_without_na[['feature1', 'feature2']], df_without_na['column_with_na'])\\n# 결측값 예측\\npredicted_values = model.predict(df_with_na[['feature1', 'feature2']])\\n# 예측된 값으로 결측값 대체\\ndf.loc[df['column_with_na'].isnull(), 'column_with_na'] = predicted_values\\n\\u200b2) 이상값 처리☑️ 이상치란?데이터셋에서 비정상적으로 큰 값이나 작은 값이상치는 분석 결과에 큰 영향을 미칠 수 있으므로, 이를 적절히 처리하는 것이 중요이상치 확인 방법 {5px}이상치 확인 방법 \\ufeff\\u200bPythonCopy# 특정 열의 이상치 확인 (IQR 방법)\\nQ1 = df['column_name'].quantile(0.25)\\nQ3 = df['column_name'].quantile(0.75)\\nIQR = Q3 - Q1\\n\\n# 이상치 범위 설정\\nlower_bound = Q1 - 1.5 * IQR\\nupper_bound = Q3 + 1.5 * IQR\\n\\n# 이상치 확인\\noutliers = df[(df['column_name'] < lower_bound) | (df['column_name'] > upper_bound)]\\nprint(outliers)\\n\\u200b☑️ 이상치 처리 방법제거: 이상치를 데이터셋에서 제거합니다.대체: 이상치를 특정 값으로 대체합니다.변환: 이상치를 변환하여 데이터의 분포를 조정합니다.이상치 처리 방법 {5px}이상치 처리 방법 \\ufeff\\u200bPythonCopy# 이상치 제거\\ndf_no_outliers = df[(df['column_name'] >= lower_bound) & (df['column_name'] <= upper_bound)]\\n# 이상치를 평균값으로 대체\\nmean_value = df['column_name'].mean()\\ndf['column_name'] = df['column_name'].apply(lambda x: mean_value if x < lower_bound or x > upper_bound else x)\\n\\u200b3) 중복값 제거☑️ 중복 데이터 제거중복 데이터 제거 {5px}중복 데이터 제거 \\ufeff\\u200bPythonCopy# 중복된 행 확인\\nprint(df.duplicated().sum())\\n# 중복된 행 제거\\ndf_no_duplicates = df.drop_duplicates()\\n\\u200b4) 데이터 타입 변환☑️ 데이터 타입 변환의 필요성잘못된 데이터 타입은 분석 결과에 영향을 미칠 수 있으며, 모델 학습에 오류를 발생시킬 수있어 적절한 데이터 타입 변환이 필요합니다데이터 타입 변환 방법 {5px}데이터 타입 변환 방법 \\ufeff\\u200bPythonCopy# 특정 열의 데이터 타입을 정수형으로 변환\\ndf['column_name'] = df['column_name'].astype(int)\\n# 특정 열의 데이터 타입을 문자열로 변환\\ndf['column_name'] = df['column_name'].astype(str)\\n# 특정 열의 데이터 타입을 부동 소수점으로 변환\\ndf['column_name'] = df['column_name'].astype(float)\\n\\u200bPandas의 astype() 메서드를 사용하여 데이터 타입을 변환의 astype() 메서드를 사용하여 데이터 타입을 변환\\ufeff\\u200b5) 인코딩☑️ 인코딩이란?범주형 데이터를 수치형 데이터로 변환하는 과정머신러닝 모델은 수치형 데이터를 입력으로 받기때문에, 범주형 데이터를 수치형으로 변환하는 것이 필요인코딩 방법 {5px}인코딩 방법 \\ufeff\\u200bPythonCopy# 범주형 데이터를 더미 변수로 변환\\ndf_encoded = pd.get_dummies(df, columns=['category_column'])\\n# 결과 출력\\nprint(df_encoded.head())\\n\\u200bPandas의 get_dummies() 메서드를 사용하여 범주형 데이터를 더미 변수로 변환의 get_dummies() 메서드를 사용하여 범주형 데이터를 더미 변수로 변환\\ufeff\\u200b6) 샘플링☑️ 샘플링이란?데이터셋의 크기를 줄이거나 늘리는 과정데이터셋의 대표성을 유지하면서 데이터의 크기를 조절하는 데 사용샘플링 방법 {5px}샘플링 방법 \\ufeff\\u200bPythonCopy# 데이터셋에서 50% 샘플 추출\\ndf_sampled = df.sample(frac=0.5)\\n# 데이터셋에서 100개의 샘플 추출\\ndf_sampled_n = df.sample(n=100)\\n\\u200bPandas의 sample() 메서드를 사용하여 데이터셋에서 샘플을 추출의 sample() 메서드를 사용하여 데이터셋에서 샘플을 추출\\ufeff\\u200b7) 특징 선택 및 추출☑️ 특징 선택 및 추출이란?특징 선택(Feature Selection) 및 추출(Feature Extraction)은 모델 성능을 높이기 위해 중요한 특징을 선택하거나 새로운 특징을 추출하는 과정특징 선택 방법 {5px}특징 선택 방법 \\ufeff\\u200bPythonCopyfrom sklearn.feature_selection import SelectKBest, f_classif\\n\\n# 특징 선택 (상위 5개의 특징 선택)\\nselector = SelectKBest(score_func=f_classif, k=5)\\nX_new = selector.fit_transform(X, y)\\n# 선택된 특징의 인덱스\\nselected_features = selector.get_support(indices=True)\\nprint(selected_features)\\n\\u200bPandas와 Scikit-learn을 사용하여 특징 선택을 수행와 Scikit-learn을 사용하여 특징 선택을 수행\\ufeff\\u200b특징 추출 방법 {5px}특징 추출 방법 \\ufeff\\u200bPythonCopy# 두 열의 곱을 새로운 특징으로 추가\\ndf['new_feature'] = df['feature1'] * df['feature2']\\n# 두 열의 합을 새로운 특징으로 추가\\ndf['new_feature_sum'] = df['feature1'] + df['feature2']\\n\\u200bCopyright ⓒ TeamSparta All rights reserved.\",\n",
       " \"[스파르타코딩클럽] 8강. 지도학습 : 회귀모델 📘[SCC] 바닥부터 시작하는 머신러닝/📚[스파르타코딩클럽] 바닥부터 시작하는 머신러닝 - 3주차/📕[스파르타코딩클럽] 8강. 지도학습 : 회귀모델 Made with📕[스파르타코딩클럽] 8강. 지도학습 : 회귀모델 [수업 목표]다양한 회귀 모델에 대해서 배워봅시다회귀(Regression)모델은 연속적인 값을 예측하는 문제입니다.회귀(Regression)모델은 연속적인 값을 예측하는 문제입니다.\\ufeff\\n오늘은 선형회귀/다항회귀/리지회귀/라쏘회귀 를 다뤄볼 예정입니다오늘은 선형회귀/다항회귀/리지회귀/라쏘회귀 를 다뤄볼 예정입니다\\ufeff\\u200b[목차]01. 회귀모델y=β0\\u200b+β1\\u200bx1\\u200b+β2\\u200bx2\\u200b+⋯+βn\\u200bxn\\u200b+ϵy=β0\\u200b+β1\\u200bx1\\u200b+β2\\u200bx2\\u200b+⋯+βn\\u200bxn\\u200b+ϵy=β0\\u200b+β1\\u200bx1\\u200b+β2\\u200bx2\\u200b+⋯+βn\\u200bxn\\u200b+ϵ\\ufeff\\n여기서 y는 종속변수, x1,x2,…,xn 은 독립변수, b0 는 절편, b1,b2,…,bn은 회귀계수, e는 오차입니다여기서 y는 종속변수, x1,x2,…,xn 은 독립변수, b0 는 절편, b1,b2,…,bn은 회귀계수, e는 오차입니다\\ufeff\\n⋄ 단순 선형 회귀일경우 ⋄ 단순 선형 회귀일경우 \\ufeff\\ny=β0\\u200b+β1\\u200bx+ϵy=β0\\u200b+β1\\u200bx+ϵ\\ufeff\\n☑️ Scikit-learn 을 사용한 선형 회귀 모델 구현 및 평가선형 회귀 모델 구현 및 평가 {5px}선형 회귀 모델 구현 및 평가 \\ufeff\\u200bPythonCopyimport numpy as np\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.metrics import mean_squared_error, r2_score\\n\\n# 데이터 생성\\nX = np.array([[1, 1], [2, 2], [3, 3], [4, 4], [5, 5],[6,6]])\\ny = np.array([1, 2, 3, 4, 5, 6])\\n# 데이터 분할 (훈련 데이터와 테스트 데이터)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n# 선형 회귀 모델 생성 및 학습\\nmodel = LinearRegression()\\nmodel.fit(X_train, y_train)\\n# 예측\\ny_pred = model.predict(X_test)\\n# 모델 평가\\nmse = mean_squared_error(y_test, y_pred)\\nr2 = r2_score(y_test, y_pred)\\nprint(f'Mean Squared Error: {mse}')\\nprint(f'R^2 Score: {r2}')\\n\\u200b2) 다항 회귀☑️ 다항 회귀다항 회귀(Polynomial Regression)는 종속 변수와 독립 변수 간의 비선형 관계를 모델링하는 방법독립변수의 다항식을 사용하여 관계를 모델링 합니다.다항 회귀의 기본 수식은 다음과 같습니다다항 회귀의 기본 수식은 다음과 같습니다\\ufeff\\ny=β0\\u200b+β1\\u200bx+β2\\u200bx2+⋯+βn\\u200bxn+ϵy=β0\\u200b+β1\\u200bx+β2\\u200bx^2+⋯+βn\\u200bx^n+ϵy=β0\\u200b+β1\\u200bx+β2\\u200bx2+⋯+βn\\u200bxn+ϵ\\ufeff\\n여기서 y는 종속변수, x1,x2,…,xn 은 독립변수, b0 는 절편, b1,b2,…,bn은 회귀계수, e는 오차입니다여기서 y는 종속변수, x1,x2,…,xn 은 독립변수, b0 는 절편, b1,b2,…,bn은 회귀계수, e는 오차입니다\\ufeff\\u200b☑️ 다항 회귀 차수 선택다항회귀 차수(degree) : 독립 변수의 최대 차수차수가 높을수록 모델이 더 복잡해지며 과적합(overfitting)의 위험 존재 → 적절한 차수 선택 필요과적합이란 학습데이터에 모델이 과도하게 적합(fitting)되는 현상입니다과적합이란 학습데이터에 모델이 과도하게 적합(fitting)되는 현상입니다\\ufeff\\u200b☑️ Scikit-learn을 사용한 다항 회귀 모델 구현 및 평가다항 회귀 모델 구현 및 평가 {5px}다항 회귀 모델 구현 및 평가 \\ufeff\\u200bPythonCopyimport numpy as np\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import PolynomialFeatures\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.metrics import mean_squared_error, r2_score\\n\\n# 데이터 생성\\nX = np.array([[1], [2], [3], [4], [5], [6]])\\ny = np.array([1, 4, 9, 16, 25, 36])\\n# 다항 특징 생성 (차수 2)\\npoly = PolynomialFeatures(degree=2)\\nX_poly = poly.fit_transform(X)\\n# 데이터 분할 (훈련 데이터와 테스트 데이터)\\nX_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=42)\\n# 다항 회귀 모델 생성 및 학습\\nmodel = LinearRegression()\\nmodel.fit(X_train, y_train)\\n# 예측\\ny_pred = model.predict(X_test)\\n# 모델 평가\\nmse = mean_squared_error(y_test, y_pred)\\nr2 = r2_score(y_test, y_pred)\\nprint(f'Mean Squared Error: {mse}')\\nprint(f'R^2 Score: {r2}')\\n\\u200b3) 리지 회귀☑️ 리지 회귀리지 회귀(Ridge Regression)는 선형 회귀의 일종회귀 계수의 크기를 제어하여 과적합을 방지하는 정규화 기법L2 정규화(regularization)를 사용하여 회귀 계수의 제곱합을 최소화 합니다리지 회귀의 기본 수식은 다음과 같습니다리지 회귀의 기본 수식은 다음과 같습니다\\ufeff\\nJ(β)=∑i=1n(yi−y^i)2+λ∑j=1pβj2J() = _{i=1}^{n} (y_i - _i)^2 +  _{j=1}^{p} _j^2J(β)=∑i=1n\\u200b(yi\\u200b−y^\\u200bi\\u200b)2+λ∑j=1p\\u200bβj2\\u200b\\ufeff\\n여기서λ는 정규화 강도를 조절하는 하이퍼파라미터 입니다. 여기서λ는 정규화 강도를 조절하는 하이퍼파라미터 입니다.\\ufeff\\u200b☑️ L2 정규화 L2 정규화는 모든 가중치를 작게 만들어 모델의 복잡도를 줄입니다.손실 함수에 제곱항을 추가하여 매끄러운 최적화가 가능합니다.정규화는 모델의 복잡도를 제어하여 과적합을 방지하는 데 필요합니다.☑️ Scikit-learn을 사용한 리지 회귀 모델 구현 및 평가리지 회귀 모델 구현 및 평가 {5px}리지 회귀 모델 구현 및 평가 \\ufeff\\u200bPythonCopyimport numpy as np\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import Ridge\\nfrom sklearn.metrics import mean_squared_error, r2_score\\n\\n# 데이터 생성\\nX = np.array([[1, 1], [2, 2], [3, 3], [4, 4], [5, 5], [6, 6]])\\ny = np.array([1, 2, 3, 4, 5, 6])\\n# 데이터 분할 (훈련 데이터와 테스트 데이터)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n# 리지 회귀 모델 생성 및 학습\\nmodel = Ridge(alpha=1.0)\\nmodel.fit(X_train, y_train)\\n# 예측\\ny_pred = model.predict(X_test)\\n# 모델 평가\\nmse = mean_squared_error(y_test, y_pred)\\nr2 = r2_score(y_test, y_pred)\\nprint(f'Mean Squared Error: {mse}')\\nprint(f'R^2 Score: {r2}')\\n\\u200b4) 라쏘 회귀☑️ 라쏘 회귀라쏘 회귀(Lasso Regression)는 선형 회귀의 일종회귀 계수의 크기를 제어하여 과적합을 방지하는 정규화 기법L1 정규화(regularization)를 사용하여 회귀 계수의 절대값 합을 최소화 합니다라쏘 회귀의 기본 수식은 다음과 같습니다라쏘 회귀의 기본 수식은 다음과 같습니다\\ufeff\\nJ(β)=∑i=1n(yi−y^i)2+λ∑j=1p∣βj∣J() = _{i=1}^{n} (y_i - _i)^2 +  _{j=1}^{p} |_j|J(β)=∑i=1n\\u200b(yi\\u200b−y^\\u200bi\\u200b)2+λ∑j=1p\\u200b∣βj\\u200b∣\\ufeff\\n여기서λ는 정규화 강도를 조절하는 하이퍼파라미터 입니다. 여기서λ는 정규화 강도를 조절하는 하이퍼파라미터 입니다.\\ufeff\\u200b☑️ L1 정규화와 특징 선택L1 정규화는 일부 회귀 계수를 0으로 만들어 특징 선택(feature selection)을 수행모델의 해석 가능성을 높이고, 불필요한 특징을 제거하는 데 유용합니다☑️ Scikit-learn을 사용한 라쏘 회귀 모델 구현 및 평가라쏘 회귀 모델 구현 및 평가 {5px}라쏘 회귀 모델 구현 및 평가 \\ufeff\\u200bPythonCopyimport numpy as np\\nimport pandas as pd\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.linear_model import Lasso\\nfrom sklearn.metrics import mean_squared_error, r2_score\\n\\n# 데이터 생성\\nX = np.array([[1, 1], [2, 2], [3, 3], [4, 4], [5, 5], [6,6]])\\ny = np.array([1, 2, 3, 4, 5, 6])\\n# 데이터 분할 (훈련 데이터와 테스트 데이터)\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n# 라쏘 회귀 모델 생성 및 학습\\nmodel = Lasso(alpha=1.0)\\nmodel.fit(X_train, y_train)\\n# 예측\\ny_pred = model.predict(X_test)\\n# 모델 평가\\nmse = mean_squared_error(y_test, y_pred)\\nr2 = r2_score(y_test, y_pred)\\nprint(f'Mean Squared Error: {mse}')\\nprint(f'R^2 Score: {r2}')\\n\\u200bCopyright ⓒ TeamSparta All rights reserved.\",\n",
       " '[스파르타코딩클럽] 9강. 지도학습 : 분류모델 - 로지스틱 회귀📘[SCC] 바닥부터 시작하는 머신러닝/📚[스파르타코딩클럽] 바닥부터 시작하는 머신러닝 - 3주차/📕[스파르타코딩클럽] 9강. 지도학습 : 분류모델 - 로지스틱 회귀Made with📕[스파르타코딩클럽] 9강. 지도학습 : 분류모델 - 로지스틱 회귀[수업 목표]지도학습 : 분류모델의 로지스틱 회귀에 대해 알아보고 실습을 통해 배워봅시다[목차]01. 로지스틱 회귀 개념02. 로지스틱 회귀분석 실습import pandas as pd\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# 데이터 로드\\ndata = load_breast_cancer()\\nX = data.data\\ny = data.target\\n\\n# 데이터 분할\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n# 데이터 스케일링\\nscaler = StandardScaler()\\nX_train = scaler.fit_transform(X_train)\\nX_test = scaler.transform(X_test)\\n\\u200bsklearn.datasets.load_breast_cancer: 유방암 데이터셋 로드return_X_y=False: 데이터와 타겟을 함께 반환할지 여부. 기본값은 False입니다._X_y=False: 데이터와 타겟을 함께 반환할지 여부. 기본값은 False입니다.\\ufeff\\u200bsklearn.model_selection.train_test_split: 데이터를 훈련 세트/ 테스트 세트로 분할test_size=0.2: 테스트 세트의 비율을 0.2로 설정합니다._size=0.2: 테스트 세트의 비율을 0.2로 설정합니다.\\ufeff\\u200brandom_state=42: 랜덤 시드 값으로, 데이터 분할의 재현성을 위해 사용됩니다._state=42: 랜덤 시드 값으로, 데이터 분할의 재현성을 위해 사용됩니다.\\ufeff\\u200bsklearn.preprocessing.StandardScaler: 데이터의 평균을 0, 분산을 1로 스케일링fit_transform(X_train): 훈련 세트를 스케일링하고 변환합니다._transform(X_train): 훈련 세트를 스케일링하고 변환합니다.\\ufeff\\u200btransform(X_test): 테스트 세트를 변환합니다.(X_test): 테스트 세트를 변환합니다.\\ufeff\\u200b☑️ 모델 학습모델 학습 {5px}모델 학습 \\ufeff\\u200bPythonCopyfrom sklearn.linear_model import LogisticRegression\\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\\n\\n# 모델 생성 및 학습\\nmodel = LogisticRegression()\\nmodel.fit(X_train, y_train)\\n# 예측\\ny_pred = model.predict(X_test)\\n# 평가\\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\\nprint(f\"Classification Report:\")\\nprint(f\"Confusion Matrix:\")\\n\\u200bsklearn.linear_model.LogisticRegression: 로지스틱 회귀 모델 생성fit(X_train, y_train): 모델을 훈련 세트에 맞추어 학습시킵니다.(X_train, y_train): 모델을 훈련 세트에 맞추어 학습시킵니다.\\ufeff\\u200bpredict(X_test): 테스트 세트에 대해 예측을 수행합니다.(X_test): 테스트 세트에 대해 예측을 수행합니다.\\ufeff\\u200bsklearn.metrics.accuracy_score: 정확도 계산accuracy_score(y_test, y_pred): 실제 값과 예측 값을 비교하여 정확도를 반환합니다._score(y_test, y_pred): 실제 값과 예측 값을 비교하여 정확도를 반환합니다.\\ufeff\\u200bsklearn.metrics.classification_report: 분류 보고서 생성classification_report(y_test, y_pred): 정확도, 정밀도, 재현율 등의 메트릭을 포함한 보고서를 출력합니다._report(y_test, y_pred): 정확도, 정밀도, 재현율 등의 메트릭을 포함한 보고서를 출력합니다.\\ufeff\\u200bsklearn.metrics.confusion_matrix: 혼동 행렬 생성confusion_matrix(y_test, y_pred): 실제 값과 예측 값의 혼동 행렬을 반환합니다._matrix(y_test, y_pred): 실제 값과 예측 값의 혼동 행렬을 반환합니다.\\ufeff\\u200b2) 타이타닉 데이터☑️ 데이터 로드 및 전처리타이타닉 데이터 로드 및 전처리 {5px}타이타닉 데이터 로드 및 전처리 \\ufeff\\u200bPythonCopyimport seaborn as sns\\n\\n# 데이터 로드\\ntitanic = sns.load_dataset(\\'titanic\\')\\n# 필요한 열 선택 및 결측값 처리\\ntitanic = titanic[[\\'survived\\', \\'pclass\\', \\'sex\\', \\'age\\', \\'sibsp\\', \\'parch\\', \\'fare\\', \\'embarked\\']].dropna()\\n# 성별과 탑승한 곳 인코딩\\ntitanic[\\'sex\\'] = titanic[\\'sex\\'].map({\\'male\\': 0, \\'female\\': 1})\\ntitanic[\\'embarked\\'] = titanic[\\'embarked\\'].map({\\'C\\': 0, \\'Q\\': 1, \\'S\\': 2})\\n# 특성과 타겟 분리\\nX = titanic.drop(\\'survived\\', axis=1)\\ny = titanic[\\'survived\\']\\n# 데이터 분할\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n# 데이터 스케일링\\nscaler = StandardScaler()\\nX_train = scaler.fit_transform(X_train)\\nX_test = scaler.transform(X_test)\\n\\u200bseaborn.load_dataset: seaborn의 내장 데이터셋 로드’titanic’: 타이타닉 데이터셋을 로드합니다.’titanic’: 타이타닉 데이터셋을 로드합니다.\\ufeff\\u200b pandas.DataFrame.dropna: 결측값이 있는 행 제거pandas.DataFrame.map: 데이터 값을 다른 값으로 매핑’male’: 0, ’female’: 1: 성별을 숫자로 매핑합니다.: 성별을 숫자로 매핑합니다.}’male’: 0, ’female’: 1: 성별을 숫자로 매핑합니다.\\ufeff\\u200b’C’: 0, ’Q’: 1, ’S’: 2: 탑승한 곳을 숫자로 매핑합니다.: 탑승한 곳을 숫자로 매핑합니다.}’C’: 0, ’Q’: 1, ’S’: 2: 탑승한 곳을 숫자로 매핑합니다.\\ufeff\\u200b☑️ 모델 학습모델 학습 {5px}모델 학습 \\ufeff\\u200bPythonCopy# 모델 생성 및 학습\\nmodel = LogisticRegression()\\nmodel.fit(X_train, y_train)\\n# 예측\\ny_pred = model.predict(X_test)\\n# 평가\\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\\nprint(f\"Classification Report:\")\\nprint(f\"Confusion Matrix:\")\\n\\u200bCopyright ⓒ TeamSparta All rights reserved.',\n",
       " '[스파르타코딩클럽] 10강. 지도학습 : 분류모델 - SVM📘[SCC] 바닥부터 시작하는 머신러닝/📚[스파르타코딩클럽] 바닥부터 시작하는 머신러닝 - 3주차/📕[스파르타코딩클럽] 10강. 지도학습 : 분류모델 - SVMMade with📕[스파르타코딩클럽] 10강. 지도학습 : 분류모델 - SVM[수업 목표]SVM(Support Vector Machine)에 대한 개념을 배우고, 데이터를 이용해 실습해 봅니다[목차]01. SVM 개념02. SVM 실습서포트 벡터 : 결정 초평면에 가장 가까이 위치한 데이터 포인트 - 결정 초평면을 정의합니다서포트 벡터 : 결정 초평면에 가장 가까이 위치한 데이터 포인트 - 결정 초평면을 정의합니다\\ufeff\\n커널 함수 : 데이터를 더 높은 차원으로 매핑하여 선형적으로 분리 할 수 없는 데이터를 분리하게 합니다. 커널 함수 : 데이터를 더 높은 차원으로 매핑하여 선형적으로 분리 할 수 없는 데이터를 분리하게 합니다. \\ufeff\\u200b☑️ SVM의 목적SVM의 목표는 마진을 최대화하면서 결정 초평면을 찾아 데이터 포인트를 정확하게 분류하는 것입니다. 이는 일반화 성능을 높이는 데 도움을 줍니다.w⋅x−b=0   - b = 0 w⋅x−b=0여기서 w는 가중치 벡터, x는 입력 벡터, b는 절편입니다.\\\\)는 가중치 벡터, \\\\(\\\\)는 입력 벡터, \\\\(b\\\\)는 절편입니다.}여기서 w는 가중치 벡터, x는 입력 벡터, b는 절편입니다.\\ufeff\\u200b02. SVM 실습✔️Scikit-learn의 유방암데이터와 Seaborn의 타이타닉 데이터로 SVM 실습을 진행합니다1) 유방암 데이터☑️ 데이터 로드 및 전처리유방암 데이터 로드 및 전처리 {5px}유방암 데이터 로드 및 전처리 \\ufeff\\u200bPythonCopyimport numpy as np\\nimport pandas as pd\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# 데이터 로드\\ndata = load_breast_cancer()\\nX = data.data\\ny = data.target\\n\\n# 데이터 분할\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n# 데이터 스케일링\\nscaler = StandardScaler()\\nX_train = scaler.fit_transform(X_train)\\nX_test = scaler.transform(X_test)\\n\\u200bsklearn.datasets.load_breast_cancer: 유방암 데이터셋 로드return_X_y=False: 데이터와 타겟을 함께 반환할지 여부. 기본값은 False입니다._X_y=False: 데이터와 타겟을 함께 반환할지 여부. 기본값은 False입니다.\\ufeff\\u200bsklearn.model_selection.train_test_split: 데이터를 훈련 세트/ 테스트 세트로 분할test_size=0.2: 테스트 세트의 비율을 0.2로 설정합니다._size=0.2: 테스트 세트의 비율을 0.2로 설정합니다.\\ufeff\\u200brandom_state=42: 랜덤 시드 값으로, 데이터 분할의 재현성을 위해 사용됩니다._state=42: 랜덤 시드 값으로, 데이터 분할의 재현성을 위해 사용됩니다.\\ufeff\\u200bsklearn.preprocessing.StandardScaler: 데이터의 평균을 0, 분산을 1로 스케일링fit_transform(X_train): 훈련 세트를 스케일링하고 변환합니다._transform(X_train): 훈련 세트를 스케일링하고 변환합니다.\\ufeff\\u200btransform(X_test): 테스트 세트를 변환합니다.(X_test): 테스트 세트를 변환합니다.\\ufeff\\u200b☑️ 모델 학습모델 학습 {5px}모델 학습 \\ufeff\\u200bPythonCopyfrom sklearn.svm import SVC\\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\\n\\n# 모델 생성 및 학습\\nmodel = SVC(kernel=\\'linear\\')\\nmodel.fit(X_train, y_train)\\n# 예측\\ny_pred = model.predict(X_test)\\n# 평가\\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\\nprint(f\"Classification Report:\")\\nprint(f\"Confusion Matrix:\")\\n\\u200bsklearn.svm.SVC: 서포트 벡터 머신 분류 모델 생성kernel=’linear’: 선형 커널을 사용하여 SVM을 학습합니다.=’linear’: 선형 커널을 사용하여 SVM을 학습합니다.\\ufeff\\u200bfit(X_train, y_train): 모델을 훈련 세트에 맞추어 학습시킵니다(X_train, y_train): 모델을 훈련 세트에 맞추어 학습시킵니다\\ufeff\\u200bpredict(X_test): 테스트 세트에 대해 예측을 수행합니다.(X_test): 테스트 세트에 대해 예측을 수행합니다.\\ufeff\\u200bsklearn.metrics.accuracy_score: 정확도 계산accuracy_score(y_test, y_pred): 실제 값과 예측 값을 비교하여 정확도를 반환합니다._score(y_test, y_pred): 실제 값과 예측 값을 비교하여 정확도를 반환합니다.\\ufeff\\u200bsklearn.metrics.classification_report: 분류 보고서 생성classification_report(y_test, y_pred): 정확도, 정밀도, 재현율 등의 메트릭을 포함한 보고서를 출력합니다._report(y_test, y_pred): 정확도, 정밀도, 재현율 등의 메트릭을 포함한 보고서를 출력합니다.\\ufeff\\u200bsklearn.metrics.confusion_matrix: 혼동 행렬 생성confusion_matrix(y_test, y_pred): 실제 값과 예측 값의 혼동 행렬을 반환합니다._matrix(y_test, y_pred): 실제 값과 예측 값의 혼동 행렬을 반환합니다.\\ufeff\\u200b2) 타이타닉 데이터☑️ 데이터 로드 및 전처리타이타닉 데이터 로드 및 전처리 {5px}타이타닉 데이터 로드 및 전처리 \\ufeff\\u200bPythonCopyimport seaborn as sns\\n\\n# 데이터 로드\\ntitanic = sns.load_dataset(\\'titanic\\')\\n# 필요한 열 선택 및 결측값 처리\\ntitanic = titanic[[\\'survived\\', \\'pclass\\', \\'sex\\', \\'age\\', \\'sibsp\\', \\'parch\\', \\'fare\\', \\'embarked\\']].dropna()\\n# 성별과 탑승한 곳 인코딩\\ntitanic[\\'sex\\'] = titanic[\\'sex\\'].map({\\'male\\': 0, \\'female\\': 1})\\ntitanic[\\'embarked\\'] = titanic[\\'embarked\\'].map({\\'C\\': 0, \\'Q\\': 1, \\'S\\': 2})\\n# 특성과 타겟 분리\\nX = titanic.drop(\\'survived\\', axis=1)\\ny = titanic[\\'survived\\']\\n# 데이터 분할\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n# 데이터 스케일링\\nscaler = StandardScaler()\\nX_train = scaler.fit_transform(X_train)\\nX_test = scaler.transform(X_test)\\n\\u200bseaborn.load_dataset: seaborn의 내장 데이터셋 로드’titanic’: 타이타닉 데이터셋을 로드합니다.’titanic’: 타이타닉 데이터셋을 로드합니다.\\ufeff\\u200b pandas.DataFrame.dropna: 결측값이 있는 행 제거pandas.DataFrame.map: 데이터 값을 다른 값으로 매핑’male’: 0, ’female’: 1: 성별을 숫자로 매핑합니다.: 성별을 숫자로 매핑합니다.}’male’: 0, ’female’: 1: 성별을 숫자로 매핑합니다.\\ufeff\\u200b’C’: 0, ’Q’: 1, ’S’: 2: 탑승한 곳을 숫자로 매핑합니다.: 탑승한 곳을 숫자로 매핑합니다.}’C’: 0, ’Q’: 1, ’S’: 2: 탑승한 곳을 숫자로 매핑합니다.\\ufeff\\u200b☑️ 모델 학습모델 학습 {5px}모델 학습 \\ufeff\\u200bPythonCopy# 모델 생성 및 학습\\nmodel = SVC(kernel=\\'linear\\')\\nmodel.fit(X_train, y_train)\\n# 예측\\ny_pred = model.predict(X_test)\\n# 평가\\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\\nprint(f\"Classification Report:\")\\nprint(f\"Confusion Matrix:\")\\n\\u200bCopyright ⓒ TeamSparta All rights reserved.',\n",
       " '[스파르타코딩클럽] 11강. 지도학습 : 분류모델 - KNN📘[SCC] 바닥부터 시작하는 머신러닝/📚[스파르타코딩클럽] 바닥부터 시작하는 머신러닝 - 3주차/📕[스파르타코딩클럽] 11강. 지도학습 : 분류모델 - KNNMade with📕[스파르타코딩클럽] 11강. 지도학습 : 분류모델 - KNN[수업 목표]KNN(K-Nearest Neighbors)에 대한 개념을 배우고, 데이터를 이용해 실습해 봅니다[목차]01. KNN 개념02. KNN 실습K값 : K는 결정 경계의 매끄러움에 영향을 미치는 하이퍼파라미터입니다. 값 : K는 결정 경계의 매끄러움에 영향을 미치는 하이퍼파라미터입니다. \\ufeff\\n 작은 K값은 더 노이즈에 민감하고,큰 K값은 더 매끄러운 경계를 만듭니다. 작은 K값은 더 노이즈에 민감하고,큰 K값은 더 매끄러운 경계를 만듭니다.\\ufeff\\n다수결 투표 : K개의 가장 가까운 이웃의 클래스중 가장 빈번한 클래스로 새로운 데이터 포인트를 분류합니다다수결 투표 : K개의 가장 가까운 이웃의 클래스중 가장 빈번한 클래스로 새로운 데이터 포인트를 분류합니다\\ufeff\\u200b☑️ KNN의 목적KNN의 목표는 학습 데이터를 기반으로 새로운 데이터 포인트의 클래스를 예측하는 것입니다이는 분류 문제에서 주로 사용되며 다양한 응용 분야에 활용될 수 있습니다02. KNN 실습✔️Scikit-learn의 유방암데이터와 Seaborn의 타이타닉 데이터로 KNN 실습을 진행합니다1) 유방암 데이터☑️ 데이터 로드 및 전처리유방암 데이터 로드 및 전처리 {5px}유방암 데이터 로드 및 전처리 \\ufeff\\u200bPythonCopyimport numpy as np\\nimport pandas as pd\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# 데이터 로드\\ndata = load_breast_cancer()\\nX = data.data\\ny = data.target\\n\\n# 데이터 분할\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n# 데이터 스케일링\\nscaler = StandardScaler()\\nX_train = scaler.fit_transform(X_train)\\nX_test = scaler.transform(X_test)\\n\\u200bsklearn.datasets.load_breast_cancer: 유방암 데이터셋 로드return_X_y=False: 데이터와 타겟을 함께 반환할지 여부. 기본값은 False입니다._X_y=False: 데이터와 타겟을 함께 반환할지 여부. 기본값은 False입니다.\\ufeff\\u200bsklearn.model_selection.train_test_split: 데이터를 훈련 세트/ 테스트 세트로 분할test_size=0.2: 테스트 세트의 비율을 0.2로 설정합니다._size=0.2: 테스트 세트의 비율을 0.2로 설정합니다.\\ufeff\\u200brandom_state=42: 랜덤 시드 값으로, 데이터 분할의 재현성을 위해 사용됩니다._state=42: 랜덤 시드 값으로, 데이터 분할의 재현성을 위해 사용됩니다.\\ufeff\\u200bsklearn.preprocessing.StandardScaler: 데이터의 평균을 0, 분산을 1로 스케일링fit_transform(X_train): 훈련 세트를 스케일링하고 변환합니다._transform(X_train): 훈련 세트를 스케일링하고 변환합니다.\\ufeff\\u200btransform(X_test): 테스트 세트를 변환합니다.(X_test): 테스트 세트를 변환합니다.\\ufeff\\u200b☑️ 모델 학습모델 학습 {5px}모델 학습 \\ufeff\\u200bPythonCopyfrom sklearn.neighbors import KNeighborsClassifier\\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\\n\\n# 모델 생성 및 학습\\nmodel = KNeighborsClassifier(n_neighbors=5)\\nmodel.fit(X_train, y_train)\\n# 예측\\ny_pred = model.predict(X_test)\\n# 평가\\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\\nprint(f\"Classification Report:\")\\nprint(f\"Confusion Matrix:\")\\n\\u200bsklearn.neighbors.KNeighborsClassifier: KNN 모델 생성n_neighbors=5: 사용할 이웃의 수(K)를 설정합니다._neighbors=5: 사용할 이웃의 수(K)를 설정합니다.\\ufeff\\u200bfit(X_train, y_train): 모델을 훈련 세트에 맞추어 학습시킵니다(X_train, y_train): 모델을 훈련 세트에 맞추어 학습시킵니다\\ufeff\\u200bpredict(X_test): 테스트 세트에 대해 예측을 수행합니다.(X_test): 테스트 세트에 대해 예측을 수행합니다.\\ufeff\\u200bsklearn.metrics.accuracy_score: 정확도 계산accuracy_score(y_test, y_pred): 실제 값과 예측 값을 비교하여 정확도를 반환합니다._score(y_test, y_pred): 실제 값과 예측 값을 비교하여 정확도를 반환합니다.\\ufeff\\u200bsklearn.metrics.classification_report: 분류 보고서 생성classification_report(y_test, y_pred): 정확도, 정밀도, 재현율 등의 메트릭을 포함한 보고서를 출력합니다._report(y_test, y_pred): 정확도, 정밀도, 재현율 등의 메트릭을 포함한 보고서를 출력합니다.\\ufeff\\u200bsklearn.metrics.confusion_matrix: 혼동 행렬 생성confusion_matrix(y_test, y_pred): 실제 값과 예측 값의 혼동 행렬을 반환합니다._matrix(y_test, y_pred): 실제 값과 예측 값의 혼동 행렬을 반환합니다.\\ufeff\\u200b2) 타이타닉 데이터☑️ 데이터 로드 및 전처리타이타닉 데이터 로드 및 전처리 {5px}타이타닉 데이터 로드 및 전처리 \\ufeff\\u200bPythonCopyimport seaborn as sns\\n\\n# 데이터 로드\\ntitanic = sns.load_dataset(\\'titanic\\')\\n# 필요한 열 선택 및 결측값 처리\\ntitanic = titanic[[\\'survived\\', \\'pclass\\', \\'sex\\', \\'age\\', \\'sibsp\\', \\'parch\\', \\'fare\\', \\'embarked\\']].dropna()\\n# 성별과 탑승한 곳 인코딩\\ntitanic[\\'sex\\'] = titanic[\\'sex\\'].map({\\'male\\': 0, \\'female\\': 1})\\ntitanic[\\'embarked\\'] = titanic[\\'embarked\\'].map({\\'C\\': 0, \\'Q\\': 1, \\'S\\': 2})\\n# 특성과 타겟 분리\\nX = titanic.drop(\\'survived\\', axis=1)\\ny = titanic[\\'survived\\']\\n# 데이터 분할\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n# 데이터 스케일링\\nscaler = StandardScaler()\\nX_train = scaler.fit_transform(X_train)\\nX_test = scaler.transform(X_test)\\n\\u200bseaborn.load_dataset: seaborn의 내장 데이터셋 로드’titanic’: 타이타닉 데이터셋을 로드합니다.’titanic’: 타이타닉 데이터셋을 로드합니다.\\ufeff\\u200bpandas.DataFrame.dropna: 결측값이 있는 행 제거pandas.DataFrame.map: 데이터 값을 다른 값으로 매핑’male’: 0, ’female’: 1: 성별을 숫자로 매핑합니다.: 성별을 숫자로 매핑합니다.}’male’: 0, ’female’: 1: 성별을 숫자로 매핑합니다.\\ufeff\\u200b’C’: 0, ’Q’: 1, ’S’: 2: 탑승한 곳을 숫자로 매핑합니다.: 탑승한 곳을 숫자로 매핑합니다.}’C’: 0, ’Q’: 1, ’S’: 2: 탑승한 곳을 숫자로 매핑합니다.\\ufeff\\u200b☑️ 모델 학습모델 학습 {5px}모델 학습 \\ufeff\\u200bPythonCopy# 모델 생성 및 학습\\nmodel = KNeighborsClassifier(n_neighbors=5)\\nmodel.fit(X_train, y_train)\\n# 예측\\ny_pred = model.predict(X_test)\\n# 평가\\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\\nprint(f\"Classification Report:\")\\nprint(f\"Confusion Matrix:\")\\n\\u200bCopyright ⓒ TeamSparta All rights reserved.',\n",
       " '[스파르타코딩클럽] 12강. 지도학습 : 분류모델 - 나이브베이즈📘[SCC] 바닥부터 시작하는 머신러닝/📚[스파르타코딩클럽] 바닥부터 시작하는 머신러닝 - 3주차/📕[스파르타코딩클럽] 12강. 지도학습 : 분류모델 - 나이브베이즈Made with📕[스파르타코딩클럽] 12강. 지도학습 : 분류모델 - 나이브베이즈[수업 목표]분류모델중 나이브베이즈에 대한 개념을 배우고, 데이터를 이용해 실습해 봅니다[목차]01. 나이브베이즈 개념02. 나이브베이즈 실습import pandas as pd\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# 데이터 로드\\ndata = load_breast_cancer()\\nX = data.data\\ny = data.target\\n\\n# 데이터 분할\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n# 데이터 스케일링\\nscaler = StandardScaler()\\nX_train = scaler.fit_transform(X_train)\\nX_test = scaler.transform(X_test)\\n\\u200bsklearn.datasets.load_breast_cancer: 유방암 데이터셋 로드return_X_y=False: 데이터와 타겟을 함께 반환할지 여부. 기본값은 False입니다._X_y=False: 데이터와 타겟을 함께 반환할지 여부. 기본값은 False입니다.\\ufeff\\u200bsklearn.model_selection.train_test_split: 데이터를 훈련 세트/ 테스트 세트로 분할test_size=0.2: 테스트 세트의 비율을 0.2로 설정합니다._size=0.2: 테스트 세트의 비율을 0.2로 설정합니다.\\ufeff\\u200brandom_state=42: 랜덤 시드 값으로, 데이터 분할의 재현성을 위해 사용됩니다._state=42: 랜덤 시드 값으로, 데이터 분할의 재현성을 위해 사용됩니다.\\ufeff\\u200bsklearn.preprocessing.StandardScaler: 데이터의 평균을 0, 분산을 1로 스케일링fit_transform(X_train): 훈련 세트를 스케일링하고 변환합니다._transform(X_train): 훈련 세트를 스케일링하고 변환합니다.\\ufeff\\u200btransform(X_test): 테스트 세트를 변환합니다.(X_test): 테스트 세트를 변환합니다.\\ufeff\\u200b☑️ 모델 학습모델 학습 {5px}모델 학습 \\ufeff\\u200bPythonCopyfrom sklearn.naive_bayes import GaussianNB\\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\\n\\n# 모델 생성 및 학습\\nmodel = GaussianNB()\\nmodel.fit(X_train, y_train)\\n# 예측\\ny_pred = model.predict(X_test)\\n# 평가\\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\\nprint(f\"Classification Report:\")\\nprint(f\"Confusion Matrix:\")\\n\\u200bsklearn.naive_bayes.GaussianNB: 가우시안 나이브베이즈 분류 모델을 생성합니다fit(X_train, y_train): 모델을 훈련 세트에 맞추어 학습시킵니다.(X_train, y_train): 모델을 훈련 세트에 맞추어 학습시킵니다.\\ufeff\\u200bpredict(X_test): 테스트 세트에 대해 예측을 수행합니다.(X_test): 테스트 세트에 대해 예측을 수행합니다.\\ufeff\\u200bsklearn.metrics.accuracy_score: 정확도 계산accuracy_score(y_test, y_pred): 실제 값과 예측 값을 비교하여 정확도를 반환합니다._score(y_test, y_pred): 실제 값과 예측 값을 비교하여 정확도를 반환합니다.\\ufeff\\u200bsklearn.metrics.classification_report: 분류 보고서 생성classification_report(y_test, y_pred): 정확도, 정밀도, 재현율 등의 메트릭을 포함한 보고서를 출력합니다._report(y_test, y_pred): 정확도, 정밀도, 재현율 등의 메트릭을 포함한 보고서를 출력합니다.\\ufeff\\u200bsklearn.metrics.confusion_matrix: 혼동 행렬 생성confusion_matrix(y_test, y_pred): 실제 값과 예측 값의 혼동 행렬을 반환합니다._matrix(y_test, y_pred): 실제 값과 예측 값의 혼동 행렬을 반환합니다.\\ufeff\\u200b2) 타이타닉 데이터☑️ 데이터 로드 및 전처리타이타닉 데이터 로드 및 전처리 {5px}타이타닉 데이터 로드 및 전처리 \\ufeff\\u200bPythonCopyimport seaborn as sns\\n\\n# 데이터 로드\\ntitanic = sns.load_dataset(\\'titanic\\')\\n# 필요한 열 선택 및 결측값 처리\\ntitanic = titanic[[\\'survived\\', \\'pclass\\', \\'sex\\', \\'age\\', \\'sibsp\\', \\'parch\\', \\'fare\\', \\'embarked\\']].dropna()\\n# 성별과 탑승한 곳 인코딩\\ntitanic[\\'sex\\'] = titanic[\\'sex\\'].map({\\'male\\': 0, \\'female\\': 1})\\ntitanic[\\'embarked\\'] = titanic[\\'embarked\\'].map({\\'C\\': 0, \\'Q\\': 1, \\'S\\': 2})\\n# 특성과 타겟 분리\\nX = titanic.drop(\\'survived\\', axis=1)\\ny = titanic[\\'survived\\']\\n# 데이터 분할\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n# 데이터 스케일링\\nscaler = StandardScaler()\\nX_train = scaler.fit_transform(X_train)\\nX_test = scaler.transform(X_test)\\n\\u200bseaborn.load_dataset: seaborn의 내장 데이터셋 로드’titanic’: 타이타닉 데이터셋을 로드합니다.’titanic’: 타이타닉 데이터셋을 로드합니다.\\ufeff\\u200bpandas.DataFrame.dropna: 결측값이 있는 행 제거pandas.DataFrame.map: 데이터 값을 다른 값으로 매핑’male’: 0, ’female’: 1: 성별을 숫자로 매핑합니다.: 성별을 숫자로 매핑합니다.}’male’: 0, ’female’: 1: 성별을 숫자로 매핑합니다.\\ufeff\\u200b’C’: 0, ’Q’: 1, ’S’: 2: 탑승한 곳을 숫자로 매핑합니다.: 탑승한 곳을 숫자로 매핑합니다.}’C’: 0, ’Q’: 1, ’S’: 2: 탑승한 곳을 숫자로 매핑합니다.\\ufeff\\u200b☑️ 모델 학습모델 학습 {5px}모델 학습 \\ufeff\\u200bPythonCopy# 모델 생성 및 학습\\nmodel = GaussianNB()\\nmodel.fit(X_train, y_train)\\n# 예측\\ny_pred = model.predict(X_test)\\n# 평가\\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\\nprint(f\"Classification Report:\")\\nprint(f\"Confusion Matrix:\")\\n\\u200bCopyright ⓒ TeamSparta All rights reserved.✔️나이브베이즈가 무엇인지 알아봅시다',\n",
       " '[스파르타코딩클럽] 13강. 지도학습 : 분류모델 - 의사결정나무📘[SCC] 바닥부터 시작하는 머신러닝/📚[스파르타코딩클럽] 바닥부터 시작하는 머신러닝 - 3주차/📕[스파르타코딩클럽] 13강. 지도학습 : 분류모델 - 의사결정나무Made with📕[스파르타코딩클럽] 13강. 지도학습 : 분류모델 - 의사결정나무[수업 목표]분류모델중 의사결정나무에 대한 개념을 배우고, 데이터를 이용해 실습해 봅니다[목차]01. 의사결정나무 개념02. 의사결정나무 실습import pandas as pd\\nfrom sklearn.datasets import load_breast_cancer\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import StandardScaler\\n\\n# 데이터 로드\\ndata = load_breast_cancer()\\nX = data.data\\ny = data.target\\n\\n# 데이터 분할\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n# 데이터 스케일링\\nscaler = StandardScaler()\\nX_train = scaler.fit_transform(X_train)\\nX_test = scaler.transform(X_test)\\n\\u200bsklearn.datasets.load_breast_cancer: 유방암 데이터셋 로드return_X_y=False: 데이터와 타겟을 함께 반환할지 여부. 기본값은 False입니다._X_y=False: 데이터와 타겟을 함께 반환할지 여부. 기본값은 False입니다.\\ufeff\\u200bsklearn.model_selection.train_test_split: 데이터를 훈련 세트/ 테스트 세트로 분할test_size=0.2: 테스트 세트의 비율을 0.2로 설정합니다._size=0.2: 테스트 세트의 비율을 0.2로 설정합니다.\\ufeff\\u200brandom_state=42: 랜덤 시드 값으로, 데이터 분할의 재현성을 위해 사용됩니다._state=42: 랜덤 시드 값으로, 데이터 분할의 재현성을 위해 사용됩니다.\\ufeff\\u200bsklearn.preprocessing.StandardScaler: 데이터의 평균을 0, 분산을 1로 스케일링fit_transform(X_train): 훈련 세트를 스케일링하고 변환합니다._transform(X_train): 훈련 세트를 스케일링하고 변환합니다.\\ufeff\\u200btransform(X_test): 테스트 세트를 변환합니다.(X_test): 테스트 세트를 변환합니다.\\ufeff\\u200b☑️ 모델 학습모델 학습 {5px}모델 학습 \\ufeff\\u200bPythonCopyfrom sklearn.tree import DecisionTreeClassifier\\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\\n\\n# 모델 생성 및 학습\\nmodel = DecisionTreeClassifier(random_state=42)\\nmodel.fit(X_train, y_train)\\n# 예측\\ny_pred = model.predict(X_test)\\n# 평가\\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\\nprint(f\"Classification Report:\")\\nprint(f\"Confusion Matrix:\")\\n\\u200bsklearn.tree.DecisionTreeClassifier: 의사결정나무 분류 모델 생성random_state=42: 랜덤 시드 값으로, 트리의 재현성을 위해 사용됩니다._state=42: 랜덤 시드 값으로, 트리의 재현성을 위해 사용됩니다.\\ufeff\\u200bfit(X_train, y_train): 모델을 훈련 세트에 맞추어 학습시킵니다.(X_train, y_train): 모델을 훈련 세트에 맞추어 학습시킵니다.\\ufeff\\u200bpredict(X_test): 테스트 세트에 대해 예측을 수행합니다.(X_test): 테스트 세트에 대해 예측을 수행합니다.\\ufeff\\u200bsklearn.metrics.accuracy_score: 정확도 계산accuracy_score(y_test, y_pred): 실제 값과 예측 값을 비교하여 정확도를 반환합니다._score(y_test, y_pred): 실제 값과 예측 값을 비교하여 정확도를 반환합니다.\\ufeff\\u200bsklearn.metrics.classification_report: 분류 보고서 생성classification_report(y_test, y_pred): 정확도, 정밀도, 재현율 등의 메트릭을 포함한 보고서를 출력합니다._report(y_test, y_pred): 정확도, 정밀도, 재현율 등의 메트릭을 포함한 보고서를 출력합니다.\\ufeff\\u200bsklearn.metrics.confusion_matrix: 혼동 행렬 생성confusion_matrix(y_test, y_pred): 실제 값과 예측 값의 혼동 행렬을 반환합니다._matrix(y_test, y_pred): 실제 값과 예측 값의 혼동 행렬을 반환합니다.\\ufeff\\u200b2) 타이타닉 데이터☑️ 데이터 로드 및 전처리타이타닉 데이터 로드 및 전처리 {5px}타이타닉 데이터 로드 및 전처리 \\ufeff\\u200bPythonCopyimport seaborn as sns\\n\\n# 데이터 로드\\ntitanic = sns.load_dataset(\\'titanic\\')\\n# 필요한 열 선택 및 결측값 처리\\ntitanic = titanic[[\\'survived\\', \\'pclass\\', \\'sex\\', \\'age\\', \\'sibsp\\', \\'parch\\', \\'fare\\', \\'embarked\\']].dropna()\\n# 성별과 탑승한 곳 인코딩\\ntitanic[\\'sex\\'] = titanic[\\'sex\\'].map({\\'male\\': 0, \\'female\\': 1})\\ntitanic[\\'embarked\\'] = titanic[\\'embarked\\'].map({\\'C\\': 0, \\'Q\\': 1, \\'S\\': 2})\\n# 특성과 타겟 분리\\nX = titanic.drop(\\'survived\\', axis=1)\\ny = titanic[\\'survived\\']\\n# 데이터 분할\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n# 데이터 스케일링\\nscaler = StandardScaler()\\nX_train = scaler.fit_transform(X_train)\\nX_test = scaler.transform(X_test)\\n\\u200bseaborn.load_dataset: seaborn의 내장 데이터셋 로드’titanic’: 타이타닉 데이터셋을 로드합니다.’titanic’: 타이타닉 데이터셋을 로드합니다.\\ufeff\\u200bpandas.DataFrame.dropna: 결측값이 있는 행 제거pandas.DataFrame.map: 데이터 값을 다른 값으로 매핑’male’: 0, ’female’: 1: 성별을 숫자로 매핑합니다.: 성별을 숫자로 매핑합니다.}’male’: 0, ’female’: 1: 성별을 숫자로 매핑합니다.\\ufeff\\u200b’C’: 0, ’Q’: 1, ’S’: 2: 탑승한 곳을 숫자로 매핑합니다.: 탑승한 곳을 숫자로 매핑합니다.}’C’: 0, ’Q’: 1, ’S’: 2: 탑승한 곳을 숫자로 매핑합니다.\\ufeff\\u200b☑️ 모델 학습모델 학습 {5px}모델 학습 \\ufeff\\u200bPythonCopy# 모델 생성 및 학습\\nmodel = DecisionTreeClassifier(random_state=42)\\nmodel.fit(X_train, y_train)\\n# 예측\\ny_pred = model.predict(X_test)\\n# 평가\\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\\nprint(f\"Classification Report:\")\\nprint(f\"Confusion Matrix:\")\\n\\u200bCopyright ⓒ TeamSparta All rights reserved.',\n",
       " \"[스파르타코딩클럽] 14강. 비지도학습 : 군집화모델 - k-means clustering📘[SCC] 바닥부터 시작하는 머신러닝/📚[스파르타코딩클럽] 바닥부터 시작하는 머신러닝 - 4주차 /📕[스파르타코딩클럽] 14강. 비지도학습 : 군집화모델 - k-means clusteringMade with📕[스파르타코딩클럽] 14강. 비지도학습 : 군집화모델 - k-means clustering[수업 목표]비지도학습 군집화모델 중 k-means clustering 에 대한 개념을 배우고, 데이터를 이용해 실습해 봅니다[목차]01. k-means clustering 개념02. k-means clustering 실습import pandas as pd\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.cluster import KMeans\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n# 데이터 로드\\ndata = pd.read_csv('Mall_Customers.csv')\\n# 필요한 열 선택 및 결측값 처리\\ndata = data[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']]\\n# 데이터 스케일링\\nscaler = StandardScaler()\\ndata_scaled = scaler.fit_transform(data)\\n\\u200bpandas.read_csv: CSV 파일을 로드하여 데이터프레임 생성’Mall_Customers.csv’: 로드할 파일의 경로입니다.’Mall_Customers.csv’: 로드할 파일의 경로입니다.\\ufeff\\u200bpandas.DataFrame.dropna: 결측값이 있는 행을 제거합니다.pandas.DataFrame.map: 데이터 값을 다른 값으로 매핑합니다.sklearn.preprocessing.StandardScaler: 데이터의 평균을 0, 분산을 1로 스케일링fit_transform(X_train): 훈련 세트를 스케일링하고 변환합니다._transform(X_train): 훈련 세트를 스케일링하고 변환합니다.\\ufeff\\u200b☑️ 모델 학습 및 군집화모델 학습 및 군집화 {5px}모델 학습 및 군집화 \\ufeff\\u200bPythonCopy# 최적의 k 찾기 (엘보우 방법)\\ninertia = []\\nK = range(1, 11)\\nfor k in K:\\n    kmeans = KMeans(n_clusters=k, random_state=42)\\n    kmeans.fit(data_scaled)\\n    inertia.append(kmeans.inertia_)\\n# 엘보우 그래프 그리기\\nplt.figure(figsize=(10, 8))\\nplt.plot(K, inertia, 'bx-')\\nplt.xlabel('k')\\nplt.ylabel('Inertia')\\nplt.title('Elbow Method For Optimal k')\\nplt.show()\\n# k=5로 모델 생성 및 학습\\nkmeans = KMeans(n_clusters=5, random_state=42)\\nkmeans.fit(data_scaled)\\n# 군집 결과 할당\\ndata['Cluster'] = kmeans.labels_\\n\\u200bsklearn.cluster.KMeans: k-means 군집화 모델을 생성합니다n_clusters=k: 군집의 수를 설정합니다._clusters=k: 군집의 수를 설정합니다.\\ufeff\\u200brandom_state=42: 랜덤 시드 값으로, 결과의 재현성을 위해 사용됩니다._state=42: 랜덤 시드 값으로, 결과의 재현성을 위해 사용됩니다.\\ufeff\\u200bfit(data_scaled): 데이터를 학습하여 군집을 형성합니다.(data_scaled): 데이터를 학습하여 군집을 형성합니다.\\ufeff\\u200blabels_: 각 데이터 포인트가 속한 군집 레이블을 반환합니다._: 각 데이터 포인트가 속한 군집 레이블을 반환합니다.\\ufeff\\u200b☑️ 군집 시각화군집 시각화 {5px}군집 시각화 \\ufeff\\u200bPythonCopy# 2차원으로 군집 시각화 (연령 vs 소득)\\nplt.figure(figsize=(10, 8))\\nsns.scatterplot(x=data['Age'], y=data['Annual Income (k$)'], hue=data['Cluster'], palette='viridis')\\nplt.title('Clusters of customers (Age vs Annual Income)')\\nplt.show()\\n# 2차원으로 군집 시각화 (소득 vs 지출 점수)\\nplt.figure(figsize=(10, 8))\\nsns.scatterplot(x=data['Annual Income (k$)'], y=data['Spending Score (1-100)'], hue=data['Cluster'], palette='viridis')\\nplt.title('Clusters of customers (Annual Income vs Spending Score)')\\nplt.show()\\n\\u200bmatplotlib.pyplot.plot: 그래프를 그립니다.K, inertia, ’bx-’: x축, y축 데이터와 그래프 스타일을 설정합니다., inertia, ’bx-’: x축, y축 데이터와 그래프 스타일을 설정합니다.\\ufeff\\u200bseaborn.scatterplot: 산점도를 그립니다.x=data[’Age’]: x축 데이터=data[’Age’]: x축 데이터\\ufeff\\u200by=data[’Annual Income (k$)’]: y축 데이터=data[’Annual Income (k$)’]: y축 데이터\\ufeff\\u200bhue=data[’Cluster’]: 색상에 따라 군집을 구분합니다.=data[’Cluster’]: 색상에 따라 군집을 구분합니다.\\ufeff\\u200bpalette=’viridis’: 색상 팔레트를 설정합니다.=’viridis’: 색상 팔레트를 설정합니다.\\ufeff\\u200bCopyright ⓒ TeamSparta All rights reserved.\",\n",
       " '[스파르타코딩클럽] 15강. 비지도학습 : 군집화모델 - 계층적 군집화📘[SCC] 바닥부터 시작하는 머신러닝/📚[스파르타코딩클럽] 바닥부터 시작하는 머신러닝 - 4주차 /📕[스파르타코딩클럽] 15강. 비지도학습 : 군집화모델 - 계층적 군집화Made with📕[스파르타코딩클럽] 15강. 비지도학습 : 군집화모델 - 계층적 군집화[수업 목표]비지도학습 군집화모델 중 계층적 군집화 에 대한 개념을 배우고, 데이터를 이용해 실습해 봅니다[목차]01. 계층적 군집화 개념02. 계층적 군집화 실습병합 군집화: 각 데이터 포인트를 개별 군집으로 시작하여, 가장 가까운 군집을 반복적으로 병합합니다.병합 군집화: 각 데이터 포인트를 개별 군집으로 시작하여, 가장 가까운 군집을 반복적으로 병합합니다.\\ufeff\\u200b분할 군집화: 모든 데이터 포인트를 하나의 군집으로 시작하여, 반복적으로 가장 멀리 떨어진 군집을 분할합니다.분할 군집화: 모든 데이터 포인트를 하나의 군집으로 시작하여, 반복적으로 가장 멀리 떨어진 군집을 분할합니다.\\ufeff\\u200b☑️ 계층적 군집화의 작동 원리거리 행렬 계산: 데이터 포인트 간의 거리를 계산하여 거리 행렬을 만듭니다.군집 병합/분할: 거리 행렬을 기반으로 가장 가까운 군집을 병합하거나, 가장 멀리 떨어진 군집을 분할합니다.덴드로그램 생성: 군집화 과정을 시각화한 덴드로그램을 생성합니다.2) 병합 군집화 vs 분할 군집화☑️ 병합 군집화(Agglomerative Clustering)병합 군집화는 각 데이터 포인트를 개별 군집으로 시작하여, 가장 가까운 군집을 반복적으로 병합합니다.병합 군집화의 특징은 아래와 같습니다단순성: 구현이 비교적 간단합니다.계산 비용: 데이터 포인트 수가 많아질수록 계산 비용이 증가합니다.덴드로그램: 군집화 과정을 시각화한 덴드로그램을 생성할 수 있습니다.☑️ 분할 군집화(Divisive Clustering)분할 군집화는 모든 데이터 포인트를 하나의 군집으로 시작하여, 반복적으로 가장 멀리 떨어진 군집을 분할합니다. 분할 군집화의 주요 특징은 다음과 같습니다:상대적으로 복잡함: 병합 군집화보다 구현이 상대적으로 복잡할 수 있습니다.효율성: 큰 데이터셋에서 병합 군집화보다 효율적일 수 있습니다.덴드로그램: 군집화 과정을 시각화한 덴드로그램을 생성할 수 있습니다.02. 계층적 군집화 실습✔️Kaggle 쇼핑몰 데이터를 이용하여 계층적 군집화 실습을 진행합니다1) 쇼핑몰 데이터☑️ 데이터셋 다운로드Kaggle에서 \"Mall_Customers.csv\" 파일을 다운로드합니다.다운로드한 파일을 작업 디렉토리에 저장합니다.☑️ 데이터 로드 및 전처리쇼핑몰 데이터 로드 및 전처리 {5px}쇼핑몰 데이터 로드 및 전처리 \\ufeff\\u200bPythonCopyimport pandas as pd\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.cluster import AgglomerativeClustering\\nimport scipy.cluster.hierarchy as sch\\n\\n# 데이터셋 불러오기\\ndf = pd.read_csv(\\'Mall_Customers.csv\\')\\n# 데이터 확인\\nprint(df.head())\\n# 필요한 열만 선택\\nX = df[[\\'Age\\', \\'Annual Income (k$)\\', \\'Spending Score (1-100)\\']]\\n# 데이터 정규화\\nscaler = StandardScaler()\\nX_scaled = scaler.fit_transform(X)\\n\\u200bpandas.read_csv: CSV 파일을 로드하여 데이터프레임 생성’Mall_Customers.csv’: 로드할 파일의 경로입니다.’Mall_Customers.csv’: 로드할 파일의 경로입니다.\\ufeff\\u200bpandas.DataFrame.dropna: 결측값이 있는 행을 제거합니다.pandas.DataFrame.map: 데이터 값을 다른 값으로 매핑합니다.sklearn.preprocessing.StandardScaler: 데이터의 평균을 0, 분산을 1로 스케일링fit_transform(X_train): 훈련 세트를 스케일링하고 변환합니다._transform(X_train): 훈련 세트를 스케일링하고 변환합니다.\\ufeff\\u200b☑️ 덴드로그램 생성덴드로그램 생성 {5px}덴드로그램 생성 \\ufeff\\u200bPythonCopy# 덴드로그램 생성\\nplt.figure(figsize=(10, 7))\\ndendrogram = sch.dendrogram(sch.linkage(X_scaled, method=\\'ward\\'))\\nplt.title(\\'Dendrogram\\')\\nplt.xlabel(\\'Customers\\')\\nplt.ylabel(\\'Euclidean distances\\')\\nplt.show()\\n\\u200bsklearn.cluster.KMeans: k-means 군집화 모델을 생성합니다n_clusters=k: 군집의 수를 설정합니다._clusters=k: 군집의 수를 설정합니다.\\ufeff\\u200brandom_state=42: 랜덤 시드 값으로, 결과의 재현성을 위해 사용됩니다._state=42: 랜덤 시드 값으로, 결과의 재현성을 위해 사용됩니다.\\ufeff\\u200bfit(data_scaled): 데이터를 학습하여 군집을 형성합니다.(data_scaled): 데이터를 학습하여 군집을 형성합니다.\\ufeff\\u200blabels_: 각 데이터 포인트가 속한 군집 레이블을 반환합니다._: 각 데이터 포인트가 속한 군집 레이블을 반환합니다.\\ufeff\\u200b☑️ 계층적 군집화 모델 구현덴드로그램을 통해 최적의 군집 수를 결정한 후, 계층적 군집화 모델을 구현합니다.계층적 군집화 모델 구현 {5px}계층적 군집화 모델 구현 \\ufeff\\u200bPythonCopy# 계층적 군집화 모델 생성\\nhc = AgglomerativeClustering(n_clusters=5, metric=\\'euclidean\\', linkage=\\'ward\\')\\n# 모델 학습 및 예측\\ny_hc = hc.fit_predict(X_scaled)\\n# 결과 시각화\\nplt.figure(figsize=(10, 7))\\nplt.scatter(X_scaled[y_hc == 0, 0], X_scaled[y_hc == 0, 1], s=100, c=\\'red\\', label=\\'Cluster 1\\')\\nplt.scatter(X_scaled[y_hc == 1, 0], X_scaled[y_hc == 1, 1], s=100, c=\\'blue\\', label=\\'Cluster 2\\')\\nplt.scatter(X_scaled[y_hc == 2, 0], X_scaled[y_hc == 2, 1], s=100, c=\\'green\\', label=\\'Cluster 3\\')\\nplt.scatter(X_scaled[y_hc == 3, 0], X_scaled[y_hc == 3, 1], s=100, c=\\'cyan\\', label=\\'Cluster 4\\')\\nplt.scatter(X_scaled[y_hc == 4, 0], X_scaled[y_hc == 4, 1], s=100, c=\\'magenta\\', label=\\'Cluster 5\\')\\nplt.title(\\'Clusters of customers\\')\\nplt.xlabel(\\'Age\\')\\nplt.ylabel(\\'Annual Income (k$)\\')\\nplt.legend()\\nplt.show()\\n\\u200bmatplotlib.pyplot.plot: 그래프를 그립니다.K, inertia, ’bx-’: x축, y축 데이터와 그래프 스타일을 설정합니다., inertia, ’bx-’: x축, y축 데이터와 그래프 스타일을 설정합니다.\\ufeff\\u200bseaborn.scatterplot: 산점도를 그립니다.x=data[’Age’]: x축 데이터=data[’Age’]: x축 데이터\\ufeff\\u200by=data[’Annual Income (k$)’]: y축 데이터=data[’Annual Income (k$)’]: y축 데이터\\ufeff\\u200bhue=data[’Cluster’]: 색상에 따라 군집을 구분합니다.=data[’Cluster’]: 색상에 따라 군집을 구분합니다.\\ufeff\\u200bpalette=’viridis’: 색상 팔레트를 설정합니다.=’viridis’: 색상 팔레트를 설정합니다.\\ufeff\\u200b☑️ 모델 평가모델 평가 {5px}모델 평가 \\ufeff\\u200bPythonCopyfrom sklearn.metrics import silhouette_score\\n\\n# 실루엣 점수 계산\\nsilhouette_avg = silhouette_score(X_scaled, y_hc)\\nprint(f\\'Silhouette Score: {silhouette_avg}\\')\\n\\u200bCopyright ⓒ TeamSparta All rights reserved.',\n",
       " \"[스파르타코딩클럽] 16강. 비지도학습 : 군집화모델 - DBSCAN📘[SCC] 바닥부터 시작하는 머신러닝/📚[스파르타코딩클럽] 바닥부터 시작하는 머신러닝 - 4주차 /📕[스파르타코딩클럽] 16강. 비지도학습 : 군집화모델 - DBSCANMade with📕[스파르타코딩클럽] 16강. 비지도학습 : 군집화모델 - DBSCAN[수업 목표]비지도학습 군집화모델 중 DBSCAN 에 대한 개념을 배우고, 데이터를 이용해 실습해 봅니다[목차]01. DBSCAN 개념02. DBSCAN 실습eps: 두 데이터 포인트가 같은 군집에 속하기 위해 가져야 하는 최대 거리입니다.: 두 데이터 포인트가 같은 군집에 속하기 위해 가져야 하는 최대 거리입니다.\\ufeff\\u200bmin_samples: 한 군집을 형성하기 위해 필요한 최소 데이터 포인트 수입니다_samples: 한 군집을 형성하기 위해 필요한 최소 데이터 포인트 수입니다\\ufeff\\u200b☑️ DBSCAN의 작동 원리임의의 데이터 포인트를 선택합니다.선택한 데이터 포인트의 eps 반경 내에 있는 모든 데이터 포인트를 찾습니다.eps 반경 내의 데이터수 ≥ min_samples : 해당 데이터 포인트를 중심으로 새로운 군집 형성.eps 반경 내의 데이터수 < min_samples : 해당 데이터 포인트를 노이즈로 간주군집에 속한 데이터 포인트에 대해 2~4 단계를 반복합니다.모든 데이터 포인트가 처리될 때까지 이 과정을 반복합니다.☑️ DBSCAN의 장점비구형 군집 탐지: DBSCAN은 비구형 군집을 탐지할 수 있습니다.노이즈 처리: DBSCAN은 노이즈를 효과적으로 처리할 수 있습니다.군집 수 자동 결정: DBSCAN은 군집 수를 사전에 지정할 필요가 없습니다.02. DBSCAN 실습✔️Kaggle 쇼핑몰 데이터를 이용하여 DBSCAN 실습을 진행합니다1) 쇼핑몰 데이터☑️ 데이터 로드 쇼핑몰 데이터 로드 {5px}쇼핑몰 데이터 로드 \\ufeff\\u200bPythonCopyimport pandas as pd\\nimport numpy as np\\nimport matplotlib.pyplot as plt\\nfrom sklearn.preprocessing import StandardScaler\\nfrom sklearn.cluster import AgglomerativeClustering\\nimport scipy.cluster.hierarchy as sch\\n\\n# 데이터셋 불러오기\\ndf = pd.read_csv('Mall_Customers.csv')\\n# 데이터 확인\\nprint(df.head())\\n# 필요한 열만 선택\\nX = df[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']]\\n# 데이터 정규화\\nscaler = StandardScaler()\\nX_scaled = scaler.fit_transform(X)\\n\\u200b☑️ DBSCAN 수행Scikit-learn의 DBSCAN을 사용하여 DBSCAN 군집화를 수행합니다.DBSCAN수행 {5px}DBSCAN수행 \\ufeff\\u200bPythonCopyfrom sklearn.cluster import DBSCAN\\nimport matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\n# DBSCAN 모델 생성\\ndbscan = DBSCAN(eps=5, min_samples=5)\\n# 모델 학습 및 예측\\ndf['Cluster'] = dbscan.fit_predict(X)\\n# 군집화 결과 시각화\\nplt.figure(figsize=(10, 7))\\nsns.scatterplot(x='Annual Income (k$)', y='Spending Score (1-100)', hue='Cluster', data=df, palette='viridis')\\nplt.title('DBSCAN Clustering of Mall Customers')\\nplt.show()\\n\\u200b☑️ 파라미터 튜닝DBSCAN의 성능은 eps와 min_samples 파라미터에 크게 의존합니다. 적절한 파라미터를 찾기 위해 여러 값을 시도해볼 수 있습니다.파라미터 튜닝 {5px}파라미터 튜닝 \\ufeff\\u200bPythonCopy# 다양한 eps와 min_samples 값 시도\\neps_values = [3, 5, 7, 10]\\nmin_samples_values = [3, 5, 7, 10]\\nfor eps in eps_values:\\nfor min_samples in min_samples_values:\\n        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\\n        df['Cluster'] = dbscan.fit_predict(X)\\n        \\n        plt.figure(figsize=(10, 7))\\n        sns.scatterplot(x='Annual Income (k$)', y='Spending Score (1-100)', hue='Cluster', data=df, palette='viridis')\\n        plt.title(f'DBSCAN Clustering (eps={eps}, min_samples={min_samples})')\\n        plt.show()\\n\\u200bCopyright ⓒ TeamSparta All rights reserved.\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "text_list = []\n",
    "\n",
    "# 파일을 읽어와서 text_list에 저장하는 함수\n",
    "def load_files_to_list(file_path, text_list):\n",
    "    if os.path.exists(file_path):  # 파일이 존재하는지 확인\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:  # 파일 열기\n",
    "            content = file.read()  # 파일 내용 읽기\n",
    "            text_list.append(content)  # text_list에 추가\n",
    "    else:\n",
    "        print(f\"File {file_path} not found.\")\n",
    "        \n",
    "    \n",
    "\n",
    "# 예시: 파일 경로 목록을 지정\n",
    "file_paths = [f\"ML_dataset_{i}.txt\" for i in range(1, 17)]  # 파일 경로 목록 (DL_dataset_1.txt, DL_dataset_2.txt, ...)\n",
    "\n",
    "print(file_paths)\n",
    "\n",
    "for j in range(len(file_paths)):\n",
    "    load_files_to_list(file_paths[j], text_list)\n",
    "\n",
    "text_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 각 교재별 retriever 생성부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "# text_list를 Document 객체로 변환\n",
    "documents = [Document(page_content=text) for text in text_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain.schema import Document\n",
    "import json\n",
    "\n",
    "def get_retriever(texts: str, identifier: str):\n",
    "    # text_list를 Document 객체로 변환\n",
    "    documents = [Document(page_content=texts)]\n",
    "\n",
    "    recursive_text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=200,\n",
    "        chunk_overlap=20,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "\n",
    "    splits_recur = recursive_text_splitter.split_documents(documents)\n",
    "    splits = splits_recur\n",
    "\n",
    "    print(\"Top 10 chunks:\")\n",
    "    for i, chunk in enumerate(splits[:10], 1):\n",
    "        pprint(f\"\\nChunk {i}:\\n{chunk.page_content}\")\n",
    "\n",
    "    # OpenAI 임베딩 모델 초기화\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\", api_key=api_key)\n",
    "    vectorstore = FAISS.from_documents(documents=splits, embedding=embeddings)\n",
    "\n",
    "    bm25_retriever = BM25Retriever.from_documents(splits)\n",
    "    faiss_retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})\n",
    "\n",
    "    retriever = EnsembleRetriever(\n",
    "        retrievers=[bm25_retriever, faiss_retriever],\n",
    "        weights=[0.5, 0.5],  # 가중치 설정 (가중치의 합은 1.0)\n",
    "    )\n",
    "\n",
    "    return retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 chunks:\n",
      "('\\n'\n",
      " 'Chunk 1:\\n'\n",
      " '[스파르타코딩클럽] 1강. 강의 소개📘[SCC] 바닥부터 시작하는 머신러닝/📚[스파르타코딩클럽] 바닥부터 시작하는 머신러닝 - '\n",
      " '1주차/📕[스파르타코딩클럽] 1강. 강의 소개Made with📕[스파르타코딩클럽] 1강. 강의 소개[수업 목표]강의 소개 및 머신러닝 '\n",
      " '기본 개념을 알아봅시다[목차]01. 강좌 소개🤔 딥러닝이 대세라던데, 왜 머신러닝부터')\n",
      "('\\n'\n",
      " 'Chunk 2:\\n'\n",
      " '대세라던데, 왜 머신러닝부터 시작할까요?02. 머신러닝 소개강의 뒤쪽에서 나오는 어려운 문법들은 강좌 내에서 설명 후에 실습을 진행할 '\n",
      " '예정이니 걱정하지 않으셔도 됩니다!강의 뒤쪽에서 나오는 어려운 문법들은 강좌 내에서 설명 후에 실습을 진행할 예정이니 걱정하지 않으셔도 '\n",
      " '됩니다!\\ufeff\\u200b🤔 딥러닝이 대세라던데, 왜 머신러닝부터 시작할까요?딥러닝의 기초는 탄탄한')\n",
      "('\\n'\n",
      " 'Chunk 3:\\n'\n",
      " '시작할까요?딥러닝의 기초는 탄탄한 머신러닝 지식입니다. 기초를 튼튼히 쌓아야 딥러닝도 쉽게 이해할 수 있죠! 차근차근 기초부터 다지면, '\n",
      " '딥러닝 정복도 문제없어요! 💪02. 머신러닝 소개✔️머신러닝에 대한 기본적인 개념 및 역사등을 소개합니다1)머신러닝 소개☑️ 머신러닝의 '\n",
      " '정의컴퓨터가 명시적으로 프로그래밍 되지 않아도 데이터를 통해 학습하고, 예측할 수')\n",
      "('\\n'\n",
      " 'Chunk 4:\\n'\n",
      " '데이터를 통해 학습하고, 예측할 수 있도록 하는 기능대량의 데이터를 알고리즘에 입력하여 학습과정을 통해 모델을 생성하고 예측을 수행기존 '\n",
      " '프로그램은 명시적으로 작성된 규칙과 논리에 따라 동작한다는것에서 머신러닝과의 차이가 있습니다!기존 프로그램은 명시적으로 작성된 규칙과 '\n",
      " '논리에 따라 동작한다는것에서 머신러닝과의 차이가 있습니다!\\ufeff\\u200b전통적인 프로그래밍:규칙과')\n",
      "('\\n'\n",
      " 'Chunk 5:\\n'\n",
      " '프로그래밍:규칙과 논리를 프로그래머가 직접 정의 🖥️명시적 명령과 조건문을 통해 문제 해결 📝머신러닝:데이터를 이용해 패턴과 규칙을 '\n",
      " '스스로 학습 📊예측 모델을 통해 새로운 데이터에 대한 결과 도출 🔮프로그램이 아닌 모델이 중심 ⚙️☑️ 머신러닝의 구성요소데이터셋 : '\n",
      " '모델을 학습시키기 위한 데이터 모음특징(Feature) : 데이터셋에서 모델이 학습할 수')\n",
      "('\\n'\n",
      " 'Chunk 6:\\n'\n",
      " ': 데이터셋에서 모델이 학습할 수 있는 개별 속성레이블(label) : 예측하고자 하는 목표 변수훈련 : 모델이 데이터를 통해 학습하는 '\n",
      " '과정테스트 : 학습된 모델의 성능을 평가하는 과정2)머신러닝의 역사 및 발전 이유 ☑️ 머신러닝의 역사1950s : 앨런 튜링의 '\n",
      " '“튜링테스트”와 퍼셉트론의 개발1980s -90s : 백프로파게이션 알고리즘의 등장2000s')\n",
      "('\\n'\n",
      " 'Chunk 7:\\n'\n",
      " '알고리즘의 등장2000s :  대규모 데이터와 고성능 컴퓨팅 자원의 발전2010s - : 딥러닝의 부상과 다양한 산업에 적용☑️ 최근 '\n",
      " '머신러닝의 발전 이유데이터의 폭발적 증가컴퓨팅 파워의 향상알고리즘의 발전오픈소스 커뮤니티와 생태계의 발전🥳이 강좌에서는 파이썬을 활용한 '\n",
      " '머신러닝의 기본 개념과 실습을 다룰 예정입니다!')\n",
      "('\\n'\n",
      " 'Chunk 8:\\n'\n",
      " '머신러닝의 기본적인 원리부터 실습을 통해 직접 모델 구축/평가하는 방법까지 모두 함께 배워봅시다!Copyright ⓒ TeamSparta '\n",
      " 'All rights reserved.')\n",
      "Top 10 chunks:\n",
      "('\\n'\n",
      " 'Chunk 1:\\n'\n",
      " '[스파르타코딩클럽] 2강. 머신러닝 개요와 구성요소📘[SCC] 바닥부터 시작하는 머신러닝/📚[스파르타코딩클럽] 바닥부터 시작하는 머신러닝 '\n",
      " '- 1주차/📕[스파르타코딩클럽] 2강. 머신러닝 개요와 구성요소Made with📕[스파르타코딩클럽] 2강. 머신러닝 개요와 구성요소[수업 '\n",
      " '목표]머신러닝의 기본적인 구성요소, 학습과정을 알려 드립니다.다양한 머신러닝의')\n",
      "('\\n'\n",
      " 'Chunk 2:\\n'\n",
      " '알려 드립니다.다양한 머신러닝의 학습방법을 소개합니다.[목차]01. 머신러닝 구성 요소02. 머신러닝의 학습💡모든 토글을 열고 닫는 '\n",
      " '단축키')\n",
      "'\\nChunk 3:\\nWindows : Ctrl + alt + t'\n",
      "('\\n'\n",
      " 'Chunk 4:\\n'\n",
      " 'Mac : ⌘ + ⌥ + t 01. 머신러닝 구성 요소✔️머신러닝의 필수 구성요소를 학습합니다1) 머신러닝의 구성요소☑️ '\n",
      " '데이터셋머신러닝은 데이터셋을 통해서 학습하며, 일반적으로 데이터셋은 입력/출력 데이터로 구성됩니다.입력 데이터 : 모델이 학습할 수 있는 '\n",
      " '정보출력 데이터(레이블) : 모델이 예측해야 하는 목표 값☑️ Feature(특징)데이터에서')\n",
      "('\\n'\n",
      " 'Chunk 5:\\n'\n",
      " 'Feature(특징)데이터에서 모델이 학습할 수 있는 개별 속성주택가격예측을 예시로 들 경우 주택의 크기, 위치, 방의 개수 등이 '\n",
      " 'Feature에 해당합니다주택가격예측을 예시로 들 경우 주택의 크기, 위치, 방의 개수 등이 Feature에 '\n",
      " '해당합니다\\ufeff\\u200b☑️ 레이블예측하고자 하는 목표 변수지도학습 모델에서는 레이블이 있는 데이터셋을 이용하여 모델을 학습')\n",
      "('\\n'\n",
      " 'Chunk 6:\\n'\n",
      " '데이터셋을 이용하여 모델을 학습 시킵니다☑️ 모델데이터의 특징으로 부터 정답(레이블)을 예측할 수 있는 지식을 학습할 수 있는 '\n",
      " '프로그램/함수입력데이터와 출력 데이터간의 관계를 학습하여 새로운 데이터에 대한 예측 수행☑️ 학습모델이 데이터를 통해서 패턴을 인식하고, '\n",
      " '이를 기반으로 예측을 수행 할 수 있도록 함수 내의 가중치를 조정하는 과정02. 머신러닝의')\n",
      "('\\n'\n",
      " 'Chunk 7:\\n'\n",
      " '조정하는 과정02. 머신러닝의 학습✔️머신러닝의 학습과정과 다양한 학습 종류에 대해서 학습합니다1) 머신러닝의 학습 과정머신러닝 학습 '\n",
      " '과정ALT데이터 수집 : 모델을 학습시키기 위한 필요 데이터 수집데이터 전처리 : 결측값 처리, 이상치 제거, 정규화 등등Feature '\n",
      " '선택 : 중요 feature(특징)을 선택하고 불필요한 피쳐를 제거하여 학습효율')\n",
      "('\\n'\n",
      " 'Chunk 8:\\n'\n",
      " '불필요한 피쳐를 제거하여 학습효율 높임모델 선택 : 문제에 적합한 머신러닝 알고리즘을 선택모델 훈련 : 트레이닝 데이터셋을 사용해서 '\n",
      " '모델을 학습시킴모델 평가 : 테스트 데이터셋을 사용하여 모델 성능을 평가모델 배포 : 학습된 모델을 실제 환경에 배포하여 예측 수행2) '\n",
      " '학습 방법☑️ 지도 학습 (Supervised Learning)레이블이 있는 데이터셋을')\n",
      "('\\n'\n",
      " 'Chunk 9:\\n'\n",
      " '있는 데이터셋을 이용하여 모델을 학습시키는 방법회귀(Regression) : 연속적인 값을 예측하는 문제ex : 주택 가격 예측, 주식 '\n",
      " '가격예측 : 주택 가격 예측, 주식 가격예측\\ufeff\\u200b분류(Classification) : 이산적인 값을 예측하는 문제ex : '\n",
      " '이메일 스팸 필터링, 이미지 분류 : 이메일 스팸 필터링, 이미지 분류\\ufeff\\u200b☑️ 비지도 학습')\n",
      "('\\n'\n",
      " 'Chunk 10:\\n'\n",
      " '이미지 분류\\ufeff\\u200b☑️ 비지도 학습 (Unsupervised Learning)레이블이 없는 데이터셋을 이용하려 모델을 '\n",
      " '학습시키는 방법군집화(Clustering) : 데이터를 유사한 그룹으로 묶는 문제 ex : 고객 세분화, 이미지 세그멘테이션 : 고객 '\n",
      " '세분화, 이미지 세그멘테이션\\ufeff\\u200b차원축소 (Dimensionality Reduction) : 고차원 데이터를')\n",
      "Top 10 chunks:\n",
      "('\\n'\n",
      " 'Chunk 1:\\n'\n",
      " '[스파르타코딩클럽] 3강. Anaconda 설치 및 라이브러리 소개📘[SCC] 바닥부터 시작하는 머신러닝/📚[스파르타코딩클럽] 바닥부터 '\n",
      " '시작하는 머신러닝 - 1주차/📕[스파르타코딩클럽] 3강. Anaconda 설치 및 라이브러리 소개Made with📕[스파르타코딩클럽] '\n",
      " '3강. Anaconda 설치 및 라이브러리 소개[수업 목표]Anaconda 를')\n",
      "('\\n'\n",
      " 'Chunk 2:\\n'\n",
      " '소개[수업 목표]Anaconda 를 설치합니다머신러닝에서 사용하는 주요 라이브러리 기능을 배우고 실습합니다[목차]01. Anaconda '\n",
      " '소개 및 설치02. 주요 라이브러리 소개conda --version')\n",
      "('\\n'\n",
      " 'Chunk 3:\\n'\n",
      " '\\u200b☑️ Anaconda 주요 기능패키지 관리: conda 명령어를 사용하여 패키지를 설치, 업데이트, 제거할 수 있습니다.환경 '\n",
      " '관리: 가상 환경을 생성하고 관리할 수 있습니다.Jupyter Notebook: 웹 기반의 대화형 개발 환경을 제공합니다.Spyder: '\n",
      " '통합 개발 환경(IDE)으로, 코드 편집기와 디버거를 포함합니다.가상환경 생성 및 관리')\n",
      "('\\n'\n",
      " 'Chunk 4:\\n'\n",
      " '포함합니다.가상환경 생성 및 관리 {5px}가상환경 생성 및 관리 \\ufeff\\u200bPythonCopy# 새로운 가상 환경 생성')\n",
      "'\\nChunk 5:\\nconda create --name myenv'\n",
      "('\\n'\n",
      " 'Chunk 6:\\n'\n",
      " '# 가상 환경 활성화\\n'\n",
      " 'conda activate myenv\\n'\n",
      " '\\n'\n",
      " '# 가상 환경 비활성화\\n'\n",
      " 'conda deactivate')\n",
      "'\\nChunk 7:\\n# 가상 환경 제거\\nconda remove --name myenv --all'\n",
      "('\\n'\n",
      " 'Chunk 8:\\n'\n",
      " '\\u200b02. 주요 라이브러리 소개✔️머신러닝 강의에서 사용하는 주요 라이브러리를 소개드리고 간단한 실습을 진행해 봅시다1) '\n",
      " 'Numpy 라이브러리 소개 ☑️ Numpy 라이브러리란?수치 계산을 위한 Python 라이브러리Numpy는 다차원 배열 객체인 '\n",
      " 'ndarray를 제공벡터화 연산을 통해 빠르고 효율적인 수치 계산을 수행 가능☑️ Numpy 설치Numpy')\n",
      "('\\n'\n",
      " 'Chunk 9:\\n'\n",
      " '가능☑️ Numpy 설치Numpy 설치 {5px}Numpy 설치 \\ufeff\\u200bPythonCopy# conda를 사용하여 '\n",
      " 'Numpy 설치')\n",
      "'\\nChunk 10:\\nconda install numpy'\n",
      "Top 10 chunks:\n",
      "('\\n'\n",
      " 'Chunk 1:\\n'\n",
      " '[스파르타코딩클럽] 4강. Jupyter Notebook 사용해보기📘[SCC] 바닥부터 시작하는 머신러닝/📚[스파르타코딩클럽] 바닥부터 '\n",
      " '시작하는 머신러닝 - 1주차/📕[스파르타코딩클럽] 4강. Jupyter Notebook 사용해보기📕[스파르타코딩클럽] 4강. '\n",
      " 'Jupyter Notebook 사용해보기[수업 목표]Jupyter Notebook')\n",
      "'\\nChunk 2:\\n목표]Jupyter Notebook 사용해보기![목차]Jupyter Notebook이란?💡모든 토글을 열고 닫는 단축키'\n",
      "'\\nChunk 3:\\nWindows : Ctrl + alt + t'\n",
      "('\\n'\n",
      " 'Chunk 4:\\n'\n",
      " 'Mac : ⌘ + ⌥ + t Jupyter Notebook이란?✔️Jupyter Notebook이 무엇인지 알아봅시다.1) Jupyter '\n",
      " 'Notebook 소개☑️ Jupyter Notebook이란Jupyter Notebook은 데이터 과학자, 연구자, 교육자들이 널리 '\n",
      " '사용하는 오픈 소스 웹 애플리케이션입니다. 이를 통해 사용자는 실시간으로 코드를 작성하고')\n",
      "('\\n'\n",
      " 'Chunk 5:\\n'\n",
      " '사용자는 실시간으로 코드를 작성하고 실행하며, 그 결과를 시각적으로 확인하고, 문서화할 수 있습니다.Anaconda를 설치하면 자동으로 '\n",
      " '설치됩니다!2) Jupyter Notebook 사용하기☑️ Jupyter Notebook 사용법환경 설정!가상환경 만들기conda '\n",
      " 'create --name myenv가상환경 활성화conda activate myenv필요')\n",
      "('\\n'\n",
      " 'Chunk 6:\\n'\n",
      " 'activate myenv필요 패키지 설치conda install jupyter numpy pandas가상환경과 Jupyter '\n",
      " '연결하기python -m ipykernel install --user --name=myenv --display-name \"Python '\n",
      " '(myenv)\"사용하기!ALTALTALT☑️ Jupyter 사용확인!세팅 확인{5px}세팅')\n",
      "('\\n'\n",
      " 'Chunk 7:\\n'\n",
      " '사용확인!세팅 확인{5px}세팅 확인\\ufeff\\u200bALTCopyright ⓒ TeamSparta All rights '\n",
      " 'reserved.')\n",
      "Top 10 chunks:\n",
      "('\\n'\n",
      " 'Chunk 1:\\n'\n",
      " '[스파르타코딩클럽] 5강. 데이터셋 불러오기📘[SCC] 바닥부터 시작하는 머신러닝/📚[스파르타코딩클럽] 바닥부터 시작하는 머신러닝 - '\n",
      " '2주차/📕[스파르타코딩클럽] 5강. 데이터셋 불러오기Made with📕[스파르타코딩클럽] 5강. 데이터셋 불러오기[수업 목표]pandas '\n",
      " '라이브러리를 이용하여 데이터를 불러오는 법을 배웁니다.캐글(Kaggle)에 대해')\n",
      "('\\n'\n",
      " 'Chunk 2:\\n'\n",
      " '배웁니다.캐글(Kaggle)에 대해 알아보고, 캐글의 데이터셋을 다운받아 불러오는 실습을 해봅니다.[목차]01. 데이터 불러오기 및 '\n",
      " '탐색02. 캐글(Kaggle) 소개 및 데이터셋 다운 받기01. 데이터 불러오기 및 탐색✔️Pandas를 이용하여 데이터를 불러오고 기본 '\n",
      " '정보를 확인해 봅니다1) 데이터 불러오기 (CSV 및 엑셀파일)☑️ Pandas')\n",
      "('\\n'\n",
      " 'Chunk 3:\\n'\n",
      " '및 엑셀파일)☑️ Pandas 라이브러리 소개데이터 조작 및 분석을 위한 Python 라이브러리pandas는 데이터 '\n",
      " '프레임(DataFrame) 구조를 사용해 데이터를 효율적으로 관리/조작 할 수있습니다☑️ CSV 파일 불러오기CSV : 콤마로 구분된 '\n",
      " '값들을 저장하는 텍스트 파일Pandas의 read_csv 함수를 사용하여 CSV 파일을 불러올 수')\n",
      "('\\n'\n",
      " 'Chunk 4:\\n'\n",
      " '사용하여 CSV 파일을 불러올 수 있습니다.CSV 불러오기 {5px}CSV 불러오기 \\ufeff\\u200bPythonCopyimport '\n",
      " 'pandas as pd')\n",
      "('\\n'\n",
      " 'Chunk 5:\\n'\n",
      " '# CSV 파일 불러오기\\n'\n",
      " \"df = pd.read_csv('data.csv')\\n\"\n",
      " '# 데이터 프레임의 첫 5행 출력\\n'\n",
      " 'print(df.head())\\n'\n",
      " '\\u200b☑️ 엑셀 파일 불러오기Pandas의 read_excel 함수를 사용하여 엑셀 파일을 불러올 수 있습니다.엑셀 불러오기 '\n",
      " '{5px}엑셀 불러오기 \\ufeff\\u200bPythonCopyimport pandas as pd')\n",
      "('\\n'\n",
      " 'Chunk 6:\\n'\n",
      " '# 엑셀 파일 불러오기\\n'\n",
      " \"df = pd.read_excel('data.xlsx', sheet_name='Sheet1')\\n\"\n",
      " '# 데이터 프레임의 첫 5행 출력\\n'\n",
      " 'print(df.head())')\n",
      "('\\n'\n",
      " 'Chunk 7:\\n'\n",
      " 'print(df.head())\\n'\n",
      " '\\u200b2) 데이터 구조 확인해 보기☑️ 데이터 프레임의 기본 정보 확인Pandas에서 제공하는 다양한 메서드들을 이용하여 데이터 '\n",
      " '프레임의 구조와 기본 정보 확인 가능데이터프레임 정보확인 {5px}데이터프레임 정보확인 \\ufeff\\u200bPythonCopy# 데이터 '\n",
      " '프레임의 크기 (행, 열) 확인\\n'\n",
      " 'print(df.shape)')\n",
      "('\\n'\n",
      " 'Chunk 8:\\n'\n",
      " 'print(df.shape)\\n'\n",
      " '# 데이터 프레임의 컬럼명 확인\\n'\n",
      " 'print(df.columns)\\n'\n",
      " '# 데이터 프레임의 데이터 타입 확인\\n'\n",
      " 'print(df.dtypes)\\n'\n",
      " '# 데이터 프레임의 요약 통계량 확인\\n'\n",
      " 'print(df.describe())\\n'\n",
      " '# 데이터 프레임의 정보 확인 (null 값, 데이터 타입 등)\\n'\n",
      " 'print(df.info())')\n",
      "('\\n'\n",
      " 'Chunk 9:\\n'\n",
      " \"\\u200b02. 캐글(Kaggle) 소개 및 데이터셋 다운 받기No accessYou don't have access to this \"\n",
      " 'synced blockRequest access1) 캐글(Kaggle) 소개☑️ 캐글이란?데이터 과학 및 머신러닝 경진대회 플랫폼데이터 '\n",
      " '사이언티스트 / 머신러닝 엔지니어들이 다양한 문제를 해결하고 데이터를 분석모델을 개발하는데')\n",
      "('\\n'\n",
      " 'Chunk 10:\\n'\n",
      " '데이터를 분석모델을 개발하는데 필요한 데이터셋과 도구를 제공☑️ 캐글의 주요 기능경진대회: 다양한 데이터 과학 및 머신러닝 문제를 '\n",
      " '해결하는 경진대회가 열립니다.데이터셋: 다양한 주제의 데이터셋을 무료로 다운로드할 수 있습니다.커뮤니티: 데이터 과학자와 머신러닝 '\n",
      " '엔지니어들이 지식을 공유하고 협업할 수 있는 커뮤니티입니다.커널: 웹 기반의 코드 실행')\n",
      "Top 10 chunks:\n",
      "('\\n'\n",
      " 'Chunk 1:\\n'\n",
      " '[스파르타코딩클럽] 6강. 데이터 전처리📘[SCC] 바닥부터 시작하는 머신러닝/📚[스파르타코딩클럽] 바닥부터 시작하는 머신러닝 - '\n",
      " '2주차/📕[스파르타코딩클럽] 6강. 데이터 전처리Made with📕[스파르타코딩클럽] 6강. 데이터 전처리[수업 목표]데이터 전처리 '\n",
      " '개념을 알아봅시다데이터 전처리 방법을 알아 봅시다[목차]01. 데이터 전처리 개념 및 API')\n",
      "('\\n'\n",
      " 'Chunk 2:\\n'\n",
      " '데이터 전처리 개념 및 API '\n",
      " '소개Xnorm=Xmax−XminX−XminXnorm=Xmax−XminX−XminXnorm=Xmax−XminX−Xmin\\ufeff\\u200bXnorm=X−XminXmax−XminXnorm=X−XminXmax−XminXnorm=X−XminXmax−XminX_{norm} '\n",
      " '= }{X_{max} -')\n",
      "('\\n'\n",
      " 'Chunk 3:\\n'\n",
      " '= }{X_{max} - '\n",
      " 'X_{min}}Xnorm=X−XminXmax−XminXnorm\\u200b=Xmax\\u200b−Xmin\\u200bX−Xmin\\u200b\\u200b\\ufeff\\u200b☑️ '\n",
      " '데이터 표준화 (Standardization)표준화는 데이터를 평균 0, 분산 1로 변환하는 과정입니다.Z-점수 표준화: '\n",
      " 'Xstd=σX−μXstd=σX−μXstd=σX−μ\\ufeff')\n",
      "('\\n'\n",
      " 'Chunk 4:\\n'\n",
      " '여기서 μμ\\ufeff는 평균, σσ\\ufeff는 표준편차입니다.Xstd=X−μσXstd=X−μσXstd=X−μσX_{std} = '\n",
      " '{}Xstd=X−μσXstd\\u200b=σX−μ\\u200b\\ufeff\\u200b☑️  특성 공학 (Feature '\n",
      " 'Engineering)특성 공학은 데이터로부터 새로운 유용한 특성을 생성하는 과정입니다.특성 생성: 기존 데이터를 기반으로 새로운 특성을 '\n",
      " '생성합니다 (예: 날짜')\n",
      "('\\n'\n",
      " 'Chunk 5:\\n'\n",
      " '특성을 생성합니다 (예: 날짜 데이터를 사용하여 요일 특성 생성).특성 선택: 모델 성능에 중요한 특성을 선택하고, 중요하지 않은 특성을 '\n",
      " '제거합니다.☑️  데이터 인코딩 (Data Encoding)비정형 데이터를 모델이 이해할 수 있는 형태로 변환합니다.레이블 인코딩 '\n",
      " '(Label Encoding): 범주형 데이터를 숫자로 변환합니다.원-핫 인코딩')\n",
      "('\\n'\n",
      " 'Chunk 6:\\n'\n",
      " '숫자로 변환합니다.원-핫 인코딩 (One-Hot Encoding): 범주형 데이터를 이진 벡터로 변환합니다.☑️  데이터 분할 (Data '\n",
      " 'Splitting)데이터를 학습용(train), 검증용(validation), 테스트용(test)으로 분할합니다. 이를 통해 모델의 '\n",
      " '일반화 성능을 평가할 수 있습니다.학습 데이터 (Training Data): 모델')\n",
      "('\\n'\n",
      " 'Chunk 7:\\n'\n",
      " '(Training Data): 모델 학습에 사용되는 데이터.검증 데이터 (Validation Data): 모델 튜닝 및 성능 검증에 '\n",
      " '사용되는 데이터.테스트 데이터 (Test Data): 최종 모델 평가에 사용되는 데이터.Copyright ⓒ TeamSparta All '\n",
      " 'rights reserved.')\n",
      "Top 10 chunks:\n",
      "('\\n'\n",
      " 'Chunk 1:\\n'\n",
      " '[스파르타코딩클럽] 7강. 데이터 전처리 실습 📘[SCC] 바닥부터 시작하는 머신러닝/📚[스파르타코딩클럽] 바닥부터 시작하는 머신러닝 - '\n",
      " '2주차/📕[스파르타코딩클럽] 7강. 데이터 전처리 실습 Made with📕[스파르타코딩클럽] 7강. 데이터 전처리 실습 [수업 '\n",
      " '목표]데이터 전처리 개념 및 Pandas에서 사용하는 API를 알아봅시다데이터 전처리 방법을')\n",
      "('\\n'\n",
      " 'Chunk 2:\\n'\n",
      " \"알아봅시다데이터 전처리 방법을 알아 봅시다[목차]01. 데이터 전처리Data'A': [1, 2, np.nan, 4, 5, 100, 1, \"\n",
      " \"2, 2, 4, '1', '2', '3', '4', '5', 10, 20, 30, 40, 50],\")\n",
      "('\\n'\n",
      " 'Chunk 3:\\n'\n",
      " \"'B': [5, np.nan, np.nan, 8, 10, 60, 10, 20, 20, 40, '10', '20', '30', '40', \"\n",
      " \"'50', 5, 4, 3, 2, 1],\\n\"\n",
      " \"'C': [1, 2, 3, 4, 5, 5, 100, 200, 200, 400, 100, 200, 300, 400, 500, 1, 2, \"\n",
      " '3, 4, 5],')\n",
      "('\\n'\n",
      " 'Chunk 4:\\n'\n",
      " \"'D': [np.nan, np.nan, 3, 3, 3, 5, 5, 5, 5, 5, np.nan, np.nan, np.nan, \"\n",
      " 'np.nan, np.nan, 2, 3, 4, 5, 6],\\n'\n",
      " \"'category_column': [np.nan]*10 + ['A', 'B', 'A', 'C', 'B'] + [np.nan]*5,\")\n",
      "('\\n'\n",
      " 'Chunk 5:\\n'\n",
      " \"'value_column': [np.nan]*10 + [1, 2, 3, 4, 5] + [np.nan]*5,\\n\"\n",
      " \"'target': [np.nan]*15 + [1, 0, 1, 0, 1]\\n\"\n",
      " '}')\n",
      "('\\n'\n",
      " 'Chunk 6:\\n'\n",
      " '}\\n'\n",
      " '\\u200b1)결측값 처리☑️ 결측값 처리 방법제거: 결측값이 포함된 행 또는 열을 제거합니다.대체: 결측값을 특정 값으로 '\n",
      " '대체합니다.예측: 머신러닝 모델을 사용하여 결측값을 예측합니다.결측값 제거 {5px}결측값 제거 '\n",
      " '\\ufeff\\u200bPythonCopy# 결측값이 포함된 행 제거\\n'\n",
      " 'df_dropped_rows = df.dropna()\\n'\n",
      " '# 결측값이 포함된 열 제거')\n",
      "('\\n'\n",
      " 'Chunk 7:\\n'\n",
      " '# 결측값이 포함된 열 제거\\n'\n",
      " 'df_dropped_cols = df.dropna(axis=1)\\n'\n",
      " '\\u200bdropna()를 사용하여 결측값 제거()를 사용하여 결측값 제거\\ufeff\\u200b결측값 대체 {5px}결측값 대체 '\n",
      " '\\ufeff\\u200bPythonCopy# 결측값을 0으로 대체\\n'\n",
      " 'df_filled = df.fillna(0)\\n'\n",
      " '# 결측값을 각 열의 평균값으로 대체')\n",
      "('\\n'\n",
      " 'Chunk 8:\\n'\n",
      " 'df_filled_mean = df.fillna(df.mean())\\n'\n",
      " '# 결측값을 각 열의 중간값으로 대체\\n'\n",
      " 'df_filled_median = df.fillna(df.median())\\n'\n",
      " '# 결측값을 각 열의 최빈값으로 대체\\n'\n",
      " 'df_filled_mode = df.fillna(df.mode().iloc[0])')\n",
      "('\\n'\n",
      " 'Chunk 9:\\n'\n",
      " '\\u200bfillna()를 사용하여 결측값 대체()를 사용하여 결측값 대체\\ufeff\\u200b결측값 예측 {5px}결측값 예측 '\n",
      " '\\ufeff\\u200bPythonCopyfrom sklearn.linear_model import LinearRegression')\n",
      "('\\n'\n",
      " 'Chunk 10:\\n'\n",
      " '# 결측값이 있는 열과 없는 열 분리\\n'\n",
      " \"df_with_na = df[df['column_with_na'].isnull()]\\n\"\n",
      " \"df_without_na = df[df['column_with_na'].notnull()]\\n\"\n",
      " '# 회귀 모델 학습\\n'\n",
      " 'model = LinearRegression()')\n",
      "Top 10 chunks:\n",
      "('\\n'\n",
      " 'Chunk 1:\\n'\n",
      " '[스파르타코딩클럽] 8강. 지도학습 : 회귀모델 📘[SCC] 바닥부터 시작하는 머신러닝/📚[스파르타코딩클럽] 바닥부터 시작하는 머신러닝 '\n",
      " '- 3주차/📕[스파르타코딩클럽] 8강. 지도학습 : 회귀모델 Made with📕[스파르타코딩클럽] 8강. 지도학습 : 회귀모델 [수업 '\n",
      " '목표]다양한 회귀 모델에 대해서 배워봅시다회귀(Regression)모델은 연속적인 값을')\n",
      "'\\nChunk 2:\\n연속적인 값을 예측하는 문제입니다.회귀(Regression)모델은 연속적인 값을 예측하는 문제입니다.\\ufeff'\n",
      "('\\n'\n",
      " 'Chunk 3:\\n'\n",
      " '오늘은 선형회귀/다항회귀/리지회귀/라쏘회귀 를 다뤄볼 예정입니다오늘은 선형회귀/다항회귀/리지회귀/라쏘회귀 를 다뤄볼 '\n",
      " '예정입니다\\ufeff\\u200b[목차]01. '\n",
      " '회귀모델y=β0\\u200b+β1\\u200bx1\\u200b+β2\\u200bx2\\u200b+⋯+βn\\u200bxn\\u200b+ϵy=β0\\u200b+β1\\u200bx1\\u200b+β2\\u200bx2\\u200b+⋯+βn\\u200bxn\\u200b+ϵy=β0\\u200b+β1\\u200bx1\\u200b+β2\\u200bx2\\u200b+⋯+βn\\u200bxn\\u200b+ϵ\\ufeff')\n",
      "('\\n'\n",
      " 'Chunk 4:\\n'\n",
      " '여기서 y는 종속변수, x1,x2,…,xn 은 독립변수, b0 는 절편, b1,b2,…,bn은 회귀계수, e는 오차입니다여기서 y는 '\n",
      " '종속변수, x1,x2,…,xn 은 독립변수, b0 는 절편, b1,b2,…,bn은 회귀계수, e는 오차입니다\\ufeff\\n'\n",
      " '⋄ 단순 선형 회귀일경우 ⋄ 단순 선형 회귀일경우 \\ufeff\\n'\n",
      " 'y=β0\\u200b+β1\\u200bx+ϵy=β0\\u200b+β1\\u200bx+ϵ\\ufeff')\n",
      "('\\n'\n",
      " 'Chunk 5:\\n'\n",
      " '☑️ Scikit-learn 을 사용한 선형 회귀 모델 구현 및 평가선형 회귀 모델 구현 및 평가 {5px}선형 회귀 모델 구현 및 평가 '\n",
      " '\\ufeff\\u200bPythonCopyimport numpy as np\\n'\n",
      " 'import pandas as pd\\n'\n",
      " 'from sklearn.model_selection import train_test_split')\n",
      "('\\n'\n",
      " 'Chunk 6:\\n'\n",
      " 'from sklearn.linear_model import LinearRegression\\n'\n",
      " 'from sklearn.metrics import mean_squared_error, r2_score')\n",
      "('\\n'\n",
      " 'Chunk 7:\\n'\n",
      " '# 데이터 생성\\n'\n",
      " 'X = np.array([[1, 1], [2, 2], [3, 3], [4, 4], [5, 5],[6,6]])\\n'\n",
      " 'y = np.array([1, 2, 3, 4, 5, 6])\\n'\n",
      " '# 데이터 분할 (훈련 데이터와 테스트 데이터)')\n",
      "('\\n'\n",
      " 'Chunk 8:\\n'\n",
      " 'X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, '\n",
      " 'random_state=42)\\n'\n",
      " '# 선형 회귀 모델 생성 및 학습\\n'\n",
      " 'model = LinearRegression()\\n'\n",
      " 'model.fit(X_train, y_train)\\n'\n",
      " '# 예측\\n'\n",
      " 'y_pred = model.predict(X_test)')\n",
      "('\\n'\n",
      " 'Chunk 9:\\n'\n",
      " '# 모델 평가\\n'\n",
      " 'mse = mean_squared_error(y_test, y_pred)\\n'\n",
      " 'r2 = r2_score(y_test, y_pred)\\n'\n",
      " \"print(f'Mean Squared Error: {mse}')\\n\"\n",
      " \"print(f'R^2 Score: {r2}')\")\n",
      "('\\n'\n",
      " 'Chunk 10:\\n'\n",
      " '\\u200b2) 다항 회귀☑️ 다항 회귀다항 회귀(Polynomial Regression)는 종속 변수와 독립 변수 간의 비선형 관계를 '\n",
      " '모델링하는 방법독립변수의 다항식을 사용하여 관계를 모델링 합니다.다항 회귀의 기본 수식은 다음과 같습니다다항 회귀의 기본 수식은 다음과 '\n",
      " '같습니다\\ufeff')\n",
      "Top 10 chunks:\n",
      "('\\n'\n",
      " 'Chunk 1:\\n'\n",
      " '[스파르타코딩클럽] 9강. 지도학습 : 분류모델 - 로지스틱 회귀📘[SCC] 바닥부터 시작하는 머신러닝/📚[스파르타코딩클럽] 바닥부터 '\n",
      " '시작하는 머신러닝 - 3주차/📕[스파르타코딩클럽] 9강. 지도학습 : 분류모델 - 로지스틱 회귀Made with📕[스파르타코딩클럽] '\n",
      " '9강. 지도학습 : 분류모델 - 로지스틱 회귀[수업 목표]지도학습 : 분류모델의 로지스틱')\n",
      "('\\n'\n",
      " 'Chunk 2:\\n'\n",
      " ': 분류모델의 로지스틱 회귀에 대해 알아보고 실습을 통해 배워봅시다[목차]01. 로지스틱 회귀 개념02. 로지스틱 회귀분석 '\n",
      " '실습import pandas as pd')\n",
      "('\\n'\n",
      " 'Chunk 3:\\n'\n",
      " 'from sklearn.datasets import load_breast_cancer\\n'\n",
      " 'from sklearn.model_selection import train_test_split\\n'\n",
      " 'from sklearn.preprocessing import StandardScaler')\n",
      "('\\n'\n",
      " 'Chunk 4:\\n'\n",
      " '# 데이터 로드\\n'\n",
      " 'data = load_breast_cancer()\\n'\n",
      " 'X = data.data\\n'\n",
      " 'y = data.target')\n",
      "('\\n'\n",
      " 'Chunk 5:\\n'\n",
      " '# 데이터 분할\\n'\n",
      " 'X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, '\n",
      " 'random_state=42)\\n'\n",
      " '# 데이터 스케일링\\n'\n",
      " 'scaler = StandardScaler()\\n'\n",
      " 'X_train = scaler.fit_transform(X_train)')\n",
      "'\\nChunk 6:\\nX_test = scaler.transform(X_test)'\n",
      "('\\n'\n",
      " 'Chunk 7:\\n'\n",
      " '\\u200bsklearn.datasets.load_breast_cancer: 유방암 데이터셋 로드return_X_y=False: 데이터와 '\n",
      " '타겟을 함께 반환할지 여부. 기본값은 False입니다._X_y=False: 데이터와 타겟을 함께 반환할지 여부. 기본값은 '\n",
      " 'False입니다.\\ufeff\\u200bsklearn.model_selection.train_test_split: 데이터를')\n",
      "('\\n'\n",
      " 'Chunk 8:\\n'\n",
      " '데이터를 훈련 세트/ 테스트 세트로 분할test_size=0.2: 테스트 세트의 비율을 0.2로 설정합니다._size=0.2: 테스트 '\n",
      " '세트의 비율을 0.2로 설정합니다.\\ufeff\\u200brandom_state=42: 랜덤 시드 값으로, 데이터 분할의 재현성을 위해 '\n",
      " '사용됩니다._state=42: 랜덤 시드 값으로, 데이터 분할의 재현성을 위해')\n",
      "('\\n'\n",
      " 'Chunk 9:\\n'\n",
      " '데이터 분할의 재현성을 위해 사용됩니다.\\ufeff\\u200bsklearn.preprocessing.StandardScaler: 데이터의 '\n",
      " '평균을 0, 분산을 1로 스케일링fit_transform(X_train): 훈련 세트를 스케일링하고 '\n",
      " '변환합니다._transform(X_train): 훈련 세트를 스케일링하고 '\n",
      " '변환합니다.\\ufeff\\u200btransform(X_test): 테스트 세트를')\n",
      "('\\n'\n",
      " 'Chunk 10:\\n'\n",
      " '테스트 세트를 변환합니다.(X_test): 테스트 세트를 변환합니다.\\ufeff\\u200b☑️ 모델 학습모델 학습 {5px}모델 학습 '\n",
      " '\\ufeff\\u200bPythonCopyfrom sklearn.linear_model import LogisticRegression')\n",
      "Top 10 chunks:\n",
      "('\\n'\n",
      " 'Chunk 1:\\n'\n",
      " '[스파르타코딩클럽] 10강. 지도학습 : 분류모델 - SVM📘[SCC] 바닥부터 시작하는 머신러닝/📚[스파르타코딩클럽] 바닥부터 시작하는 '\n",
      " '머신러닝 - 3주차/📕[스파르타코딩클럽] 10강. 지도학습 : 분류모델 - SVMMade with📕[스파르타코딩클럽] 10강. 지도학습 '\n",
      " ': 분류모델 - SVM[수업 목표]SVM(Support Vector Machine)에')\n",
      "('\\n'\n",
      " 'Chunk 2:\\n'\n",
      " 'Vector Machine)에 대한 개념을 배우고, 데이터를 이용해 실습해 봅니다[목차]01. SVM 개념02. SVM 실습서포트 벡터 '\n",
      " ': 결정 초평면에 가장 가까이 위치한 데이터 포인트 - 결정 초평면을 정의합니다서포트 벡터 : 결정 초평면에 가장 가까이 위치한 데이터 '\n",
      " '포인트 - 결정 초평면을 정의합니다\\ufeff')\n",
      "('\\n'\n",
      " 'Chunk 3:\\n'\n",
      " '커널 함수 : 데이터를 더 높은 차원으로 매핑하여 선형적으로 분리 할 수 없는 데이터를 분리하게 합니다. 커널 함수 : 데이터를 더 높은 '\n",
      " '차원으로 매핑하여 선형적으로 분리 할 수 없는 데이터를 분리하게 합니다. \\ufeff\\u200b☑️ SVM의 목적SVM의 목표는 마진을 '\n",
      " '최대화하면서 결정 초평면을 찾아 데이터 포인트를 정확하게 분류하는 것입니다. 이는 일반화 성능을')\n",
      "('\\n'\n",
      " 'Chunk 4:\\n'\n",
      " '것입니다. 이는 일반화 성능을 높이는 데 도움을 줍니다.w⋅x−b=0   - b = 0 w⋅x−b=0여기서 w는 가중치 벡터, x는 입력 '\n",
      " '벡터, b는 절편입니다.\\\\)는 가중치 벡터, \\\\(\\\\)는 입력 벡터, \\\\(b\\\\)는 절편입니다.}여기서 w는 가중치 벡터, x는 '\n",
      " '입력 벡터, b는 절편입니다.\\ufeff\\u200b02. SVM 실습✔️Scikit-learn의 유방암데이터와')\n",
      "('\\n'\n",
      " 'Chunk 5:\\n'\n",
      " '유방암데이터와 Seaborn의 타이타닉 데이터로 SVM 실습을 진행합니다1) 유방암 데이터☑️ 데이터 로드 및 전처리유방암 데이터 로드 '\n",
      " '및 전처리 {5px}유방암 데이터 로드 및 전처리 \\ufeff\\u200bPythonCopyimport numpy as np')\n",
      "('\\n'\n",
      " 'Chunk 6:\\n'\n",
      " 'import pandas as pd\\n'\n",
      " 'from sklearn.datasets import load_breast_cancer\\n'\n",
      " 'from sklearn.model_selection import train_test_split\\n'\n",
      " 'from sklearn.preprocessing import StandardScaler')\n",
      "('\\n'\n",
      " 'Chunk 7:\\n'\n",
      " '# 데이터 로드\\n'\n",
      " 'data = load_breast_cancer()\\n'\n",
      " 'X = data.data\\n'\n",
      " 'y = data.target')\n",
      "('\\n'\n",
      " 'Chunk 8:\\n'\n",
      " '# 데이터 분할\\n'\n",
      " 'X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, '\n",
      " 'random_state=42)\\n'\n",
      " '# 데이터 스케일링\\n'\n",
      " 'scaler = StandardScaler()\\n'\n",
      " 'X_train = scaler.fit_transform(X_train)')\n",
      "'\\nChunk 9:\\nX_test = scaler.transform(X_test)'\n",
      "('\\n'\n",
      " 'Chunk 10:\\n'\n",
      " '\\u200bsklearn.datasets.load_breast_cancer: 유방암 데이터셋 로드return_X_y=False: 데이터와 '\n",
      " '타겟을 함께 반환할지 여부. 기본값은 False입니다._X_y=False: 데이터와 타겟을 함께 반환할지 여부. 기본값은 '\n",
      " 'False입니다.\\ufeff\\u200bsklearn.model_selection.train_test_split: 데이터를')\n",
      "Top 10 chunks:\n",
      "('\\n'\n",
      " 'Chunk 1:\\n'\n",
      " '[스파르타코딩클럽] 11강. 지도학습 : 분류모델 - KNN📘[SCC] 바닥부터 시작하는 머신러닝/📚[스파르타코딩클럽] 바닥부터 시작하는 '\n",
      " '머신러닝 - 3주차/📕[스파르타코딩클럽] 11강. 지도학습 : 분류모델 - KNNMade with📕[스파르타코딩클럽] 11강. 지도학습 '\n",
      " ': 분류모델 - KNN[수업 목표]KNN(K-Nearest Neighbors)에 대한')\n",
      "('\\n'\n",
      " 'Chunk 2:\\n'\n",
      " 'Neighbors)에 대한 개념을 배우고, 데이터를 이용해 실습해 봅니다[목차]01. KNN 개념02. KNN 실습K값 : K는 결정 '\n",
      " '경계의 매끄러움에 영향을 미치는 하이퍼파라미터입니다. 값 : K는 결정 경계의 매끄러움에 영향을 미치는 하이퍼파라미터입니다. \\ufeff')\n",
      "('\\n'\n",
      " 'Chunk 3:\\n'\n",
      " '작은 K값은 더 노이즈에 민감하고,큰 K값은 더 매끄러운 경계를 만듭니다. 작은 K값은 더 노이즈에 민감하고,큰 K값은 더 매끄러운 '\n",
      " '경계를 만듭니다.\\ufeff')\n",
      "('\\n'\n",
      " 'Chunk 4:\\n'\n",
      " '다수결 투표 : K개의 가장 가까운 이웃의 클래스중 가장 빈번한 클래스로 새로운 데이터 포인트를 분류합니다다수결 투표 : K개의 가장 '\n",
      " '가까운 이웃의 클래스중 가장 빈번한 클래스로 새로운 데이터 포인트를 분류합니다\\ufeff\\u200b☑️ KNN의 목적KNN의 목표는 학습 '\n",
      " '데이터를 기반으로 새로운 데이터 포인트의 클래스를 예측하는 것입니다이는 분류 문제에서 주로 사용되며')\n",
      "('\\n'\n",
      " 'Chunk 5:\\n'\n",
      " '분류 문제에서 주로 사용되며 다양한 응용 분야에 활용될 수 있습니다02. KNN 실습✔️Scikit-learn의 유방암데이터와 '\n",
      " 'Seaborn의 타이타닉 데이터로 KNN 실습을 진행합니다1) 유방암 데이터☑️ 데이터 로드 및 전처리유방암 데이터 로드 및 전처리 '\n",
      " '{5px}유방암 데이터 로드 및 전처리 \\ufeff\\u200bPythonCopyimport numpy as np')\n",
      "('\\n'\n",
      " 'Chunk 6:\\n'\n",
      " 'import pandas as pd\\n'\n",
      " 'from sklearn.datasets import load_breast_cancer\\n'\n",
      " 'from sklearn.model_selection import train_test_split\\n'\n",
      " 'from sklearn.preprocessing import StandardScaler')\n",
      "('\\n'\n",
      " 'Chunk 7:\\n'\n",
      " '# 데이터 로드\\n'\n",
      " 'data = load_breast_cancer()\\n'\n",
      " 'X = data.data\\n'\n",
      " 'y = data.target')\n",
      "('\\n'\n",
      " 'Chunk 8:\\n'\n",
      " '# 데이터 분할\\n'\n",
      " 'X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, '\n",
      " 'random_state=42)\\n'\n",
      " '# 데이터 스케일링\\n'\n",
      " 'scaler = StandardScaler()\\n'\n",
      " 'X_train = scaler.fit_transform(X_train)')\n",
      "'\\nChunk 9:\\nX_test = scaler.transform(X_test)'\n",
      "('\\n'\n",
      " 'Chunk 10:\\n'\n",
      " '\\u200bsklearn.datasets.load_breast_cancer: 유방암 데이터셋 로드return_X_y=False: 데이터와 '\n",
      " '타겟을 함께 반환할지 여부. 기본값은 False입니다._X_y=False: 데이터와 타겟을 함께 반환할지 여부. 기본값은 '\n",
      " 'False입니다.\\ufeff\\u200bsklearn.model_selection.train_test_split: 데이터를')\n",
      "Top 10 chunks:\n",
      "('\\n'\n",
      " 'Chunk 1:\\n'\n",
      " '[스파르타코딩클럽] 12강. 지도학습 : 분류모델 - 나이브베이즈📘[SCC] 바닥부터 시작하는 머신러닝/📚[스파르타코딩클럽] 바닥부터 '\n",
      " '시작하는 머신러닝 - 3주차/📕[스파르타코딩클럽] 12강. 지도학습 : 분류모델 - 나이브베이즈Made with📕[스파르타코딩클럽] '\n",
      " '12강. 지도학습 : 분류모델 - 나이브베이즈[수업 목표]분류모델중 나이브베이즈에 대한')\n",
      "('\\n'\n",
      " 'Chunk 2:\\n'\n",
      " '목표]분류모델중 나이브베이즈에 대한 개념을 배우고, 데이터를 이용해 실습해 봅니다[목차]01. 나이브베이즈 개념02. 나이브베이즈 '\n",
      " '실습import pandas as pd')\n",
      "('\\n'\n",
      " 'Chunk 3:\\n'\n",
      " 'from sklearn.datasets import load_breast_cancer\\n'\n",
      " 'from sklearn.model_selection import train_test_split\\n'\n",
      " 'from sklearn.preprocessing import StandardScaler')\n",
      "('\\n'\n",
      " 'Chunk 4:\\n'\n",
      " '# 데이터 로드\\n'\n",
      " 'data = load_breast_cancer()\\n'\n",
      " 'X = data.data\\n'\n",
      " 'y = data.target')\n",
      "('\\n'\n",
      " 'Chunk 5:\\n'\n",
      " '# 데이터 분할\\n'\n",
      " 'X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, '\n",
      " 'random_state=42)\\n'\n",
      " '# 데이터 스케일링\\n'\n",
      " 'scaler = StandardScaler()\\n'\n",
      " 'X_train = scaler.fit_transform(X_train)')\n",
      "'\\nChunk 6:\\nX_test = scaler.transform(X_test)'\n",
      "('\\n'\n",
      " 'Chunk 7:\\n'\n",
      " '\\u200bsklearn.datasets.load_breast_cancer: 유방암 데이터셋 로드return_X_y=False: 데이터와 '\n",
      " '타겟을 함께 반환할지 여부. 기본값은 False입니다._X_y=False: 데이터와 타겟을 함께 반환할지 여부. 기본값은 '\n",
      " 'False입니다.\\ufeff\\u200bsklearn.model_selection.train_test_split: 데이터를')\n",
      "('\\n'\n",
      " 'Chunk 8:\\n'\n",
      " '데이터를 훈련 세트/ 테스트 세트로 분할test_size=0.2: 테스트 세트의 비율을 0.2로 설정합니다._size=0.2: 테스트 '\n",
      " '세트의 비율을 0.2로 설정합니다.\\ufeff\\u200brandom_state=42: 랜덤 시드 값으로, 데이터 분할의 재현성을 위해 '\n",
      " '사용됩니다._state=42: 랜덤 시드 값으로, 데이터 분할의 재현성을 위해')\n",
      "('\\n'\n",
      " 'Chunk 9:\\n'\n",
      " '데이터 분할의 재현성을 위해 사용됩니다.\\ufeff\\u200bsklearn.preprocessing.StandardScaler: 데이터의 '\n",
      " '평균을 0, 분산을 1로 스케일링fit_transform(X_train): 훈련 세트를 스케일링하고 '\n",
      " '변환합니다._transform(X_train): 훈련 세트를 스케일링하고 '\n",
      " '변환합니다.\\ufeff\\u200btransform(X_test): 테스트 세트를')\n",
      "('\\n'\n",
      " 'Chunk 10:\\n'\n",
      " '테스트 세트를 변환합니다.(X_test): 테스트 세트를 변환합니다.\\ufeff\\u200b☑️ 모델 학습모델 학습 {5px}모델 학습 '\n",
      " '\\ufeff\\u200bPythonCopyfrom sklearn.naive_bayes import GaussianNB')\n",
      "Top 10 chunks:\n",
      "('\\n'\n",
      " 'Chunk 1:\\n'\n",
      " '[스파르타코딩클럽] 13강. 지도학습 : 분류모델 - 의사결정나무📘[SCC] 바닥부터 시작하는 머신러닝/📚[스파르타코딩클럽] 바닥부터 '\n",
      " '시작하는 머신러닝 - 3주차/📕[스파르타코딩클럽] 13강. 지도학습 : 분류모델 - 의사결정나무Made with📕[스파르타코딩클럽] '\n",
      " '13강. 지도학습 : 분류모델 - 의사결정나무[수업 목표]분류모델중 의사결정나무에 대한')\n",
      "('\\n'\n",
      " 'Chunk 2:\\n'\n",
      " '목표]분류모델중 의사결정나무에 대한 개념을 배우고, 데이터를 이용해 실습해 봅니다[목차]01. 의사결정나무 개념02. 의사결정나무 '\n",
      " '실습import pandas as pd')\n",
      "('\\n'\n",
      " 'Chunk 3:\\n'\n",
      " 'from sklearn.datasets import load_breast_cancer\\n'\n",
      " 'from sklearn.model_selection import train_test_split\\n'\n",
      " 'from sklearn.preprocessing import StandardScaler')\n",
      "('\\n'\n",
      " 'Chunk 4:\\n'\n",
      " '# 데이터 로드\\n'\n",
      " 'data = load_breast_cancer()\\n'\n",
      " 'X = data.data\\n'\n",
      " 'y = data.target')\n",
      "('\\n'\n",
      " 'Chunk 5:\\n'\n",
      " '# 데이터 분할\\n'\n",
      " 'X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, '\n",
      " 'random_state=42)\\n'\n",
      " '# 데이터 스케일링\\n'\n",
      " 'scaler = StandardScaler()\\n'\n",
      " 'X_train = scaler.fit_transform(X_train)')\n",
      "'\\nChunk 6:\\nX_test = scaler.transform(X_test)'\n",
      "('\\n'\n",
      " 'Chunk 7:\\n'\n",
      " '\\u200bsklearn.datasets.load_breast_cancer: 유방암 데이터셋 로드return_X_y=False: 데이터와 '\n",
      " '타겟을 함께 반환할지 여부. 기본값은 False입니다._X_y=False: 데이터와 타겟을 함께 반환할지 여부. 기본값은 '\n",
      " 'False입니다.\\ufeff\\u200bsklearn.model_selection.train_test_split: 데이터를')\n",
      "('\\n'\n",
      " 'Chunk 8:\\n'\n",
      " '데이터를 훈련 세트/ 테스트 세트로 분할test_size=0.2: 테스트 세트의 비율을 0.2로 설정합니다._size=0.2: 테스트 '\n",
      " '세트의 비율을 0.2로 설정합니다.\\ufeff\\u200brandom_state=42: 랜덤 시드 값으로, 데이터 분할의 재현성을 위해 '\n",
      " '사용됩니다._state=42: 랜덤 시드 값으로, 데이터 분할의 재현성을 위해')\n",
      "('\\n'\n",
      " 'Chunk 9:\\n'\n",
      " '데이터 분할의 재현성을 위해 사용됩니다.\\ufeff\\u200bsklearn.preprocessing.StandardScaler: 데이터의 '\n",
      " '평균을 0, 분산을 1로 스케일링fit_transform(X_train): 훈련 세트를 스케일링하고 '\n",
      " '변환합니다._transform(X_train): 훈련 세트를 스케일링하고 '\n",
      " '변환합니다.\\ufeff\\u200btransform(X_test): 테스트 세트를')\n",
      "('\\n'\n",
      " 'Chunk 10:\\n'\n",
      " '테스트 세트를 변환합니다.(X_test): 테스트 세트를 변환합니다.\\ufeff\\u200b☑️ 모델 학습모델 학습 {5px}모델 학습 '\n",
      " '\\ufeff\\u200bPythonCopyfrom sklearn.tree import DecisionTreeClassifier')\n",
      "Top 10 chunks:\n",
      "('\\n'\n",
      " 'Chunk 1:\\n'\n",
      " '[스파르타코딩클럽] 14강. 비지도학습 : 군집화모델 - k-means clustering📘[SCC] 바닥부터 시작하는 '\n",
      " '머신러닝/📚[스파르타코딩클럽] 바닥부터 시작하는 머신러닝 - 4주차 /📕[스파르타코딩클럽] 14강. 비지도학습 : 군집화모델 - '\n",
      " 'k-means clusteringMade with📕[스파르타코딩클럽] 14강. 비지도학습 : 군집화모델 -')\n",
      "('\\n'\n",
      " 'Chunk 2:\\n'\n",
      " '비지도학습 : 군집화모델 - k-means clustering[수업 목표]비지도학습 군집화모델 중 k-means clustering 에 '\n",
      " '대한 개념을 배우고, 데이터를 이용해 실습해 봅니다[목차]01. k-means clustering 개념02. k-means '\n",
      " 'clustering 실습import pandas as pd')\n",
      "('\\n'\n",
      " 'Chunk 3:\\n'\n",
      " 'from sklearn.preprocessing import StandardScaler\\n'\n",
      " 'from sklearn.cluster import KMeans\\n'\n",
      " 'import matplotlib.pyplot as plt\\n'\n",
      " 'import seaborn as sns')\n",
      "('\\n'\n",
      " 'Chunk 4:\\n'\n",
      " '# 데이터 로드\\n'\n",
      " \"data = pd.read_csv('Mall_Customers.csv')\\n\"\n",
      " '# 필요한 열 선택 및 결측값 처리\\n'\n",
      " \"data = data[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']]\\n\"\n",
      " '# 데이터 스케일링\\n'\n",
      " 'scaler = StandardScaler()')\n",
      "'\\nChunk 5:\\ndata_scaled = scaler.fit_transform(data)'\n",
      "('\\n'\n",
      " 'Chunk 6:\\n'\n",
      " '\\u200bpandas.read_csv: CSV 파일을 로드하여 데이터프레임 생성’Mall_Customers.csv’: 로드할 파일의 '\n",
      " '경로입니다.’Mall_Customers.csv’: 로드할 파일의 '\n",
      " '경로입니다.\\ufeff\\u200bpandas.DataFrame.dropna: 결측값이 있는 행을 '\n",
      " '제거합니다.pandas.DataFrame.map: 데이터 값을 다른 값으로')\n",
      "('\\n'\n",
      " 'Chunk 7:\\n'\n",
      " '데이터 값을 다른 값으로 매핑합니다.sklearn.preprocessing.StandardScaler: 데이터의 평균을 0, 분산을 1로 '\n",
      " '스케일링fit_transform(X_train): 훈련 세트를 스케일링하고 변환합니다._transform(X_train): 훈련 세트를 '\n",
      " '스케일링하고 변환합니다.\\ufeff\\u200b☑️ 모델 학습 및 군집화모델 학습 및 군집화')\n",
      "('\\n'\n",
      " 'Chunk 8:\\n'\n",
      " '학습 및 군집화모델 학습 및 군집화 {5px}모델 학습 및 군집화 \\ufeff\\u200bPythonCopy# 최적의 k 찾기 (엘보우 '\n",
      " '방법)')\n",
      "('\\n'\n",
      " 'Chunk 9:\\n'\n",
      " 'inertia = []\\n'\n",
      " 'K = range(1, 11)\\n'\n",
      " 'for k in K:\\n'\n",
      " '    kmeans = KMeans(n_clusters=k, random_state=42)\\n'\n",
      " '    kmeans.fit(data_scaled)\\n'\n",
      " '    inertia.append(kmeans.inertia_)\\n'\n",
      " '# 엘보우 그래프 그리기\\n'\n",
      " 'plt.figure(figsize=(10, 8))')\n",
      "('\\n'\n",
      " 'Chunk 10:\\n'\n",
      " \"plt.plot(K, inertia, 'bx-')\\n\"\n",
      " \"plt.xlabel('k')\\n\"\n",
      " \"plt.ylabel('Inertia')\\n\"\n",
      " \"plt.title('Elbow Method For Optimal k')\\n\"\n",
      " 'plt.show()\\n'\n",
      " '# k=5로 모델 생성 및 학습\\n'\n",
      " 'kmeans = KMeans(n_clusters=5, random_state=42)')\n",
      "Top 10 chunks:\n",
      "('\\n'\n",
      " 'Chunk 1:\\n'\n",
      " '[스파르타코딩클럽] 15강. 비지도학습 : 군집화모델 - 계층적 군집화📘[SCC] 바닥부터 시작하는 머신러닝/📚[스파르타코딩클럽] '\n",
      " '바닥부터 시작하는 머신러닝 - 4주차 /📕[스파르타코딩클럽] 15강. 비지도학습 : 군집화모델 - 계층적 군집화Made '\n",
      " 'with📕[스파르타코딩클럽] 15강. 비지도학습 : 군집화모델 - 계층적 군집화[수업 목표]비지도학습')\n",
      "('\\n'\n",
      " 'Chunk 2:\\n'\n",
      " '계층적 군집화[수업 목표]비지도학습 군집화모델 중 계층적 군집화 에 대한 개념을 배우고, 데이터를 이용해 실습해 봅니다[목차]01. '\n",
      " '계층적 군집화 개념02. 계층적 군집화 실습병합 군집화: 각 데이터 포인트를 개별 군집으로 시작하여, 가장 가까운 군집을 반복적으로 '\n",
      " '병합합니다.병합 군집화: 각 데이터 포인트를 개별 군집으로 시작하여, 가장 가까운 군집을')\n",
      "('\\n'\n",
      " 'Chunk 3:\\n'\n",
      " '시작하여, 가장 가까운 군집을 반복적으로 병합합니다.\\ufeff\\u200b분할 군집화: 모든 데이터 포인트를 하나의 군집으로 시작하여, '\n",
      " '반복적으로 가장 멀리 떨어진 군집을 분할합니다.분할 군집화: 모든 데이터 포인트를 하나의 군집으로 시작하여, 반복적으로 가장 멀리 떨어진 '\n",
      " '군집을 분할합니다.\\ufeff\\u200b☑️ 계층적 군집화의 작동 원리거리 행렬 계산: 데이터 포인트 간의 거리를')\n",
      "('\\n'\n",
      " 'Chunk 4:\\n'\n",
      " '계산: 데이터 포인트 간의 거리를 계산하여 거리 행렬을 만듭니다.군집 병합/분할: 거리 행렬을 기반으로 가장 가까운 군집을 병합하거나, '\n",
      " '가장 멀리 떨어진 군집을 분할합니다.덴드로그램 생성: 군집화 과정을 시각화한 덴드로그램을 생성합니다.2) 병합 군집화 vs 분할 '\n",
      " '군집화☑️ 병합 군집화(Agglomerative Clustering)병합 군집화는 각 데이터')\n",
      "('\\n'\n",
      " 'Chunk 5:\\n'\n",
      " '군집화는 각 데이터 포인트를 개별 군집으로 시작하여, 가장 가까운 군집을 반복적으로 병합합니다.병합 군집화의 특징은 아래와 '\n",
      " '같습니다단순성: 구현이 비교적 간단합니다.계산 비용: 데이터 포인트 수가 많아질수록 계산 비용이 증가합니다.덴드로그램: 군집화 과정을 '\n",
      " '시각화한 덴드로그램을 생성할 수 있습니다.☑️ 분할 군집화(Divisive Clustering)분할')\n",
      "('\\n'\n",
      " 'Chunk 6:\\n'\n",
      " 'Clustering)분할 군집화는 모든 데이터 포인트를 하나의 군집으로 시작하여, 반복적으로 가장 멀리 떨어진 군집을 분할합니다. 분할 '\n",
      " '군집화의 주요 특징은 다음과 같습니다:상대적으로 복잡함: 병합 군집화보다 구현이 상대적으로 복잡할 수 있습니다.효율성: 큰 데이터셋에서 '\n",
      " '병합 군집화보다 효율적일 수 있습니다.덴드로그램: 군집화 과정을 시각화한 덴드로그램을')\n",
      "('\\n'\n",
      " 'Chunk 7:\\n'\n",
      " '군집화 과정을 시각화한 덴드로그램을 생성할 수 있습니다.02. 계층적 군집화 실습✔️Kaggle 쇼핑몰 데이터를 이용하여 계층적 군집화 '\n",
      " '실습을 진행합니다1) 쇼핑몰 데이터☑️ 데이터셋 다운로드Kaggle에서 \"Mall_Customers.csv\" 파일을 '\n",
      " '다운로드합니다.다운로드한 파일을 작업 디렉토리에 저장합니다.☑️ 데이터 로드 및 전처리쇼핑몰 데이터 로드')\n",
      "('\\n'\n",
      " 'Chunk 8:\\n'\n",
      " '로드 및 전처리쇼핑몰 데이터 로드 및 전처리 {5px}쇼핑몰 데이터 로드 및 전처리 \\ufeff\\u200bPythonCopyimport '\n",
      " 'pandas as pd')\n",
      "('\\n'\n",
      " 'Chunk 9:\\n'\n",
      " 'import numpy as np\\n'\n",
      " 'import matplotlib.pyplot as plt\\n'\n",
      " 'from sklearn.preprocessing import StandardScaler\\n'\n",
      " 'from sklearn.cluster import AgglomerativeClustering\\n'\n",
      " 'import scipy.cluster.hierarchy as sch')\n",
      "('\\n'\n",
      " 'Chunk 10:\\n'\n",
      " '# 데이터셋 불러오기\\n'\n",
      " \"df = pd.read_csv('Mall_Customers.csv')\\n\"\n",
      " '# 데이터 확인\\n'\n",
      " 'print(df.head())\\n'\n",
      " '# 필요한 열만 선택\\n'\n",
      " \"X = df[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']]\\n\"\n",
      " '# 데이터 정규화\\n'\n",
      " 'scaler = StandardScaler()')\n",
      "Top 10 chunks:\n",
      "('\\n'\n",
      " 'Chunk 1:\\n'\n",
      " '[스파르타코딩클럽] 16강. 비지도학습 : 군집화모델 - DBSCAN📘[SCC] 바닥부터 시작하는 머신러닝/📚[스파르타코딩클럽] 바닥부터 '\n",
      " '시작하는 머신러닝 - 4주차 /📕[스파르타코딩클럽] 16강. 비지도학습 : 군집화모델 - DBSCANMade with📕[스파르타코딩클럽] '\n",
      " '16강. 비지도학습 : 군집화모델 - DBSCAN[수업 목표]비지도학습 군집화모델')\n",
      "('\\n'\n",
      " 'Chunk 2:\\n'\n",
      " '목표]비지도학습 군집화모델 중 DBSCAN 에 대한 개념을 배우고, 데이터를 이용해 실습해 봅니다[목차]01. DBSCAN 개념02. '\n",
      " 'DBSCAN 실습eps: 두 데이터 포인트가 같은 군집에 속하기 위해 가져야 하는 최대 거리입니다.: 두 데이터 포인트가 같은 군집에 '\n",
      " '속하기 위해 가져야 하는 최대 거리입니다.\\ufeff\\u200bmin_samples: 한 군집을 형성하기')\n",
      "('\\n'\n",
      " 'Chunk 3:\\n'\n",
      " '한 군집을 형성하기 위해 필요한 최소 데이터 포인트 수입니다_samples: 한 군집을 형성하기 위해 필요한 최소 데이터 포인트 '\n",
      " '수입니다\\ufeff\\u200b☑️ DBSCAN의 작동 원리임의의 데이터 포인트를 선택합니다.선택한 데이터 포인트의 eps 반경 내에 있는 '\n",
      " '모든 데이터 포인트를 찾습니다.eps 반경 내의 데이터수 ≥ min_samples : 해당 데이터 포인트를')\n",
      "('\\n'\n",
      " 'Chunk 4:\\n'\n",
      " ': 해당 데이터 포인트를 중심으로 새로운 군집 형성.eps 반경 내의 데이터수 < min_samples : 해당 데이터 포인트를 노이즈로 '\n",
      " '간주군집에 속한 데이터 포인트에 대해 2~4 단계를 반복합니다.모든 데이터 포인트가 처리될 때까지 이 과정을 반복합니다.☑️ '\n",
      " 'DBSCAN의 장점비구형 군집 탐지: DBSCAN은 비구형 군집을 탐지할 수 있습니다.노이즈')\n",
      "('\\n'\n",
      " 'Chunk 5:\\n'\n",
      " '군집을 탐지할 수 있습니다.노이즈 처리: DBSCAN은 노이즈를 효과적으로 처리할 수 있습니다.군집 수 자동 결정: DBSCAN은 군집 '\n",
      " '수를 사전에 지정할 필요가 없습니다.02. DBSCAN 실습✔️Kaggle 쇼핑몰 데이터를 이용하여 DBSCAN 실습을 진행합니다1) '\n",
      " '쇼핑몰 데이터☑️ 데이터 로드 쇼핑몰 데이터 로드 {5px}쇼핑몰 데이터 로드')\n",
      "'\\nChunk 6:\\n로드 {5px}쇼핑몰 데이터 로드 \\ufeff\\u200bPythonCopyimport pandas as pd'\n",
      "('\\n'\n",
      " 'Chunk 7:\\n'\n",
      " 'import numpy as np\\n'\n",
      " 'import matplotlib.pyplot as plt\\n'\n",
      " 'from sklearn.preprocessing import StandardScaler\\n'\n",
      " 'from sklearn.cluster import AgglomerativeClustering\\n'\n",
      " 'import scipy.cluster.hierarchy as sch')\n",
      "('\\n'\n",
      " 'Chunk 8:\\n'\n",
      " '# 데이터셋 불러오기\\n'\n",
      " \"df = pd.read_csv('Mall_Customers.csv')\\n\"\n",
      " '# 데이터 확인\\n'\n",
      " 'print(df.head())\\n'\n",
      " '# 필요한 열만 선택\\n'\n",
      " \"X = df[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']]\\n\"\n",
      " '# 데이터 정규화\\n'\n",
      " 'scaler = StandardScaler()')\n",
      "('\\n'\n",
      " 'Chunk 9:\\n'\n",
      " 'X_scaled = scaler.fit_transform(X)\\n'\n",
      " '\\u200b☑️ DBSCAN 수행Scikit-learn의 DBSCAN을 사용하여 DBSCAN 군집화를 수행합니다.DBSCAN수행 '\n",
      " '{5px}DBSCAN수행 \\ufeff\\u200bPythonCopyfrom sklearn.cluster import DBSCAN\\n'\n",
      " 'import matplotlib.pyplot as plt')\n",
      "'\\nChunk 10:\\nimport seaborn as sns'\n"
     ]
    }
   ],
   "source": [
    "ml_retriever_list = []\n",
    "\n",
    "for idx, text in enumerate(text_list, start=1):\n",
    "    identifier = f\"ml_doc_{idx}\"  # 고유 파일명 생성 (예: doc_1, doc_2, ...)\n",
    "    retriever = get_retriever(text, identifier)\n",
    "    retriever_list.append(retriever)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DebugPassThrough(RunnablePassthrough):\n",
    "    def invoke(self, *args, **kwargs):\n",
    "        output = super().invoke(*args, **kwargs)\n",
    "        print(\"Debug Output:\", output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt 및 Chain 구성\n",
    "class ContextToText(RunnablePassthrough):\n",
    "    def invoke(self, inputs, config=None, **kwargs):  # config 인수 추가\n",
    "        # context의 각 문서를 문자열로 결합\n",
    "        context_text = \" \".join([doc.page_content for doc in inputs[\"context\"]])\n",
    "        print(f\"Context output: {context_text}\")\n",
    "        return {\"context\": context_text, \"quiz_list\": inputs[\"quiz_list\"]}\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"\n",
    "    당신은 딥러닝을 가르치는 AI 강사입니다.\n",
    "    아래의 {context} 바탕으로만 한국말로 된 하나의 질문을 생성해주세요.\n",
    "    (최대한 코드에 관한 시나리오적 질문이면 더 좋습니다.)\n",
    "     \n",
    "    [중요]\n",
    "    이전에 만들었던 질문과 유사한 질문은 절대 생성하지 마세요.\n",
    "    주관적인 변수에 대한 의견을 묻는 질문은 피해야 합니다.\n",
    "    예를 들어, \"타이타닉 데이터셋의 'embarked' 열을 숫자로 매핑할 때, 'S'는 어떤 숫자로 매핑되나요?\"와 같은 질문은 피해야 합니다.\n",
    "\n",
    "    아래의 제약 조건과 출제 방식에 맞춘 질문을 생성해주세요.\n",
    "     \n",
    "    제약 조건:\n",
    "    1. \"Context\"에서 제공된 내용만 기반으로 질문을 생성하세요.\n",
    "    2. AI 관련 내용이 아닌 질문은 생성하지 마세요\n",
    "    3. \"QuizList\"에 이미 있는 질문과 유사하지 않은 새로운 질문을 생성하세요.\n",
    "\n",
    "    출제 방식:\n",
    "    - 질문은 반드시 보기가 있는 객관식(MCQ) 또는 O,X 형태로 출제하세요.\n",
    "    - \"Context\"에 명시적으로 언급된 개념, 정의, 또는 내용을 활용하세요.\n",
    "    \n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    QuizList:\n",
    "    {quiz_list}\n",
    "     \n",
    "    \n",
    "\n",
    "    \"\"\")\n",
    "])\n",
    "\n",
    "# 아래의 금지리스트들과 유사한 질문을 절대 생성하지 마시오.\n",
    "#     금지리스트: {quiz_list}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG Chain 생성 함수\n",
    "def create_rag_chain(retriever):\n",
    "    return (\n",
    "        {\n",
    "            \"context\": retriever,\n",
    "            \"quiz_list\": DebugPassThrough()\n",
    "        }\n",
    "        | DebugPassThrough()  # DebugPassThrough()가 실제로 어떤 역할을 하는지 확인\n",
    "        | ContextToText()     # Text 변환을 위한 ContextToText\n",
    "        | prompt              # prompt 사용\n",
    "        | llm                 # LLM 호출\n",
    "        | StrOutputParser()   # 출력 파서\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 동작부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest 5 Quizzes: []\n",
      "Debug Output: \n",
      "Debug Output: {'context': [Document(metadata={}, page_content='print(f\"Confusion Matrix:\")\\n\\u200bCopyright ⓒ TeamSparta All rights reserved.'), Document(metadata={}, page_content='print(f\"Confusion Matrix:\")'), Document(metadata={}, page_content='숫자로 매핑합니다.\\ufeff\\u200b☑️ 모델 학습모델 학습 {5px}모델 학습 \\ufeff\\u200bPythonCopy# 모델 생성 및 학습'), Document(metadata={}, page_content=': 분류모델의 로지스틱 회귀에 대해 알아보고 실습을 통해 배워봅시다[목차]01. 로지스틱 회귀 개념02. 로지스틱 회귀분석 실습import pandas as pd'), Document(metadata={}, page_content='# 데이터 로드\\ndata = load_breast_cancer()\\nX = data.data\\ny = data.target'), Document(metadata={}, page_content='from sklearn.datasets import load_breast_cancer\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import StandardScaler'), Document(metadata={}, page_content='[스파르타코딩클럽] 9강. 지도학습 : 분류모델 - 로지스틱 회귀📘[SCC] 바닥부터 시작하는 머신러닝/📚[스파르타코딩클럽] 바닥부터 시작하는 머신러닝 - 3주차/📕[스파르타코딩클럽] 9강. 지도학습 : 분류모델 - 로지스틱 회귀Made with📕[스파르타코딩클럽] 9강. 지도학습 : 분류모델 - 로지스틱 회귀[수업 목표]지도학습 : 분류모델의 로지스틱'), Document(metadata={}, page_content='행렬을 반환합니다.\\ufeff\\u200b2) 타이타닉 데이터☑️ 데이터 로드 및 전처리타이타닉 데이터 로드 및 전처리 {5px}타이타닉 데이터 로드 및 전처리 \\ufeff\\u200bPythonCopyimport seaborn as sns')], 'quiz_list': ''}\n",
      "Context output: print(f\"Confusion Matrix:\")\n",
      "​Copyright ⓒ TeamSparta All rights reserved. print(f\"Confusion Matrix:\") 숫자로 매핑합니다.﻿​☑️ 모델 학습모델 학습 {5px}모델 학습 ﻿​PythonCopy# 모델 생성 및 학습 : 분류모델의 로지스틱 회귀에 대해 알아보고 실습을 통해 배워봅시다[목차]01. 로지스틱 회귀 개념02. 로지스틱 회귀분석 실습import pandas as pd # 데이터 로드\n",
      "data = load_breast_cancer()\n",
      "X = data.data\n",
      "y = data.target from sklearn.datasets import load_breast_cancer\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler [스파르타코딩클럽] 9강. 지도학습 : 분류모델 - 로지스틱 회귀📘[SCC] 바닥부터 시작하는 머신러닝/📚[스파르타코딩클럽] 바닥부터 시작하는 머신러닝 - 3주차/📕[스파르타코딩클럽] 9강. 지도학습 : 분류모델 - 로지스틱 회귀Made with📕[스파르타코딩클럽] 9강. 지도학습 : 분류모델 - 로지스틱 회귀[수업 목표]지도학습 : 분류모델의 로지스틱 행렬을 반환합니다.﻿​2) 타이타닉 데이터☑️ 데이터 로드 및 전처리타이타닉 데이터 로드 및 전처리 {5px}타이타닉 데이터 로드 및 전처리 ﻿​PythonCopyimport seaborn as sns\n",
      "Cache 길이 : 0\n",
      "Quiz : **질문:** 로지스틱 회귀 모델을 사용하여 유방암 데이터를 학습시키기 위해 필요한 데이터 전처리 단계 중 하나로, 데이터를 스케일링하기 위해 어떤 모듈을 사용하나요?\n",
      "\n",
      "a) sklearn.preprocessing.StandardScaler  \n",
      "b) pandas.DataFrame  \n",
      "c) seaborn.sns  \n",
      "d) sklearn.model_selection.train_test_split  \n",
      "\n",
      "**정답:** a) sklearn.preprocessing.StandardScaler\n",
      "TEXT 파일 저장 완료: quiz_list_1.txt\n",
      "Feedback:\n",
      "AIMessage(content='- 정답 여부: \"예\"\\n- 추가 설명: 로지스틱 회귀 모델을 훈련할 때, 특히 데이터의 특성들이 서로 다른 스케일을 가질 경우 데이터를 스케일링하는 것이 중요합니다. `sklearn.preprocessing.StandardScaler`는 각 특성(feature)을 평균이 0이고 분산이 1이 되도록 표준화(scaling)하는 도구입니다. 이는 모델의 훈련 속도를 높이고, 과적합을 방지하며 더 나은 성능을 얻을 수 있게 합니다. 다른 옵션들은 데이터 스케일링에 적합하지 않습니다. 예를 들어, `pandas.DataFrame`은 데이터 구조를 정의하는 라이브러리이고, `seaborn.sns`는 데이터 시각화 라이브러리이며, `sklearn.model_selection.train_test_split`는 데이터를 훈련 및 테스트 세트로 나누는 데 사용됩니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 198, 'prompt_tokens': 185, 'total_tokens': 383, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_831e067d82', 'finish_reason': 'stop', 'logprobs': None}, id='run-50bd79c8-ef70-4eeb-b6d1-11cbfef6e01a-0', usage_metadata={'input_tokens': 185, 'output_tokens': 198, 'total_tokens': 383, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})\n",
      "Latest 5 Quizzes: [\"[ChatGeneration(text='**질문:** 로지스틱 회귀 모델을 사용하여 유방암 데이터를 학습시키기 위해 필요한 데이터 전처리 단계 중 하나로, 데이터를 스케일링하기 위해 어떤 모듈을 사용하나요?\\\\n\\\\na) sklearn.preprocessing.StandardScaler  \\\\nb) pandas.DataFrame  \\\\nc) seaborn.sns  \\\\nd) sklearn.model_selection.train_test_split  \\\\n\\\\n**정답:** a) sklearn.preprocessing.StandardScaler', generation_info={'finish_reason': 'stop', 'logprobs': None}, message=AIMessage(content='**질문:** 로지스틱 회귀 모델을 사용하여 유방암 데이터를 학습시키기 위해 필요한 데이터 전처리 단계 중 하나로, 데이터를 스케일링하기 위해 어떤 모듈을 사용하나요?\\\\n\\\\na) sklearn.preprocessing.StandardScaler  \\\\nb) pandas.DataFrame  \\\\nc) seaborn.sns  \\\\nd) sklearn.model_selection.train_test_split  \\\\n\\\\n**정답:** a) sklearn.preprocessing.StandardScaler', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 88, 'prompt_tokens': 1056, 'total_tokens': 1144, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_831e067d82', 'finish_reason': 'stop', 'logprobs': None}, id='run-ad56ff5a-af84-4bd7-a11c-a80ab3db26ca-0', usage_metadata={'input_tokens': 1056, 'output_tokens': 88, 'total_tokens': 1144, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}))]\", '[ChatGeneration(text=\\'- 정답 여부: \"예\"\\\\n- 추가 설명: 로지스틱 회귀 모델을 훈련할 때, 특히 데이터의 특성들이 서로 다른 스케일을 가질 경우 데이터를 스케일링하는 것이 중요합니다. `sklearn.preprocessing.StandardScaler`는 각 특성(feature)을 평균이 0이고 분산이 1이 되도록 표준화(scaling)하는 도구입니다. 이는 모델의 훈련 속도를 높이고, 과적합을 방지하며 더 나은 성능을 얻을 수 있게 합니다. 다른 옵션들은 데이터 스케일링에 적합하지 않습니다. 예를 들어, `pandas.DataFrame`은 데이터 구조를 정의하는 라이브러리이고, `seaborn.sns`는 데이터 시각화 라이브러리이며, `sklearn.model_selection.train_test_split`는 데이터를 훈련 및 테스트 세트로 나누는 데 사용됩니다.\\', generation_info={\\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, message=AIMessage(content=\\'- 정답 여부: \"예\"\\\\n- 추가 설명: 로지스틱 회귀 모델을 훈련할 때, 특히 데이터의 특성들이 서로 다른 스케일을 가질 경우 데이터를 스케일링하는 것이 중요합니다. `sklearn.preprocessing.StandardScaler`는 각 특성(feature)을 평균이 0이고 분산이 1이 되도록 표준화(scaling)하는 도구입니다. 이는 모델의 훈련 속도를 높이고, 과적합을 방지하며 더 나은 성능을 얻을 수 있게 합니다. 다른 옵션들은 데이터 스케일링에 적합하지 않습니다. 예를 들어, `pandas.DataFrame`은 데이터 구조를 정의하는 라이브러리이고, `seaborn.sns`는 데이터 시각화 라이브러리이며, `sklearn.model_selection.train_test_split`는 데이터를 훈련 및 테스트 세트로 나누는 데 사용됩니다.\\', additional_kwargs={\\'refusal\\': None}, response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 198, \\'prompt_tokens\\': 185, \\'total_tokens\\': 383, \\'completion_tokens_details\\': {\\'accepted_prediction_tokens\\': 0, \\'audio_tokens\\': 0, \\'reasoning_tokens\\': 0, \\'rejected_prediction_tokens\\': 0}, \\'prompt_tokens_details\\': {\\'audio_tokens\\': 0, \\'cached_tokens\\': 0}}, \\'model_name\\': \\'gpt-4o-2024-08-06\\', \\'system_fingerprint\\': \\'fp_831e067d82\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-50bd79c8-ef70-4eeb-b6d1-11cbfef6e01a-0\\', usage_metadata={\\'input_tokens\\': 185, \\'output_tokens\\': 198, \\'total_tokens\\': 383, \\'input_token_details\\': {\\'audio\\': 0, \\'cache_read\\': 0}, \\'output_token_details\\': {\\'audio\\': 0, \\'reasoning\\': 0}}))]']\n",
      "Debug Output: [ChatGeneration(text='**질문:** 로지스틱 회귀 모델을 사용하여 유방암 데이터를 학습시키기 위해 필요한 데이터 전처리 단계 중 하나로, 데이터를 스케일링하기 위해 어떤 모듈을 사용하나요?\\n\\na) sklearn.preprocessing.StandardScaler  \\nb) pandas.DataFrame  \\nc) seaborn.sns  \\nd) sklearn.model_selection.train_test_split  \\n\\n**정답:** a) sklearn.preprocessing.StandardScaler', generation_info={'finish_reason': 'stop', 'logprobs': None}, message=AIMessage(content='**질문:** 로지스틱 회귀 모델을 사용하여 유방암 데이터를 학습시키기 위해 필요한 데이터 전처리 단계 중 하나로, 데이터를 스케일링하기 위해 어떤 모듈을 사용하나요?\\n\\na) sklearn.preprocessing.StandardScaler  \\nb) pandas.DataFrame  \\nc) seaborn.sns  \\nd) sklearn.model_selection.train_test_split  \\n\\n**정답:** a) sklearn.preprocessing.StandardScaler', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 88, 'prompt_tokens': 1056, 'total_tokens': 1144, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_831e067d82', 'finish_reason': 'stop', 'logprobs': None}, id='run-ad56ff5a-af84-4bd7-a11c-a80ab3db26ca-0', usage_metadata={'input_tokens': 1056, 'output_tokens': 88, 'total_tokens': 1144, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}))] [ChatGeneration(text='- 정답 여부: \"예\"\\n- 추가 설명: 로지스틱 회귀 모델을 훈련할 때, 특히 데이터의 특성들이 서로 다른 스케일을 가질 경우 데이터를 스케일링하는 것이 중요합니다. `sklearn.preprocessing.StandardScaler`는 각 특성(feature)을 평균이 0이고 분산이 1이 되도록 표준화(scaling)하는 도구입니다. 이는 모델의 훈련 속도를 높이고, 과적합을 방지하며 더 나은 성능을 얻을 수 있게 합니다. 다른 옵션들은 데이터 스케일링에 적합하지 않습니다. 예를 들어, `pandas.DataFrame`은 데이터 구조를 정의하는 라이브러리이고, `seaborn.sns`는 데이터 시각화 라이브러리이며, `sklearn.model_selection.train_test_split`는 데이터를 훈련 및 테스트 세트로 나누는 데 사용됩니다.', generation_info={'finish_reason': 'stop', 'logprobs': None}, message=AIMessage(content='- 정답 여부: \"예\"\\n- 추가 설명: 로지스틱 회귀 모델을 훈련할 때, 특히 데이터의 특성들이 서로 다른 스케일을 가질 경우 데이터를 스케일링하는 것이 중요합니다. `sklearn.preprocessing.StandardScaler`는 각 특성(feature)을 평균이 0이고 분산이 1이 되도록 표준화(scaling)하는 도구입니다. 이는 모델의 훈련 속도를 높이고, 과적합을 방지하며 더 나은 성능을 얻을 수 있게 합니다. 다른 옵션들은 데이터 스케일링에 적합하지 않습니다. 예를 들어, `pandas.DataFrame`은 데이터 구조를 정의하는 라이브러리이고, `seaborn.sns`는 데이터 시각화 라이브러리이며, `sklearn.model_selection.train_test_split`는 데이터를 훈련 및 테스트 세트로 나누는 데 사용됩니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 198, 'prompt_tokens': 185, 'total_tokens': 383, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_831e067d82', 'finish_reason': 'stop', 'logprobs': None}, id='run-50bd79c8-ef70-4eeb-b6d1-11cbfef6e01a-0', usage_metadata={'input_tokens': 185, 'output_tokens': 198, 'total_tokens': 383, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}))]\n",
      "Debug Output: {'context': [Document(metadata={}, page_content='데이터 분할의 재현성을 위해 사용됩니다.\\ufeff\\u200bsklearn.preprocessing.StandardScaler: 데이터의 평균을 0, 분산을 1로 스케일링fit_transform(X_train): 훈련 세트를 스케일링하고 변환합니다._transform(X_train): 훈련 세트를 스케일링하고 변환합니다.\\ufeff\\u200btransform(X_test): 테스트 세트를'), Document(metadata={}, page_content='\\u200bseaborn.load_dataset: seaborn의 내장 데이터셋 로드’titanic’: 타이타닉 데이터셋을 로드합니다.’titanic’: 타이타닉 데이터셋을 로드합니다.\\ufeff\\u200b pandas.DataFrame.dropna: 결측값이 있는 행 제거pandas.DataFrame.map: 데이터 값을 다른 값으로 매핑’male’: 0, ’female’: 1:'), Document(metadata={}, page_content='데이터를 훈련 세트/ 테스트 세트로 분할test_size=0.2: 테스트 세트의 비율을 0.2로 설정합니다._size=0.2: 테스트 세트의 비율을 0.2로 설정합니다.\\ufeff\\u200brandom_state=42: 랜덤 시드 값으로, 데이터 분할의 재현성을 위해 사용됩니다._state=42: 랜덤 시드 값으로, 데이터 분할의 재현성을 위해'), Document(metadata={}, page_content=': 분류모델의 로지스틱 회귀에 대해 알아보고 실습을 통해 배워봅시다[목차]01. 로지스틱 회귀 개념02. 로지스틱 회귀분석 실습import pandas as pd'), Document(metadata={}, page_content='\\u200bsklearn.linear_model.LogisticRegression: 로지스틱 회귀 모델 생성fit(X_train, y_train): 모델을 훈련 세트에 맞추어 학습시킵니다.(X_train, y_train): 모델을 훈련 세트에 맞추어 학습시킵니다.\\ufeff\\u200bpredict(X_test): 테스트 세트에 대해 예측을 수행합니다.(X_test): 테스트 세트에'), Document(metadata={}, page_content='행렬을 반환합니다.\\ufeff\\u200b2) 타이타닉 데이터☑️ 데이터 로드 및 전처리타이타닉 데이터 로드 및 전처리 {5px}타이타닉 데이터 로드 및 전처리 \\ufeff\\u200bPythonCopyimport seaborn as sns'), Document(metadata={}, page_content='from sklearn.datasets import load_breast_cancer\\nfrom sklearn.model_selection import train_test_split\\nfrom sklearn.preprocessing import StandardScaler')], 'quiz_list': '[ChatGeneration(text=\\'**질문:** 로지스틱 회귀 모델을 사용하여 유방암 데이터를 학습시키기 위해 필요한 데이터 전처리 단계 중 하나로, 데이터를 스케일링하기 위해 어떤 모듈을 사용하나요?\\\\n\\\\na) sklearn.preprocessing.StandardScaler  \\\\nb) pandas.DataFrame  \\\\nc) seaborn.sns  \\\\nd) sklearn.model_selection.train_test_split  \\\\n\\\\n**정답:** a) sklearn.preprocessing.StandardScaler\\', generation_info={\\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, message=AIMessage(content=\\'**질문:** 로지스틱 회귀 모델을 사용하여 유방암 데이터를 학습시키기 위해 필요한 데이터 전처리 단계 중 하나로, 데이터를 스케일링하기 위해 어떤 모듈을 사용하나요?\\\\n\\\\na) sklearn.preprocessing.StandardScaler  \\\\nb) pandas.DataFrame  \\\\nc) seaborn.sns  \\\\nd) sklearn.model_selection.train_test_split  \\\\n\\\\n**정답:** a) sklearn.preprocessing.StandardScaler\\', additional_kwargs={\\'refusal\\': None}, response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 88, \\'prompt_tokens\\': 1056, \\'total_tokens\\': 1144, \\'completion_tokens_details\\': {\\'accepted_prediction_tokens\\': 0, \\'audio_tokens\\': 0, \\'reasoning_tokens\\': 0, \\'rejected_prediction_tokens\\': 0}, \\'prompt_tokens_details\\': {\\'audio_tokens\\': 0, \\'cached_tokens\\': 0}}, \\'model_name\\': \\'gpt-4o-2024-08-06\\', \\'system_fingerprint\\': \\'fp_831e067d82\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-ad56ff5a-af84-4bd7-a11c-a80ab3db26ca-0\\', usage_metadata={\\'input_tokens\\': 1056, \\'output_tokens\\': 88, \\'total_tokens\\': 1144, \\'input_token_details\\': {\\'audio\\': 0, \\'cache_read\\': 0}, \\'output_token_details\\': {\\'audio\\': 0, \\'reasoning\\': 0}}))] [ChatGeneration(text=\\'- 정답 여부: \"예\"\\\\n- 추가 설명: 로지스틱 회귀 모델을 훈련할 때, 특히 데이터의 특성들이 서로 다른 스케일을 가질 경우 데이터를 스케일링하는 것이 중요합니다. `sklearn.preprocessing.StandardScaler`는 각 특성(feature)을 평균이 0이고 분산이 1이 되도록 표준화(scaling)하는 도구입니다. 이는 모델의 훈련 속도를 높이고, 과적합을 방지하며 더 나은 성능을 얻을 수 있게 합니다. 다른 옵션들은 데이터 스케일링에 적합하지 않습니다. 예를 들어, `pandas.DataFrame`은 데이터 구조를 정의하는 라이브러리이고, `seaborn.sns`는 데이터 시각화 라이브러리이며, `sklearn.model_selection.train_test_split`는 데이터를 훈련 및 테스트 세트로 나누는 데 사용됩니다.\\', generation_info={\\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, message=AIMessage(content=\\'- 정답 여부: \"예\"\\\\n- 추가 설명: 로지스틱 회귀 모델을 훈련할 때, 특히 데이터의 특성들이 서로 다른 스케일을 가질 경우 데이터를 스케일링하는 것이 중요합니다. `sklearn.preprocessing.StandardScaler`는 각 특성(feature)을 평균이 0이고 분산이 1이 되도록 표준화(scaling)하는 도구입니다. 이는 모델의 훈련 속도를 높이고, 과적합을 방지하며 더 나은 성능을 얻을 수 있게 합니다. 다른 옵션들은 데이터 스케일링에 적합하지 않습니다. 예를 들어, `pandas.DataFrame`은 데이터 구조를 정의하는 라이브러리이고, `seaborn.sns`는 데이터 시각화 라이브러리이며, `sklearn.model_selection.train_test_split`는 데이터를 훈련 및 테스트 세트로 나누는 데 사용됩니다.\\', additional_kwargs={\\'refusal\\': None}, response_metadata={\\'token_usage\\': {\\'completion_tokens\\': 198, \\'prompt_tokens\\': 185, \\'total_tokens\\': 383, \\'completion_tokens_details\\': {\\'accepted_prediction_tokens\\': 0, \\'audio_tokens\\': 0, \\'reasoning_tokens\\': 0, \\'rejected_prediction_tokens\\': 0}, \\'prompt_tokens_details\\': {\\'audio_tokens\\': 0, \\'cached_tokens\\': 0}}, \\'model_name\\': \\'gpt-4o-2024-08-06\\', \\'system_fingerprint\\': \\'fp_831e067d82\\', \\'finish_reason\\': \\'stop\\', \\'logprobs\\': None}, id=\\'run-50bd79c8-ef70-4eeb-b6d1-11cbfef6e01a-0\\', usage_metadata={\\'input_tokens\\': 185, \\'output_tokens\\': 198, \\'total_tokens\\': 383, \\'input_token_details\\': {\\'audio\\': 0, \\'cache_read\\': 0}, \\'output_token_details\\': {\\'audio\\': 0, \\'reasoning\\': 0}}))]'}\n",
      "Context output: 데이터 분할의 재현성을 위해 사용됩니다.﻿​sklearn.preprocessing.StandardScaler: 데이터의 평균을 0, 분산을 1로 스케일링fit_transform(X_train): 훈련 세트를 스케일링하고 변환합니다._transform(X_train): 훈련 세트를 스케일링하고 변환합니다.﻿​transform(X_test): 테스트 세트를 ​seaborn.load_dataset: seaborn의 내장 데이터셋 로드’titanic’: 타이타닉 데이터셋을 로드합니다.’titanic’: 타이타닉 데이터셋을 로드합니다.﻿​ pandas.DataFrame.dropna: 결측값이 있는 행 제거pandas.DataFrame.map: 데이터 값을 다른 값으로 매핑’male’: 0, ’female’: 1: 데이터를 훈련 세트/ 테스트 세트로 분할test_size=0.2: 테스트 세트의 비율을 0.2로 설정합니다._size=0.2: 테스트 세트의 비율을 0.2로 설정합니다.﻿​random_state=42: 랜덤 시드 값으로, 데이터 분할의 재현성을 위해 사용됩니다._state=42: 랜덤 시드 값으로, 데이터 분할의 재현성을 위해 : 분류모델의 로지스틱 회귀에 대해 알아보고 실습을 통해 배워봅시다[목차]01. 로지스틱 회귀 개념02. 로지스틱 회귀분석 실습import pandas as pd ​sklearn.linear_model.LogisticRegression: 로지스틱 회귀 모델 생성fit(X_train, y_train): 모델을 훈련 세트에 맞추어 학습시킵니다.(X_train, y_train): 모델을 훈련 세트에 맞추어 학습시킵니다.﻿​predict(X_test): 테스트 세트에 대해 예측을 수행합니다.(X_test): 테스트 세트에 행렬을 반환합니다.﻿​2) 타이타닉 데이터☑️ 데이터 로드 및 전처리타이타닉 데이터 로드 및 전처리 {5px}타이타닉 데이터 로드 및 전처리 ﻿​PythonCopyimport seaborn as sns from sklearn.datasets import load_breast_cancer\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "Cache 길이 : 2\n",
      "Quiz : **질문:** 타이타닉 데이터셋을 로드하고 결측값이 있는 행을 제거하기 위해 어떤 pandas 메서드를 사용하나요?\n",
      "\n",
      "a) pandas.DataFrame.fillna  \n",
      "b) pandas.DataFrame.dropna  \n",
      "c) pandas.DataFrame.replace  \n",
      "d) pandas.DataFrame.drop_duplicates  \n",
      "\n",
      "**정답:** b) pandas.DataFrame.dropna\n",
      "TEXT 파일 저장 완료: quiz_list_2.txt\n",
      "대화를 종료합니다.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from langchain.globals import set_llm_cache, get_llm_cache\n",
    "from langchain.cache import InMemoryCache\n",
    "\n",
    "\n",
    "order = int(input(\"선택하실 딥러닝 챕터의 교재 번호를 입력해주세요! 종료를 원하신다면 0을 입력해주세요!\"))\n",
    "set_llm_cache(InMemoryCache()) # 캐시 메모리 설정\n",
    "llm_cache = get_llm_cache()\n",
    "j = 0\n",
    "\n",
    "while True: \n",
    "\n",
    "    if (order == 0):\n",
    "        break\n",
    "\n",
    "    latest_quizzes = []\n",
    "\n",
    "    # 캐시 데이터 추출 및 확인\n",
    "    if isinstance(llm_cache, InMemoryCache):\n",
    "        cached_data = list(llm_cache._cache.items())  # 캐시된 모든 데이터 가져오기 (키-값 쌍)\n",
    "        latest_quizzes = [str(value) for _, value in cached_data[-5:]]  # 최신 5개의 값을 문자열로 변환\n",
    "        print(\"Latest 5 Quizzes:\", latest_quizzes)\n",
    "    else:\n",
    "        print(\"현재 캐시가 InMemoryCache가 아닙니다.\")\n",
    "\n",
    "    cached_quiz_list = latest_quizzes\n",
    "    cached_quiz_str = \" \".join(cached_quiz_list)\n",
    "    rag_chain = create_rag_chain(retriever_list[order-1])\n",
    "    \n",
    "    try:\n",
    "        response = rag_chain.invoke(cached_quiz_str)\n",
    "    except Exception as e:\n",
    "        print(f\"Quiz 생성 중 오류 발생: {e}\")\n",
    "        break\n",
    "    \n",
    "    quiz = response\n",
    "    print(f\"Cache 길이 : {len(latest_quizzes)}\")\n",
    "    print(f\"Quiz : {quiz}\")\n",
    "    j+=1\n",
    "    save_file(''.join(str(quiz)), f\"quiz_list_{j}.txt\")\n",
    "    \n",
    "    \n",
    "    # 2. 사용자 답변 수집\n",
    "    user_answer = input(\"답변을 입력하세요 종료를 원하시면 exit을 입력해주세요.: \").strip()\n",
    "\n",
    "    if user_answer.strip().lower() == \"exit\":\n",
    "        print(\"대화를 종료합니다.\")\n",
    "        break\n",
    "    \n",
    "    if not user_answer:\n",
    "        print(\"답변이 비어 있습니다. 다시 입력해주세요.\")\n",
    "        continue\n",
    "\n",
    "    # 3. 사용자 답변에 대한 피드백 생성\n",
    "    feedback_prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", f\"\"\"\n",
    "    AI 강사로서 다음 퀴즈의 정답 여부를 확인하고 피드백을 제공하세요.\n",
    "    피드백은 아래와 같은 형식이어야 합니다:\n",
    "    \n",
    "    - 정답 여부: \"N번\" 또는 \"예/아니오\"\n",
    "    - 추가 설명: (정답과 관련된 추가 정보를 제공하세요)\n",
    "    \n",
    "    퀴즈 : {{quiz}}\n",
    "    답변 : {{user_answer}}\n",
    "      \n",
    "    \"\"\")\n",
    "    ])\n",
    "\n",
    "\n",
    "    feedback_chain = feedback_prompt | llm\n",
    "    feedback = feedback_chain.invoke({\"quiz\": quiz, \"user_answer\": user_answer})\n",
    "    print(\"Feedback:\")\n",
    "    pprint(feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
