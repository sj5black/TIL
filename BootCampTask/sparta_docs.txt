Pandas ì„¤ì¹˜ ë° Jupyter Notebook ì„¤ì •í•˜ê¸° ì¸ê³µì§€ëŠ¥ì„ ìœ„í•œ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬/ì¸ê³µì§€ëŠ¥ì„ ìœ„í•œ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ - 1ì£¼ì°¨/Pandas ì„¤ì¹˜ ë° Jupyter Notebook ì„¤ì •í•˜ê¸° ì œì‘:Pandas ì„¤ì¹˜ ë° Jupyter Notebook ì„¤ì •í•˜ê¸° ìˆ˜ì—… ëª©í‘œPandas ì„¤ì¹˜ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.Jupyter Notebook ì„¤ì • ë°©ë²•ì— ëŒ€í•´ì„œ í•™ìŠµí•©ë‹ˆë‹¤.ëª©ì°¨íŒë‹¤ìŠ¤(Pandas) ì„¤ì¹˜í•˜ê¸°Jupyter Notebook ì„¤ì •í•˜ê¸° Condaì™€ ê°€ìƒí™˜ê²½ ì„¤ì •í•˜ê¸°ê°€ìƒí™˜ê²½ì„ Jupyter Notebookê³¼ ì—°ê²°í•˜ê¸°ì•Œë©´ ì¢‹ì€ Tip!íŒë‹¤ìŠ¤(Pandas) ì„¤ì¹˜í•˜ê¸°íŒë‹¤ìŠ¤(Pandas)ë€?ğŸ“šíŒë‹¤ìŠ¤ëŠ” ë°ì´í„° ì¡°ì‘ ë° ë¶„ì„ì„ ìœ„í•œ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.
ë°ì´í„°í”„ë ˆì„(DataFrame)ì„ ì‚¬ìš©í•´ ì—‘ì…€ê³¼ ìœ ì‚¬í•œ ë°©ì‹ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë‹¤ë£° ìˆ˜ ìˆìŠµë‹ˆë‹¤.íŒë‹¤ìŠ¤ ì„¤ì¹˜í•˜ê¸°Shellë³µì‚¬pip install pandas

â€‹ìœ„ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ë©´ pipì„ í†µí•´ íŒë‹¤ìŠ¤ê°€ ì„¤ì¹˜ë©ë‹ˆë‹¤!Jupyter Notebook ì„¤ì •í•˜ê¸° â€‹Jupyter Notebookì´ë€?ğŸ“šJupyter Notebookì€ ì½”ë“œ, í…ìŠ¤íŠ¸, ì´ë¯¸ì§€ ë“±ì„ í•œ ê³³ì—ì„œ ê´€ë¦¬í•  ìˆ˜ ìˆëŠ” ëŒ€í™”í˜• ì»´í“¨íŒ… í™˜ê²½ì…ë‹ˆë‹¤.
ë°ì´í„° ë¶„ì„, ë¨¸ì‹ ëŸ¬ë‹, êµìœ¡ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤.Jupyter Notebook ì„¤ì¹˜í•˜ê¸°Shellë³µì‚¬pip install jupyter

â€‹ì´ ëª…ë ¹ì–´ë¡œ Jupyter Notebookì„ ì„¤ì¹˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Jupyter Notebook ì‹¤í–‰í•˜ê¸°Shellë³µì‚¬jupyter notebook

â€‹ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ë©´ ì›¹ ë¸Œë¼ìš°ì €ê°€ ì—´ë¦¬ë©° Jupyter Notebook ì¸í„°í˜ì´ìŠ¤ë¡œ ì´ë™í•©ë‹ˆë‹¤.Condaì™€ ê°€ìƒí™˜ê²½ ì„¤ì •í•˜ê¸°Condaë€?CondaëŠ” íŒŒì´ì¬ íŒ¨í‚¤ì§€ ê´€ë¦¬ ë° ê°€ìƒí™˜ê²½ ê´€ë¦¬ë¥¼ ë•ëŠ” ë„êµ¬ì…ë‹ˆë‹¤.ì—¬ëŸ¬ í”„ë¡œì íŠ¸ì— ì„œë¡œ ë‹¤ë¥¸ íŒ¨í‚¤ì§€ ë²„ì „ì„ ì‚¬ìš©í•´ì•¼ í•  ë•Œ ìœ ìš©í•©ë‹ˆë‹¤.

Conda ì„¤ì¹˜í•˜ê¸°Anacondaë‚˜ Minicondaë¥¼ ì„¤ì¹˜í•˜ì—¬ Condaë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ì„¤ì¹˜ í›„, Conda ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ê°€ìƒí™˜ê²½ ë§Œë“¤ê¸°Shellë³µì‚¬conda create --name myenv

â€‹ìœ„ ëª…ë ¹ì–´ë¡œ myenvë¼ëŠ” ì´ë¦„ì˜ ê°€ìƒí™˜ê²½ì„ ë§Œë“­ë‹ˆë‹¤.Shellë³µì‚¬conda create --name myenv python=3.8 pandas
â€‹ì›í•˜ëŠ” íŒŒì´ì¬ ë²„ì „ê³¼ íŒ¨í‚¤ì§€ë¥¼ ì§€ì •í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.ê°€ìƒí™˜ê²½ í™œì„±í™” ë° ë¹„í™œì„±í™”ê°€ìƒí™˜ê²½ í™œì„±í™”:Shellë³µì‚¬conda activate myenv

â€‹ê°€ìƒí™˜ê²½ ë¹„í™œì„±í™”:Shellë³µì‚¬conda deactivate
â€‹ê°€ìƒí™˜ê²½ì„ Jupyter Notebookê³¼ ì—°ê²°í•˜ê¸°ipykernel ì„¤ì¹˜ê°€ìƒí™˜ê²½ì„ Jupyter Notebookì—ì„œ ì‚¬ìš©í•˜ë ¤ë©´ ipykernelì„ ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.Shellë³µì‚¬pip install ipykernel

â€‹ê°€ìƒí™˜ê²½ì„ Jupyter Notebookì— ì¶”ê°€í•˜ê¸°Shellë³µì‚¬python -m ipykernel install --user --name myenv --display-name "My Env"

â€‹ì´ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ë©´ Jupyter Notebookì—ì„œ My Envë¼ëŠ” ì´ë¦„ìœ¼ë¡œ ê°€ìƒí™˜ê²½ì„ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Jupyter Notebookì—ì„œ ê°€ìƒí™˜ê²½ ì‚¬ìš©í•˜ê¸°ALTJupyter Notebookì—ì„œ ìƒˆë¡œìš´ ë…¸íŠ¸ë¶ì„ ì—´ ë•Œ,  My Envë¥¼ ì„ íƒí•©ë‹ˆë‹¤.ì•Œë©´ ì¢‹ì€ Tip!Conda ê°€ìƒí™˜ê²½ ê´€ë¦¬ conda env list ëª…ë ¹ì–´ë¡œ í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥í•œ ê°€ìƒí™˜ê²½ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Jupyter Notebook í™•ì¥ ë‹¤ì–‘í•œ Jupyter Notebook í™•ì¥ì„ ì„¤ì¹˜í•´ ê¸°ëŠ¥ì„ í™•ì¥í•´ ë³´ì„¸ìš”! nbextensions íŒ¨í‚¤ì§€ë¥¼ ì‚¬ìš©í•˜ë©´ ë§ì€ ìœ ìš©í•œ í™•ì¥ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ğŸ“šì´ë ‡ê²Œ í•˜ë©´ íŒë‹¤ìŠ¤ì™€ Jupyter Notebook ì„¤ì •ì´ ëª¨ë‘ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! â€‹ìµëª…9ì›” 25ì¼ì•„ì´ì½˜ ì œê±°
[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 1ê°•. ê°•ì˜ ì†Œê°œ[SCC] ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ - 1ì£¼ì°¨/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 1ê°•. ê°•ì˜ ì†Œê°œì œì‘:[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 1ê°•. ê°•ì˜ ì†Œê°œ[ìˆ˜ì—… ëª©í‘œ]ê°•ì˜ ì†Œê°œ ë° ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë³¸ ê°œë…ì„ ì•Œì•„ë´…ì‹œë‹¤[ëª©ì°¨]01. ê°•ì¢Œ ì†Œê°œ ë”¥ëŸ¬ë‹ì´ ëŒ€ì„¸ë¼ë˜ë°, ì™œ ë¨¸ì‹ ëŸ¬ë‹ë¶€í„° ì‹œì‘í• ê¹Œìš”?02. ë¨¸ì‹ ëŸ¬ë‹ ì†Œê°œ
 
 01. ê°•ì¢Œ ì†Œê°œì´ë²ˆ ë¨¸ì‹ ëŸ¬ë‹ ê°•ì¢Œëª©í‘œì™€ ì»¤ë¦¬í˜ëŸ¼ì— ëŒ€í•´ ì•ˆë‚´í•©ë‹ˆë‹¤.1) ê°•ì¢Œ ëª©í‘œë¨¸ì‹ ëŸ¬ë‹ì˜ ì •ì˜, ì£¼ìš” ê°œë…, ë‹¤ì–‘í•œ ì•Œê³ ë¦¬ì¦˜ í•™ìŠµê°œë…ì  ì´í•´ì™€ ì‹¤ìŠµì„ í†µí•œ ì‹¬í™” í•™ìŠµScikit-learn, TensorFlow, Keras ë“± íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ í™œìš© ì¸ê³µì§€ëŠ¥, ë¨¸ì‹ ëŸ¬ë‹, ë”¥ëŸ¬ë‹? ë­ê°€ ë‹¤ë¥¸ ê±¸ê¹Œìš”?ì¸ê³µì§€ëŠ¥(AI): ì‚¬ëŒì²˜ëŸ¼ ìƒê°í•˜ê³  í–‰ë™í•˜ëŠ” ê¸°ìˆ  â€‹ë¨¸ì‹ ëŸ¬ë‹(ML): ë°ì´í„°ë¥¼ í†µí•´ ìŠ¤ìŠ¤ë¡œ ë°°ìš°ê³  ë˜‘ë˜‘í•´ì§€ëŠ” ê¸°ìˆ  â€‹ë”¥ëŸ¬ë‹(DL): ì´ë¯¸ì§€ ì¸ì‹, ìì—°ì–´ ì²˜ë¦¬ ë“±ì— ë›°ì–´ë‚œ ë¨¸ì‹ ëŸ¬ë‹ì˜ í•œ ë¶„ì•¼ â€‹ ìˆ˜í•™? ê±±ì • ë§ˆì„¸ìš”!ìˆ˜í•™ ê³µì‹ì„ ëª°ë¼ë„ ê´œì°®ì•„ìš”! ì´ ê°•ì˜ì—ì„œëŠ” ë¨¸ì‹ ëŸ¬ë‹ì˜ ì‘ë™ ì›ë¦¬ì™€ ì½”ë”©ì— ì§‘ì¤‘í•©ë‹ˆë‹¤. í•„ìš”í•œ ìˆ˜í•™ ê°œë…ì€ ê·¸ë•Œê·¸ë•Œ í•¨ê»˜ í•™ìŠµí•˜ë©´ ë©ë‹ˆë‹¤! â€‹2) ê°•ì¢Œ ì»¤ë¦¬í˜ëŸ¼ë¨¸ì‹ ëŸ¬ë‹ ì •ì˜ ë° ê°œë… ë‹¤ì–‘í•œ ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ë°ì´í„°ì…‹ í™œìš©ê³¼ ì‹¤ìŠµì‹¤ë¬´ ë°ì´í„° í”„ë¡œì íŠ¸ì´ë²ˆ ê°•ì¢Œì—ì„œëŠ” íŒŒì´ì¬ ê¸°ë³¸ ê°œë…ì´ í•„ìš”í•©ë‹ˆë‹¤.
ê°•ì˜ ë’¤ìª½ì—ì„œ ë‚˜ì˜¤ëŠ” ì–´ë ¤ìš´ ë¬¸ë²•ë“¤ì€ ê°•ì¢Œ ë‚´ì—ì„œ ì„¤ëª… í›„ì— ì‹¤ìŠµì„ ì§„í–‰í•  ì˜ˆì •ì´ë‹ˆ ê±±ì •í•˜ì§€ ì•Šìœ¼ì…”ë„ ë©ë‹ˆë‹¤!ê°•ì˜ ë’¤ìª½ì—ì„œ ë‚˜ì˜¤ëŠ” ì–´ë ¤ìš´ ë¬¸ë²•ë“¤ì€ ê°•ì¢Œ ë‚´ì—ì„œ ì„¤ëª… í›„ì— ì‹¤ìŠµì„ ì§„í–‰í•  ì˜ˆì •ì´ë‹ˆ ê±±ì •í•˜ì§€ ì•Šìœ¼ì…”ë„ ë©ë‹ˆë‹¤!ï»¿â€‹ ë”¥ëŸ¬ë‹ì´ ëŒ€ì„¸ë¼ë˜ë°, ì™œ ë¨¸ì‹ ëŸ¬ë‹ë¶€í„° ì‹œì‘í• ê¹Œìš”?ë”¥ëŸ¬ë‹ì˜ ê¸°ì´ˆëŠ” íƒ„íƒ„í•œ ë¨¸ì‹ ëŸ¬ë‹ ì§€ì‹ì…ë‹ˆë‹¤. ê¸°ì´ˆë¥¼ íŠ¼íŠ¼íˆ ìŒ“ì•„ì•¼ ë”¥ëŸ¬ë‹ë„ ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆì£ ! ì°¨ê·¼ì°¨ê·¼ ê¸°ì´ˆë¶€í„° ë‹¤ì§€ë©´, ë”¥ëŸ¬ë‹ ì •ë³µë„ ë¬¸ì œì—†ì–´ìš”! â€‹02. ë¨¸ì‹ ëŸ¬ë‹ ì†Œê°œë¨¸ì‹ ëŸ¬ë‹ì— ëŒ€í•œ ê¸°ë³¸ì ì¸ ê°œë… ë° ì—­ì‚¬ë“±ì„ ì†Œê°œí•©ë‹ˆë‹¤1)ë¨¸ì‹ ëŸ¬ë‹ ì†Œê°œ ë¨¸ì‹ ëŸ¬ë‹ì˜ ì •ì˜ì»´í“¨í„°ê°€ ëª…ì‹œì ìœ¼ë¡œ í”„ë¡œê·¸ë˜ë° ë˜ì§€ ì•Šì•„ë„ ë°ì´í„°ë¥¼ í†µí•´ í•™ìŠµí•˜ê³ , ì˜ˆì¸¡í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ê¸°ëŠ¥ëŒ€ëŸ‰ì˜ ë°ì´í„°ë¥¼ ì•Œê³ ë¦¬ì¦˜ì— ì…ë ¥í•˜ì—¬ í•™ìŠµê³¼ì •ì„ í†µí•´ ëª¨ë¸ì„ ìƒì„±í•˜ê³  ì˜ˆì¸¡ì„ ìˆ˜í–‰ê¸°ì¡´ í”„ë¡œê·¸ë¨ì€ ëª…ì‹œì ìœ¼ë¡œ ì‘ì„±ëœ ê·œì¹™ê³¼ ë…¼ë¦¬ì— ë”°ë¼ ë™ì‘í•œë‹¤ëŠ”ê²ƒì—ì„œ ë¨¸ì‹ ëŸ¬ë‹ê³¼ì˜ ì°¨ì´ê°€ ìˆìŠµë‹ˆë‹¤!ê¸°ì¡´ í”„ë¡œê·¸ë¨ì€ ëª…ì‹œì ìœ¼ë¡œ ì‘ì„±ëœ ê·œì¹™ê³¼ ë…¼ë¦¬ì— ë”°ë¼ ë™ì‘í•œë‹¤ëŠ”ê²ƒì—ì„œ ë¨¸ì‹ ëŸ¬ë‹ê³¼ì˜ ì°¨ì´ê°€ ìˆìŠµë‹ˆë‹¤!ï»¿â€‹ì „í†µì ì¸ í”„ë¡œê·¸ë˜ë°:ê·œì¹™ê³¼ ë…¼ë¦¬ë¥¼ í”„ë¡œê·¸ë˜ë¨¸ê°€ ì§ì ‘ ì •ì˜ â€‹ëª…ì‹œì  ëª…ë ¹ê³¼ ì¡°ê±´ë¬¸ì„ í†µí•´ ë¬¸ì œ í•´ê²° â€‹ë¨¸ì‹ ëŸ¬ë‹:ë°ì´í„°ë¥¼ ì´ìš©í•´ íŒ¨í„´ê³¼ ê·œì¹™ì„ ìŠ¤ìŠ¤ë¡œ í•™ìŠµ â€‹ì˜ˆì¸¡ ëª¨ë¸ì„ í†µí•´ ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•œ ê²°ê³¼ ë„ì¶œ â€‹í”„ë¡œê·¸ë¨ì´ ì•„ë‹Œ ëª¨ë¸ì´ ì¤‘ì‹¬ â€‹ ë¨¸ì‹ ëŸ¬ë‹ì˜ êµ¬ì„±ìš”ì†Œë°ì´í„°ì…‹ : ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•œ ë°ì´í„° ëª¨ìŒíŠ¹ì§•(Feature) : ë°ì´í„°ì…‹ì—ì„œ ëª¨ë¸ì´ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ê°œë³„ ì†ì„±ë ˆì´ë¸”(label) : ì˜ˆì¸¡í•˜ê³ ì í•˜ëŠ” ëª©í‘œ ë³€ìˆ˜í›ˆë ¨ : ëª¨ë¸ì´ ë°ì´í„°ë¥¼ í†µí•´ í•™ìŠµí•˜ëŠ” ê³¼ì •í…ŒìŠ¤íŠ¸ : í•™ìŠµëœ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ê³¼ì •2)ë¨¸ì‹ ëŸ¬ë‹ì˜ ì—­ì‚¬ ë° ë°œì „ ì´ìœ   ë¨¸ì‹ ëŸ¬ë‹ì˜ ì—­ì‚¬1950s : ì•¨ëŸ° íŠœë§ì˜ â€œíŠœë§í…ŒìŠ¤íŠ¸â€ì™€ í¼ì…‰íŠ¸ë¡ ì˜ ê°œë°œ1980s -90s : ë°±í”„ë¡œíŒŒê²Œì´ì…˜ ì•Œê³ ë¦¬ì¦˜ì˜ ë“±ì¥2000s :  ëŒ€ê·œëª¨ ë°ì´í„°ì™€ ê³ ì„±ëŠ¥ ì»´í“¨íŒ… ìì›ì˜ ë°œì „2010s - : ë”¥ëŸ¬ë‹ì˜ ë¶€ìƒê³¼ ë‹¤ì–‘í•œ ì‚°ì—…ì— ì ìš© ìµœê·¼ ë¨¸ì‹ ëŸ¬ë‹ì˜ ë°œì „ ì´ìœ ë°ì´í„°ì˜ í­ë°œì  ì¦ê°€ì»´í“¨íŒ… íŒŒì›Œì˜ í–¥ìƒì•Œê³ ë¦¬ì¦˜ì˜ ë°œì „ì˜¤í”ˆì†ŒìŠ¤ ì»¤ë®¤ë‹ˆí‹°ì™€ ìƒíƒœê³„ì˜ ë°œì „ì´ ê°•ì¢Œì—ì„œëŠ” íŒŒì´ì¬ì„ í™œìš©í•œ ë¨¸ì‹ ëŸ¬ë‹ì˜ ê¸°ë³¸ ê°œë…ê³¼ ì‹¤ìŠµì„ ë‹¤ë£° ì˜ˆì •ì…ë‹ˆë‹¤!
ë¨¸ì‹ ëŸ¬ë‹ì˜ ê¸°ë³¸ì ì¸ ì›ë¦¬ë¶€í„° ì‹¤ìŠµì„ í†µí•´ ì§ì ‘ ëª¨ë¸ êµ¬ì¶•/í‰ê°€í•˜ëŠ” ë°©ë²•ê¹Œì§€ ëª¨ë‘ í•¨ê»˜ ë°°ì›Œë´…ì‹œë‹¤!.
[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 1. ë”¥ëŸ¬ë‹ ê°œë…ì„ ì¡ì•„ë´…ì‹œë‹¤![SCC] ê¸°ì´ˆê°€ íƒ„íƒ„í•œ ë”¥ëŸ¬ë‹/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] ê¸°ì´ˆê°€ íƒ„íƒ„í•œ ë”¥ëŸ¬ë‹ - 1ì£¼ì°¨/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 1. ë”¥ëŸ¬ë‹ ê°œë…ì„ ì¡ì•„ë´…ì‹œë‹¤!ì œì‘:[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 1. ë”¥ëŸ¬ë‹ ê°œë…ì„ ì¡ì•„ë´…ì‹œë‹¤![ìˆ˜ì—… ëª©í‘œ]ë”¥ëŸ¬ë‹ì´ ë¬´ì—‡ì¸ì§€ ê°œë…ì— ëŒ€í•´ ì•Œì•„ë´…ì‹œë‹¤.ë”¥ëŸ¬ë‹ì˜ ì—­ì‚¬ì™€ ì–´ë””ì— ì‚¬ìš©í•  ìˆ˜ ìˆì„ì§€ ì•Œì•„ë´…ì‹œë‹¤[ëª©ì°¨]01. ë”¥ëŸ¬ë‹ì´ë€ ë¬´ì—‡ì¼ê¹Œìš”?02. ë”¥ëŸ¬ë‹ì˜ ì—­ì‚¬ì™€ í™œìš© ë°©ì•ˆ03. ë”¥ëŸ¬ë‹ì„ ë°°ì›Œì•¼ í•˜ëŠ” ì´ìœ ğŸ’¡
 
 01. ë”¥ëŸ¬ë‹ì´ë€ ë¬´ì—‡ì¼ê¹Œìš”?âœ”ï¸ë”¥ëŸ¬ë‹ì´ë€ ë¬´ì—‡ì¸ì§€ ê°œë…ì— ëŒ€í•´ì„œ ì•Œì•„ë´…ì‹œë‹¤!1) ë”¥ëŸ¬ë‹ì´ë€? ë”¥ëŸ¬ë‹ ê°œë…ë”¥ëŸ¬ë‹ì€ ì¸ê³µì‹ ê²½ë§(Artificial Neural Networks)ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ê¸°ê³„ í•™ìŠµì˜ í•œ ë¶„ì•¼ì…ë‹ˆë‹¤.ë‹¤ì¸µ ì‹ ê²½ë§ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¡œë¶€í„° íŠ¹ì§•ì„ ìë™ìœ¼ë¡œ í•™ìŠµí•˜ê³ , ì´ë¥¼ í†µí•´ ë³µì¡í•œ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤.ì…ë ¥ ë°ì´í„°ì—ì„œ ì¤‘ìš”í•œ íŒ¨í„´ì„ ì¶”ì¶œí•˜ê³ , ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì˜ˆì¸¡, ë¶„ë¥˜, ìƒì„± ë“±ì˜ ë‹¤ì–‘í•œ ì‘ì—…ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ALT ë”¥ëŸ¬ë‹ì˜ íŠ¹ì§•ë¹„ì„ í˜• ì¶”ë¡ : ë”¥ëŸ¬ë‹ì€ ë¹„ì„ í˜• ì¶”ë¡ ì„ í†µí•´ ë³µì¡í•œ ë°ì´í„°ì˜ íŒ¨í„´ì„ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ë‹¤ì¸µ êµ¬ì¡°: ì—¬ëŸ¬ ì¸µì˜ ì‹ ê²½ë§ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ì˜ ê³ ì°¨ì› íŠ¹ì§•ì„ í•™ìŠµí•©ë‹ˆë‹¤.ìë™ íŠ¹ì§• ì¶”ì¶œ: ë°ì´í„°ë¡œë¶€í„° ì¤‘ìš”í•œ íŠ¹ì§•ì„ ìë™ìœ¼ë¡œ ì¶”ì¶œí•˜ì—¬ ë³„ë„ì˜ íŠ¹ì§• ê³µí•™(feature engineering) ê³¼ì •ì´ í•„ìš” ì—†ìŠµë‹ˆë‹¤.02. ë”¥ëŸ¬ë‹ì˜ ì—­ì‚¬ì™€ í™œìš© ë°©ì•ˆâœ”ï¸ë”¥ëŸ¬ë‹ì˜ ì—­ì‚¬ì™€ ì–´ë””ì— ë”¥ëŸ¬ë‹ì„ ì“¸ ìˆ˜ ìˆì„ì§€ ë°°ì›Œë´…ì‹œë‹¤!1) ë”¥ëŸ¬ë‹ì˜ ì—­ì‚¬ì™€ ë°œì „ ë°œì „ ê³¼ì •ALT ì¸ê³µì§€ëŠ¥, ë¨¸ì‹ ëŸ¬ë‹, ë”¥ëŸ¬ë‹ì˜ ê´€ê³„ì¸ê³µì§€ëŠ¥(AI) : ì¸ê³µì§€ëŠ¥ì€ ì¸ê°„ì˜ ì§€ëŠ¥ì„ ëª¨ë°©í•˜ì—¬ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ê¸°ìˆ ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. AIëŠ” ê·œì¹™ ê¸°ë°˜ ì‹œìŠ¤í…œë¶€í„° ììœ¨ í•™ìŠµ ì‹œìŠ¤í…œê¹Œì§€ ë‹¤ì–‘í•œ ì ‘ê·¼ ë°©ì‹ì„ í¬í•¨í•©ë‹ˆë‹¤.ë¨¸ì‹ ëŸ¬ë‹(ML) : ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„°ë¥¼ ì´ìš©í•´ ëª¨ë¸ì„ í•™ìŠµí•˜ê³ , ì´ë¥¼ í†µí•´ ì˜ˆì¸¡ì´ë‚˜ ê²°ì •ì„ ë‚´ë¦¬ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. ë¨¸ì‹ ëŸ¬ë‹ì€ AIì˜ í•˜ìœ„ ë¶„ì•¼ë¡œ, ì§€ë„ í•™ìŠµ, ë¹„ì§€ë„ í•™ìŠµ, ê°•í™” í•™ìŠµ ë“±ì˜ ë°©ë²•ì„ í¬í•¨í•©ë‹ˆë‹¤.ë”¥ëŸ¬ë‹(DL) : ë”¥ëŸ¬ë‹ì€ ë¨¸ì‹ ëŸ¬ë‹ì˜ í•˜ìœ„ ë¶„ì•¼ë¡œ, ë‹¤ì¸µ ì‹ ê²½ë§ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤. ë”¥ëŸ¬ë‹ì€ íŠ¹íˆ ëŒ€ê·œëª¨ ë°ì´í„°ì™€ ë³µì¡í•œ ë¬¸ì œë¥¼ ë‹¤ë£¨ëŠ” ë° ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë°œíœ˜í•©ë‹ˆë‹¤.ALT2) ìµœê·¼ì˜ í™œìš© ë°©ì•ˆ ì´ë¯¸ì§€ ì¸ì‹ë”¥ëŸ¬ë‹ì€ ì´ë¯¸ì§€ ë¶„ë¥˜, ê°ì²´ ê²€ì¶œ, ì´ë¯¸ì§€ ìƒì„± ë“± ë‹¤ì–‘í•œ ì´ë¯¸ì§€ ì²˜ë¦¬ ì‘ì—…ì— í™œìš©ë©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ììœ¨ ì£¼í–‰ ìë™ì°¨ëŠ” ë”¥ëŸ¬ë‹ì„ ì‚¬ìš©í•˜ì—¬ ë„ë¡œ ìƒí™©ì„ ì¸ì‹í•˜ê³ , ë³´í–‰ìì™€ ì°¨ëŸ‰ì„ ê°ì§€í•©ë‹ˆë‹¤. ìì—°ì–´ ì²˜ë¦¬ë²ˆì—­, ìš”ì•½, ê°ì • ë¶„ì„ ë“± ìì—°ì–´ ì²˜ë¦¬ ì‘ì—…ì— ì‚¬ìš©ë©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, êµ¬ê¸€ ë²ˆì—­ì€ ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ì–‘í•œ ì–¸ì–´ ê°„ì˜ ë²ˆì—­ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ìŒì„± ì¸ì‹ë”¥ëŸ¬ë‹ì€ ìŒì„± ì¸ì‹ ì‹œìŠ¤í…œì˜ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤.ì˜ˆë¥¼ ë“¤ì–´, ì• í”Œì˜ Siri, ì•„ë§ˆì¡´ì˜ Alexaì™€ ê°™ì€ ê°€ìƒ ë¹„ì„œëŠ” ë”¥ëŸ¬ë‹ì„ ì‚¬ìš©í•˜ì—¬ ì‚¬ìš©ìì˜ ìŒì„±ì„ ì¸ì‹í•˜ê³  ëª…ë ¹ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì˜ë£Œ ë¶„ì•¼ì˜ë£Œ ì˜ìƒ ë¶„ì„, ì§ˆë³‘ ì˜ˆì¸¡, ì‹ ì•½ ê°œë°œ ë“± ë‹¤ì–‘í•œ ì˜ë£Œ ë¶„ì•¼ì—ì„œë„ í™œìš©ë©ë‹ˆë‹¤.ì˜ˆë¥¼ ë“¤ì–´, ë”¥ëŸ¬ë‹ ëª¨ë¸ì€ MRIë‚˜ CT ìŠ¤ìº” ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ì—¬ ì•”ì„ ì¡°ê¸°ì— ë°œê²¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.03. ë”¥ëŸ¬ë‹ì„ ë°°ì›Œì•¼ í•˜ëŠ” ì´ìœ âœ”ï¸ê·¸ëŸ¬ë©´ ì™œ? ë”¥ëŸ¬ë‹ì„ ë°°ì›Œì•¼ í•˜ëŠ”ì§€ ì´ìœ ë¥¼ ì•Œì•„ ë´…ì‹œë‹¤1) ë”¥ëŸ¬ë‹ì„ ë°°ì›Œì•¼ í•˜ëŠ” ì´ìœ ALT.
LLMì˜ ì‚¬ìš© ì¤€ë¹„í•˜ê¸° LLM & RAGë¥¼ í™œìš©í•œ AI ì„œë¹„ìŠ¤ ë§Œë“¤ê¸°/ LLM & RAGë¥¼ í™œìš©í•œ AI ì„œë¹„ìŠ¤ ë§Œë“¤ê¸° - 5ì£¼ì°¨/LLMì˜ ì‚¬ìš© ì¤€ë¹„í•˜ê¸°LLMì˜ ì‚¬ìš© ì¤€ë¹„í•˜ê¸°ìˆ˜ì—… ëª©í‘œì´ë²ˆ ì£¼ì°¨ì—ì„œ ë°°ìš¸ ë‚´ìš©ë“¤ì„ ì‚´í´ë´…ë‹ˆë‹¤.ë³´ì•ˆ ë¬¸ì œ, API ê°œë…, ê·¸ë¦¬ê³  ì˜¤í”ˆ ì†ŒìŠ¤ LLM êµ¬ì¶•ì— ëŒ€í•œ ê¸°ë³¸ ë‚´ìš©ì„ í•™ìŠµí•©ë‹ˆë‹¤.ëª©ì°¨ì´ë²ˆì— ë°°ìš¸ ê²ƒë³´ì•ˆ ë¬¸ì œLLMì„ ì‚¬ìš©í•  ë•Œì˜ ë°ì´í„° ë³´ì•ˆ ë¬¸ì œë³´ì•ˆì„ ê°•í™”í•˜ëŠ” ë°©ë²•API ê°œë…ì¡ê¸°APIë€ ë¬´ì—‡ì¸ê°€?APIì˜ ê¸°ë³¸ ê°œë…ChatGPTë¥¼ APIë¡œ ì‚¬ìš©í•˜ëŠ” ê²ƒì˜ ì¥ì 
API ì‚¬ìš©ì˜ ì£¼ìš” ì¥ì ë²¡í„° DBì™€ LangChain í™œìš©í•˜ê¸°ë²¡í„° DBë€?ë²¡í„° DBì™€ LangChainì˜ ì—­í• LLM + ë²¡í„° DB + LangChain êµ¬ì¶• í”Œë¡œìš°ê²°ë¡ ì´ë²ˆì— ë°°ìš¸ ê²ƒì´ë²ˆ ì±•í„°ì—ì„œëŠ” LLMì„ ì‚¬ìš©í•˜ê¸° ì „ì— ì•Œì•„ì•¼ í•  ì¤‘ìš”í•œ ê°œë…ë“¤ì„ ë‹¤ë£° ê±°ì˜ˆìš”.LLMì„ ì‹¤ë¬´ì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ë³´ì•ˆ ë¬¸ì œ, API ê°œë…, ê·¸ë¦¬ê³  ì˜¤í”ˆ ì†ŒìŠ¤ LLM êµ¬ì¶•ì— ëŒ€í•œ ì´í•´ê°€ í•„ìˆ˜ì…ë‹ˆë‹¤.ê° ê°œë…ì„ ì°¨ê·¼ì°¨ê·¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤! â€‹ë³´ì•ˆ ë¬¸ì œLLMì„ ì‚¬ìš©í•  ë•ŒëŠ” ë°ì´í„° ë³´ì•ˆì— ëŒ€í•´ ë°˜ë“œì‹œ ì‹ ê²½ ì¨ì•¼ í•´ìš”.íŠ¹íˆ ëŒ€í™”í˜• AIì¸ LLMì€ ì‚¬ìš©ìì˜ ë¯¼ê°í•œ ì •ë³´ë¥¼ ì²˜ë¦¬í•  ê°€ëŠ¥ì„±ì´ ìˆê¸° ë•Œë¬¸ì— ë°ì´í„° ìœ ì¶œê³¼ í”„ë¼ì´ë²„ì‹œ ì¹¨í•´ê°€ ë°œìƒí•˜ì§€ ì•Šë„ë¡ ì£¼ì˜í•´ì•¼ í•©ë‹ˆë‹¤.LLMì„ ì‚¬ìš©í•  ë•Œì˜ ë°ì´í„° ë³´ì•ˆ ë¬¸ì œê°œì¸ ì •ë³´ ë³´í˜¸
LLMì€ ëŒ€í™”ë¥¼ í†µí•´ ê°œì¸ ì •ë³´(ì´ë¦„, ì „í™”ë²ˆí˜¸, ì‹ ìš©ì¹´ë“œ ì •ë³´ ë“±)ë¥¼ ì ‘í•  ìˆ˜ ìˆì–´ìš”.
ì´ëŸ° ì •ë³´ê°€ ì €ì¥ë˜ê±°ë‚˜ ì œ3ìì—ê²Œ ê³µìœ ë  ê²½ìš° í° ë¬¸ì œê°€ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ë°ì´í„° ì €ì¥ ë° ì „ì†¡
LLMì´ ì²˜ë¦¬í•œ ë°ì´í„°ê°€ ì–´ë””ì— ì €ì¥ë˜ëŠ”ì§€, ì–´ë–»ê²Œ ì „ì†¡ë˜ëŠ”ì§€ë¥¼ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤. 
ì•”í˜¸í™”ëœ ì „ì†¡ ë°©ì‹(ì˜ˆ: HTTPS)ì„ ì‚¬ìš©í•´ ë°ì´í„°ë¥¼ ë³´í˜¸í•´ì•¼ í•´ìš”.ëª¨ë¸ í•™ìŠµ ë°ì´í„°
LLMì€ í•™ìŠµì— ì‚¬ìš©ëœ ë°ì´í„°ì— ì˜ì¡´í•´ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
ë§Œì•½ í•™ìŠµ ë°ì´í„°ì— ë¯¼ê°í•œ ì •ë³´ê°€ í¬í•¨ëœë‹¤ë©´, í•´ë‹¹ ì •ë³´ê°€ ì˜ˆê¸°ì¹˜ ì•Šê²Œ ëª¨ë¸ì˜ ì¶œë ¥ìœ¼ë¡œ ë“±ì¥í•  ê°€ëŠ¥ì„±ë„ ìˆìŠµë‹ˆë‹¤.ë³´ì•ˆì„ ê°•í™”í•˜ëŠ” ë°©ë²•ë¯¼ê° ì •ë³´ í•„í„°ë§
ì…ë ¥ëœ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê¸° ì „ì— ë¯¼ê°í•œ ì •ë³´ë¥¼ ìë™ìœ¼ë¡œ ê±¸ëŸ¬ë‚´ëŠ” í•„í„°ë§ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ì„¸ìš”.ì•”í˜¸í™”
ë°ì´í„°ëŠ” ì €ì¥ ë° ì „ì†¡ ì¤‘ì— ì•”í˜¸í™”ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
íŠ¹íˆ SSL/TLSì™€ ê°™ì€ ì•ˆì „í•œ ì „ì†¡ í”„ë¡œí† ì½œì„ ì‚¬ìš©í•´ì•¼ í•´ìš”.ë°ì´í„° ì €ì¥ ìµœì†Œí™”
í•„ìš” ì´ìƒìœ¼ë¡œ ë°ì´í„°ë¥¼ ì €ì¥í•˜ì§€ ë§ê³ , í•„ìš”í•œ ê²½ìš°ì—ë„ ë°ì´í„° ë³´ì¡´ ì£¼ê¸°ë¥¼ ì„¤ì •í•´ ìë™ ì‚­ì œí•˜ë„ë¡ í•˜ì„¸ìš”.ì ‘ê·¼ í†µì œ
LLMì„ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì‚¬ëŒì˜ ê¶Œí•œì„ ì œí•œí•˜ê³ , ëª¨ë¸ì´ ë¯¼ê°í•œ ë°ì´í„°ì— ì ‘ê·¼í•˜ì§€ ì•Šë„ë¡ ì œí•œí•´ì•¼ í•´ìš”.API ê°œë…ì¡ê¸°APIë€ ë¬´ì—‡ì¸ê°€?API(Application Programming Interface)ëŠ” ì„œë¡œ ë‹¤ë¥¸ ì†Œí”„íŠ¸ì›¨ì–´ ì‹œìŠ¤í…œ ê°„ì— ë°ì´í„°ì™€ ê¸°ëŠ¥ì„ ì£¼ê³ ë°›ì„ ìˆ˜ ìˆë„ë¡ í•´ì£¼ëŠ” ì¸í„°í˜ì´ìŠ¤ì—ìš”.ì‰½ê²Œ ë§í•´, ë‘ í”„ë¡œê·¸ë¨ì´ ì„œë¡œ ì†Œí†µí•  ìˆ˜ ìˆëŠ” ë‹¤ë¦¬ë¼ê³  ë³´ë©´ ë©ë‹ˆë‹¤.APIì˜ ê¸°ë³¸ ê°œë…í´ë¼ì´ì–¸íŠ¸-ì„œë²„ ëª¨ë¸
í´ë¼ì´ì–¸íŠ¸(ìš”ì²­í•˜ëŠ” ìª½)ê°€ ì„œë²„(ì‘ë‹µí•˜ëŠ” ìª½)ì—ê²Œ ë°ì´í„°ë¥¼ ìš”ì²­í•˜ë©´, ì„œë²„ëŠ” í•´ë‹¹ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ì—¬ í´ë¼ì´ì–¸íŠ¸ì—ê²Œ ë°˜í™˜í•©ë‹ˆë‹¤.HTTP/HTTPS í”„ë¡œí† ì½œ
ëŒ€ë¶€ë¶„ì˜ APIëŠ” HTTPë‚˜ HTTPSë¥¼ í†µí•´ ìš”ì²­ê³¼ ì‘ë‹µì´ ì´ë£¨ì–´ì ¸ìš”.RESTful API
ê°€ì¥ í”íˆ ì‚¬ìš©ë˜ëŠ” API ì„¤ê³„ ë°©ì‹ìœ¼ë¡œ, ê° ì—”ë“œí¬ì¸íŠ¸(URI)ì™€ HTTP ë©”ì†Œë“œ(GET, POST, PUT, DELETE ë“±)ë¥¼ ì‚¬ìš©í•´ ë°ì´í„°ë¥¼ ì£¼ê³ ë°›ì•„ìš”.ChatGPTë¥¼ APIë¡œ ì‚¬ìš©í•˜ëŠ” ê²ƒì˜ ì¥ì ChatGPT ê°™ì€ LLMì„ APIë¡œ ì‚¬ìš©í•˜ëŠ” ê²ƒì€ ë§¤ìš° íš¨ìœ¨ì ì´ì—ìš”.ì´ APIë¥¼ í†µí•´ ë‹¤ì–‘í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ì— LLM ê¸°ëŠ¥ì„ ì‰½ê²Œ í†µí•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
API ì‚¬ìš©ì˜ ì£¼ìš” ì¥ì ìœ ì—°ì„±: í•„ìš”í•  ë•Œë§ˆë‹¤ ìš”ì²­ì„ ë³´ë‚´ì–´ ê²°ê³¼ë¥¼ ë°›ì„ ìˆ˜ ìˆì–´, ì‹¤ì‹œê°„ìœ¼ë¡œ ë‹¤ì–‘í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ì— ì ìš©í•  ìˆ˜ ìˆì–´ìš”.í™•ì¥ì„±: ë‹¤ì–‘í•œ ì„œë¹„ìŠ¤ë‚˜ í”Œë«í¼(ì˜ˆ: ì›¹ì‚¬ì´íŠ¸, ì•±)ì— ì‰½ê²Œ í†µí•©í•  ìˆ˜ ìˆì–´, ì—¬ëŸ¬ ì‚¬ìš©ìê°€ ë™ì‹œì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í™•ì¥ì„±ì„ ê°€ì§‘ë‹ˆë‹¤.ì—…ë°ì´íŠ¸: LLM API ì œê³µì‚¬ê°€ ëª¨ë¸ì„ ì—…ë°ì´íŠ¸í•˜ë©´ ë³„ë‹¤ë¥¸ ìˆ˜ì • ì—†ì´ ìµœì‹  ê¸°ëŠ¥ì„ ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆì–´ìš”.ë¹„ìš© íš¨ìœ¨ì„±: API í˜¸ì¶œì— ë”°ë¼ ë¹„ìš©ì´ ì²­êµ¬ë˜ë¯€ë¡œ, ëŒ€ê·œëª¨ ì„œë²„ë¥¼ ìœ ì§€í•  í•„ìš” ì—†ì´ í•„ìš”í•œ ë§Œí¼ë§Œ ì‚¬ìš© ê°€ëŠ¥í•´ìš”.ë²¡í„° DBì™€ LangChain í™œìš©í•˜ê¸°ì˜¤í”ˆ ì†ŒìŠ¤ LLMì„ êµ¬ì¶•í•  ë•Œ ë²¡í„° DBì™€ LangChainì„ í•¨ê»˜ ì‚¬ìš©í•˜ë©´ ë” ê°•ë ¥í•œ ì‹œìŠ¤í…œì„ ë§Œë“¤ ìˆ˜ ìˆì–´ìš”.ë²¡í„° DBë€?ë²¡í„° DBëŠ” ë¬¸ì„œë‚˜ ë°ì´í„°ì˜ ì„ë² ë”©(embedding) ë²¡í„°ë¥¼ ì €ì¥í•˜ê³ , ì´ ë²¡í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìœ ì‚¬í•œ ë°ì´í„°ë¥¼ ë¹ ë¥´ê²Œ ì°¾ì„ ìˆ˜ ìˆëŠ” ë°ì´í„°ë² ì´ìŠ¤ì…ë‹ˆë‹¤.LLMê³¼ ê²°í•©í•˜ë©´ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ì´ë‚˜ ì¶”ì²œ ì‹œìŠ¤í…œ ë“±ì— í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ë²¡í„° DBì™€ LangChainì˜ ì—­í• ë²¡í„° DB
í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•œ í›„, ìœ ì‚¬í•œ í…ìŠ¤íŠ¸ë‚˜ ë°ì´í„°ë¥¼ ë¹ ë¥´ê²Œ ì°¾ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ìœ ì‚¬í•œ ì§ˆë¬¸ì„ ì°¾ê±°ë‚˜ ì¶”ì²œì„ í•  ë•Œ ìœ ìš©í•´ìš”.LangChain
LLMê³¼ ë²¡í„° DBë¥¼ ì—°ê²°í•´ì£¼ëŠ” í”„ë ˆì„ì›Œí¬ë¡œ, ë°ì´í„° íë¦„ì„ ê´€ë¦¬í•˜ê³  API í˜¸ì¶œì„ ë” ê°„í¸í•˜ê²Œ ë§Œë“¤ì–´ì¤ë‹ˆë‹¤.LLM + ë²¡í„° DB + LangChain êµ¬ì¶• í”Œë¡œìš°í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
ë¨¼ì € LLMì„ í†µí•´ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë²¡í„°(ì„ë² ë”©)ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.ë²¡í„° DB ì €ì¥
ë³€í™˜ëœ ì„ë² ë”©ì„ ë²¡í„° DBì— ì €ì¥í•©ë‹ˆë‹¤.ì§ˆë¬¸ ì²˜ë¦¬
ì‚¬ìš©ìê°€ ì§ˆë¬¸ì„ í•˜ë©´, ì§ˆë¬¸ë„ ë²¡í„°ë¡œ ë³€í™˜í•˜ê³  ë²¡í„° DBì—ì„œ ìœ ì‚¬í•œ ì„ë² ë”©ì„ ì°¾ìŠµë‹ˆë‹¤.ë‹µë³€ ìƒì„±
ì°¾ì€ ìœ ì‚¬í•œ ë°ì´í„°ì™€ í•¨ê»˜ LLMì„ í†µí•´ ìµœì¢… ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.APIë¡œ ì œê³µ
ì´ ê³¼ì •ì„ LangChainì„ í†µí•´ API í˜•íƒœë¡œ ì œê³µí•˜ì—¬, ì‹¤ì œ ì„œë¹„ìŠ¤ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•©ë‹ˆë‹¤.ê²°ë¡ ì´ë²ˆ ì±•í„°ì—ì„œëŠ” LLMì˜ ì‚¬ìš© ì¤€ë¹„ë¥¼ ìœ„í•œ ë³´ì•ˆ ë¬¸ì œ, API ê°œë…, ê·¸ë¦¬ê³  ì˜¤í”ˆ ì†ŒìŠ¤ LLM êµ¬ì¶• ë°©ë²•ì„ ë°°ì› ìŠµë‹ˆë‹¤.ë°ì´í„°ë¥¼ ì•ˆì „í•˜ê²Œ ê´€ë¦¬í•˜ë©´ì„œ LLMì„ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•˜ê³ , í•„ìš”í•œ ê²½ìš°ì—ëŠ” ì˜¤í”ˆ ì†ŒìŠ¤ LLMì„ êµ¬ì¶•í•˜ì—¬ ìµœì ì˜ ì‹œìŠ¤í…œì„ ë§Œë“¤ ìˆ˜ ìˆì£ !ì´ì œ LLMì„ ì‹¤ë¬´ì—ì„œ ì‚¬ìš©í•  ì¤€ë¹„ê°€ ë‹¤ ë˜ì…¨ìŠµë‹ˆë‹¤! â€‹
NumPy ì†Œê°œ ë° ì„¤ì¹˜ì¸ê³µì§€ëŠ¥ì„ ìœ„í•œ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬/ì¸ê³µì§€ëŠ¥ì„ ìœ„í•œ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ - 2ì£¼ì°¨/NumPy ì†Œê°œ ë° ì„¤ì¹˜ì œì‘:NumPy ì†Œê°œ ë° ì„¤ì¹˜ìˆ˜ì—… ëª©í‘œNumpyê°€ ë¬´ì—‡ì¸ì§€ ì•Œì•„ë´…ë‹ˆë‹¤.Numpy ì„¸íŒ…ì„ ì§„í–‰í•©ë‹ˆë‹¤.ëª©ì°¨NumPyë€? NumPy ì„¤ì¹˜í•˜ê¸°NumPyì˜ ê¸°ë³¸ ì‚¬ìš©ë²• ìš”ì•½NumPyë€? NumPyëŠ” Numerical Pythonì˜ ì¤„ì„ë§ë¡œ,
ê³¼í•™ ê³„ì‚°ì— ê°•ë ¥í•œ ì„±ëŠ¥ì„ ì œê³µí•˜ëŠ” íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.NumPy?ë‹¤ì°¨ì› ë°°ì—´ ê°ì²´ì¸ ndarrayì™€ ë°°ì—´ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ë‹¤ì–‘í•œ í•¨ìˆ˜ë“¤ì„ ì œê³µí•©ë‹ˆë‹¤.ë°ì´í„° ë¶„ì„, ë¨¸ì‹ ëŸ¬ë‹, ë”¥ëŸ¬ë‹ì—ì„œ ê¸°ì´ˆê°€ ë˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ, íŒë‹¤ìŠ¤ì™€ í•¨ê»˜ ìì£¼ ì‚¬ìš©ë©ë‹ˆë‹¤.NumPyì˜ ì£¼ìš” íŠ¹ì§•ê³ ì† ë°°ì—´ ì—°ì‚°: Cì–¸ì–´ë¡œ ì‘ì„±ë˜ì–´ ìˆì–´ íŒŒì´ì¬ ê¸°ë³¸ ë¦¬ìŠ¤íŠ¸ë³´ë‹¤ í›¨ì”¬ ë¹ ë¥¸ ì—°ì‚° ê°€ëŠ¥.ë‹¤ì–‘í•œ ìˆ˜í•™ í•¨ìˆ˜: ë°°ì—´ ê°„ì˜ ë§ì…ˆ, ëº„ì…ˆ, ê³±ì…ˆ ë“±ì˜ ì—°ì‚°ì„ íš¨ìœ¨ì ìœ¼ë¡œ ìˆ˜í–‰.ì„ í˜•ëŒ€ìˆ˜, í†µê³„ í•¨ìˆ˜: ë³µì¡í•œ ìˆ˜í•™ ê³„ì‚°ë„ ê°„ë‹¨íˆ ì²˜ë¦¬ ê°€ëŠ¥.NumPy ì„¤ì¹˜í•˜ê¸°NumPyë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ ë¨¼ì € ì„¤ì¹˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.
ê°€ìƒí™˜ê²½ì„ ì‚¬ìš©í•˜ê³  ìˆë‹¤ë©´ í•´ë‹¹ ê°€ìƒí™˜ê²½ì— ì„¤ì¹˜í•˜ë©´ ë©ë‹ˆë‹¤.Shellë³µì‚¬pip install numpy

â€‹ì„¤ì¹˜ê°€ ì™„ë£Œë˜ë©´, íŒŒì´ì¬ ì½”ë“œì—ì„œ import numpy as npë¡œ NumPyë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.NumPyì˜ ê¸°ë³¸ ì‚¬ìš©ë²• ë°°ì—´(ndarray) ìƒì„±í•˜ê¸°ë¦¬ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•´ ë°°ì—´ì„ ìƒì„±í•  ìˆ˜ ìˆì–´ìš”.Pythonë³µì‚¬import numpy as np

# 1ì°¨ì› ë°°ì—´ ìƒì„±
arr = np.array([1, 2, 3, 4, 5])
print(arr)

â€‹arrì€ ë‹¤ìŒê³¼ ê°™ì´ ì¶œë ¥ë©ë‹ˆë‹¤:Plain Textë³µì‚¬[1 2 3 4 5]

â€‹ë°°ì—´ì˜ ì—°ì‚°NumPy ë°°ì—´ì€ ë²¡í„°í™” ì—°ì‚°ì´ ê°€ëŠ¥í•˜ì—¬, ë°˜ë³µë¬¸ ì—†ì´ ë°°ì—´ ì „ì²´ì— ì—°ì‚°ì„ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Pythonë³µì‚¬# ë°°ì—´ ë§ì…ˆ
arr2 = arr + 10
print(arr2)

â€‹arr2ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì¶œë ¥ë©ë‹ˆë‹¤:Plain Textë³µì‚¬[11 12 13 14 15]

â€‹ë°°ì—´ì˜ ë‹¤ì–‘í•œ ê¸°ëŠ¥ë°°ì—´ì˜ ëª¨ì–‘ ë³€ê²½, ì›ì†Œ ì ‘ê·¼ ë“± ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Pythonë³µì‚¬# ë°°ì—´ì˜ ëª¨ì–‘ ë³€ê²½
arr_reshaped = arr.reshape((1, 5))
print(arr_reshaped)

â€‹arr_reshapedëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì¶œë ¥ë©ë‹ˆë‹¤:Plain Textë³µì‚¬[[1 2 3 4 5]]â€‹ìš”ì•½ìš”ì•½NumPyëŠ” ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ ìˆ˜ì¹˜ ê³„ì‚°ì„ ìœ„í•œ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.pip ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜ ê°€ëŠ¥í•˜ë©°, ë‹¤ì–‘í•œ ìˆ˜í•™ ì—°ì‚°ê³¼ ë°°ì—´ ì¡°ì‘ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.NumPyë¥¼ í†µí•´ ë³µì¡í•œ ê³„ì‚°ì„ ë‹¨ìˆœí•˜ê²Œ ë§Œë“¤ ìˆ˜ ìˆì–´, ë°ì´í„° ë¶„ì„ì—ì„œ ìì£¼ ì‚¬ìš©ë©ë‹ˆë‹¤.NumPyë¡œ ë°ì´í„° ì²˜ë¦¬ì˜ ê¸°ì´ˆë¥¼ ë‹¤ì§€ê³ , ë” ë‚˜ì•„ê°€ íŒë‹¤ìŠ¤ì™€ í•¨ê»˜ í™œìš©í•´ë³´ì„¸ìš”! â€‹
[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 2ê°•. ë¨¸ì‹ ëŸ¬ë‹ ê°œìš”ì™€ êµ¬ì„±ìš”ì†Œ[SCC] ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ - 1ì£¼ì°¨/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 2ê°•. ë¨¸ì‹ ëŸ¬ë‹ ê°œìš”ì™€ êµ¬ì„±ìš”ì†Œì œì‘:[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 2ê°•. ë¨¸ì‹ ëŸ¬ë‹ ê°œìš”ì™€ êµ¬ì„±ìš”ì†Œ[ìˆ˜ì—… ëª©í‘œ]ë¨¸ì‹ ëŸ¬ë‹ì˜ ê¸°ë³¸ì ì¸ êµ¬ì„±ìš”ì†Œ, í•™ìŠµê³¼ì •ì„ ì•Œë ¤ ë“œë¦½ë‹ˆë‹¤.ë‹¤ì–‘í•œ ë¨¸ì‹ ëŸ¬ë‹ì˜ í•™ìŠµë°©ë²•ì„ ì†Œê°œí•©ë‹ˆë‹¤.[ëª©ì°¨]01. ë¨¸ì‹ ëŸ¬ë‹ êµ¬ì„± ìš”ì†Œ02. ë¨¸ì‹ ëŸ¬ë‹ì˜ í•™ìŠµğŸ’¡
 
 01. ë¨¸ì‹ ëŸ¬ë‹ êµ¬ì„± ìš”ì†Œâœ”ï¸ë¨¸ì‹ ëŸ¬ë‹ì˜ í•„ìˆ˜ êµ¬ì„±ìš”ì†Œë¥¼ í•™ìŠµí•©ë‹ˆë‹¤1) ë¨¸ì‹ ëŸ¬ë‹ì˜ êµ¬ì„±ìš”ì†Œ ë°ì´í„°ì…‹ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„°ì…‹ì„ í†µí•´ì„œ í•™ìŠµí•˜ë©°, ì¼ë°˜ì ìœ¼ë¡œ ë°ì´í„°ì…‹ì€ ì…ë ¥/ì¶œë ¥ ë°ì´í„°ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.ì…ë ¥ ë°ì´í„° : ëª¨ë¸ì´ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ì •ë³´ì¶œë ¥ ë°ì´í„°(ë ˆì´ë¸”) : ëª¨ë¸ì´ ì˜ˆì¸¡í•´ì•¼ í•˜ëŠ” ëª©í‘œ ê°’ Feature(íŠ¹ì§•)ë°ì´í„°ì—ì„œ ëª¨ë¸ì´ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ê°œë³„ ì†ì„±ì£¼íƒê°€ê²©ì˜ˆì¸¡ì„ ì˜ˆì‹œë¡œ ë“¤ ê²½ìš° ì£¼íƒì˜ í¬ê¸°, ìœ„ì¹˜, ë°©ì˜ ê°œìˆ˜ ë“±ì´ Featureì— í•´ë‹¹í•©ë‹ˆë‹¤ì£¼íƒê°€ê²©ì˜ˆì¸¡ì„ ì˜ˆì‹œë¡œ ë“¤ ê²½ìš° ì£¼íƒì˜ í¬ê¸°, ìœ„ì¹˜, ë°©ì˜ ê°œìˆ˜ ë“±ì´ Featureì— í•´ë‹¹í•©ë‹ˆë‹¤ï»¿â€‹ ë ˆì´ë¸”ì˜ˆì¸¡í•˜ê³ ì í•˜ëŠ” ëª©í‘œ ë³€ìˆ˜ì§€ë„í•™ìŠµ ëª¨ë¸ì—ì„œëŠ” ë ˆì´ë¸”ì´ ìˆëŠ” ë°ì´í„°ì…‹ì„ ì´ìš©í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµ ì‹œí‚µë‹ˆë‹¤ ëª¨ë¸ë°ì´í„°ì˜ íŠ¹ì§•ìœ¼ë¡œ ë¶€í„° ì •ë‹µ(ë ˆì´ë¸”)ì„ ì˜ˆì¸¡í•  ìˆ˜ ìˆëŠ” ì§€ì‹ì„ í•™ìŠµí•  ìˆ˜ ìˆëŠ” í”„ë¡œê·¸ë¨/í•¨ìˆ˜ì…ë ¥ë°ì´í„°ì™€ ì¶œë ¥ ë°ì´í„°ê°„ì˜ ê´€ê³„ë¥¼ í•™ìŠµí•˜ì—¬ ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ ìˆ˜í–‰ í•™ìŠµëª¨ë¸ì´ ë°ì´í„°ë¥¼ í†µí•´ì„œ íŒ¨í„´ì„ ì¸ì‹í•˜ê³ , ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì˜ˆì¸¡ì„ ìˆ˜í–‰ í•  ìˆ˜ ìˆë„ë¡ í•¨ìˆ˜ ë‚´ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì •í•˜ëŠ” ê³¼ì •02. ë¨¸ì‹ ëŸ¬ë‹ì˜ í•™ìŠµâœ”ï¸ë¨¸ì‹ ëŸ¬ë‹ì˜ í•™ìŠµê³¼ì •ê³¼ ë‹¤ì–‘í•œ í•™ìŠµ ì¢…ë¥˜ì— ëŒ€í•´ì„œ í•™ìŠµí•©ë‹ˆë‹¤1) ë¨¸ì‹ ëŸ¬ë‹ì˜ í•™ìŠµ ê³¼ì •ë¨¸ì‹ ëŸ¬ë‹ í•™ìŠµ ê³¼ì •ALTë°ì´í„° ìˆ˜ì§‘ : ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•œ í•„ìš” ë°ì´í„° ìˆ˜ì§‘ë°ì´í„° ì „ì²˜ë¦¬ : ê²°ì¸¡ê°’ ì²˜ë¦¬, ì´ìƒì¹˜ ì œê±°, ì •ê·œí™” ë“±ë“±Feature ì„ íƒ : ì¤‘ìš” feature(íŠ¹ì§•)ì„ ì„ íƒí•˜ê³  ë¶ˆí•„ìš”í•œ í”¼ì³ë¥¼ ì œê±°í•˜ì—¬ í•™ìŠµíš¨ìœ¨ ë†’ì„ëª¨ë¸ ì„ íƒ : ë¬¸ì œì— ì í•©í•œ ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì„ ì„ íƒëª¨ë¸ í›ˆë ¨ : íŠ¸ë ˆì´ë‹ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•´ì„œ ëª¨ë¸ì„ í•™ìŠµì‹œí‚´ëª¨ë¸ í‰ê°€ : í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ ì„±ëŠ¥ì„ í‰ê°€ëª¨ë¸ ë°°í¬ : í•™ìŠµëœ ëª¨ë¸ì„ ì‹¤ì œ í™˜ê²½ì— ë°°í¬í•˜ì—¬ ì˜ˆì¸¡ ìˆ˜í–‰2) í•™ìŠµ ë°©ë²• ì§€ë„ í•™ìŠµ (Supervised Learning)ë ˆì´ë¸”ì´ ìˆëŠ” ë°ì´í„°ì…‹ì„ ì´ìš©í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ëŠ” ë°©ë²•íšŒê·€(Regression) : ì—°ì†ì ì¸ ê°’ì„ ì˜ˆì¸¡í•˜ëŠ” ë¬¸ì œex : ì£¼íƒ ê°€ê²© ì˜ˆì¸¡, ì£¼ì‹ ê°€ê²©ì˜ˆì¸¡ : ì£¼íƒ ê°€ê²© ì˜ˆì¸¡, ì£¼ì‹ ê°€ê²©ì˜ˆì¸¡ï»¿â€‹ë¶„ë¥˜(Classification) : ì´ì‚°ì ì¸ ê°’ì„ ì˜ˆì¸¡í•˜ëŠ” ë¬¸ì œex : ì´ë©”ì¼ ìŠ¤íŒ¸ í•„í„°ë§, ì´ë¯¸ì§€ ë¶„ë¥˜ : ì´ë©”ì¼ ìŠ¤íŒ¸ í•„í„°ë§, ì´ë¯¸ì§€ ë¶„ë¥˜ï»¿â€‹ ë¹„ì§€ë„ í•™ìŠµ (Unsupervised Learning)ë ˆì´ë¸”ì´ ì—†ëŠ” ë°ì´í„°ì…‹ì„ ì´ìš©í•˜ë ¤ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ëŠ” ë°©ë²•êµ°ì§‘í™”(Clustering) : ë°ì´í„°ë¥¼ ìœ ì‚¬í•œ ê·¸ë£¹ìœ¼ë¡œ ë¬¶ëŠ” ë¬¸ì œ ex : ê³ ê° ì„¸ë¶„í™”, ì´ë¯¸ì§€ ì„¸ê·¸ë©˜í…Œì´ì…˜ : ê³ ê° ì„¸ë¶„í™”, ì´ë¯¸ì§€ ì„¸ê·¸ë©˜í…Œì´ì…˜ï»¿â€‹ì°¨ì›ì¶•ì†Œ (Dimensionality Reduction) : ê³ ì°¨ì› ë°ì´í„°ë¥¼ ì €ì°¨ì›ìœ¼ë¡œ ë³€í™˜ex : PCA, t-SNE : PCA, t-SNEï»¿â€‹ ì•™ìƒë¸” í•™ìŠµ (Ensemble Learning)ì—¬ëŸ¬ê°œì˜ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ ê²°í•©í•˜ì—¬ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ì–»ëŠ” ë°©ë²•ë°°ê¹…(Bagging) : ì—¬ëŸ¬ ëª¨ë¸ì„ ë…ë¦½ì ìœ¼ë¡œ í•™ìŠµì‹œí‚¤ê³ , ì˜ˆì¸¡ì„ í‰ê· ë‚´ê±°ë‚˜ ë‹¤ìˆ˜ê²° íˆ¬í‘œë¡œ ìµœì¢… ì˜ˆì¸¡ex : ëœë¤í¬ë ˆìŠ¤íŠ¸ : ëœë¤í¬ë ˆìŠ¤íŠ¸ï»¿â€‹ë¶€ìŠ¤íŒ…(Boosting) : ì—¬ëŸ¬ ëª¨ë¸ì„ ìˆœì°¨ì ìœ¼ë¡œ í•™ìŠµì‹œí‚¤ê³ , ì´ì „ ëª¨ë¸ì˜ ì˜¤ì°¨ë¥¼ ë³´ì™„í•˜ì—¬ ìµœì¢… ì˜ˆì¸¡ì„ ìˆ˜í–‰ex : ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…, XGboost : ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…, XGboostï»¿â€‹ìŠ¤íƒœí‚¹(Stacking) : ì—¬ëŸ¬ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê³  ì˜ˆì¸¡ê²°ê³¼ë¥¼ ìƒˆë¡œìš´ ë°ì´í„°ë¡œ ì‚¬ìš©í•˜ì—¬ ë©”íƒ€ ëª¨ë¸ì„ í•™ìŠµ ê³¼ì í•©ì´ë€?ê³¼ì í•©(Overfitting):ëª¨ë¸ì´ í›ˆë ¨ ë°ì´í„°ì— ì§€ë‚˜ì¹˜ê²Œ ì ì‘í•˜ì—¬ ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•œ ì¼ë°˜í™” ì„±ëŠ¥ì´ ë–¨ì–´ì§€ëŠ” í˜„ìƒì…ë‹ˆë‹¤. ëª¨ë¸ì´ ë„ˆë¬´ ë³µì¡í•˜ì—¬ í›ˆë ¨ ë°ì´í„°ì˜ ë…¸ì´ì¦ˆê¹Œì§€ í•™ìŠµí•´ë²„ë¦¬ëŠ” ê²½ìš° ë°œìƒí•©ë‹ˆë‹¤. â€‹ë°©ì§€ ë°©ë²•:ë” ë§ì€ ë°ì´í„° ìˆ˜ì§‘ â€‹êµì°¨ ê²€ì¦(Cross-validation) ì‚¬ìš© â€‹ì •ê·œí™”(Regularization) ê¸°ë²• ì ìš© â€‹ê°„ë‹¨í•œ ëª¨ë¸ ì‚¬ìš© â€‹ ë¨¸ì‹ ëŸ¬ë‹ì—ì„œëŠ” "ì ˆëŒ€ë¡œ ì¢‹ë‹¤"ë¼ëŠ” ê°œë…ì´ ì—†ë‹¤!ëª¨ë¸ì˜ ì„±ëŠ¥:ëª¨ë“  ë°ì´í„°ì…‹ì— ëŒ€í•´ ì™„ë²½í•œ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ëª¨ë¸ì€ ì—†ìŠµë‹ˆë‹¤. ê° ëª¨ë¸ì€ íŠ¹ì • ë°ì´í„°ì™€ ìƒí™©ì—ì„œë§Œ ìµœì ì˜ ì„±ëŠ¥ì„ ë°œíœ˜í•©ë‹ˆë‹¤. â€‹íŠ¸ë ˆì´ë“œì˜¤í”„:ëª¨ë¸ì˜ ë³µì¡ì„±ê³¼ ì¼ë°˜í™” ì„±ëŠ¥ ì‚¬ì´ì—ëŠ” í•­ìƒ ê· í˜•ì´ í•„ìš”í•©ë‹ˆë‹¤. ë„ˆë¬´ ë³µì¡í•œ ëª¨ë¸ì€ ê³¼ì í•©ì˜ ìœ„í—˜ì´ ìˆê³ , ë„ˆë¬´ ë‹¨ìˆœí•œ ëª¨ë¸ì€ ì¶©ë¶„íˆ í•™ìŠµí•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. â€‹.
[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 2. ì‹ ê²½ë§ì˜ ê¸°ë³¸ ì›ë¦¬[SCC] ê¸°ì´ˆê°€ íƒ„íƒ„í•œ ë”¥ëŸ¬ë‹/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] ê¸°ì´ˆê°€ íƒ„íƒ„í•œ ë”¥ëŸ¬ë‹ - 1ì£¼ì°¨/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 2. ì‹ ê²½ë§ì˜ ê¸°ë³¸ ì›ë¦¬ì œì‘:[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 2. ì‹ ê²½ë§ì˜ ê¸°ë³¸ ì›ë¦¬[ìˆ˜ì—… ëª©í‘œ]í¼ì…‰íŠ¸ë¡ ì˜ ê°œë…ê³¼ ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ ì— ëŒ€í•´ ë°°ì›Œë´…ì‹œë‹¤.ì‹ ê²½ë§ì„ ê°•í™”í•˜ê¸° ìœ„í•œ í™œì„±í™” í•¨ìˆ˜/ì†ì‹¤ í•¨ìˆ˜/ì—­ì „íŒŒ/ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì— ëŒ€í•´ ë°°ì›Œë´…ì‹œë‹¤[ëª©ì°¨]01. í¼ì…‰íŠ¸ë¡ ê³¼ ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ (XOR ë¬¸ì œ í¬í•¨)02. ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ (MLP)03. í™œì„±í™” í•¨ìˆ˜04. ì†ì‹¤ í•¨ìˆ˜ì™€ ìµœì í™” ì•Œê³ ë¦¬ì¦˜05. ì—­ì „íŒŒì— ëŒ€í•´ ì•Œì•„ë³¼ê¹Œìš”?
 
 01. í¼ì…‰íŠ¸ë¡ ê³¼ ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ (XOR ë¬¸ì œ í¬í•¨)ì¸ê³µì‹ ê³µë§ì˜ ê°€ì¥ ê¸°ë³¸ ë‹¨ìœ„ì¸ í¼ì…‰íŠ¸ë¡ ì— ëŒ€í•´ ë°°ì›Œë´…ì‹œë‹¤1) ë‹¨ì¼ í¼ì…‰íŠ¸ë¡ ì˜ ì›ë¦¬ ë‹¨ì¼ í¼ì…‰íŠ¸ë¡ ì˜ ê°œë…í¼ì…‰íŠ¸ë¡ (Perceptron)ì€ ì¸ê³µ ì‹ ê²½ë§ì˜ ê°€ì¥ ê¸°ë³¸ì ì¸ ë‹¨ìœ„ë¡œ, í•˜ë‚˜ì˜ ë‰´ëŸ°ì„ ëª¨ë¸ë§í•œ ê²ƒì…ë‹ˆë‹¤.í¼ì…‰íŠ¸ë¡ ì€ ì…ë ¥ ê°’ì„ ë°›ì•„ ê°€ì¤‘ì¹˜(weight)ë¥¼ ê³±í•˜ê³ , ì´ë¥¼ ëª¨ë‘ ë”í•œ í›„ í™œì„±í™” í•¨ìˆ˜(activation function)ë¥¼ í†µí•´ ì¶œë ¥ ê°’ì„ ê²°ì •í•©ë‹ˆë‹¤.ALT í¼ì…‰íŠ¸ë¡ ì˜ ìˆ˜í•™ì  í‘œí˜„y=f(âˆ‘i=1nwixi+b)y = f(_{i=1}^{n} w_i x_i + b) y=f(i=1âˆ‘nâ€‹wiâ€‹xiâ€‹+b)ì—¬ê¸°ì„œ xiëŠ” ì…ë ¥ ê°’, wiëŠ” ê°€ì¤‘ì¹˜, bëŠ” ë°”ì´ì–´ìŠ¤(bias), fëŠ” í™œì„±í™” í•¨ìˆ˜ì…ë‹ˆë‹¤.ì—¬ê¸°ì„œ xiâ€‹ëŠ” ì…ë ¥ ê°’, wiâ€‹ëŠ” ê°€ì¤‘ì¹˜, bëŠ” ë°”ì´ì–´ìŠ¤(bias), fëŠ” í™œì„±í™” í•¨ìˆ˜ì…ë‹ˆë‹¤.ï»¿â€‹02. ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ (MLP)ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ ì— ëŒ€í•´ ë°°ì›Œë´…ì‹œë‹¤1) ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ (MLP)ê³¼ XOR ë¬¸ì œ í•´ê²° ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ (MLP)ì˜ ê°œë…ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡ (Multi-Layer Perceptron, MLP)ì€ ì—¬ëŸ¬ ì¸µì˜ í¼ì…‰íŠ¸ë¡ ì„ ìŒ“ì•„ ì˜¬ë¦° ì‹ ê²½ë§ êµ¬ì¡°ì…ë‹ˆë‹¤.MLPëŠ” ì…ë ¥ì¸µ(input layer), ì€ë‹‰ì¸µ(hidden layer), ì¶œë ¥ì¸µ(output layer)ìœ¼ë¡œ êµ¬ì„±ë˜ë©°, ê° ì¸µì˜ ë‰´ëŸ°ë“¤ì´ ì„œë¡œ ì—°ê²°ë˜ì–´ ìˆìŠµë‹ˆë‹¤.ALT ì…ë ¥, ì€ë‹‰, ì¶œë ¥ ë ˆì´ì–´ì˜ ê°œë…ì…ë ¥ ë ˆì´ì–´(Input Layer) : ì™¸ë¶€ ë°ì´í„°ê°€ ì‹ ê²½ë§ì— ì…ë ¥ë˜ëŠ” ë¶€ë¶„ì…ë‹ˆë‹¤. ì…ë ¥ ë ˆì´ì–´ì˜ ë‰´ëŸ° ìˆ˜ëŠ” ì…ë ¥ ë°ì´í„°ì˜ íŠ¹ì§• ìˆ˜ì™€ ë™ì¼í•©ë‹ˆë‹¤.ì€ë‹‰ ë ˆì´ì–´(Hidden Layer) : ì€ë‹‰ ë ˆì´ì–´ëŠ” ì…ë ¥ ë ˆì´ì–´ì™€ ì¶œë ¥ ë ˆì´ì–´ ì‚¬ì´ì— ìœ„ì¹˜í•œ ì¸µìœ¼ë¡œ, ì…ë ¥ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê³  íŠ¹ì§•ì„ ì¶”ì¶œí•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. ì€ë‹‰ ë ˆì´ì–´ì˜ ë‰´ëŸ° ìˆ˜ì™€ ì¸µ ìˆ˜ëŠ” ëª¨ë¸ì˜ ë³µì¡ì„±ê³¼ ì„±ëŠ¥ì— ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤.ì¶œë ¥ ë ˆì´ì–´(Output Layer) : ì¶œë ¥ ë ˆì´ì–´ëŠ” ì‹ ê²½ë§ì˜ ë§ˆì§€ë§‰ ì¸µìœ¼ë¡œ, ìµœì¢… ì˜ˆì¸¡ ê°’ì„ ì¶œë ¥í•©ë‹ˆë‹¤. ì¶œë ¥ ë ˆì´ì–´ì˜ ë‰´ëŸ° ìˆ˜ëŠ” ì˜ˆì¸¡í•˜ë ¤ëŠ” í´ë˜ìŠ¤ ìˆ˜ ë˜ëŠ” íšŒê·€ ë¬¸ì œì˜ ì¶œë ¥ ì°¨ì›ê³¼ ë™ì¼í•©ë‹ˆë‹¤. XOR ë¬¸ì œì™€ MLPë‹¨ì¼ í¼ì…‰íŠ¸ë¡ ì€ ì„ í˜• ë¶„ë¥˜ê¸°ì´ê¸° ë•Œë¬¸ì— XOR ë¬¸ì œì™€ ê°™ì€ ë¹„ì„ í˜• ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.XOR ë¬¸ì œëŠ” ë‘ ì…ë ¥ ê°’ì´ ë‹¤ë¥¼ ë•Œë§Œ 1ì„ ì¶œë ¥í•˜ëŠ” ë¬¸ì œë¡œ, ë‹¨ì¼ í¼ì…‰íŠ¸ë¡ ìœ¼ë¡œëŠ” í•´ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ MLPëŠ” ì€ë‹‰ì¸µì„ í†µí•´ ë¹„ì„ í˜•ì„±ì„ í•™ìŠµí•  ìˆ˜ ìˆì–´ XOR ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.03. í™œì„±í™” í•¨ìˆ˜í™œì„±í™” í•¨ìˆ˜ë¼ëŠ”ê²Œ ë¬´ì—‡ì¸ì§€, ì‹ ê²½ë§ì—ì„œ ì–´ë–¤ ì—­í• ì„ í•˜ëŠ”ì§€ ì•Œì•„ë³´ê³ , ì–´ë–¤ ì¢…ë¥˜ì˜ í™œì„±í™” í•¨ìˆ˜ê°€ ìˆëŠ”ì§€ ë°°ì›Œë´…ì‹œë‹¤1) í™œì„±í™” í•¨ìˆ˜ì˜ í•„ìš”ì„±ê³¼ ì¢…ë¥˜ í™œì„±í™” í•¨ìˆ˜ì˜ í•„ìš”ì„±í™œì„±í™” í•¨ìˆ˜ëŠ” ì‹ ê²½ë§ì˜ ê° ë‰´ëŸ°ì—ì„œ ì…ë ¥ê°’ì„ ì¶œë ¥ê°’ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.í™œì„±í™” í•¨ìˆ˜ê°€ ì—†ë‹¤ë©´ ì‹ ê²½ë§ì€ ë‹¨ìˆœ ì„ í˜•ë³€í™˜ë§Œ ìˆ˜í–‰í•˜ê²Œ ë˜ì–´ ë³µì¡í•œ íŒ¨í„´ì„ í•™ìŠµí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.í™œì„±í™” í•¨ìˆ˜ëŠ” ë¹„ ì„ í˜•ì„±ì„ ë„ì…í•˜ì—¬ ì‹ ê²½ë§ì´ ë³µì¡í•œ íŒ¨í„´ì„ í•™ìŠµí•  ìˆ˜ ìˆê²Œí•©ë‹ˆë‹¤. í™œì„±í™” í•¨ìˆ˜ì˜ ì¢…ë¥˜ReLU (Rectified Linear Unit)f(x)=maxâ¡(0,x)f(x) = (0, x)f(x)=max(0,x)ì¥ì : ê³„ì‚°ì´ ê°„ë‹¨í•˜ê³ , ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œ(vanishing gradient problem)ë¥¼ ì™„í™”í•©ë‹ˆë‹¤.ë‹¨ì : ìŒìˆ˜ ì…ë ¥ì— ëŒ€í•´ ê¸°ìš¸ê¸°ê°€ 0ì´ ë˜ëŠ” 'ì£½ì€ ReLU' ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Sigmoidf(x)=11+eâˆ’xf(x) = {1 + e^{-x}} f(x)=1+eâˆ’x1â€‹ì¥ì : ì¶œë ¥ ê°’ì´ 0ê³¼ 1 ì‚¬ì´ë¡œ ì œí•œë˜ì–´ í™•ë¥ ì„ í‘œí˜„í•˜ê¸°ì— ì í•©í•©ë‹ˆë‹¤.ë‹¨ì : ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œì™€ ì¶œë ¥ ê°’ì´ 0 ë˜ëŠ” 1ì— ê°€ê¹Œì›Œì§ˆ ë•Œ í•™ìŠµì´ ëŠë ¤ì§€ëŠ” ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.Tanh (Hyperbolic Tangent)f(x)=tanhâ¡(x)=exâˆ’eâˆ’xex+eâˆ’xf(x) = (x) = }{e^x + e^{-x}}f(x)=tanh(x)=ex+eâˆ’xexâˆ’eâˆ’xâ€‹ì¥ì : ì¶œë ¥ ê°’ì´ -1ê³¼ 1 ì‚¬ì´ë¡œ ì œí•œë˜ì–´ ì¤‘ì‹¬ì´ 0ì— ê°€ê¹Œì›Œì§‘ë‹ˆë‹¤.ë‹¨ì : ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.04. ì†ì‹¤ í•¨ìˆ˜ì™€ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì†ì‹¤í•¨ìˆ˜ì™€ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì´ ë¬´ì—‡ì¸ì§€ ë°°ìš°ê³  ì£¼ìš” ì‚¬ìš©í•˜ëŠ” í•¨ìˆ˜ì˜ ì¢…ë¥˜ë¥¼ ë°°ì›Œë´…ì‹œë‹¤.1) ì†ì‹¤ í•¨ìˆ˜ì˜ ì—­í• ê³¼ ì£¼ìš” ì¢…ë¥˜ ì†ì‹¤í•¨ìˆ˜ì˜ ì—­í• ì†ì‹¤ í•¨ìˆ˜(Loss Function)ëŠ” ëª¨ë¸ì˜ ì˜ˆì¸¡ ê°’ê³¼ ì‹¤ì œ ê°’ ì‚¬ì´ì˜ ì°¨ì´ë¥¼ ì¸¡ì •í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.ì†ì‹¤ í•¨ìˆ˜ëŠ” ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê³ , ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ì£¼ìš” ì†ì‹¤ í•¨ìˆ˜ì˜ ì¢…ë¥˜MSE (Mean Squared Error)MSE=1nâˆ‘i=1n(yiâˆ’y^i)2 = {n} _{i=1}^{n} (y_i - _i)^2 MSE=n1â€‹i=1âˆ‘nâ€‹(yiâ€‹âˆ’y^â€‹iâ€‹)2ì‚¬ìš© ë¶„ì•¼: íšŒê·€ ë¬¸ì œì—ì„œ ì£¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.íŠ¹ì§•: ì˜ˆì¸¡ ê°’ê³¼ ì‹¤ì œ ê°’ì˜ ì°¨ì´ë¥¼ ì œê³±í•˜ì—¬ í‰ê· ì„ êµ¬í•©ë‹ˆë‹¤.Cross-EntropyCross-Entropy=âˆ’âˆ‘i=1nyilogâ¡(y^i) = -_{i=1}^{n} y_i (_i)Cross-Entropy=âˆ’i=1âˆ‘nâ€‹yiâ€‹log(y^â€‹iâ€‹)ì‚¬ìš© ë¶„ì•¼: ë¶„ë¥˜ ë¬¸ì œì—ì„œ ì£¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.íŠ¹ì§•: ì˜ˆì¸¡ í™•ë¥ ê³¼ ì‹¤ì œ í´ë˜ìŠ¤ ê°„ì˜ ì°¨ì´ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤.2) ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì˜ ê°œë…ê³¼ ì¢…ë¥˜ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì˜ ê°œë…ìµœì í™” ì•Œê³ ë¦¬ì¦˜(Optimization Algorithm)ì€ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì •í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì€ ì†ì‹¤ í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•˜ê³ , ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. ì£¼ìš” ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì˜ ì¢…ë¥˜SGD (Stochastic Gradient Descent)ê°œë…: ì „ì²´ ë°ì´í„°ì…‹ì´ ì•„ë‹Œ ë¬´ì‘ìœ„ë¡œ ì„ íƒëœ ì¼ë¶€ ë°ì´í„°(ë¯¸ë‹ˆë°°ì¹˜)ë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•˜ê³  ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.ì¥ì : ê³„ì‚°ì´ ë¹ ë¥´ê³ , í° ë°ì´í„°ì…‹ì—ì„œë„ íš¨ìœ¨ì ìœ¼ë¡œ ë™ì‘í•©ë‹ˆë‹¤.ë‹¨ì : ìµœì ì ì— ë„ë‹¬í•˜ê¸°ê¹Œì§€ ì§„ë™ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Adam (Adaptive Moment Estimation)ê°œë…: ëª¨ë©˜í…€ê³¼ RMSPropì„ ê²°í•©í•œ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ, í•™ìŠµë¥ ì„ ì ì‘ì ìœ¼ë¡œ ì¡°ì •í•©ë‹ˆë‹¤.ì¥ì : ë¹ ë¥¸ ìˆ˜ë ´ ì†ë„ì™€ ì•ˆì •ì ì¸ í•™ìŠµì„ ì œê³µí•©ë‹ˆë‹¤.ë‹¨ì : í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •ì´ ë³µì¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.05. ì—­ì „íŒŒì— ëŒ€í•´ ì•Œì•„ë³¼ê¹Œìš”?ì—­ì „íŒŒì— ëŒ€í•´ ë°°ì›Œë´…ì‹œë‹¤1) ì—­ì „íŒŒ ì•Œê³ ë¦¬ì¦˜ì˜ ê°œë…ê³¼ ìˆ˜í•™ì  ì›ë¦¬ ì—­ì „íŒŒ ì•Œê³ ë¦¬ì¦˜ì˜ ê°œë…ì—­ì „íŒŒ(Backpropagation)ëŠ” ì‹ ê²½ë§ì˜ ê°€ì¤‘ì¹˜ë¥¼ í•™ìŠµì‹œí‚¤ê¸° ìœ„í•´ ì‚¬ìš©ë˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤. ì¶œë ¥ì—ì„œ ì…ë ¥ ë°©í–¥ìœ¼ë¡œ ì†ì‹¤ í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•˜ê³ , ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. ì—­ì „íŒŒì˜ ìˆ˜í•™ì  ì›ë¦¬ì—°ì‡„ ë²•ì¹™(Chain Rule)ì„ ì‚¬ìš©í•´ ì†ì‹¤í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.ê° ì¸µì˜ ê¸°ìš¸ê¸°ëŠ” ì´ì „ ì¸µì˜ ê¸°ìš¸ê¸°ì™€ í˜„ì¬ ì¸µì˜ ê¸°ìš¸ê¸°ë¥¼ ê³±í•˜ì—¬ ê³„ì‚°í•©ë‹ˆë‹¤.ì´ë¥¼ í†µí•´ ì‹ ê²½ë§ì˜ ëª¨ë“  ê°€ì¤‘ì¹˜ê°€ ì—…ë°ì´íŠ¸ ë©ë‹ˆë‹¤ALT.
Vector DB ê°œë… ë° RAG (Retrieval-Augmented Generation) ê°œë… LLM & RAGë¥¼ í™œìš©í•œ AI ì„œë¹„ìŠ¤ ë§Œë“¤ê¸°/ LLM & RAGë¥¼ í™œìš©í•œ AI ì„œë¹„ìŠ¤ ë§Œë“¤ê¸° - 5ì£¼ì°¨/Vector DB ê°œë… ë° RAG (Retrieval-Augmented Generation) ê°œë…ì œì‘:Vector DB ê°œë… ë° RAG (Retrieval-Augmented Generation) ê°œë…ìˆ˜ì—… ëª©í‘œvectorDBì™€ RAGì˜ ê°œë…ì— ëŒ€í•´ì„œ í•™ìŠµí•˜ê³ , í•œêµ­ì–´ ì„ë² ë”© ì‹¤ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤.ëª©ì°¨Vector DB ê°œë… Vector DBë€?Faissë€?ì„ë² ë”©(Embedding) ê°œë… ì„ë² ë”©ì´ë€?RAG (Retrieval-Augmented Generation) ê°œë…RAGì˜ ë™ì‘ ì›ë¦¬Retrieval (ê²€ìƒ‰) ë‹¨ê³„Generation (ìƒì„±) ë‹¨ê³„RAGì˜ ì¥ì Vector DBì™€ RAGì˜ ê²°í•©ë™ì‘ íë¦„í•œêµ­ì–´ ì„ë² ë”© ì‹¤ìŠµí•œêµ­ì–´ ë°ì´í„° ì„ë² ë”©Vector DBì™€ RAGë¡œ LLM ì‹œìŠ¤í…œ êµ¬ì¶•í•˜ê¸°Vector DB ê°œë… Vector DBë€?â—Vector DBëŠ” ë°ì´í„°ë¥¼ ë²¡í„° í˜•ì‹ìœ¼ë¡œ ì €ì¥í•˜ê³ , ê·¸ ë²¡í„°ë“¤ì„ íš¨ìœ¨ì ìœ¼ë¡œ ê²€ìƒ‰í•  ìˆ˜ ìˆëŠ” ë°ì´í„°ë² ì´ìŠ¤ì—ìš”. ì¼ë°˜ì ì¸ ë°ì´í„°ë² ì´ìŠ¤ëŠ” ì •í™•í•œ ì¼ì¹˜ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë°ì´í„°ë¥¼ ê²€ìƒ‰í•˜ì§€ë§Œ, Vector DBëŠ” ìœ ì‚¬í•œ ë²¡í„° ê°„ì˜ ê²€ìƒ‰ì„ ì§€ì›í•˜ì£ .ë²¡í„°(ì„ë² ë”©)ì˜ ì—­í• í…ìŠ¤íŠ¸ë‚˜ ì´ë¯¸ì§€ ë“±ì˜ ë¹„ì •í˜• ë°ì´í„°ë¥¼ ë²¡í„°í™”(ì„ë² ë”©)í•´ì„œ ì €ì¥í•´ìš”.
ì´ ë²¡í„°ëŠ” ë°ì´í„°ì˜ ì˜ë¯¸ë‚˜ íŠ¹ì§•ì„ ìˆ˜ì¹˜ë¡œ í‘œí˜„í•œ ê²ƒì´ë©°, ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•´ ê´€ë ¨ì„±ì´ ë†’ì€ í•­ëª©ì„ ì°¾ìŠµë‹ˆë‹¤.ì˜ˆë¥¼ ë“¤ì–´, "ê°•ì•„ì§€"ë¼ëŠ” í…ìŠ¤íŠ¸ëŠ” ë²¡í„°ë¡œ ë³€í™˜ë˜ë©°, ë¹„ìŠ·í•œ ì˜ë¯¸ë¥¼ ê°€ì§„ "ë°˜ë ¤ê²¬"ë„ ë²¡í„°í™”ë˜ì–´ ìœ ì‚¬ë„ê°€ ë†’ì€ í•­ëª©ìœ¼ë¡œ ê²€ìƒ‰ë  ìˆ˜ ìˆì–´ìš”.Faissë€?ğŸ’¡FaissëŠ” Facebook AI Researchì—ì„œ ê°œë°œí•œ ë²¡í„° ê²€ìƒ‰ ì—”ì§„ìœ¼ë¡œ, Vector DBë¥¼ êµ¬í˜„í•  ë•Œ ìì£¼ ì‚¬ìš©ë¼ìš”. ëŒ€ê·œëª¨ ë²¡í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê²€ìƒ‰í•˜ê³ , ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ëŠ” ë° íƒì›”í•œ ì„±ëŠ¥ì„ ë°œíœ˜í•©ë‹ˆë‹¤. íŠ¹íˆ ë¹ ë¥¸ ì†ë„ì™€ í™•ì¥ì„±ì´ í•„ìš”í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ë§ì´ ì“°ì´ì£ .ì„ë² ë”©(Embedding) ê°œë… ì„ë² ë”©ì´ë€?ğŸ’¡ì„ë² ë”©ì€ í…ìŠ¤íŠ¸, ì´ë¯¸ì§€ ë“±ì˜ ë°ì´í„°ë¥¼ ê³ ì°¨ì› ê³µê°„ì—ì„œ ë²¡í„°(ìˆ«ì ë°°ì—´)ë¡œ ë³€í™˜í•˜ëŠ” ì‘ì—…ì´ì—ìš”. LLM(ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸)ì´ ë¬¸ì¥ì„ ì´í•´í•˜ê¸° ìœ„í•´ì„œëŠ” ë‹¨ì–´ì™€ ë¬¸ì¥ì„ ë²¡í„°ë¡œ ë³€í™˜í•´ì•¼, ì»´í“¨í„°ê°€ ì˜ë¯¸ì  ìœ ì‚¬ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ì„ë² ë”©ì˜ ì‘ë™ ë°©ì‹ì„ë² ë”©ì€ ë‹¨ì–´ ê°„ì˜ ì˜ë¯¸ì  ê´€ê³„ë¥¼ ë²¡í„° ê³µê°„ì— íˆ¬ì˜í•´ìš”.
ì˜ˆë¥¼ ë“¤ì–´, "ê³ ì–‘ì´"ì™€ "ê°œ"ëŠ” ë¹„ìŠ·í•œ ì˜ë¯¸ë¥¼ ê°€ì§€ë¯€ë¡œ ë²¡í„° ê³µê°„ì—ì„œë„ ì„œë¡œ ê°€ê¹Œìš´ ìœ„ì¹˜ì— ì¡´ì¬í•˜ê²Œ ë©ë‹ˆë‹¤. 
ë°˜ëŒ€ë¡œ "ì‚¬ê³¼"ì™€ "ìë™ì°¨"ì²˜ëŸ¼ ì „í˜€ ë‹¤ë¥¸ ì˜ë¯¸ë¥¼ ê°€ì§„ ë‹¨ì–´ë“¤ì€ ë²¡í„° ê³µê°„ì—ì„œ ë©€ë¦¬ ë–¨ì–´ì§„ ìœ„ì¹˜ì— ë†“ì´ê²Œ ë˜ì£ .RAG (Retrieval-Augmented Generation) ê°œë…â—RAGëŠ” Retrieval-Augmented Generationì˜ ì•½ìë¡œ, 
LLM(ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸)ê³¼ ê²€ìƒ‰ ì‹œìŠ¤í…œì„ ê²°í•©í•œ ê°œë…ì´ì—ìš”.
RAGëŠ” ê¸°ì¡´ì˜ LLMë§Œìœ¼ë¡œëŠ” í•´ê²°í•  ìˆ˜ ì—†ëŠ” ë¬¸ì œë¥¼, ì™¸ë¶€ ì •ë³´ ê²€ìƒ‰ì„ í†µí•´ ë³´ì™„í•  ìˆ˜ ìˆì–´ìš”. 
ìµœì‹  ì •ë³´ë¥¼ í¬í•¨í•œ ë‹µë³€ì„ ì œê³µí•˜ëŠ” ë° ë§¤ìš° ìœ ë¦¬í•˜ì£ .RAGì˜ ë™ì‘ ì›ë¦¬1ï¸âƒ£Retrieval (ê²€ìƒ‰) ë‹¨ê³„ì‚¬ìš©ìê°€ ì§ˆë¬¸ì„ í•˜ë©´, ë²¡í„° DBì—ì„œ ì§ˆë¬¸ê³¼ ìœ ì‚¬í•œ ë¬¸ì„œë‚˜ ë°ì´í„°ë¥¼ ê²€ìƒ‰í•´ìš”. ì´ë•Œ ì„ë² ë”© ëª¨ë¸ì„ ì‚¬ìš©í•´ ì§ˆë¬¸ì„ ë²¡í„°ë¡œ ë³€í™˜í•˜ê³ , ë²¡í„° ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•´ ê´€ë ¨ ë°ì´í„°ë¥¼ ì°¾ì•„ëƒ…ë‹ˆë‹¤.2ï¸âƒ£Generation (ìƒì„±) ë‹¨ê³„ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ LLMì— ì „ë‹¬í•˜ê³ , ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤. ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ì°¸ì¡°í•´ ìµœì‹  ì •ë³´ë¥¼ í¬í•¨í•œ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ì£ .RAGì˜ ì¥ì ìµœì‹  ì •ë³´ ì œê³µ: LLMì´ í•™ìŠµí•œ ë°ì´í„° ì™¸ì˜ ìµœì‹  ë¬¸ì„œë¥¼ ê²€ìƒ‰í•´ ì •ë³´ì˜ ì •í™•ë„ë¥¼ ë†’ì¼ ìˆ˜ ìˆì–´ìš”.ìœ ì—°ì„±: LLMì´ ëª¨ë¥´ëŠ” ì •ë³´ë„ ì™¸ë¶€ ê²€ìƒ‰ì„ í†µí•´ ë‹µë³€í•  ìˆ˜ ìˆì–´ ì§€ì‹ì˜ í™•ì¥ì„±ì´ ë›°ì–´ë‚©ë‹ˆë‹¤.ì§€ì‹ì˜ í•œê³„ ê·¹ë³µ: í•™ìŠµ ë°ì´í„°ì—ë§Œ ì˜ì¡´í•˜ì§€ ì•Šê³ , ì™¸ë¶€ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì‹¤ì‹œê°„ ì •ë³´ë¥¼ ì œê³µë°›ì•„ ë”ìš± í’ë¶€í•œ ë‹µë³€ì„ í•  ìˆ˜ ìˆì–´ìš”.Vector DBì™€ RAGì˜ ê²°í•©ğŸ’¡Vector DBì™€ RAGì˜ ê²°í•©ì€ ë§¤ìš° ê°•ë ¥í•´ìš”. Vector DBëŠ” ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•´ì£¼ê³ , RAGëŠ” ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•œ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ê³¼ì •ì´ì£ .ë™ì‘ íë¦„ALTí•œêµ­ì–´ ì„ë² ë”© ì‹¤ìŠµí•œêµ­ì–´ ë°ì´í„° ì„ë² ë”©í•œêµ­ì–´ ë¬¸ì¥ì„ ì„ë² ë”©í•˜ë ¤ë©´ ì‚¬ì „ í•™ìŠµëœ í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ì´ í•„ìš”í•´ìš”. [ì½”ë“œìŠ¤ë‹ˆí«] í•œêµ­ì–´ ì„ë² ë”© ì‹¤ìŠµ ì½”ë“œPythonë³µì‚¬from sentence_transformers import SentenceTransformer
import numpy as np

# Multilingual-E5-large-instruct ëª¨ë¸ ë¡œë“œ
model = SentenceTransformer('intfloat/multilingual-e5-large')
# ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
sentences = [
"ì°¸ìƒˆëŠ” ì§¹ì§¹í•˜ê³  ì›ë‹ˆë‹¤.",
"LangChainê³¼ Faissë¥¼ í™œìš©í•œ ì˜ˆì‹œì…ë‹ˆë‹¤.",
"ìì—°ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©ë²•ì„ ë°°ì›Œë´…ì‹œë‹¤.",
"ìœ ì‚¬í•œ ë¬¸ì¥ì„ ê²€ìƒ‰í•˜ëŠ” ë°©ë²•ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.",
"ê°•ì¢Œë¥¼ ìˆ˜ê°•í•˜ì‹œëŠ” ìˆ˜ê°•ìƒ ì—¬ëŸ¬ë¶„ ê°ì‚¬í•©ë‹ˆë‹¤!"
]
# ë¬¸ì¥ë“¤ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
embeddings = model.encode(sentences)
# ì„ë² ë”© ë²¡í„° ì¶œë ¥
print(embeddings.shape) # (4, 1024) - 4ê°œì˜ ë¬¸ì¥ì´ 1024 ì°¨ì›ì˜ ë²¡í„°ë¡œ ë³€í™˜ë¨
â€‹Pythonë³µì‚¬from sentence_transformers import SentenceTransformer
import numpy as np

# Multilingual-E5-large-instruct ëª¨ë¸ ë¡œë“œ
model = SentenceTransformer('intfloat/multilingual-e5-large')
# ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
sentences = [
"ì°¸ìƒˆëŠ” ì§¹ì§¹í•˜ê³  ì›ë‹ˆë‹¤.",
"LangChainê³¼ Faissë¥¼ í™œìš©í•œ ì˜ˆì‹œì…ë‹ˆë‹¤.",
"ìì—°ì–´ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©ë²•ì„ ë°°ì›Œë´…ì‹œë‹¤.",
"ìœ ì‚¬í•œ ë¬¸ì¥ì„ ê²€ìƒ‰í•˜ëŠ” ë°©ë²•ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.",
"ê°•ì¢Œë¥¼ ìˆ˜ê°•í•˜ì‹œëŠ” ìˆ˜ê°•ìƒ ì—¬ëŸ¬ë¶„ ê°ì‚¬í•©ë‹ˆë‹¤!"
]
# ë¬¸ì¥ë“¤ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
embeddings = model.encode(sentences)
# ì„ë² ë”© ë²¡í„° ì¶œë ¥
print(embeddings.shape) # (4, 1024) - 4ê°œì˜ ë¬¸ì¥ì´ 1024 ì°¨ì›ì˜ ë²¡í„°ë¡œ ë³€í™˜ë¨
â€‹Vector DBì™€ RAGë¡œ LLM ì‹œìŠ¤í…œ êµ¬ì¶•í•˜ê¸°1ï¸âƒ£ì§ˆë¬¸ì„ ì„ë² ë”©
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.2ï¸âƒ£ë²¡í„° DBì—ì„œ ê²€ìƒ‰
ë²¡í„° DBì—ì„œ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.3ï¸âƒ£ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ LLMì´ ë‹µë³€ ìƒì„±
ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ LLMì— ì „ë‹¬í•˜ê³ , ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.ğŸ˜€ì´ëŸ¬í•œ ê³¼ì •ì„ í†µí•´ RAGì™€ Vector DBëŠ” ìµœì‹  ì •ë³´ ê¸°ë°˜ì˜ ëŒ€í™”í˜• AI ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ëŠ” ë° ë§¤ìš° ìœ ìš©í•´ìš”!
Vector DBì™€ RAG ê°œë…ì„ ê¸°ë°˜ìœ¼ë¡œ LLM ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ë•Œ, ë” ë‚˜ì€ ì •ë³´ ê²€ìƒ‰ê³¼ ë‹µë³€ ìƒì„±ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
NumPy ë°°ì—´(array) ìƒì„± ë° ê¸°ì´ˆ ì—°ì‚°ì¸ê³µì§€ëŠ¥ì„ ìœ„í•œ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬/ì¸ê³µì§€ëŠ¥ì„ ìœ„í•œ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ - 2ì£¼ì°¨/NumPy ë°°ì—´(array) ìƒì„± ë° ê¸°ì´ˆ ì—°ì‚°ì œì‘:NumPy ë°°ì—´(array) ìƒì„± ë° ê¸°ì´ˆ ì—°ì‚°ìˆ˜ì—… ëª©í‘œNumPy ë°°ì—´ ìƒì„± ë° ê¸°ì´ˆ ì—°ì‚°ì— ëŒ€í•´ í•™ìŠµí•©ë‹ˆë‹¤.ëª©ì°¨NumPy ë°°ì—´(array) ìƒì„±í•˜ê¸°NumPy ë°°ì—´ì˜ ê¸°ì´ˆ ì—°ì‚° ë°°ì—´ì˜ ì¸ë±ì‹±ê³¼ ìŠ¬ë¼ì´ì‹±ìš”ì•½NumPy ë°°ì—´(array) ìƒì„±í•˜ê¸°ğŸ“šNumPy ë°°ì—´ì€ íŒŒì´ì¬ì˜ ë¦¬ìŠ¤íŠ¸ì™€ ë¹„ìŠ·í•˜ì§€ë§Œ, ë” ê°•ë ¥í•˜ê³  íš¨ìœ¨ì ì¸ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤. ë°°ì—´ì„ ìƒì„±í•˜ëŠ” ë‹¤ì–‘í•œ ë°©ë²•ì„ ì‚´í´ë³¼ê¹Œìš”?ë¦¬ìŠ¤íŠ¸ë¡œë¶€í„° ë°°ì—´ ìƒì„±ë¦¬ìŠ¤íŠ¸ë¥¼ ë°°ì—´ë¡œ ë³€í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Pythonë³µì‚¬import numpy as np

# ë¦¬ìŠ¤íŠ¸ë¡œë¶€í„° ë°°ì—´ ìƒì„±
arr = np.array([1, 2, 3, 4, 5])
print(arr)

â€‹ì¶œë ¥ ê²°ê³¼:Plain Textë³µì‚¬[1 2 3 4 5]

â€‹ë‹¤ì–‘í•œ í˜•íƒœì˜ ë°°ì—´ ìƒì„±0ìœ¼ë¡œ ì±„ì›Œì§„ ë°°ì—´ ìƒì„±Pythonë³µì‚¬zero_arr = np.zeros((2, 3)) # 2x3 í¬ê¸°ì˜ ë°°ì—´
print(zero_arr)

â€‹ì¶œë ¥ ê²°ê³¼Plain Textë³µì‚¬[[0. 0. 0.]
 [0. 0. 0.]]â€‹1ë¡œ ì±„ì›Œì§„ ë°°ì—´ ìƒì„±Pythonë³µì‚¬ones_arr = np.ones((3, 2)) # 3x2 í¬ê¸°ì˜ ë°°ì—´
print(ones_arr)

â€‹ì¶œë ¥ ê²°ê³¼Plain Textë³µì‚¬[[1. 1.]
 [1. 1.]
 [1. 1.]]â€‹íŠ¹ì • ê°’ìœ¼ë¡œ ì±„ì›Œì§„ ë°°ì—´ ìƒì„±Pythonë³µì‚¬full_arr = np.full((2, 2), 7) # 2x2 í¬ê¸°ì˜ 7ë¡œ ì±„ì›Œì§„ ë°°ì—´
print(full_arr)

â€‹ì¶œë ¥ ê²°ê³¼Plain Textë³µì‚¬[[7 7]
 [7 7]]

â€‹ì—°ì†ì ì¸ ê°’ìœ¼ë¡œ ì±„ì›Œì§„ ë°°ì—´ ìƒì„±:Pythonë³µì‚¬range_arr = np.arange(10) # 0ë¶€í„° 9ê¹Œì§€ì˜ ì—°ì†ëœ ê°’
print(range_arr)

â€‹ì¶œë ¥ ê²°ê³¼Plain Textë³µì‚¬[0 1 2 3 4 5 6 7 8 9]

â€‹NumPy ë°°ì—´ì˜ ê¸°ì´ˆ ì—°ì‚° ğŸ“šNumPy ë°°ì—´ì—ì„œëŠ” ë°˜ë³µë¬¸ ì—†ì´ ë²¡í„°í™” ì—°ì‚°ì„ í†µí•´ ì‰½ê²Œ ì—°ì‚°ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ê¸°ë³¸ ì—°ì‚°ë§ì…ˆ, ëº„ì…ˆ, ê³±ì…ˆ, ë‚˜ëˆ—ì…ˆ ëª¨ë‘ ê°€ëŠ¥í•©ë‹ˆë‹¤.Pythonë³µì‚¬arr = np.array([1, 2, 3, 4, 5])
# ê° ì›ì†Œì— 2ë¥¼ ë”í•˜ê¸°
arr_add = arr + 2
print(arr_add)
# ê° ì›ì†Œì— 2ë¥¼ ê³±í•˜ê¸°
arr_mul = arr * 2
print(arr_mul)

â€‹ì¶œë ¥ ê²°ê³¼Plain Textë³µì‚¬[3 4 5 6 7]
[ 2  4  6  8 10]

â€‹ë°°ì—´ ê°„ì˜ ì—°ì‚°ë°°ì—´ ê°„ì˜ ë§ì…ˆê³¼ ê³±ì…ˆë„ ì‰½ìŠµë‹ˆë‹¤.Pythonë³µì‚¬arr1 = np.array([1, 2, 3])
arr2 = np.array([4, 5, 6])
# ë°°ì—´ ê°„ ë§ì…ˆ
arr_sum = arr1 + arr2
print(arr_sum)
# ë°°ì—´ ê°„ ê³±ì…ˆ
arr_mul = arr1 * arr2
print(arr_mul)

â€‹ì¶œë ¥ ê²°ê³¼Plain Textë³µì‚¬[5 7 9]
[ 4 10 18]

â€‹ë°°ì—´ì˜ ì¸ë±ì‹±ê³¼ ìŠ¬ë¼ì´ì‹±ì¸ë±ì‹±(Indexing)ë°°ì—´ì˜ íŠ¹ì • ìœ„ì¹˜ì— ì ‘ê·¼í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.Pythonë³µì‚¬arr = np.array([10, 20, 30, 40, 50])
# ì²« ë²ˆì§¸ ì›ì†Œ
print(arr[0])
# ë§ˆì§€ë§‰ ì›ì†Œ
print(arr[-1])

â€‹ì¶œë ¥ ê²°ê³¼Plain Textë³µì‚¬10
50

â€‹ìŠ¬ë¼ì´ì‹±(Slicing)ë°°ì—´ì˜ ì¼ë¶€ë¶„ì„ ì˜ë¼ë‚´ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.Pythonë³µì‚¬arr = np.array([10, 20, 30, 40, 50])
# ë‘ ë²ˆì§¸ë¶€í„° ë„¤ ë²ˆì§¸ ì›ì†Œê¹Œì§€
sliced_arr = arr[1:4]
print(sliced_arr)

â€‹ì¶œë ¥ ê²°ê³¼Plain Textë³µì‚¬[20 30 40]

â€‹ë‹¤ì°¨ì› ë°°ì—´ì˜ ì¸ë±ì‹± ë° ìŠ¬ë¼ì´ì‹±ë‹¤ì°¨ì› ë°°ì—´ì˜ ê²½ìš°, ì½¤ë§ˆë¥¼ ì‚¬ìš©í•´ ì ‘ê·¼í•©ë‹ˆë‹¤.Pythonë³µì‚¬arr2d = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
# íŠ¹ì • ì›ì†Œ ì ‘ê·¼ (2í–‰ 3ì—´)
print(arr2d[1, 2])
# ìŠ¬ë¼ì´ì‹± (2í–‰ê¹Œì§€, 2ì—´ê¹Œì§€)
sliced_arr2d = arr2d[:2, :2]
print(sliced_arr2d)

â€‹ì¶œë ¥ ê²°ê³¼Plain Textë³µì‚¬6
[[1 2]
 [4 5]]

â€‹ìš”ì•½ğŸ“šìš”ì•½NumPy ë°°ì—´ì€ ë¦¬ìŠ¤íŠ¸ë³´ë‹¤ ê°•ë ¥í•œ ê¸°ëŠ¥ì„ ì œê³µí•˜ë©°, ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ë°°ì—´ ê°„ì˜ ë²¡í„°í™” ì—°ì‚°ì„ í†µí•´ ë³µì¡í•œ ìˆ˜í•™ ì—°ì‚°ì„ ì‰½ê²Œ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ì¸ë±ì‹±ê³¼ ìŠ¬ë¼ì´ì‹±ì„ ì‚¬ìš©í•´ ë°°ì—´ì˜ íŠ¹ì • ì›ì†Œë‚˜ ë¶€ë¶„ ë°°ì—´ì— ì‰½ê²Œ ì ‘ê·¼ ê°€ëŠ¥í•©ë‹ˆë‹¤.ì´ì œ NumPy ë°°ì—´ì„ ììœ ìì¬ë¡œ ë‹¤ë¤„ë³´ì„¸ìš”! â€‹Son minsuck9ì›” 25ì¼í† ê¸€ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 3ê°•. Anaconda ì„¤ì¹˜ ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì†Œê°œ[SCC] ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ - 1ì£¼ì°¨/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 3ê°•. Anaconda ì„¤ì¹˜ ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì†Œê°œì œì‘:[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 3ê°•. Anaconda ì„¤ì¹˜ ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì†Œê°œ[ìˆ˜ì—… ëª©í‘œ]Anaconda ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì£¼ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ê¸°ëŠ¥ì„ ë°°ìš°ê³  ì‹¤ìŠµí•©ë‹ˆë‹¤[ëª©ì°¨]01. Anaconda ì†Œê°œ ë° ì„¤ì¹˜02. ì£¼ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì†Œê°œğŸ’¡
 
 01. Anaconda ì†Œê°œ ë° ì„¤ì¹˜âœ”ï¸Anacondaê°€ ë¬´ì—‡ì¸ì§€ ì•Œì•„ë³´ê³ , ê°™ì´ ì„¤ì¹˜ë¥¼ ì§„í–‰í•´ ë´…ì‹œë‹¤1) Anaconda ì†Œê°œ Anaconda ë€ë°ì´í„° ê³¼í•™ ë° ë¨¸ì‹ ëŸ¬ë‹ì„ ìœ„í•œ í†µí•© ê°œë°œ í™˜ê²½(IDE)ë‹¤ì–‘í•œ ë°ì´í„° ê³¼í•™ ë„êµ¬ì™€ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í¬í•¨í•˜ê³  ìˆì–´, ë°ì´í„° ë¶„ì„ ë° ë¨¸ì‹ ëŸ¬ë‹ ì‘ì—…ì„ ì‰½ê²Œ ì‹œì‘ê°€ëŠ¥AnacondaëŠ” Pythonê³¼ R í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë¥¼ ì§€ì›2) Anaconda ì„¤ì¹˜ Anaconda ì„¤ì¹˜ ë°©ë²•Anaconda ë‹¤ìš´ë¡œë“œ: Anaconda ê³µì‹ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ìš´ì˜ ì²´ì œì— ë§ëŠ” ì„¤ì¹˜ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œ í•©ë‹ˆë‹¤ì„¤ì¹˜ íŒŒì¼ ì‹¤í–‰: ë‹¤ìš´ë¡œë“œí•œ ì„¤ì¹˜ íŒŒì¼ì„ ì‹¤í–‰í•˜ì—¬ ì„¤ì¹˜ë¥¼ ì§„í–‰ í•©ë‹ˆë‹¤í™˜ê²½ ë³€ìˆ˜ ì„¤ì •: ì„¤ì¹˜ ê³¼ì •ì—ì„œ "Add Anaconda to my PATH environment variable" ì˜µì…˜ì„ ì„ íƒí•˜ì—¬ í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì • í•©ë‹ˆë‹¤ì„¤ì¹˜ ì™„ë£Œ: ì„¤ì¹˜ê°€ ì™„ë£Œë˜ë©´ Anaconda Navigatorì™€ Anaconda Promptë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Anaconda ì„¤ì¹˜ í™•ì¸Anaconda ì„¤ì¹˜ í™•ì¸ {5px}Anaconda ì„¤ì¹˜ í™•ì¸ ï»¿â€‹Pythonë³µì‚¬# Anaconda ë²„ì „ í™•ì¸
conda --version
â€‹ Anaconda ì£¼ìš” ê¸°ëŠ¥íŒ¨í‚¤ì§€ ê´€ë¦¬: conda ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜, ì—…ë°ì´íŠ¸, ì œê±°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.í™˜ê²½ ê´€ë¦¬: ê°€ìƒ í™˜ê²½ì„ ìƒì„±í•˜ê³  ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Jupyter Notebook: ì›¹ ê¸°ë°˜ì˜ ëŒ€í™”í˜• ê°œë°œ í™˜ê²½ì„ ì œê³µí•©ë‹ˆë‹¤.Spyder: í†µí•© ê°œë°œ í™˜ê²½(IDE)ìœ¼ë¡œ, ì½”ë“œ í¸ì§‘ê¸°ì™€ ë””ë²„ê±°ë¥¼ í¬í•¨í•©ë‹ˆë‹¤.ê°€ìƒí™˜ê²½ ìƒì„± ë° ê´€ë¦¬ {5px}ê°€ìƒí™˜ê²½ ìƒì„± ë° ê´€ë¦¬ ï»¿â€‹Pythonë³µì‚¬# ìƒˆë¡œìš´ ê°€ìƒ í™˜ê²½ ìƒì„±
conda create --name myenv

# ê°€ìƒ í™˜ê²½ í™œì„±í™”
conda activate myenv

# ê°€ìƒ í™˜ê²½ ë¹„í™œì„±í™”
conda deactivate

# ê°€ìƒ í™˜ê²½ ì œê±°
conda remove --name myenv --all
â€‹02. ì£¼ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì†Œê°œâœ”ï¸ë¨¸ì‹ ëŸ¬ë‹ ê°•ì˜ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì£¼ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì†Œê°œë“œë¦¬ê³  ê°„ë‹¨í•œ ì‹¤ìŠµì„ ì§„í–‰í•´ ë´…ì‹œë‹¤1) Numpy ë¼ì´ë¸ŒëŸ¬ë¦¬ ì†Œê°œ  Numpy ë¼ì´ë¸ŒëŸ¬ë¦¬ë€?ìˆ˜ì¹˜ ê³„ì‚°ì„ ìœ„í•œ Python ë¼ì´ë¸ŒëŸ¬ë¦¬NumpyëŠ” ë‹¤ì°¨ì› ë°°ì—´ ê°ì²´ì¸ ndarrayë¥¼ ì œê³µë²¡í„°í™” ì—°ì‚°ì„ í†µí•´ ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ ìˆ˜ì¹˜ ê³„ì‚°ì„ ìˆ˜í–‰ ê°€ëŠ¥ Numpy ì„¤ì¹˜Numpy ì„¤ì¹˜ {5px}Numpy ì„¤ì¹˜ ï»¿â€‹Pythonë³µì‚¬# condaë¥¼ ì‚¬ìš©í•˜ì—¬ Numpy ì„¤ì¹˜
conda install numpy

# pipë¥¼ ì‚¬ìš©í•˜ì—¬ Numpy ì„¤ì¹˜
pip install numpy
â€‹ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” conda ë˜ëŠ”pipë¥¼ ì‚¬ìš©í•˜ì—¬ ì„¤ì¹˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. { conda }ë¼ì´ë¸ŒëŸ¬ë¦¬ëŠ” conda ë˜ëŠ”pipë¥¼ ì‚¬ìš©í•˜ì—¬ ì„¤ì¹˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ï»¿â€‹ Numpy ì£¼ìš” ê¸°ëŠ¥ë‹¤ì°¨ì› ë°°ì—´ ê°ì²´(ndarray): ë‹¤ì°¨ì› ë°°ì—´ì„ ìƒì„±í•˜ê³  ì¡°ì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ë²¡í„°í™” ì—°ì‚°: ë°°ì—´ ê°„ì˜ ì—°ì‚°ì„ ë¹ ë¥´ê²Œ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ìˆ˜í•™ í•¨ìˆ˜: ë‹¤ì–‘í•œ ìˆ˜í•™ í•¨ìˆ˜ë¥¼ ì œê³µí•©ë‹ˆë‹¤. Numpy ì˜ˆì œë‹¤ì°¨ì› ë°°ì—´ì„ ìƒì„±í•˜ê³  ê¸°ë³¸ ì—°ì‚°ì„ ìˆ˜í–‰í•˜ëŠ” ì˜ˆì œë¥¼ ì‹¤ìŠµí•´ ë´…ì‹œë‹¤Numpy ì˜ˆì œ {5px}Numpy ì˜ˆì œ ï»¿â€‹Pythonë³µì‚¬import numpy as np

# 1ì°¨ì› ë°°ì—´ ìƒì„±
arr1 = np.array([1, 2, 3, 4, 5])
# 2ì°¨ì› ë°°ì—´ ìƒì„±
arr2 = np.array([[1, 2, 3], [4, 5, 6]])
# ë°°ì—´ì˜ í¬ê¸° í™•ì¸
print(arr1.shape)
print(arr2.shape)
# ë°°ì—´ì˜ ë°ì´í„° íƒ€ì… í™•ì¸
print(arr1.dtype)
print(arr2.dtype)
# ë°°ì—´ ê°„ì˜ ì—°ì‚°
arr_sum = arr1 + arr1
arr_product = arr1 * arr1

print(arr_sum)
print(arr_product)
â€‹2) Pandas ë¼ì´ë¸ŒëŸ¬ë¦¬ ì†Œê°œ Pandas ë¼ì´ë¸ŒëŸ¬ë¦¬ë€?ë°ì´í„° ì¡°ì‘ ë° ë¶„ì„ì„ ìœ„í•œ Python ë¼ì´ë¸ŒëŸ¬ë¦¬ë°ì´í„° í”„ë ˆì„(DataFrame)ì´ë¼ëŠ” êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬í•˜ê³  ì¡°ì‘ ê°€ëŠ¥ì´ ë°ì´í„° í”„ë ˆì„ì´ë¼ëŠ” êµ¬ì¡° ë•ì— í…Œì´ë¸” í˜•ì‹ì˜ ë°ì´í„°ë¥¼ ë‹¤ë£¨ëŠ” ë° ë§¤ìš° ìœ ìš© í•©ë‹ˆë‹¤ì´ ë°ì´í„° í”„ë ˆì„ì´ë¼ëŠ” êµ¬ì¡° ë•ì— í…Œì´ë¸” í˜•ì‹ì˜ ë°ì´í„°ë¥¼ ë‹¤ë£¨ëŠ” ë° ë§¤ìš° ìœ ìš© í•©ë‹ˆë‹¤ï»¿â€‹ Pandas ì„¤ì¹˜Pandas ì„¤ì¹˜ {5px}Pandas ì„¤ì¹˜ ï»¿â€‹Pythonë³µì‚¬# condaë¥¼ ì‚¬ìš©í•˜ì—¬ Pandas ì„¤ì¹˜
conda install pandas

# pipë¥¼ ì‚¬ìš©í•˜ì—¬ Pandas ì„¤ì¹˜
pip install pandas
â€‹ Pandas ì£¼ìš” ê¸°ëŠ¥ë°ì´í„° í”„ë ˆì„(DataFrame): í…Œì´ë¸” í˜•ì‹ì˜ ë°ì´í„°ë¥¼ ìƒì„±í•˜ê³  ì¡°ì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ë° ì €ì¥: CSV, ì—‘ì…€, SQL ë“± ë‹¤ì–‘í•œ í˜•ì‹ì˜ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ê³  ì €ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ë°ì´í„° ì¡°ì‘: í•„í„°ë§, ê·¸ë£¹í™”, ë³‘í•© ë“± ë‹¤ì–‘í•œ ë°ì´í„° ì¡°ì‘ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤. Pandas ì˜ˆì œë°ì´í„°í”„ë ˆì„ì„ ìƒì„±í•˜ê³  ê¸°ë³¸ ì¡°ì‘ì„ ìˆ˜í–‰í•˜ëŠ” ì˜ˆì œë¥¼ ì‹¤ìŠµí•´ ë´…ì‹œë‹¤Pandas ì˜ˆì œ {5px}Pandas ì˜ˆì œ ï»¿â€‹Pythonë³µì‚¬import pandas as pd

# ë°ì´í„° í”„ë ˆì„ ìƒì„±
data = {'Name': ['Alice', 'Bob', 'Charlie'],
'Age': [25, 30, 35],
'City': ['New York', 'Los Angeles', 'Chicago']}
df = pd.DataFrame(data)
# ë°ì´í„° í”„ë ˆì„ì˜ ì²« 5í–‰ ì¶œë ¥
print(df.head())
# ë°ì´í„° í”„ë ˆì„ì˜ í¬ê¸° í™•ì¸
print(df.shape)
# ë°ì´í„° í”„ë ˆì„ì˜ ì»¬ëŸ¼ëª… í™•ì¸
print(df.columns)
# ë°ì´í„° í”„ë ˆì„ì˜ ë°ì´í„° íƒ€ì… í™•ì¸
print(df.dtypes)
# íŠ¹ì • ì»¬ëŸ¼ ì„ íƒ
print(df['Name'])
# ì¡°ê±´ì— ë§ëŠ” í–‰ ì„ íƒ
print(df[df['Age'] > 30])
â€‹3) Scikit-learn ë¼ì´ë¸ŒëŸ¬ë¦¬ ì†Œê°œ Scikit-learn ë¼ì´ë¸ŒëŸ¬ë¦¬ë€?ë¨¸ì‹ ëŸ¬ë‹ì„ ìœ„í•œ Python ë¼ì´ë¸ŒëŸ¬ë¦¬ë‹¤ì–‘í•œ ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì„ ì œê³µí•˜ë©°, ë°ì´í„° ì „ì²˜ë¦¬, ëª¨ë¸ í•™ìŠµ, í‰ê°€ ë° ì˜ˆì¸¡ì„ ìœ„í•œ ë„êµ¬ë¥¼ í¬í•¨ Scikit-learn ì„¤ì¹˜Scikit-learn ì„¤ì¹˜{5px}Scikit-learn ì„¤ì¹˜ï»¿â€‹Pythonë³µì‚¬# condaë¥¼ ì‚¬ìš©í•˜ì—¬ Scikit-learn ì„¤ì¹˜
conda install scikit-learn

# pipë¥¼ ì‚¬ìš©í•˜ì—¬ Scikit-learn ì„¤ì¹˜
pip install scikit-learn
â€‹ Scikit-learn  ì£¼ìš” ê¸°ëŠ¥ë°ì´í„° ì „ì²˜ë¦¬: ìŠ¤ì¼€ì¼ë§, ì¸ì½”ë”©, ê²°ì¸¡ê°’ ì²˜ë¦¬ ë“± ë‹¤ì–‘í•œ ë°ì´í„° ì „ì²˜ë¦¬ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.ëª¨ë¸ í•™ìŠµ: íšŒê·€, ë¶„ë¥˜, êµ°ì§‘í™” ë“± ë‹¤ì–‘í•œ ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì„ ì œê³µí•©ë‹ˆë‹¤.ëª¨ë¸ í‰ê°€: êµì°¨ ê²€ì¦, ì„±ëŠ¥ í‰ê°€ ì§€í‘œ ë“± ëª¨ë¸ í‰ê°€ë¥¼ ìœ„í•œ ë„êµ¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤.ëª¨ë¸ ì˜ˆì¸¡: í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Scikit-learn  ì˜ˆì œScikit-learn ì˜ˆì œ{5px}Scikit-learn ì˜ˆì œï»¿â€‹Pythonë³µì‚¬from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# ë°ì´í„° ìƒì„±
X = [[1], [2], [3], [4], [5]]
y = [1, 4, 9, 16, 25]
# ë°ì´í„° ë¶„í•  (í›ˆë ¨ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# íšŒê·€ ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
model = LinearRegression()
model.fit(X_train, y_train)
# ì˜ˆì¸¡
y_pred = model.predict(X_test)
# ëª¨ë¸ í‰ê°€
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')
â€‹.
[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 3. ë”¥ëŸ¬ë‹ ì‹¤ìŠµ í™˜ê²½ êµ¬ì¶•[SCC] ê¸°ì´ˆê°€ íƒ„íƒ„í•œ ë”¥ëŸ¬ë‹/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] ê¸°ì´ˆê°€ íƒ„íƒ„í•œ ë”¥ëŸ¬ë‹ - 1ì£¼ì°¨/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 3. ë”¥ëŸ¬ë‹ ì‹¤ìŠµ í™˜ê²½ êµ¬ì¶•ì œì‘:[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 3. ë”¥ëŸ¬ë‹ ì‹¤ìŠµ í™˜ê²½ êµ¬ì¶•[ìˆ˜ì—… ëª©í‘œ]ë”¥ëŸ¬ë‹ ì‹¤ìŠµì„ ìœ„í•œ í™˜ê²½ì„ êµ¬ì¶•í•´ ë´…ì‹œë‹¤[ëª©ì°¨]01. condaë¥¼ ì´ìš©í•œ í™˜ê²½ ì„¤ì •02. jupyter notebook 03. ê°€ìƒí™˜ê²½ ì„¤ì¹˜ ë° jupyter notebook ì—°ê²° 04. pytorch ì„¤ì¹˜ğŸ’¡
 
 01. condaë¥¼ ì´ìš©í•œ í™˜ê²½ ì„¤ì •âœ”ï¸condaë¥¼ ì„¤ì¹˜í•˜ê³  í™˜ê²½ì„¤ì •ì„ ì§„í–‰í•´ ë´…ì‹œë‹¤1) condaë¥¼ ì´ìš©í•œ í™˜ê²½ ì„¤ì • condaë€ ë¬´ì—‡ì¸ê°€?CondaëŠ” íŒ¨í‚¤ì§€ ê´€ë¦¬ì™€ í™˜ê²½ ê´€ë¦¬ë¥¼ ìœ„í•œ ì˜¤í”ˆ ì†ŒìŠ¤ í”Œë«í¼ì…ë‹ˆë‹¤. ë‹¤ì–‘í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ íŒ¨í‚¤ì§€ë¥¼ ì‰½ê²Œ ì„¤ì¹˜í•˜ê³  ê´€ë¦¬í•  ìˆ˜ ìˆìœ¼ë©°, ì„œë¡œ ë‹¤ë¥¸ í”„ë¡œì íŠ¸ ê°„ì˜ ì¢…ì†ì„±ì„ ê²©ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. conda ì„¤ì¹˜Anacondahttps://www.anaconda.com/downloadALTMinicondahttps://docs.anaconda.com/miniconda/ALT conda í™˜ê²½ ì„¤ì •ë‹¤ìŒê³¼ ê°™ì´ ìƒˆë¡œìš´ í™˜ê²½ì„ ìƒì„±í•˜ê³  í•„ìš”í•œ íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜ í•©ë‹ˆë‹¤.ìƒˆë¡œìš´ í™˜ê²½ ìƒì„± {5px}ìƒˆë¡œìš´ í™˜ê²½ ìƒì„± ï»¿â€‹Pythonë³µì‚¬conda create --name myenv python=3.8
â€‹í™˜ê²½ í™œì„±í™” {5px}í™˜ê²½ í™œì„±í™” ï»¿â€‹Pythonë³µì‚¬conda activate myenv
â€‹í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ {5px}í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ï»¿â€‹Pythonë³µì‚¬conda install numpy pandas matplotlib
â€‹02. jupyter notebook âœ”ï¸jupyter notebookì´ ë¬´ì—‡ì¸ì§€ ì•Œì•„ë³´ê³  ì„¤ì¹˜í•´ ë´…ì‹œë‹¤.1)  jupyter notebook ì‚¬ìš©ë²• jupyter notebookì´ë€?Jupyter Notebookì€ ë°ì´í„° ê³¼í•™ìì™€ ì—°êµ¬ìë“¤ì´ ì½”ë“œë¥¼ ì‘ì„±í•˜ê³  ì‹¤í–‰í•˜ë©°, ê²°ê³¼ë¥¼ ì‹œê°í™”í•˜ê³  ë¬¸ì„œí™”í•  ìˆ˜ ìˆëŠ” ëŒ€í™”í˜• í™˜ê²½ì…ë‹ˆë‹¤. jupyter notebook ì„¤ì¹˜Jupyter Notebookì€ condaë¥¼ í†µí•´ ì‰½ê²Œ ì„¤ì¹˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤jupyter notebook ì„¤ì¹˜ {5px}jupyter notebook ì„¤ì¹˜ ï»¿â€‹Pythonë³µì‚¬conda install jupyter
â€‹ jupyter notebook ì‹œì‘í•˜ê¸°Jupyter Notebook ì‹¤í–‰jupyter notebook ì‹¤í–‰ {5px}jupyter notebook ì‹¤í–‰ ï»¿â€‹Pythonë³µì‚¬jupyter notebook
â€‹ì´ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ë©´ ì›¹ ë¸Œë¼ìš°ì €ê°€ ì—´ë¦¬ê³ , Jupyter Notebook ì¸í„°í˜ì´ìŠ¤ê°€ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.ìƒˆë¡œìš´ ë…¸íŠ¸ë¶ ìƒì„±:Jupyter Notebook ì¸í„°í˜ì´ìŠ¤ì—ì„œ "New" ë²„íŠ¼ì„ í´ë¦­í•˜ê³ , "Python (myenv)"ë¥¼ ì„ íƒí•˜ì—¬ ìƒˆë¡œìš´ ë…¸íŠ¸ë¶ì„ ìƒì„±í•©ë‹ˆë‹¤.ì½”ë“œ ì‘ì„± ë° ì‹¤í–‰:ì…€(Cell)ì— ì½”ë“œë¥¼ ì‘ì„±í•˜ê³ , ì…€ì„ ì„ íƒí•œ í›„ "Shift + Enter"ë¥¼ ëˆŒëŸ¬ ì½”ë“œë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.03. ê°€ìƒí™˜ê²½ ì„¤ì¹˜ ë° jupyter notebook ì—°ê²° âœ”ï¸ê°€ìƒí™˜ê²½ì´ ë¬´ì—‡ì¸ì§€ ì•Œì•„ë³´ê³  jupyter notebookê³¼ ê°€ìƒí™˜ê²½ì„ ì—°ê²°í•´ ë´…ì‹œë‹¤1) ê°€ìƒí™˜ê²½ ì„¤ì¹˜  ê°€ìƒí™˜ê²½ì´ë€ ë¬´ì—‡ì¸ê°€?ê°€ìƒí™˜ê²½(Virtual Environment)ì€ í”„ë¡œì íŠ¸ë§ˆë‹¤ ë…ë¦½ì ì¸ íŒŒì´ì¬ í™˜ê²½ì„ ì œê³µí•©ë‹ˆë‹¤.ê°€ìƒí™˜ê²½ì„ ì´ìš©í•´ ì„œë¡œ ë‹¤ë¥¸ í”„ë¡œì íŠ¸ ê°„ì˜ íŒ¨í‚¤ì§€ ì¶©ëŒì„ ë°©ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Condaë¥¼ ì´ìš©í•˜ë©´ ì‰½ê²Œ ê°€ìƒí™˜ê²½ì„ ìƒì„±í•˜ê³  ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê°€ìƒí™˜ê²½ ìƒì„± ë° ê´€ë¦¬ê°€ìƒ í™˜ê²½ ìƒì„± {5px}ê°€ìƒ í™˜ê²½ ìƒì„± ï»¿â€‹Pythonë³µì‚¬conda create --name myenv python=3.8
â€‹ì—¬ê¸°ì„œ myenv ëŠ” ê°€ìƒí™˜ê²½ì˜ ì´ë¦„ì´ë©° python=3.8ì€ ì„¤ì¹˜í•  íŒŒì´ì¬ ë²„ì „ì…ë‹ˆë‹¤ì—¬ê¸°ì„œ myenv ëŠ” ê°€ìƒí™˜ê²½ì˜ ì´ë¦„ì´ë©° python=3.8ì€ ì„¤ì¹˜í•  íŒŒì´ì¬ ë²„ì „ì…ë‹ˆë‹¤ï»¿â€‹ê°€ìƒ í™˜ê²½ ë¹„í™œì„±í™” {5px}ê°€ìƒ í™˜ê²½ ë¹„í™œì„±í™” ï»¿â€‹Pythonë³µì‚¬conda deactivate
â€‹ê°€ìƒ í™˜ê²½ ì‚­ì œ {5px}ê°€ìƒ í™˜ê²½ ì‚­ì œ ï»¿â€‹Pythonë³µì‚¬conda remove --name myenv --all
â€‹2) jupyter notebook ì—°ê²° jupyter notebook ì—°ê²°ê°€ìƒí™˜ê²½ì„ Jupyter Notebookê³¼ ì—°ê²°í•˜ì—¬ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤ì´ë¥¼ ìœ„í•´ ipykernel íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•˜ê³ , ê°€ìƒí™˜ê²½ì„ Jupyter Notebookì— ì»¤ë„ë¡œ ì¶”ê°€í•©ë‹ˆë‹¤.ipykernel ì„¤ì¹˜ {5px}ipykernel ì„¤ì¹˜ ï»¿â€‹Pythonë³µì‚¬conda install ipykernel
â€‹ê°€ìƒí™˜ê²½ì„ Jupyter Notebookì— ì»¤ë„ë¡œ ì¶”ê°€ {5px}ê°€ìƒí™˜ê²½ì„ Jupyter Notebookì— ì»¤ë„ë¡œ ì¶”ê°€ ï»¿â€‹Pythonë³µì‚¬python -m ipykernel install --user --name myenv --display-name "Python (myenv)"
â€‹ì—¬ê¸°ì„œ myenvëŠ” ê°€ìƒí™˜ê²½ì˜ ì´ë¦„ì´ë©°, "Python (myenv)"ëŠ” Jupyter Notebookì—ì„œ í‘œì‹œë  ì»¤ë„ ì´ë¦„ì…ë‹ˆë‹¤.ì—¬ê¸°ì„œ myenvëŠ” ê°€ìƒí™˜ê²½ì˜ ì´ë¦„ì´ë©°, "Python (myenv)"ëŠ” Jupyter Notebookì—ì„œ í‘œì‹œë  ì»¤ë„ ì´ë¦„ì…ë‹ˆë‹¤.ï»¿â€‹04. pytorch ì„¤ì¹˜âœ”ï¸condaë¥¼ ì´ìš©í•´ pytorch ì„¤ì¹˜í•´ ë´…ì‹œë‹¤1) pytorch ì„¤ì¹˜ pytorchë€ ë¬´ì—‡ì¸ê°€?PyTorchëŠ” Facebookì—ì„œ ê°œë°œí•œ ì˜¤í”ˆ ì†ŒìŠ¤ ë”¥ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ, ë™ì  ê³„ì‚° ê·¸ë˜í”„(dynamic computation graph)ë¥¼ ì§€ì›í•˜ì—¬ ìœ ì—°í•˜ê³  ì§ê´€ì ì¸ ëª¨ë¸ ì„¤ê³„ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤. pytorch ì„¤ì¹˜PyTorchëŠ” condaë¥¼ í†µí•´ ì‰½ê²Œ ì„¤ì¹˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ì„¤ì¹˜ ëª…ë ¹ì–´ëŠ” ìš´ì˜ì²´ì œì™€ CUDA ë²„ì „ì— ë”°ë¼ ë‹¤ë¥´ë¯€ë¡œ, PyTorch ê³µì‹ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì„¤ì¹˜ ëª…ë ¹ì–´ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, CUDA 10.2ë¥¼ ì‚¬ìš©í•˜ëŠ” Windows í™˜ê²½ì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì„¤ì¹˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:pytorch ì„¤ì¹˜ {5px}pytorch ì„¤ì¹˜ ï»¿â€‹Pythonë³µì‚¬conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch
â€‹.
í…ìŠ¤íŠ¸ ì²˜ë¦¬ì˜ í•µì‹¬ ê¸°ë²•ê³¼ ì„ë² ë”© í™œìš©í•˜ê¸° LLM & RAGë¥¼ í™œìš©í•œ AI ì„œë¹„ìŠ¤ ë§Œë“¤ê¸°/ LLM & RAGë¥¼ í™œìš©í•œ AI ì„œë¹„ìŠ¤ ë§Œë“¤ê¸° - 5ì£¼ì°¨/í…ìŠ¤íŠ¸ ì²˜ë¦¬ì˜ í•µì‹¬ ê¸°ë²•ê³¼ ì„ë² ë”© í™œìš©í•˜ê¸°ì œì‘:í…ìŠ¤íŠ¸ ì²˜ë¦¬ì˜ í•µì‹¬ ê¸°ë²•ê³¼ ì„ë² ë”© í™œìš©í•˜ê¸°ìˆ˜ì—… ëª©í‘œì£¼ìš” í…ìŠ¤íŠ¸ ì²˜ë¦¬ ê¸°ë²•ì¸ í† í°í™”, ì •ê·œí™” ë“±ì„ í•™ìŠµí•©ë‹ˆë‹¤.ì„ë² ë”©ì˜ ê°œë…ê³¼ ì¤‘ìš”ì„±ì— ëŒ€í•´ í•™ìŠµí•©ë‹ˆë‹¤.ëª©ì°¨í…ìŠ¤íŠ¸ ì²˜ë¦¬ê°€ ì¤‘ìš”í•œ ì´ìœ í…ìŠ¤íŠ¸ ì²˜ë¦¬ì˜ ëª©í‘œì£¼ìš” í…ìŠ¤íŠ¸ ì²˜ë¦¬ ê¸°ë²•í† í°í™” (Tokenization)ì •ê·œí™” (Normalization)ë¶ˆìš©ì–´ ì œê±° (Stopword Removal)í˜•íƒœì†Œ ë¶„ì„ (Morphological Analysis)ì–´ê°„ ì¶”ì¶œê³¼ í‘œì œì–´ ì¶”ì¶œ (Stemming and Lemmatization)ë¬¸ì¥ ë¶„ë¦¬ ë° ê¸¸ì´ ì¡°ì •ì„ë² ë”©ì˜ ê°œë…ê³¼ ì¤‘ìš”ì„±ì„ë² ë”©(Embedding)ë€?ëŒ€í‘œì ì¸ ì„ë² ë”© ê¸°ë²•Bag of Words (BoW)TF-IDF (Term Frequency-Inverse Document Frequency)Word2Vec, GloVeTransformer ê¸°ë°˜ ì„ë² ë”© (BERT, GPT)ë°°ìš´ ë‚´ìš© ì •ë¦¬í•˜ê¸°!LLM(ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸)ì„ íš¨ìœ¨ì ìœ¼ë¡œ í™œìš©í•˜ë ¤ë©´ í…ìŠ¤íŠ¸ ì²˜ë¦¬ê°€ ê¸°ë³¸ì´ ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. í…ìŠ¤íŠ¸ ì²˜ë¦¬ëŠ” ëª¨ë¸ì´ ìì—°ì–´ë¥¼ ì´í•´í•˜ê³  ë¶„ì„í•  ìˆ˜ ìˆê²Œ ë•ëŠ” ì²« ë‹¨ê³„ì´ì£ . ë˜í•œ, í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ì„ë² ë”©(embedding) ê¸°ë²•ì€ ë¬¸ì¥ì˜ ì˜ë¯¸ë¥¼ ìˆ«ìë¡œ í‘œí˜„í•´ ëª¨ë¸ì´ ë¬¸ì„œì˜ ìœ ì‚¬ì„±ì„ íŒŒì•…í•˜ê±°ë‚˜ ì •í™•í•œ ì¶”ë¡ ì„ í•  ìˆ˜ ìˆê²Œ í•´ì¤˜ìš”.ì´ë²ˆ ê°•ì˜ì—ì„œëŠ” í…ìŠ¤íŠ¸ ì²˜ë¦¬ì˜ ì£¼ìš” ê¸°ë²•ê³¼ ì„ë² ë”©ì˜ ê°œë…ì„ ì‚´í´ë³´ë©°, ì´ ë‘ ê°€ì§€ê°€ LLMì—ì„œ ì™œ ì¤‘ìš”í•œì§€, ê·¸ë¦¬ê³  ì‹¤ì œë¡œ ì–´ë–»ê²Œ í™œìš©ë˜ëŠ”ì§€ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.í…ìŠ¤íŠ¸ ì²˜ë¦¬ê°€ ì¤‘ìš”í•œ ì´ìœ í…ìŠ¤íŠ¸ ì²˜ë¦¬(Text Preprocessing)ëŠ” ë°ì´í„°ì˜ í’ˆì§ˆì„ ë†’ì´ê³  ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ í•„ìˆ˜ ì‘ì—…ì´ì—ìš”. ìì—°ì–´ëŠ” ë§¤ìš° ë³µì¡í•˜ê³  ë‹¤ì–‘í•˜ê¸° ë•Œë¬¸ì—, LLMì´ í…ìŠ¤íŠ¸ë¥¼ ì •í™•í•˜ê²Œ ì´í•´í•˜ê³  ì²˜ë¦¬í•˜ê¸° ìœ„í•´ì„œëŠ” ë°ì´í„°ê°€ êµ¬ì¡°í™”ë˜ê³  ì •ì œë  í•„ìš”ê°€ ìˆì–´ìš”. ì˜ëª»ëœ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ê³¼ì •ì€ ëª¨ë¸ì´ í˜¼ë™í•˜ê±°ë‚˜ ì˜ëª»ëœ ì¶”ë¡ ì„ í•˜ê²Œ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.í…ìŠ¤íŠ¸ ì²˜ë¦¬ì˜ ëª©í‘œë…¸ì´ì¦ˆ ì œê±°: í…ìŠ¤íŠ¸ ë‚´ ë¶ˆí•„ìš”í•œ ì •ë³´ë‚˜ ì˜¤ë¥˜ë¥¼ ì œê±°í•´ ì •í™•í•œ ë¶„ì„ì„ í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.ì¼ê´€ì„± í™•ë³´: ë¬¸ì¥ì˜ êµ¬ì¡°ë‚˜ í˜•íƒœë¥¼ ì¼ê´€ë˜ê²Œ ìœ ì§€í•˜ì—¬ ëª¨ë¸ì´ ë” ì‰½ê²Œ íŒ¨í„´ì„ í•™ìŠµí•˜ê²Œ ë•ìŠµë‹ˆë‹¤.íš¨ìœ¨ì ì¸ ì²˜ë¦¬: ë¶ˆí•„ìš”í•œ ë‹¨ì–´ë¥¼ ì œê±°í•˜ê³  ì¤‘ìš”í•œ ì •ë³´ë§Œ ë‚¨ê²¨, ëª¨ë¸ì´ ë” ë¹ ë¥´ê²Œ ê³„ì‚°í•  ìˆ˜ ìˆë„ë¡ í•´ì¤ë‹ˆë‹¤.ì£¼ìš” í…ìŠ¤íŠ¸ ì²˜ë¦¬ ê¸°ë²•í† í°í™” (Tokenization)í† í°í™”ëŠ” í…ìŠ¤íŠ¸ë¥¼ ë‹¨ì–´ ë˜ëŠ” ì„œë¸Œì›Œë“œ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ëŠ” ì‘ì—…ì…ë‹ˆë‹¤. ì´ ê³¼ì •ì€ í…ìŠ¤íŠ¸ë¥¼ ìˆ«ìë¡œ ë³€í™˜í•˜ê¸° ì „ì˜ ê°€ì¥ ì¤‘ìš”í•œ ë‹¨ê³„ì— í•´ë‹¹í•´ìš”.ë‹¨ì–´ ë‹¨ìœ„ í† í°í™”: í…ìŠ¤íŠ¸ë¥¼ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ëŠ” ê¸°ë³¸ ë°©ë²•ì…ë‹ˆë‹¤.ì˜ˆ: "ë‚˜ëŠ” ì˜¤ëŠ˜ ì±…ì„ ì½ì—ˆë‹¤." â†’ ["ë‚˜ëŠ”", "ì˜¤ëŠ˜", "ì±…ì„", "ì½ì—ˆë‹¤"]ì„œë¸Œì›Œë“œ í† í°í™”: ë‹¨ì–´ë¥¼ ë” ì‘ì€ ë‹¨ìœ„ë¡œ ë¶„ë¦¬í•´ ìƒˆë¡œìš´ ë‹¨ì–´ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. BPEë‚˜ WordPiece ê°™ì€ ë°©ë²•ì´ ìˆì–´ìš”.ì˜ˆ: "ì½ì—ˆë‹¤" â†’ ["ì½", "ì—ˆë‹¤"]ì´ë ‡ê²Œ ë‚˜ëˆ ì§„ í† í°ì€ ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜ë©ë‹ˆë‹¤.ì •ê·œí™” (Normalization)ì •ê·œí™”ëŠ” í…ìŠ¤íŠ¸ë¥¼ í‘œì¤€í™”ëœ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì‘ì—…ì´ì—ìš”. í…ìŠ¤íŠ¸ì— í¬í•¨ëœ ëŒ€ì†Œë¬¸ì, íŠ¹ìˆ˜ë¬¸ì ë“±ì„ ì¼ê´€ë˜ê²Œ ë³€í™˜í•˜ì—¬, ëª¨ë¸ì´ ë¶ˆí•„ìš”í•œ ë³€ë™ì— í˜¼ë€ì„ ê²ªì§€ ì•Šë„ë¡ í•©ë‹ˆë‹¤.ì†Œë¬¸ì ë³€í™˜: ëŒ€ë¬¸ìì™€ ì†Œë¬¸ìë¥¼ í†µì¼í•˜ì—¬ ê°™ì€ ë‹¨ì–´ë¡œ ì¸ì‹í•˜ê²Œ í•©ë‹ˆë‹¤.ì˜ˆ: "OpenAI" â†’ "openai"ë¶ˆí•„ìš”í•œ ê¸°í˜¸ ì œê±°: ë¶„ì„ì— í•„ìš” ì—†ëŠ” íŠ¹ìˆ˜ë¬¸ìë‚˜ ê¸°í˜¸ë¥¼ ì œê±°í•©ë‹ˆë‹¤.ì˜ˆ: "ì„œìš¸, 2023ë…„!" â†’ "ì„œìš¸ 2023ë…„"ì •ê·œí™”ë¥¼ í†µí•´ ëª¨ë¸ì´ í…ìŠ¤íŠ¸ì˜ ì˜ë¯¸ì— ì§‘ì¤‘í•˜ê²Œ í•  ìˆ˜ ìˆì–´ìš”.ë¶ˆìš©ì–´ ì œê±° (Stopword Removal)ë¶ˆìš©ì–´ë€ ìì£¼ ë“±ì¥í•˜ì§€ë§Œ ì •ë³´ê°€ ì—†ëŠ” ë‹¨ì–´ë¥¼ ë§í•´ìš”. ì˜ˆë¥¼ ë“¤ì–´, "ê·¸ë¦¬ê³ ", "ì´", "ëŠ”" ê°™ì€ ë‹¨ì–´ë“¤ì€ ë¬¸ë§¥ì— í° ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ë¶ˆìš©ì–´ë¡œ ì²˜ë¦¬í•´ìš”. ì´ë¥¼ ì œê±°í•˜ë©´ ëª¨ë¸ì´ ì¤‘ìš”í•œ ë‹¨ì–´ì—ë§Œ ì§‘ì¤‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ì˜ˆ: "ë‚˜ëŠ” ì±…ì„ ì½ì—ˆë‹¤." â†’ ["ì±…", "ì½ì—ˆë‹¤"]í˜•íƒœì†Œ ë¶„ì„ (Morphological Analysis)í•œêµ­ì–´ì™€ ê°™ì€ êµì°©ì–´ì—ì„œëŠ” í˜•íƒœì†Œ ë¶„ì„ì´ í•„ìˆ˜ì ì´ì—ìš”. í˜•íƒœì†ŒëŠ” ë‹¨ì–´ì˜ ìµœì†Œ ì˜ë¯¸ ë‹¨ìœ„ë¡œ, í•œêµ­ì–´ì—ì„œ íŠ¹íˆ ì¡°ì‚¬ë‚˜ ì–´ë¯¸ì™€ ê°™ì€ ë¶€ë¶„ì„ ì •í™•í•˜ê²Œ ë¶„ë¦¬í•´ë‚´ëŠ” ë° ìœ ìš©í•©ë‹ˆë‹¤.ì˜ˆ: "ì±…ì„ ì½ì—ˆë‹¤." â†’ [ì±…(Noun), ì„(Postposition), ì½ì—ˆë‹¤(Verb)]ì–´ê°„ ì¶”ì¶œê³¼ í‘œì œì–´ ì¶”ì¶œ (Stemming and Lemmatization)í…ìŠ¤íŠ¸ì—ì„œ ë™ì‚¬ë‚˜ í˜•ìš©ì‚¬ì˜ ë³€í˜•ì„ ê¸°ë³¸ í˜•íƒœë¡œ ëŒë¦¬ëŠ” ì‘ì—…ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë™ì¼í•œ ë‹¨ì–´ë¥¼ ì¼ê´€ë˜ê²Œ ì²˜ë¦¬í•  ìˆ˜ ìˆì–´ìš”.ì–´ê°„ ì¶”ì¶œì€ ë‹¨ì–´ì—ì„œ ì–´ë¯¸ë¥¼ ì œê±°í•˜ê³ , ê¸°ë³¸ ì–´ê°„ë§Œ ë‚¨ê¹ë‹ˆë‹¤.ì˜ˆ: "studying", "studied", "study" â†’ "study"í‘œì œì–´ ì¶”ì¶œì€ ë‹¨ì–´ë¥¼ ì‚¬ì „ì  ê¸°ë³¸í˜•ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.ì˜ˆ: "am", "is", "are" â†’ "be"ì´ ê³¼ì •ì€ ë™ì‚¬ë‚˜ í˜•ìš©ì‚¬ì²˜ëŸ¼ ë³€í˜•ì´ ë§ì€ ë‹¨ì–´ë¥¼ ì²˜ë¦¬í•  ë•Œ ë§¤ìš° ìœ ìš©í•´ìš”.ë¬¸ì¥ ë¶„ë¦¬ ë° ê¸¸ì´ ì¡°ì •í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ê±°ë‚˜ ë³µì¡í•  ê²½ìš°, ì´ë¥¼ ì ì ˆí•˜ê²Œ ë‚˜ëˆ„ê±°ë‚˜ ê¸¸ì´ë¥¼ ì¡°ì •í•´ì•¼ í•´ìš”. ê¸´ ë¬¸ì¥ì„ ì²˜ë¦¬í•  ë•Œ, ëª¨ë¸ì˜ ë©”ëª¨ë¦¬ ì œí•œì´ë‚˜ ì„±ëŠ¥ ì €í•˜ë¥¼ ë°©ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ì„ë² ë”©ì˜ ê°œë…ê³¼ ì¤‘ìš”ì„±ì„ë² ë”©(Embedding)ë€?ì„ë² ë”©ì€ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ **ë²¡í„°(ìˆ«ì ë°°ì—´)**ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì´ì—ìš”. 
LLMì´ í…ìŠ¤íŠ¸ì˜ ì˜ë¯¸ë¥¼ ì´í•´í•˜ë ¤ë©´ í…ìŠ¤íŠ¸ê°€ ìˆ«ìë¡œ ë³€í™˜ë˜ì–´ì•¼ í•˜ëŠ”ë°, ê·¸ ê³¼ì •ì—ì„œ ê° ë¬¸ì¥ì´ë‚˜ ë‹¨ì–´ë¥¼ ë²¡í„° ê³µê°„ì— ë§¤í•‘í•˜ê²Œ ë©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ëª¨ë¸ì´ ì˜ë¯¸ì  ìœ ì‚¬ì„±ì„ íŒŒì•…í•˜ê³ , íš¨ìœ¨ì ì¸ ê²€ìƒ‰ì´ë‚˜ ë¬¸ì„œ ë¶„ë¥˜ë¥¼ í•  ìˆ˜ ìˆì–´ìš”.ì„ë² ë”© ë²¡í„°ëŠ” ë‹¨ì–´ì˜ ì˜ë¯¸ë‚˜ ë¬¸ì¥ì˜ ë¬¸ë§¥ì„ ë°˜ì˜í•˜ì—¬, 
ìœ ì‚¬í•œ ì˜ë¯¸ë¥¼ ê°€ì§„ í…ìŠ¤íŠ¸ëŠ” ìœ ì‚¬í•œ ë²¡í„° ê°’ì„ ê°€ì§‘ë‹ˆë‹¤.ëŒ€í‘œì ì¸ ì„ë² ë”© ê¸°ë²•Bag of Words (BoW)Bag of WordsëŠ” ë‹¨ì–´ì˜ ë¹ˆë„ë§Œì„ ê¸°ë°˜ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°í™”í•˜ëŠ” ê°€ì¥ ë‹¨ìˆœí•œ ë°©ë²•ì…ë‹ˆë‹¤. ë‹¨ì–´ì˜ ìˆœì„œë‚˜ ë¬¸ë§¥ì„ ê³ ë ¤í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ì˜ë¯¸ íŒŒì•…ì— í•œê³„ê°€ ìˆì§€ë§Œ, ê°„ë‹¨í•œ ë¬¸ì„œ ë¶„ë¥˜ë‚˜ í…ìŠ¤íŠ¸ ë¶„ì„ì— ìœ ìš©í•´ìš”.ë¬¸ì¥: "ê³ ì–‘ì´ê°€ ì•¼ì˜¹í–ˆë‹¤"ë²¡í„° í‘œí˜„: [1, 1, 1, 0, 0] (ê° ë‹¨ì–´ì˜ ë¹ˆë„ìˆ˜)TF-IDF (Term Frequency-Inverse Document Frequency)TF-IDFëŠ” ë‹¨ìˆœí•œ ë‹¨ì–´ ë¹ˆë„ ì™¸ì—ë„ ë‹¨ì–´ì˜ ì¤‘ìš”ë„ë¥¼ ë°˜ì˜í•œ ì„ë² ë”© ê¸°ë²•ì´ì—ìš”. íŠ¹ì • ë‹¨ì–´ê°€ ë¬¸ì„œ ë‚´ì—ì„œ ìì£¼ ë“±ì¥í•˜ì§€ë§Œ ì „ì²´ ë¬¸ì„œì—ì„œ ë“œë¬¼ê²Œ ë“±ì¥í•œë‹¤ë©´, ê·¸ ë‹¨ì–´ëŠ” í•´ë‹¹ ë¬¸ì„œì—ì„œ ì¤‘ìš”í•œ ë‹¨ì–´ë¡œ ê°„ì£¼ë©ë‹ˆë‹¤.TF: ë‹¨ì–´ì˜ ë¹ˆë„IDF: ë‹¨ì–´ì˜ ì „ì²´ ë¬¸ì„œì—ì„œì˜ ë“±ì¥ ë¹ˆë„ ë°˜ë¹„ë¡€ê°’ì´ë¥¼ í†µí•´ ë¬¸ì„œ ë‚´ì—ì„œ ì˜ë¯¸ ìˆëŠ” ë‹¨ì–´ë¥¼ ê°•ì¡°í•  ìˆ˜ ìˆì–´ìš”.Word2Vec, GloVeWord2Vecê³¼ GloVeëŠ” ë‹¨ì–´ ê°„ì˜ ì˜ë¯¸ì  ìœ ì‚¬ì„±ì„ ë°˜ì˜í•˜ëŠ” ì„ë² ë”© ê¸°ë²•ì´ì—ìš”. ë‹¨ì–´ë¥¼ ê³ ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬, ë‹¨ì–´ ê°„ì˜ ê´€ê³„ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.Word2Vec: ì£¼ìœ„ ë‹¨ì–´ë“¤ì— ê¸°ë°˜í•´ ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ í•™ìŠµGloVe: ì „ì²´ ë¬¸ë§¥ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¨ì–´ ê°„ì˜ ê³µí†µ íŒ¨í„´ì„ í•™ìŠµì´ëŸ¬í•œ ì„ë² ë”© ê¸°ë²•ì„ ì‚¬ìš©í•˜ë©´, ë‹¨ì–´ì˜ ì˜ë¯¸ë¥¼ ë²¡í„°ë¡œ ë¹„êµí•´ ë¬¸ë§¥ ìœ ì‚¬ì„±ì„ íŒŒì•…í•  ìˆ˜ ìˆì–´ìš”.Transformer ê¸°ë°˜ ì„ë² ë”© (BERT, GPT)BERTë‚˜ GPT ê°™ì€ **Transformer* ëª¨ë¸ë“¤ì€ ë¬¸ì¥ì˜ ë¬¸ë§¥ì„ ê³ ë ¤í•˜ì—¬ ë” ê¹Šì´ ìˆëŠ” ì˜ë¯¸ë¥¼ ë°˜ì˜í•œ ì„ë² ë”©ì„ ìƒì„±í•´ìš”. íŠ¹íˆ, ì´ë“¤ì€ ë¬¸ì¥ ë‹¨ìœ„ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°í™”í•  ìˆ˜ ìˆì–´ ë¬¸ì¥ ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ì •í™•í•˜ê²Œ íŒŒì•…í•©ë‹ˆë‹¤.BERT: ì–‘ë°©í–¥ìœ¼ë¡œ ë¬¸ë§¥ì„ ê³ ë ¤í•œ ì„ë² ë”© ìƒì„±GPT: ìë™ ì™„ì„± ë° ìƒì„±ì— ê°•ì ì„ ë‘” ì„ë² ë”© ìƒì„±ì„ë² ë”©ì˜ í™œìš©ì„ë² ë”©ì„ í™œìš©í•˜ë©´ í…ìŠ¤íŠ¸ ê²€ìƒ‰, ë¬¸ì„œ ë¶„ë¥˜, ëŒ€í™”í˜• AI ë“± ë‹¤ì–‘í•œ ì‘ìš© ë¶„ì•¼ì—ì„œ ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰ê³¼ ìœ ì‚¬ì„± ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ë°°ìš´ ë‚´ìš© ì •ë¦¬í•˜ê¸°!í…ìŠ¤íŠ¸ ì²˜ë¦¬ì™€ ì„ë² ë”©ì€ í•„ìˆ˜ì ì¸ ê¸°ì´ˆ!í…ìŠ¤íŠ¸ ì²˜ë¦¬ì™€ ì„ë² ë”©ì€ LLM ì‹œìŠ¤í…œ êµ¬ì¶•ì˜ í•µì‹¬ ê¸°ì´ˆì…ë‹ˆë‹¤. ì •í™•í•œ í…ìŠ¤íŠ¸ ì²˜ë¦¬ì™€ ì ì ˆí•œ ì„ë² ë”© ê¸°ë²•ì„ í™œìš©í•˜ë©´, LLMì˜ ì„±ëŠ¥ì„ ìµœëŒ€í•œ ëŒì–´ì˜¬ë¦¬ê³  ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆì–´ìš”.í…ìŠ¤íŠ¸ ì²˜ë¦¬ëŠ” ëª¨ë¸ì˜ ì…ë ¥ì„ ì •ì œí•˜ê³ , ì„ë² ë”©ì€ ëª¨ë¸ì´ ì¶”ë¡ í•  ìˆ˜ ìˆë„ë¡ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°í™”í•©ë‹ˆë‹¤. ì´ ë‘˜ì„ ì˜ ì´í•´í•˜ê³  ì ìš©í•˜ëŠ” ê²ƒì´ LLM ì‹œìŠ¤í…œ êµ¬ì¶•ì˜ ì²«ê±¸ìŒì´ì—ìš”!
Pandas ì„¤ì¹˜ ë° Jupyter Notebook ì„¤ì •í•˜ê¸° ì¸ê³µì§€ëŠ¥ì„ ìœ„í•œ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬/ì¸ê³µì§€ëŠ¥ì„ ìœ„í•œ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ - 1ì£¼ì°¨/Pandas ì„¤ì¹˜ ë° Jupyter Notebook ì„¤ì •í•˜ê¸° ì œì‘:Pandas ì„¤ì¹˜ ë° Jupyter Notebook ì„¤ì •í•˜ê¸° ìˆ˜ì—… ëª©í‘œPandas ì„¤ì¹˜ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.Jupyter Notebook ì„¤ì • ë°©ë²•ì— ëŒ€í•´ì„œ í•™ìŠµí•©ë‹ˆë‹¤.ëª©ì°¨íŒë‹¤ìŠ¤(Pandas) ì„¤ì¹˜í•˜ê¸°Jupyter Notebook ì„¤ì •í•˜ê¸° Condaì™€ ê°€ìƒí™˜ê²½ ì„¤ì •í•˜ê¸°ê°€ìƒí™˜ê²½ì„ Jupyter Notebookê³¼ ì—°ê²°í•˜ê¸°ì•Œë©´ ì¢‹ì€ Tip!íŒë‹¤ìŠ¤(Pandas) ì„¤ì¹˜í•˜ê¸°íŒë‹¤ìŠ¤(Pandas)ë€?ğŸ“šíŒë‹¤ìŠ¤ëŠ” ë°ì´í„° ì¡°ì‘ ë° ë¶„ì„ì„ ìœ„í•œ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.
ë°ì´í„°í”„ë ˆì„(DataFrame)ì„ ì‚¬ìš©í•´ ì—‘ì…€ê³¼ ìœ ì‚¬í•œ ë°©ì‹ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë‹¤ë£° ìˆ˜ ìˆìŠµë‹ˆë‹¤.íŒë‹¤ìŠ¤ ì„¤ì¹˜í•˜ê¸°Shellë³µì‚¬pip install pandas

â€‹ìœ„ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ë©´ pipì„ í†µí•´ íŒë‹¤ìŠ¤ê°€ ì„¤ì¹˜ë©ë‹ˆë‹¤!Jupyter Notebook ì„¤ì •í•˜ê¸° â€‹Jupyter Notebookì´ë€?ğŸ“šJupyter Notebookì€ ì½”ë“œ, í…ìŠ¤íŠ¸, ì´ë¯¸ì§€ ë“±ì„ í•œ ê³³ì—ì„œ ê´€ë¦¬í•  ìˆ˜ ìˆëŠ” ëŒ€í™”í˜• ì»´í“¨íŒ… í™˜ê²½ì…ë‹ˆë‹¤.
ë°ì´í„° ë¶„ì„, ë¨¸ì‹ ëŸ¬ë‹, êµìœ¡ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤.Jupyter Notebook ì„¤ì¹˜í•˜ê¸°Shellë³µì‚¬pip install jupyter

â€‹ì´ ëª…ë ¹ì–´ë¡œ Jupyter Notebookì„ ì„¤ì¹˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Jupyter Notebook ì‹¤í–‰í•˜ê¸°Shellë³µì‚¬jupyter notebook

â€‹ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ë©´ ì›¹ ë¸Œë¼ìš°ì €ê°€ ì—´ë¦¬ë©° Jupyter Notebook ì¸í„°í˜ì´ìŠ¤ë¡œ ì´ë™í•©ë‹ˆë‹¤.Condaì™€ ê°€ìƒí™˜ê²½ ì„¤ì •í•˜ê¸°Condaë€?CondaëŠ” íŒŒì´ì¬ íŒ¨í‚¤ì§€ ê´€ë¦¬ ë° ê°€ìƒí™˜ê²½ ê´€ë¦¬ë¥¼ ë•ëŠ” ë„êµ¬ì…ë‹ˆë‹¤.ì—¬ëŸ¬ í”„ë¡œì íŠ¸ì— ì„œë¡œ ë‹¤ë¥¸ íŒ¨í‚¤ì§€ ë²„ì „ì„ ì‚¬ìš©í•´ì•¼ í•  ë•Œ ìœ ìš©í•©ë‹ˆë‹¤.

Conda ì„¤ì¹˜í•˜ê¸°Anacondaë‚˜ Minicondaë¥¼ ì„¤ì¹˜í•˜ì—¬ Condaë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ì„¤ì¹˜ í›„, Conda ëª…ë ¹ì–´ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ê°€ìƒí™˜ê²½ ë§Œë“¤ê¸°Shellë³µì‚¬conda create --name myenv

â€‹ìœ„ ëª…ë ¹ì–´ë¡œ myenvë¼ëŠ” ì´ë¦„ì˜ ê°€ìƒí™˜ê²½ì„ ë§Œë“­ë‹ˆë‹¤.Shellë³µì‚¬conda create --name myenv python=3.8 pandas
â€‹ì›í•˜ëŠ” íŒŒì´ì¬ ë²„ì „ê³¼ íŒ¨í‚¤ì§€ë¥¼ ì§€ì •í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.ê°€ìƒí™˜ê²½ í™œì„±í™” ë° ë¹„í™œì„±í™”ê°€ìƒí™˜ê²½ í™œì„±í™”:Shellë³µì‚¬conda activate myenv

â€‹ê°€ìƒí™˜ê²½ ë¹„í™œì„±í™”:Shellë³µì‚¬conda deactivate
â€‹ê°€ìƒí™˜ê²½ì„ Jupyter Notebookê³¼ ì—°ê²°í•˜ê¸°ipykernel ì„¤ì¹˜ê°€ìƒí™˜ê²½ì„ Jupyter Notebookì—ì„œ ì‚¬ìš©í•˜ë ¤ë©´ ipykernelì„ ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.Shellë³µì‚¬pip install ipykernel

â€‹ê°€ìƒí™˜ê²½ì„ Jupyter Notebookì— ì¶”ê°€í•˜ê¸°Shellë³µì‚¬python -m ipykernel install --user --name myenv --display-name "My Env"

â€‹ì´ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ë©´ Jupyter Notebookì—ì„œ My Envë¼ëŠ” ì´ë¦„ìœ¼ë¡œ ê°€ìƒí™˜ê²½ì„ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Jupyter Notebookì—ì„œ ê°€ìƒí™˜ê²½ ì‚¬ìš©í•˜ê¸°ALTJupyter Notebookì—ì„œ ìƒˆë¡œìš´ ë…¸íŠ¸ë¶ì„ ì—´ ë•Œ,  My Envë¥¼ ì„ íƒí•©ë‹ˆë‹¤.ì•Œë©´ ì¢‹ì€ Tip!Conda ê°€ìƒí™˜ê²½ ê´€ë¦¬ conda env list ëª…ë ¹ì–´ë¡œ í˜„ì¬ ì‚¬ìš© ê°€ëŠ¥í•œ ê°€ìƒí™˜ê²½ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Jupyter Notebook í™•ì¥ ë‹¤ì–‘í•œ Jupyter Notebook í™•ì¥ì„ ì„¤ì¹˜í•´ ê¸°ëŠ¥ì„ í™•ì¥í•´ ë³´ì„¸ìš”! nbextensions íŒ¨í‚¤ì§€ë¥¼ ì‚¬ìš©í•˜ë©´ ë§ì€ ìœ ìš©í•œ í™•ì¥ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ğŸ“šì´ë ‡ê²Œ í•˜ë©´ íŒë‹¤ìŠ¤ì™€ Jupyter Notebook ì„¤ì •ì´ ëª¨ë‘ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! â€‹
[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 4ê°•. Jupyter Notebook ì‚¬ìš©í•´ë³´ê¸°[SCC] ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ - 1ì£¼ì°¨/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 4ê°•. Jupyter Notebook ì‚¬ìš©í•´ë³´ê¸°ì œì‘:[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 4ê°•. Jupyter Notebook ì‚¬ìš©í•´ë³´ê¸°[ìˆ˜ì—… ëª©í‘œ]Jupyter Notebook ì‚¬ìš©í•´ë³´ê¸°![ëª©ì°¨]Jupyter Notebookì´ë€?
 
 Jupyter Notebookì´ë€?Jupyter Notebookì´ ë¬´ì—‡ì¸ì§€ ì•Œì•„ë´…ì‹œë‹¤.1) Jupyter Notebook ì†Œê°œ Jupyter Notebookì´ë€Jupyter Notebookì€ ë°ì´í„° ê³¼í•™ì, ì—°êµ¬ì, êµìœ¡ìë“¤ì´ ë„ë¦¬ ì‚¬ìš©í•˜ëŠ” ì˜¤í”ˆ ì†ŒìŠ¤ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì‚¬ìš©ìëŠ” ì‹¤ì‹œê°„ìœ¼ë¡œ ì½”ë“œë¥¼ ì‘ì„±í•˜ê³  ì‹¤í–‰í•˜ë©°, ê·¸ ê²°ê³¼ë¥¼ ì‹œê°ì ìœ¼ë¡œ í™•ì¸í•˜ê³ , ë¬¸ì„œí™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Anacondaë¥¼ ì„¤ì¹˜í•˜ë©´ ìë™ìœ¼ë¡œ ì„¤ì¹˜ë©ë‹ˆë‹¤!2) Jupyter Notebook ì‚¬ìš©í•˜ê¸° Jupyter Notebook ì‚¬ìš©ë²•í™˜ê²½ ì„¤ì •!ê°€ìƒí™˜ê²½ ë§Œë“¤ê¸°conda create --name myenvê°€ìƒí™˜ê²½ í™œì„±í™”conda activate myenví•„ìš” íŒ¨í‚¤ì§€ ì„¤ì¹˜conda install jupyter numpy pandasê°€ìƒí™˜ê²½ê³¼ Jupyter ì—°ê²°í•˜ê¸°python -m ipykernel install --user --name=myenv --display-name "Python (myenv)"ì‚¬ìš©í•˜ê¸°!ALTALTALT Jupyter ì‚¬ìš©í™•ì¸!ì„¸íŒ… í™•ì¸{5px}ì„¸íŒ… í™•ì¸ï»¿â€‹ALT.
[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 4. ì¸ê³µ ì‹ ê²½ë§(ANN)[SCC] ê¸°ì´ˆê°€ íƒ„íƒ„í•œ ë”¥ëŸ¬ë‹/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] ê¸°ì´ˆê°€ íƒ„íƒ„í•œ ë”¥ëŸ¬ë‹ - 2ì£¼ì°¨/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 4. ì¸ê³µ ì‹ ê²½ë§(ANN)ì œì‘:[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 4. ì¸ê³µ ì‹ ê²½ë§(ANN)[ìˆ˜ì—… ëª©í‘œ]ì¸ê³µì‹ ê²½ë§ì˜ ê°œë…ì— ëŒ€í•´ì„œ ë°°ì›Œë³´ê³  ì–´ë–¤ ì›ë¦¬ë¡œ ë™ì‘í•˜ëŠ”ì§€ ì•Œì•„ë´…ì‹œë‹¤Pytorchë¡œ ê°„ë‹¨í•œ ì¸ê³µì‹ ê²½ë§ ëª¨ë¸ êµ¬í˜„ ì‹¤ìŠµì„ ì§„í–‰í•´ ë´…ì‹œë‹¤[ëª©ì°¨]01. ê¸°ë³¸ êµ¬ì¡°ì™€ ë™ì‘ì›ë¦¬02. ì‹¤ìŠµ: ê°„ë‹¨í•œ ì¸ê³µ ì‹ ê²½ë§ ëª¨ë¸ êµ¬í˜„ (PyTorch)ğŸ’¡
 
 01. ê¸°ë³¸ êµ¬ì¡°ì™€ ë™ì‘ì›ë¦¬âœ”ï¸ANNì˜ ê¸°ë³¸ êµ¬ì„±ìš”ì†Œì™€ ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ë™ì‘í•˜ëŠ”ì§€ ë°°ì›Œë³´ê³ , ANNì˜ ì¶œë ¥ ë ˆì´ì–´ì˜ ìœ í˜•ì— ë”°ë¼ ì–´ë–»ê²Œ í™œìš© í•  ìˆ˜ ìˆëŠ”ì§€ í™•ì¸í•´ ë´…ì‹œë‹¤1) ANNì˜ ê¸°ë³¸ êµ¬ì„± ìš”ì†Œì™€ ë™ì‘ ë°©ì‹ ANNì˜ ê¸°ë³¸ êµ¬ì„± ìš”ì†Œì¸ê³µ ì‹ ê²½ë§(Artificial Neural Network, ANN)ì€ ìƒë¬¼í•™ì  ì‹ ê²½ë§ì„ ëª¨ë°©í•˜ì—¬ ì„¤ê³„ëœ ì»´í“¨íŒ… ì‹œìŠ¤í…œì…ë‹ˆë‹¤ANNì€ ì…ë ¥ì¸µ(Input Layer), ì€ë‹‰ì¸µ(Hidden Layer), ì¶œë ¥ì¸µ(Output Layer)ìœ¼ë¡œ êµ¬ì„±ë˜ë©°, ê° ì¸µì€ ë‰´ëŸ°(Neuron)ìœ¼ë¡œ ì´ë£¨ì–´ì ¸ ìˆìŠµë‹ˆë‹¤.ALTì…ë ¥ì¸µ
 - ì…ë ¥ë°ì´í„°ë¥¼ ë°›ì•„ë“¤ì´ëŠ” ì¸µ, ì…ë ¥ì¸µì˜ ë‰´ëŸ°ìˆ˜ëŠ” ì…ë ¥ë°ì´í„° í”¼ì³ìˆ˜ì™€ ë™ì¼ - ì…ë ¥ë°ì´í„°ë¥¼ ë°›ì•„ë“¤ì´ëŠ” ì¸µ, ì…ë ¥ì¸µì˜ ë‰´ëŸ°ìˆ˜ëŠ” ì…ë ¥ë°ì´í„° í”¼ì³ìˆ˜ì™€ ë™ì¼ï»¿
ì€ë‹‰ì¸µ
 - ì…ë ¥ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê³  íŠ¹ì§•ì„ ì¶”ì¶œí•˜ëŠ” ì¸µ, ì€ë‹‰ì¸µì˜ ë‰´ëŸ°ìˆ˜ì™€ ì¸µìˆ˜ëŠ” ëª¨ë¸ì˜ ë³µì¡ì„±ê³¼ ì„±ëŠ¥ì— ì˜í–¥ - ì…ë ¥ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê³  íŠ¹ì§•ì„ ì¶”ì¶œí•˜ëŠ” ì¸µ, ì€ë‹‰ì¸µì˜ ë‰´ëŸ°ìˆ˜ì™€ ì¸µìˆ˜ëŠ” ëª¨ë¸ì˜ ë³µì¡ì„±ê³¼ ì„±ëŠ¥ì— ì˜í–¥ï»¿
ì¶œë ¥ì¸µ
 - ìµœì¢… ì˜ˆì¸¡ê°’ì„ ì¶œë ¥í•˜ëŠ” ì¸µ, ì¶œë ¥ì¸µì˜ ë‰´ëŸ° ìˆ˜ëŠ” ì˜ˆì¸¡í•˜ë ¤ëŠ” í´ë˜ìŠ¤ ìˆ˜ ë˜ëŠ” íšŒê·€ë¬¸ì œ ì¶œë ¥ì°¨ì›ê³¼ ë™ì¼ - ìµœì¢… ì˜ˆì¸¡ê°’ì„ ì¶œë ¥í•˜ëŠ” ì¸µ, ì¶œë ¥ì¸µì˜ ë‰´ëŸ° ìˆ˜ëŠ” ì˜ˆì¸¡í•˜ë ¤ëŠ” í´ë˜ìŠ¤ ìˆ˜ ë˜ëŠ” íšŒê·€ë¬¸ì œ ì¶œë ¥ì°¨ì›ê³¼ ë™ì¼ï»¿â€‹ ë™ì‘ ë°©ì‹ìˆœì „íŒŒ (Forward Propagation)ì…ë ¥ ë°ì´í„°ë¥¼ í†µí•´ ê° ì¸µì˜ ë‰´ëŸ°ì´ í™œì„±í™”ë˜ê³ , ìµœì¢… ì¶œë ¥ ê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤.ê° ë‰´ëŸ°ì€ ì…ë ¥ ê°’ì— ê°€ì¤‘ì¹˜(weight)ë¥¼ ê³±í•˜ê³ , ë°”ì´ì–´ìŠ¤(bias)ë¥¼ ë”í•œ í›„ í™œì„±í™” í•¨ìˆ˜(activation function)ë¥¼ í†µí•´ ì¶œë ¥ ê°’ì„ ê²°ì •í•©ë‹ˆë‹¤.ì†ì‹¤ ê³„ì‚° (Loss Calculation)ì˜ˆì¸¡ ê°’ê³¼ ì‹¤ì œ ê°’ì˜ ì°¨ì´ë¥¼ ì†ì‹¤ í•¨ìˆ˜(Loss Function)ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.ì—­ì „íŒŒ (Backpropagation)ì†ì‹¤ í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°ë¥¼ ì¶œë ¥ì¸µì—ì„œ ì…ë ¥ì¸µ ë°©í–¥ìœ¼ë¡œ ê³„ì‚°í•˜ê³ , ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.ALT2) ì¶œë ¥ ë ˆì´ì–´ì˜ êµ¬ì„± ì¶œë ¥ë ˆì´ì–´ì˜ ìœ í˜•ê³¼ í™œìš©ì¶œë ¥ ë ˆì´ì–´ëŠ” ì‹ ê²½ë§ì˜ ìµœì¢… ì˜ˆì¸¡ ê°’ì„ ì¶œë ¥í•˜ëŠ” ì¸µìœ¼ë¡œ, ë¬¸ì œì˜ ìœ í˜•ì— ë”°ë¼ ë‹¤ì–‘í•œ í˜•íƒœë¡œ êµ¬ì„±ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.íšŒê·€ ë¬¸ì œ (Regression):ì¶œë ¥ ë ˆì´ì–´ì˜ ë‰´ëŸ° ìˆ˜ëŠ” ì˜ˆì¸¡í•˜ë ¤ëŠ” ì—°ì†ì ì¸ ê°’ì˜ ì°¨ì›ê³¼ ë™ì¼í•©ë‹ˆë‹¤.í™œì„±í™” í•¨ìˆ˜ë¡œëŠ” ì£¼ë¡œ ì„ í˜• í•¨ìˆ˜(linear function)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.ì´ì§„ ë¶„ë¥˜ ë¬¸ì œ (Binary Classification):ì¶œë ¥ ë ˆì´ì–´ì˜ ë‰´ëŸ° ìˆ˜ëŠ” 1ì…ë‹ˆë‹¤.í™œì„±í™” í•¨ìˆ˜ë¡œëŠ” ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜(Sigmoid Function)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¶œë ¥ ê°’ì„ 0ê³¼ 1 ì‚¬ì´ì˜ í™•ë¥ ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ ë¬¸ì œ (Multi-Class Classification):ì¶œë ¥ ë ˆì´ì–´ì˜ ë‰´ëŸ° ìˆ˜ëŠ” ì˜ˆì¸¡í•˜ë ¤ëŠ” í´ë˜ìŠ¤ ìˆ˜ì™€ ë™ì¼í•©ë‹ˆë‹¤.í™œì„±í™” í•¨ìˆ˜ë¡œëŠ” ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜(Softmax Function)ë¥¼ ì‚¬ìš©í•˜ì—¬ ê° í´ë˜ìŠ¤ì— ëŒ€í•œ í™•ë¥ ì„ ì¶œë ¥í•©ë‹ˆë‹¤.02. ì‹¤ìŠµ: ê°„ë‹¨í•œ ì¸ê³µ ì‹ ê²½ë§ ëª¨ë¸ êµ¬í˜„ (PyTorch)âœ”ï¸ PyTorchë¥¼ ì‚¬ìš©í•˜ì—¬ ê°„ë‹¨í•œ ì¸ê³µ ì‹ ê²½ë§ ëª¨ë¸ì„ êµ¬ì¶•í•˜ê³  í•™ìŠµí•´ë³´ê² ìŠµë‹ˆë‹¤. ì˜ˆì œë¡œëŠ” MNIST ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ ìˆ«ì ì´ë¯¸ì§€ë¥¼ ë¶„ë¥˜í•˜ëŠ” ëª¨ë¸ì„ êµ¬í˜„í•˜ê² ìŠµë‹ˆë‹¤.1)  ê°„ë‹¨í•œ ANN ëª¨ë¸ êµ¬ì¶• ë° í•™ìŠµ PyTorch ë° í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸PyTorch ë° í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ {5px}PyTorch ë° í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ï»¿â€‹Pythonë³µì‚¬import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
â€‹ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬ {5px}ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬ ï»¿â€‹Pythonë³µì‚¬# ë°ì´í„°ì…‹ ì „ì²˜ë¦¬
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])
# MNIST ë°ì´í„°ì…‹ ë¡œë“œ
trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)

testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)
â€‹ ê°„ë‹¨í•œ ANN ëª¨ë¸ ì •ì˜ê°„ë‹¨í•œ ANN ëª¨ë¸ ì •ì˜ {5px}ê°„ë‹¨í•œ ANN ëª¨ë¸ ì •ì˜ ï»¿â€‹Pythonë³µì‚¬class SimpleANN(nn.Module):
def __init__(self):
super(SimpleANN, self).__init__()
        self.fc1 = nn.Linear(28 * 28, 128) # ì…ë ¥ì¸µì—ì„œ ì€ë‹‰ì¸µìœ¼ë¡œ
        self.fc2 = nn.Linear(128, 64) # ì€ë‹‰ì¸µì—ì„œ ì€ë‹‰ì¸µìœ¼ë¡œ
        self.fc3 = nn.Linear(64, 10) # ì€ë‹‰ì¸µì—ì„œ ì¶œë ¥ì¸µìœ¼ë¡œ
def forward(self, x):
        x = x.view(-1, 28 * 28) # ì…ë ¥ ì´ë¯¸ì§€ë¥¼ 1ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
return x
â€‹torch.nn.Module: ëª¨ë“  ì‹ ê²½ë§ ëª¨ë“ˆì˜ ê¸°ë³¸ í´ë˜ìŠ¤ì…ë‹ˆë‹¤.  ì‚¬ìš©ì ì •ì˜ ì‹ ê²½ë§ì€ ì´ í´ë˜ìŠ¤ë¥¼ ìƒì†ë°›ì•„ì•¼ í•©ë‹ˆë‹¤.nn.Linear: ì„ í˜• ë³€í™˜ì„ ì ìš©í•˜ëŠ” ì™„ì „ ì—°ê²°(fully connected) ë ˆì´ì–´ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.nn.Linear(in_features, out_features)ëŠ” ì…ë ¥ íŠ¹ì§•ì˜ ìˆ˜ì™€ ì¶œë ¥ íŠ¹ì§•ì˜ ìˆ˜ë¥¼ ì§€ì •í•©ë‹ˆë‹¤..Linear(in_features, out_features)ëŠ” ì…ë ¥ íŠ¹ì§•ì˜ ìˆ˜ì™€ ì¶œë ¥ íŠ¹ì§•ì˜ ìˆ˜ë¥¼ ì§€ì •í•©ë‹ˆë‹¤.ï»¿â€‹torch.relu: ReLU í™œì„±í™” í•¨ìˆ˜ë¥¼ ì ìš©í•©ë‹ˆë‹¤.view: í…ì„œì˜ í¬ê¸°ë¥¼ ë³€ê²½í•©ë‹ˆë‹¤.x.view(-1, 28 * 28)ì€ ì…ë ¥ ì´ë¯¸ì§€ë¥¼ 1ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤..view(-1, 28 * 28)ì€ ì…ë ¥ ì´ë¯¸ì§€ë¥¼ 1ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.ï»¿â€‹ ëª¨ë¸ í•™ìŠµëª¨ë¸ í•™ìŠµ {5px}ëª¨ë¸ í•™ìŠµ ï»¿â€‹Pythonë³µì‚¬# ëª¨ë¸ ì´ˆê¸°í™”
model = SimpleANN()
# ì†ì‹¤ í•¨ìˆ˜ì™€ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ ì •ì˜
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
# ëª¨ë¸ í•™ìŠµ
for epoch in range(10): # 10 ì—í¬í¬ ë™ì•ˆ í•™ìŠµ
    running_loss = 0.0
for i, data in enumerate(trainloader, 0):
        inputs, labels = data

        # ê¸°ìš¸ê¸° ì´ˆê¸°í™”
        optimizer.zero_grad()
# ìˆœì „íŒŒ + ì—­ì „íŒŒ + ìµœì í™”
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
# ì†ì‹¤ ì¶œë ¥
        running_loss += loss.item()
if i % 100 == 99: # ë§¤ 100 ë¯¸ë‹ˆë°°ì¹˜ë§ˆë‹¤ ì¶œë ¥
print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 100:.3f}')
            running_loss = 0.0
print('Finished Training')
â€‹nn.CrossEntropyLoss: ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ ë¬¸ì œì—ì„œ ì£¼ë¡œ ì‚¬ìš©ë˜ëŠ” ì†ì‹¤ í•¨ìˆ˜ì…ë‹ˆë‹¤. ì˜ˆì¸¡ ê°’ê³¼ ì‹¤ì œ ê°’ ì‚¬ì´ì˜ êµì°¨ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤ì„ ê³„ì‚°í•©ë‹ˆë‹¤.optim.SGD: í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•(Stochastic Gradient Descent) ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì„ ì •ì˜í•©ë‹ˆë‹¤.  lrì€ í•™ìŠµë¥ , momentumì€ ëª¨ë©˜í…€ ê°’ì„ ì§€ì •í•©ë‹ˆë‹¤.ì€ í•™ìŠµë¥ , momentumì€ ëª¨ë©˜í…€ ê°’ì„ ì§€ì •í•©ë‹ˆë‹¤.ï»¿â€‹optimizer.zero_grad(): ì´ì „ ë‹¨ê³„ì—ì„œ ê³„ì‚°ëœ ê¸°ìš¸ê¸°ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.loss.backward(): ì—­ì „íŒŒë¥¼ í†µí•´ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.optimizer.step(): ê³„ì‚°ëœ ê¸°ìš¸ê¸°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. ëª¨ë¸  í‰ê°€ëª¨ë¸ í‰ê°€ {5px}ëª¨ë¸ í‰ê°€ ï»¿â€‹Pythonë³µì‚¬correct = 0
total = 0
with torch.no_grad():
for data in testloader:
        images, labels = data
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')

â€‹torch.no_grad(): í‰ê°€ ë‹¨ê³„ì—ì„œëŠ” ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•  í•„ìš”ê°€ ì—†ìœ¼ë¯€ë¡œ, ì´ë¥¼ ë¹„í™œì„±í™”í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ì„ ì¤„ì…ë‹ˆë‹¤.torch.max: í…ì„œì˜ ìµœëŒ€ ê°’ì„ ì°¾ìŠµë‹ˆë‹¤. torch.max(outputs.data, 1)ì€ ê° ìƒ˜í”Œì— ëŒ€í•´ ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ í´ë˜ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤..max(outputs.data, 1)ì€ ê° ìƒ˜í”Œì— ëŒ€í•´ ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ í´ë˜ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.ï»¿â€‹labels.size(0): ë°°ì¹˜ í¬ê¸°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.(predicted == labels).sum().item(): ì˜ˆì¸¡ ê°’ê³¼ ì‹¤ì œ ê°’ì´ ì¼ì¹˜í•˜ëŠ” ìƒ˜í”Œì˜ ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤..
LangChain: ê°œë…ê³¼ í™œìš©  LLM & RAGë¥¼ í™œìš©í•œ AI ì„œë¹„ìŠ¤ ë§Œë“¤ê¸°/ LLM & RAGë¥¼ í™œìš©í•œ AI ì„œë¹„ìŠ¤ ë§Œë“¤ê¸° - 5ì£¼ì°¨/LangChain: ê°œë…ê³¼ í™œìš© ì œì‘:LangChain: ê°œë…ê³¼ í™œìš© ìˆ˜ì—… ëª©í‘œLangchainì— ëŒ€í•´ì„œ í•™ìŠµí•˜ê³ , ì‚¬ìš© ì‚¬ë¡€ì™€ ì£¼ìš” ì¥ì ì„ ì‚´í´ë´…ë‹ˆë‹¤.ëª©ì°¨ LangChainì´ë€?LangChainì´ ì™œ ì¢‹ì€ê°€LangChainì˜ ì£¼ìš” ê°œë…LangChain ì‚¬ìš© ì‚¬ë¡€ë°°ìš´ ë‚´ìš© ì •ë¦¬í•˜ê¸°!â—LangChainì€ LLM(ëŒ€í˜• ì–¸ì–´ ëª¨ë¸)ê³¼ ë‹¤ì–‘í•œ ì»´í¬ë„ŒíŠ¸ë¥¼ ì—°ê²°í•˜ì—¬ ë³µì¡í•œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ê³  ì²´ì¸ í˜•íƒœë¡œ êµ¬ì„±ëœ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ë§Œë“¤ ìˆ˜ ìˆëŠ” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ LangChainì€ ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìƒì„±ì—ì„œ ë³µì¡í•œ ë©€í‹°ìŠ¤í… ì›Œí¬í”Œë¡œìš°ê¹Œì§€ ë‹¤ì–‘í•œ AI ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ê°œë°œí•˜ëŠ” ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤. LangChainì´ë€?ğŸ’¡LangChainì€ ì–¸ì–´ ëª¨ë¸ì„ ì¤‘ì‹¬ìœ¼ë¡œ ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤ì™€ íˆ´ì„ ì—°ê²°í•˜ì—¬ ì²´ì¸ ê¸°ë°˜ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì¶•í•  ìˆ˜ ìˆëŠ” Python ê¸°ë°˜ í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤. 

LangChainì„ ì‚¬ìš©í•˜ë©´ í•˜ë‚˜ì˜ ì–¸ì–´ ëª¨ë¸ ì‘ë‹µë§Œ ë°›ëŠ” ëŒ€ì‹ ,
ì—¬ëŸ¬ ë‹¨ê³„ë¡œ êµ¬ì„±ëœ ì²´ì¸ êµ¬ì¡°ë¥¼ í†µí•´ ë‹¤ì–‘í•œ ì—°ì‚°ê³¼ ë°ì´í„° ì²˜ë¦¬, ë©€í‹°ìŠ¤í… ë¶„ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.

ì˜ˆë¥¼ ë“¤ì–´, íŠ¹ì • ì§ˆë¬¸ì— ëŒ€í•´ ì™¸ë¶€ ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ê³ , ì´ë¥¼ ì¢…í•©í•˜ì—¬ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ê³¼ì • ë“±ì„ ìë™í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.LangChainì´ ì™œ ì¢‹ì€ê°€ìœ ì—°í•œ êµ¬ì„±
LangChainì€ ì–¸ì–´ ëª¨ë¸ê³¼ ë‹¤ì–‘í•œ ì»´í¬ë„ŒíŠ¸ë¥¼ ì‰½ê²Œ ì—°ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª¨ë¸ì˜ ì‘ë‹µì„ ë‹¤ë¥¸ ì»´í¬ë„ŒíŠ¸ë¡œ ë³´ë‚´ê±°ë‚˜, ì—¬ëŸ¬ ë‹¨ê³„ì— ê±¸ì¹œ ë°ì´í„° ì²˜ë¦¬ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.ëª¨ë“ˆí™”ëœ ì»´í¬ë„ŒíŠ¸
í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿, ì¶œë ¥ íŒŒì„œ, ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤, ì—ì´ì „íŠ¸ ë“±ì„ í†µí•´ ê° ì»´í¬ë„ŒíŠ¸ë¥¼ í•„ìš”ì— ë”°ë¼ ì¡°í•©í•  ìˆ˜ ìˆì–´ ì¬ì‚¬ìš©ì„±ê³¼ í™•ì¥ì„±ì´ ë†’ìŠµë‹ˆë‹¤.ì²´ì¸ê³¼ ì—ì´ì „íŠ¸
ë‹¨ìˆœí•œ ì§ˆë¬¸-ì‘ë‹µì„ ë„˜ì–´ì„œ ì—¬ëŸ¬ ì‘ì—…ì„ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰í•˜ëŠ” ì²´ì¸ê³¼ ìƒí™©ì— ë”°ë¼ í–‰ë™ì„ ê²°ì •í•˜ëŠ” ì—ì´ì „íŠ¸ë¥¼ í†µí•´ ë³µì¡í•œ ì‘ì—…ì„ ìë™í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ê°•ë ¥í•œ í†µí•© ê¸°ëŠ¥
LangChainì€ OpenAI, HuggingFace, FAISS, ElasticSearch ë“± ë‹¤ì–‘í•œ ì–¸ì–´ ëª¨ë¸, ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì™€ì˜ í†µí•©ì´ ê°€ëŠ¥í•´ ë°ì´í„° ì†ŒìŠ¤ í™•ì¥ê³¼ ë¹ ë¥¸ ê²€ìƒ‰ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.LangChainì˜ ì£¼ìš” ê°œë…ì–¸ì–´ ëª¨ë¸ (LLM)ì–¸ì–´ ëª¨ë¸ì€ ì£¼ì–´ì§„ ì…ë ¥ì„ ë°”íƒ•ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. LangChainì€ OpenAIì˜ GPT ëª¨ë¸ì„ í¬í•¨í•´ ë‹¤ì–‘í•œ ì–¸ì–´ ëª¨ë¸ê³¼ì˜ í†µí•©ì„ ì§€ì›í•©ë‹ˆë‹¤.ì˜ˆì‹œ: "LangChainì„ ì‚¬ìš©í•˜ì—¬ OpenAIì˜ GPT-4 ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ ê¸°ë³¸ì ì¸ ì§ˆë¬¸ì— ë‹µë³€í•˜ë„ë¡ ì„¤ì •"í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (Prompt Templates)í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì€ í”„ë¡¬í”„íŠ¸ë¥¼ ë™ì ìœ¼ë¡œ ìƒì„±í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. íŠ¹ì • ì…ë ¥ ê°’ì— ë”°ë¼ í…œí”Œë¦¿ì´ ì±„ì›Œì ¸ ëª¨ë¸ì— ì „ë‹¬ë˜ë¯€ë¡œ ë°˜ë³µì ì¸ ì‘ì—…ì„ ë‹¨ìˆœí™”í•©ë‹ˆë‹¤.ì˜ˆì‹œ: ì‚¬ìš©ì ì§ˆë¬¸ì— ë”°ë¼ íŠ¹ì • ì •ë³´ ê²€ìƒ‰ì„ ìš”êµ¬í•˜ëŠ” ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±ì²´ì¸ (Chains)ì—¬ëŸ¬ ë‹¨ê³„ë¥¼ ê±°ì¹˜ëŠ” ì›Œí¬í”Œë¡œìš°ë¥¼ í•˜ë‚˜ë¡œ ë¬¶ì–´ì£¼ëŠ” ê¸°ëŠ¥ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•´ í•„ìš”í•œ ë°ì´í„°ë¥¼ ê²€ìƒ‰í•˜ê³ , ê²€ìƒ‰ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì¼ë ¨ì˜ ê³¼ì •ì„ ì²´ì¸ìœ¼ë¡œ êµ¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ì˜ˆì‹œ: ì‚¬ìš©ì ì§ˆë¬¸ì„ ë°›ì€ í›„ ê²€ìƒ‰ -> ë¶„ì„ -> ì‘ë‹µ ìƒì„±ì˜ 3ë‹¨ê³„ ì²´ì¸ ìƒì„±ì—ì´ì „íŠ¸ (Agents)ì—ì´ì „íŠ¸ëŠ” ë™ì ìœ¼ë¡œ í•„ìš”í•œ ì‘ì—…ì„ ê²°ì •í•˜ê³  ìˆ˜í–‰í•˜ëŠ” ì»´í¬ë„ŒíŠ¸ì…ë‹ˆë‹¤. ì§ˆë¬¸ì— ë”°ë¼ ë‹µë³€í•˜ê¸° ìœ„í•´ API í˜¸ì¶œì´ í•„ìš”í•œì§€, ë˜ëŠ” ë‹¨ìˆœíˆ í…ìŠ¤íŠ¸ ìƒì„±ì„ í•´ì•¼ í•˜ëŠ”ì§€ë¥¼ íŒë‹¨í•´ ì‘ì—…ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.ì˜ˆì‹œ: ì§ˆë¬¸ì´ ì§€ë¦¬ ì •ë³´ì— ê´€ë ¨ëœ ê²½ìš°, APIë¥¼ í˜¸ì¶œí•´ ìµœì‹  ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ì—ì´ì „íŠ¸ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ (Vector Databases)ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ëŠ” í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•´ ì €ì¥í•˜ê³ , ì´í›„ ìœ ì‚¬í•œ ë²¡í„°ë¥¼ ë¹ ë¥´ê²Œ ê²€ìƒ‰í•  ìˆ˜ ìˆë„ë¡ ë•ìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´, ì €ì¥ëœ ë°ì´í„°ì™€ ìœ ì‚¬í•œ ì§ˆë¬¸ì— ë¹ ë¥´ê²Œ ì‘ë‹µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ì˜ˆì‹œ: FAISSì™€ ê°™ì€ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ë¬¸ì„œë¥¼ ì €ì¥í•´ ìœ ì‚¬ì„± ê²€ìƒ‰ì„ ìˆ˜í–‰LangChain ì‚¬ìš© ì‚¬ë¡€1ï¸âƒ£ê²€ìƒ‰ ê¸°ë°˜ ìƒì„±(RAG)
LangChainì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•  ë•Œ ê´€ë ¨ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•˜ê³ , í•´ë‹¹ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ìµœì‹  ì •ë³´ì— ê¸°ë°˜í•œ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.2ï¸âƒ£FAQ ì‹œìŠ¤í…œ
ë‹¤ì–‘í•œ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ê³ , ìœ ì‚¬ì„± ê²€ìƒ‰ì„ í†µí•´ ë¹ ë¥´ê²Œ ì ì ˆí•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.3ï¸âƒ£ë‹¤ë‹¨ê³„ ì±—ë´‡ ì›Œí¬í”Œë¡œìš°
ë³µì¡í•œ ì§ˆë¬¸ì— ëŒ€í•´ ì—¬ëŸ¬ ë‹¨ê³„ë¥¼ ê±°ì³ ë‹µë³€ì„ êµ¬ì„±í•˜ëŠ” ì±—ë´‡ì„ ì„¤ê³„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì§ˆë¬¸ì„ ë°›ì•„ ìš”ì•½í•œ í›„, ê´€ë ¨ëœ ì¶”ê°€ ì •ë³´ë¥¼ ê²€ìƒ‰í•´ ì œê³µí•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.4ï¸âƒ£ì§€ëŠ¥í˜• ì—ì´ì „íŠ¸
ì£¼ì‹ ê°€ê²©ì„ í™•ì¸í•˜ê±°ë‚˜ ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ê²€ìƒ‰í•´ ìµœì‹  ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ì±—ë´‡ ì—ì´ì „íŠ¸ë¥¼ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.LangChainì˜ ì£¼ìš” ì¥ì  ìš”ì•½í™•ì¥ì„±: ì–¸ì–´ ëª¨ë¸ê³¼ ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤ë¥¼ ì—°ê²°í•˜ì—¬ ìœ ì—°í•˜ê²Œ í™•ì¥ ê°€ëŠ¥ìë™í™”: ë³µì¡í•œ ë©€í‹°ìŠ¤í… í”„ë¡œì„¸ìŠ¤ë¥¼ ì²´ì¸ í˜•íƒœë¡œ êµ¬ì„±í•˜ì—¬ ìë™í™” ê°€ëŠ¥ìœ ì‚¬ì„± ê²€ìƒ‰: ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ í™œìš©í•´ ìœ ì‚¬ ë¬¸ì„œë¥¼ ë¹ ë¥´ê²Œ ê²€ìƒ‰ ë° ì‘ë‹µ ê°€ëŠ¥ì¬ì‚¬ìš©ì„±: í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿, ì²´ì¸, ì—ì´ì „íŠ¸ ë“±ì„ í™œìš©í•´ ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ êµ¬ì„± ê°€ëŠ¥ë°°ìš´ ë‚´ìš© ì •ë¦¬í•˜ê¸°!ğŸ˜€LangChainì€ LLMì˜ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤ì™€ í†µí•©í•´ ìœ ì—°í•˜ê³  í™•ì¥ì„± ìˆëŠ” AI ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ê°œë°œí•˜ëŠ” ë° í° ë„ì›€ì„ ì¤ë‹ˆë‹¤. LangChainì˜ ë‹¤ì–‘í•œ ì»´í¬ë„ŒíŠ¸ë¥¼ ì´í•´í•˜ê³  ì‚¬ìš©í•˜ëŠ” ê²ƒì€ AI ì• í”Œë¦¬ì¼€ì´ì…˜ì˜ íš¨ìœ¨ì„±ê³¼ ìƒì‚°ì„±ì„ ë†’ì´ëŠ” ë° ì¤‘ìš”í•œ ì—­í• ì„ í•©ë‹ˆë‹¤.
ë°°ì—´ ì—°ì‚° ë° ë¸Œë¡œë“œìºìŠ¤íŒ…ì¸ê³µì§€ëŠ¥ì„ ìœ„í•œ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬/ì¸ê³µì§€ëŠ¥ì„ ìœ„í•œ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ - 2ì£¼ì°¨/ë°°ì—´ ì—°ì‚° ë° ë¸Œë¡œë“œìºìŠ¤íŒ…ì œì‘:ë°°ì—´ ì—°ì‚° ë° ë¸Œë¡œë“œìºìŠ¤íŒ…ìˆ˜ì—… ëª©í‘œNumpyì˜ ë°°ì—´ ì—°ì‚° ë° ë¸Œë¡œë“œìºìŠ¤íŒ…ì— ëŒ€í•´ì„œ í•™ìŠµí•©ë‹ˆë‹¤.ëª©ì°¨ë°°ì—´ ì—°ì‚° ë° ë¸Œë¡œë“œìºìŠ¤íŒ…ê¸°ë³¸ì ì¸ ìˆ˜í•™ í•¨ìˆ˜ì™€ í†µê³„ í•¨ìˆ˜ìš”ì•½ë°°ì—´ ì—°ì‚° ë° ë¸Œë¡œë“œìºìŠ¤íŒ…ğŸ“šNumPyì—ì„œëŠ” ë°°ì—´ ê°„ì˜ ì—°ì‚°ì„ ë§¤ìš° íš¨ìœ¨ì ìœ¼ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ ë¸Œë¡œë“œìºìŠ¤íŒ…(Broadcasting) ê¸°ëŠ¥ì€ í¬ê¸°ê°€ ë‹¤ë¥¸ ë°°ì—´ ê°„ì˜ ì—°ì‚°ì„ ê°€ëŠ¥í•˜ê²Œ í•´ì¤ë‹ˆë‹¤.ë°°ì—´ ê°„ì˜ ê¸°ë³¸ ì—°ì‚°NumPy ë°°ì—´ ê°„ì˜ ë§ì…ˆ, ëº„ì…ˆ, ê³±ì…ˆ, ë‚˜ëˆ—ì…ˆ ëª¨ë‘ ìš”ì†Œë³„(element-wise)ë¡œ ìˆ˜í–‰ë©ë‹ˆë‹¤.Pythonë³µì‚¬import numpy as np

arr1 = np.array([1, 2, 3])
arr2 = np.array([4, 5, 6])
# ë§ì…ˆ
add_result = arr1 + arr2
print(add_result)
# ê³±ì…ˆ
mul_result = arr1 * arr2
print(mul_result)

â€‹ì¶œë ¥ ê²°ê³¼Plain Textë³µì‚¬[5 7 9]
[ 4 10 18]

â€‹ë¸Œë¡œë“œìºìŠ¤íŒ…(Broadcasting)ğŸ“šë¸Œë¡œë“œìºìŠ¤íŒ…ì´ë€?ì‘ì€ ë°°ì—´ì´ í° ë°°ì—´ì˜ í˜•íƒœì— ë§ê²Œ ìë™ìœ¼ë¡œ í™•ì¥ë˜ì–´ ì—°ì‚°ì´ ì´ë£¨ì–´ì§€ëŠ” ê²ƒì„ ì˜ë¯¸í•´ìš”. 
ì´ ê¸°ëŠ¥ ë•ë¶„ì— ì½”ë“œë¥¼ ë” ê°„ê²°í•˜ê²Œ ì‘ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ì„œë¡œ ë‹¤ë¥¸ í¬ê¸°ì˜ ë°°ì—´ ê°„ì—ë„ ì—°ì‚°ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. ì‘ì€ ë°°ì—´ì´ ìë™ìœ¼ë¡œ í™•ì¥ë˜ì–´ ì—°ì‚°ë©ë‹ˆë‹¤.Pythonë³µì‚¬arr1 = np.array([1, 2, 3])
arr2 = np.array([[10], [20], [30]])
# ë¸Œë¡œë“œìºìŠ¤íŒ…ì„ ì´ìš©í•œ ë§ì…ˆ
broadcast_result = arr1 + arr2
print(broadcast_result)

â€‹ì¶œë ¥ ê²°ê³¼Plain Textë³µì‚¬[[11 12 13]
 [21 22 23]
 [31 32 33]]

â€‹ê¸°ë³¸ì ì¸ ìˆ˜í•™ í•¨ìˆ˜ì™€ í†µê³„ í•¨ìˆ˜ğŸ“šNumPyëŠ” ë°°ì—´ì„ ë‹¤ë£° ë•Œ ìì£¼ ì‚¬ìš©í•˜ëŠ” ë‹¤ì–‘í•œ ìˆ˜í•™ í•¨ìˆ˜ì™€ í†µê³„ í•¨ìˆ˜ë¥¼ ì œê³µí•©ë‹ˆë‹¤.ê¸°ë³¸ì ì¸ ìˆ˜í•™ í•¨ìˆ˜ë°°ì—´ì˜ í•©(sum)ê³¼ ê³±(prod)Pythonë³µì‚¬arr = np.array([1, 2, 3, 4])
# í•©ê³„
sum_result = np.sum(arr)
print(sum_result)
# ê³±
prod_result = np.prod(arr)
print(prod_result)

â€‹ì¶œë ¥ ê²°ê³¼Plain Textë³µì‚¬10
24

â€‹ì œê³±ê·¼ê³¼ ë¡œê·¸Pythonë³µì‚¬# ì œê³±ê·¼
sqrt_result = np.sqrt(arr)
print(sqrt_result)
# ìì—°ë¡œê·¸
log_result = np.log(arr)
print(log_result)

â€‹ì¶œë ¥ ê²°ê³¼Plain Textë³µì‚¬[1.         1.41421356 1.73205081 2.        ]
[0.         0.69314718 1.09861229 1.38629436]

â€‹ê¸°ë³¸ì ì¸ í†µê³„ í•¨ìˆ˜í‰ê· (mean), ì¤‘ê°„ê°’(median), í‘œì¤€í¸ì°¨(std)Pythonë³µì‚¬arr = np.array([1, 2, 3, 4, 5])
# í‰ê· 
mean_result = np.mean(arr)
print(mean_result)
# ì¤‘ê°„ê°’
median_result = np.median(arr)
print(median_result)
# í‘œì¤€í¸ì°¨
std_result = np.std(arr)
print(std_result)

â€‹ì¶œë ¥ ê²°ê³¼Plain Textë³µì‚¬3.0
3.0
1.4142135623730951

â€‹ìµœëŒ€ê°’(max)ê³¼ ìµœì†Œê°’(min)Pythonë³µì‚¬arr = np.array([1, 3, 2, 7, 5])
# ìµœëŒ€ê°’
max_result = np.max(arr)
print(max_result)
# ìµœì†Œê°’
min_result = np.min(arr)
print(min_result)

â€‹ì¶œë ¥ ê²°ê³¼Plain Textë³µì‚¬7
1

â€‹ìš”ì•½ğŸ“šìš”ì•½ë°°ì—´ ê°„ ì—°ì‚°ì€ ìš”ì†Œë³„ë¡œ ìˆ˜í–‰ë˜ë©°, í¬ê¸°ê°€ ë‹¤ë¥¸ ë°°ì—´ ê°„ì—ë„ ë¸Œë¡œë“œìºìŠ¤íŒ…ì„ í†µí•´ ì—°ì‚°ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.NumPyëŠ” í•©ê³„, ê³±, ì œê³±ê·¼, ë¡œê·¸ì™€ ê°™ì€ ë‹¤ì–‘í•œ ìˆ˜í•™ í•¨ìˆ˜ë¥¼ ì œê³µí•˜ë©°, ì´ë¥¼ í†µí•´ ë³µì¡í•œ ê³„ì‚°ì„ ê°„ë‹¨í•˜ê²Œ ìˆ˜í–‰í•  ìˆ˜ ìˆì–´ìš”.í‰ê· , ì¤‘ê°„ê°’, í‘œì¤€í¸ì°¨, ìµœëŒ€ê°’, ìµœì†Œê°’ ë“±ì˜ í†µê³„ í•¨ìˆ˜ë„ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆì–´ ë°ì´í„° ë¶„ì„ì— ë§¤ìš° ìœ ìš©í•©ë‹ˆë‹¤.ì´ì œ NumPyì˜ ê°•ë ¥í•œ ì—°ì‚° ëŠ¥ë ¥ì„ í™œìš©í•´ ë” ë³µì¡í•œ ë°ì´í„° ë¶„ì„ì„ ì‹œë„í•´ë³´ì„¸ìš”! â€‹
[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 5ê°•. ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°[SCC] ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ - 2ì£¼ì°¨/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 5ê°•. ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°ì œì‘:[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 5ê°•. ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°[ìˆ˜ì—… ëª©í‘œ]pandas ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì´ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ë²•ì„ ë°°ì›ë‹ˆë‹¤.ìºê¸€(Kaggle)ì— ëŒ€í•´ ì•Œì•„ë³´ê³ , ìºê¸€ì˜ ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë°›ì•„ ë¶ˆëŸ¬ì˜¤ëŠ” ì‹¤ìŠµì„ í•´ë´…ë‹ˆë‹¤.[ëª©ì°¨]01. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ë° íƒìƒ‰02. ìºê¸€(Kaggle) ì†Œê°œ ë° ë°ì´í„°ì…‹ ë‹¤ìš´ ë°›ê¸°01. ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ë° íƒìƒ‰Pandasë¥¼ ì´ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ê³  ê¸°ë³¸ ì •ë³´ë¥¼ í™•ì¸í•´ ë´…ë‹ˆë‹¤1) ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° (CSV ë° ì—‘ì…€íŒŒì¼) Pandas ë¼ì´ë¸ŒëŸ¬ë¦¬ ì†Œê°œë°ì´í„° ì¡°ì‘ ë° ë¶„ì„ì„ ìœ„í•œ Python ë¼ì´ë¸ŒëŸ¬ë¦¬pandasëŠ” ë°ì´í„° í”„ë ˆì„(DataFrame) êµ¬ì¡°ë¥¼ ì‚¬ìš©í•´ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬/ì¡°ì‘ í•  ìˆ˜ìˆìŠµë‹ˆë‹¤ CSV íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°CSV : ì½¤ë§ˆë¡œ êµ¬ë¶„ëœ ê°’ë“¤ì„ ì €ì¥í•˜ëŠ” í…ìŠ¤íŠ¸ íŒŒì¼Pandasì˜ read_csv í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ CSV íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.CSV ë¶ˆëŸ¬ì˜¤ê¸° {5px}CSV ë¶ˆëŸ¬ì˜¤ê¸° ï»¿â€‹Pythonë³µì‚¬import pandas as pd

# CSV íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
df = pd.read_csv('data.csv')
# ë°ì´í„° í”„ë ˆì„ì˜ ì²« 5í–‰ ì¶œë ¥
print(df.head())
â€‹ ì—‘ì…€ íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°Pandasì˜ read_excel í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì—‘ì…€ íŒŒì¼ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.ì—‘ì…€ ë¶ˆëŸ¬ì˜¤ê¸° {5px}ì—‘ì…€ ë¶ˆëŸ¬ì˜¤ê¸° ï»¿â€‹Pythonë³µì‚¬import pandas as pd

# ì—‘ì…€ íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
df = pd.read_excel('data.xlsx', sheet_name='Sheet1')
# ë°ì´í„° í”„ë ˆì„ì˜ ì²« 5í–‰ ì¶œë ¥
print(df.head())
â€‹2) ë°ì´í„° êµ¬ì¡° í™•ì¸í•´ ë³´ê¸° ë°ì´í„° í”„ë ˆì„ì˜ ê¸°ë³¸ ì •ë³´ í™•ì¸Pandasì—ì„œ ì œê³µí•˜ëŠ” ë‹¤ì–‘í•œ ë©”ì„œë“œë“¤ì„ ì´ìš©í•˜ì—¬ ë°ì´í„° í”„ë ˆì„ì˜ êµ¬ì¡°ì™€ ê¸°ë³¸ ì •ë³´ í™•ì¸ ê°€ëŠ¥ë°ì´í„°í”„ë ˆì„ ì •ë³´í™•ì¸ {5px}ë°ì´í„°í”„ë ˆì„ ì •ë³´í™•ì¸ ï»¿â€‹Pythonë³µì‚¬# ë°ì´í„° í”„ë ˆì„ì˜ í¬ê¸° (í–‰, ì—´) í™•ì¸
print(df.shape)
# ë°ì´í„° í”„ë ˆì„ì˜ ì»¬ëŸ¼ëª… í™•ì¸
print(df.columns)
# ë°ì´í„° í”„ë ˆì„ì˜ ë°ì´í„° íƒ€ì… í™•ì¸
print(df.dtypes)
# ë°ì´í„° í”„ë ˆì„ì˜ ìš”ì•½ í†µê³„ëŸ‰ í™•ì¸
print(df.describe())
# ë°ì´í„° í”„ë ˆì„ì˜ ì •ë³´ í™•ì¸ (null ê°’, ë°ì´í„° íƒ€ì… ë“±)
print(df.info())
â€‹02. ìºê¸€(Kaggle) ì†Œê°œ ë° ë°ì´í„°ì…‹ ë‹¤ìš´ ë°›ê¸°ì‚¬ìš© ê¶Œí•œ ì—†ìŒë™ê¸°í™”ëœ ì´ ë¸”ë¡ì— ëŒ€í•œ ì‚¬ìš© ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤ì‚¬ìš© ê¶Œí•œ ìš”ì²­1) ìºê¸€(Kaggle) ì†Œê°œ ìºê¸€ì´ë€?ë°ì´í„° ê³¼í•™ ë° ë¨¸ì‹ ëŸ¬ë‹ ê²½ì§„ëŒ€íšŒ í”Œë«í¼ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸ / ë¨¸ì‹ ëŸ¬ë‹ ì—”ì§€ë‹ˆì–´ë“¤ì´ ë‹¤ì–‘í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê³  ë°ì´í„°ë¥¼ ë¶„ì„ëª¨ë¸ì„ ê°œë°œí•˜ëŠ”ë° í•„ìš”í•œ ë°ì´í„°ì…‹ê³¼ ë„êµ¬ë¥¼ ì œê³µ ìºê¸€ì˜ ì£¼ìš” ê¸°ëŠ¥ê²½ì§„ëŒ€íšŒ: ë‹¤ì–‘í•œ ë°ì´í„° ê³¼í•™ ë° ë¨¸ì‹ ëŸ¬ë‹ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ê²½ì§„ëŒ€íšŒê°€ ì—´ë¦½ë‹ˆë‹¤.ë°ì´í„°ì…‹: ë‹¤ì–‘í•œ ì£¼ì œì˜ ë°ì´í„°ì…‹ì„ ë¬´ë£Œë¡œ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ì»¤ë®¤ë‹ˆí‹°: ë°ì´í„° ê³¼í•™ìì™€ ë¨¸ì‹ ëŸ¬ë‹ ì—”ì§€ë‹ˆì–´ë“¤ì´ ì§€ì‹ì„ ê³µìœ í•˜ê³  í˜‘ì—…í•  ìˆ˜ ìˆëŠ” ì»¤ë®¤ë‹ˆí‹°ì…ë‹ˆë‹¤.ì»¤ë„: ì›¹ ê¸°ë°˜ì˜ ì½”ë“œ ì‹¤í–‰ í™˜ê²½ìœ¼ë¡œ, Jupyter ë…¸íŠ¸ë¶ê³¼ ìœ ì‚¬í•œ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.2) ë°ì´í„°ì…‹ ë‹¤ìš´ë°›ê¸° ìºê¸€ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ë°©ë²•ìºê¸€ì—ì„œ ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œ í•˜ê¸° ìœ„í•´ì„œëŠ” ë¨¼ì € ìºê¸€ ê³„ì •ì´ í•„ìš”í•©ë‹ˆë‹¤ìºê¸€ì—ì„œ ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œ í•˜ê¸° ìœ„í•´ì„œëŠ” ë¨¼ì € ìºê¸€ ê³„ì •ì´ í•„ìš”í•©ë‹ˆë‹¤ï»¿â€‹ìºê¸€ API ì„¤ì¹˜Pythonë³µì‚¬pip install kaggle
â€‹ìºê¸€ API í‚¤ ì„¤ì •ìºê¸€ ê³„ì •ì—ì„œ APIí‚¤ ìƒì„±í•˜ê³  ë¡œì»¬ í™˜ê²½ì— ì €ì¥API í‚¤ëŠ” ~/.kaggle/kaggle.json íŒŒì¼ì— ì €ì¥ë©ë‹ˆë‹¤.ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œPythonë³µì‚¬kaggle datasets download -d <dataset-identifier>
â€‹ ì˜ˆì‹œ : íƒ€ì´íƒ€ë‹‰ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œíƒ€ì´íƒ€ë‹‰ ìƒì¡´ì ì˜ˆì¸¡ ê²½ì§„ëŒ€íšŒì˜ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì˜ˆì‹œíƒ€ì´íƒ€ë‹‰ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ {5px}íƒ€ì´íƒ€ë‹‰ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ï»¿â€‹Pythonë³µì‚¬# íƒ€ì´íƒ€ë‹‰ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ
kaggle competitions download -c titanic

# ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ ì••ì¶• í•´ì œ
unzip titanic.zip
â€‹ë‹¤ìš´ë¡œë“œí•œ ë°ì´í„° import {5px}ë‹¤ìš´ë¡œë“œí•œ ë°ì´í„° import ï»¿â€‹Pythonë³µì‚¬import pandas as pd

# íƒ€ì´íƒ€ë‹‰ ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°
train_df = pd.read_csv('train.csv')
test_df = pd.read_csv('test.csv')
# ë°ì´í„° í”„ë ˆì„ì˜ ì²« 5í–‰ ì¶œë ¥
print(train_df.head())
print(test_df.head())
â€‹.
[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 5. í•©ì„±ê³± ì‹ ê²½ë§(CNN)[SCC] ê¸°ì´ˆê°€ íƒ„íƒ„í•œ ë”¥ëŸ¬ë‹/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] ê¸°ì´ˆê°€ íƒ„íƒ„í•œ ë”¥ëŸ¬ë‹ - 2ì£¼ì°¨/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 5. í•©ì„±ê³± ì‹ ê²½ë§(CNN)ì œì‘:[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 5. í•©ì„±ê³± ì‹ ê²½ë§(CNN)[ìˆ˜ì—… ëª©í‘œ]í•©ì„±ê³± ì‹ ê²½ë§ì˜ ê°œë…ì— ëŒ€í•´ì„œ ë°°ì›Œë³´ê³  ì–´ë–¤ ì›ë¦¬ë¡œ ë™ì‘í•˜ëŠ”ì§€ ì•Œì•„ë´…ì‹œë‹¤Pytorchë¡œ ê°„ë‹¨í•œ CNN ëª¨ë¸ êµ¬í˜„ ì‹¤ìŠµì„ ì§„í–‰í•´ ë´…ì‹œë‹¤[ëª©ì°¨]01. CNNì˜ ê¸°ë³¸ êµ¬ì¡°ì™€ ë™ì‘ ì›ë¦¬02. ì‹¤ìŠµ: CNNì„ ì´ìš©í•œ ì´ë¯¸ì§€ ë¶„ë¥˜ (PyTorch)
 
 01. CNNì˜ ê¸°ë³¸ êµ¬ì¡°ì™€ ë™ì‘ ì›ë¦¬CNNì˜ ê¸°ë³¸ êµ¬ì„±ìš”ì†Œì™€ ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ë™ì‘í•˜ëŠ”ì§€ ë°°ì›Œë´…ì‹œë‹¤1) CNNì˜ ê¸°ë³¸ êµ¬ì¡° CNNì˜ ê¸°ë³¸ êµ¬ì¡°í•©ì„±ê³± ì‹ ê²½ë§(Convolutional Neural Network, CNN)ì€ ì´ë¯¸ì§€ì™€ ê°™ì€ 2ì°¨ì› ë°ì´í„°ì˜ íŠ¹ì§•ì„ íš¨ê³¼ì ìœ¼ë¡œ ì¶”ì¶œí•˜ê¸° ìœ„í•´ ì„¤ê³„ëœ ì‹ ê²½ë§ì…ë‹ˆë‹¤.CNNì€ ì£¼ë¡œ í•©ì„±ê³± ì¸µ(Convolutional Layer), í’€ë§ ì¸µ(Pooling Layer), ì™„ì „ ì—°ê²° ì¸µ(Fully Connected Layer)ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.ALTí•©ì„±ê³± ì¸µ (Convolutional Layer):
 - ì…ë ¥ ì´ë¯¸ì§€ì— í•„í„°(ì»¤ë„)ë¥¼ ì ìš©í•˜ì—¬ íŠ¹ì§• ë§µ(feature map)ì„ ìƒì„±í•©ë‹ˆë‹¤. - ì…ë ¥ ì´ë¯¸ì§€ì— í•„í„°(ì»¤ë„)ë¥¼ ì ìš©í•˜ì—¬ íŠ¹ì§• ë§µ(feature map)ì„ ìƒì„±í•©ë‹ˆë‹¤.ï»¿
 - í•„í„°ëŠ” ì´ë¯¸ì§€ì˜ êµ­ì†Œì ì¸ íŒ¨í„´ì„ í•™ìŠµí•©ë‹ˆë‹¤. - í•„í„°ëŠ” ì´ë¯¸ì§€ì˜ êµ­ì†Œì ì¸ íŒ¨í„´ì„ í•™ìŠµí•©ë‹ˆë‹¤.ï»¿
í’€ë§ ì¸µ (Pooling Layer)
 - íŠ¹ì§• ë§µì˜ í¬ê¸°ë¥¼ ì¤„ì´ê³ , ì¤‘ìš”í•œ íŠ¹ì§•ì„ ì¶”ì¶œí•©ë‹ˆë‹¤. - íŠ¹ì§• ë§µì˜ í¬ê¸°ë¥¼ ì¤„ì´ê³ , ì¤‘ìš”í•œ íŠ¹ì§•ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.ï»¿
 - ì£¼ë¡œ Max Poolingê³¼ Average Poolingì´ ì‚¬ìš©ë©ë‹ˆë‹¤. - ì£¼ë¡œ Max Poolingê³¼ Average Poolingì´ ì‚¬ìš©ë©ë‹ˆë‹¤.ï»¿
ì™„ì „ ì—°ê²° ì¸µ (Fully Connected Layer)
 - ì¶”ì¶œëœ íŠ¹ì§•ì„ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. - ì¶”ì¶œëœ íŠ¹ì§•ì„ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.ï»¿
 - CNNì´ë¼ëŠ” ë¶„ì„ë ˆì´ì–´ë¥¼ í†µí•´ ì¶”ì¶œí•œ íŠ¹ì„±ì„ ë°”íƒ•ìœ¼ë¡œ ê²°ë¡ ì„ ë‚´ë¦¬ëŠ” ë¶€ë¶„ - CNNì´ë¼ëŠ” ë¶„ì„ë ˆì´ì–´ë¥¼ í†µí•´ ì¶”ì¶œí•œ íŠ¹ì„±ì„ ë°”íƒ•ìœ¼ë¡œ ê²°ë¡ ì„ ë‚´ë¦¬ëŠ” ë¶€ë¶„ï»¿â€‹2) í•©ì„±ê³± ì—°ì‚°ê³¼ í•„í„° í•©ì„±ê³± ì—°ì‚°ì˜ ì›ë¦¬ì™€ í•„í„°ì˜ ì—­í• í•©ì„±ê³± ì—°ì‚°ì€ ì…ë ¥ ì´ë¯¸ì§€ì— í•„í„°(ì»¤ë„)ë¥¼ ì ìš©í•˜ì—¬ íŠ¹ì§• ë§µì„ ìƒì„±í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. í•„í„°ëŠ” ì‘ì€ í¬ê¸°ì˜ í–‰ë ¬ë¡œ, ì´ë¯¸ì§€ì˜ êµ­ì†Œì ì¸ íŒ¨í„´ì„ í•™ìŠµí•©ë‹ˆë‹¤.í•©ì„±ê³± ì—°ì‚°:í•„í„°ë¥¼ ì´ë¯¸ì§€ì˜ ê° ìœ„ì¹˜ì— ìŠ¬ë¼ì´ë”©í•˜ë©°, í•„í„°ì™€ ì´ë¯¸ì§€ì˜ í•´ë‹¹ ë¶€ë¶„ ê°„ì˜ ì ê³±(dot product)ì„ ê³„ì‚°í•©ë‹ˆë‹¤.ê³„ì‚°ëœ ê°’ì€ íŠ¹ì§• ë§µì˜ í•´ë‹¹ ìœ„ì¹˜ì— ì €ì¥ë©ë‹ˆë‹¤.í•„í„°ì˜ ì—­í• :í•„í„°ëŠ” ì´ë¯¸ì§€ì˜ ì—ì§€(edge), ì½”ë„ˆ(corner), í…ìŠ¤ì²˜(texture) ë“± ë‹¤ì–‘í•œ êµ­ì†Œì ì¸ íŒ¨í„´ì„ í•™ìŠµí•©ë‹ˆë‹¤.ì—¬ëŸ¬ ê°œì˜ í•„í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ì–‘í•œ íŠ¹ì§• ë§µì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.3) í’€ë§ ë ˆì´ì–´, í”Œë˜íŠ¼ í’€ë§ ë ˆì´ì–´ì˜ í•„ìš”ì„±ê³¼ ì¢…ë¥˜í’€ë§ ì¸µì€ íŠ¹ì§• ë§µì˜ í¬ê¸°ë¥¼ ì¤„ì´ê³ , ì¤‘ìš”í•œ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. í’€ë§ ì¸µì€ ì£¼ë¡œ Max Poolingê³¼ Average Poolingì´ ì‚¬ìš©ë©ë‹ˆë‹¤.Max Pooling:í•„í„° í¬ê¸° ë‚´ì—ì„œ ìµœëŒ€ ê°’ì„ ì„ íƒí•©ë‹ˆë‹¤.ì¤‘ìš”í•œ íŠ¹ì§•ì„ ê°•ì¡°í•˜ê³ , ë¶ˆí•„ìš”í•œ ì •ë³´ë¥¼ ì œê±°í•©ë‹ˆë‹¤.Average Pooling:í•„í„° í¬ê¸° ë‚´ì—ì„œ í‰ê·  ê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤.íŠ¹ì§• ë§µì˜ í¬ê¸°ë¥¼ ì¤„ì´ë©´ì„œ, ì •ë³´ì˜ ì†ì‹¤ì„ ìµœì†Œí™”í•©ë‹ˆë‹¤. í”Œë˜íŠ¼ ë ˆì´ì–´ì˜ ì—­í• í”Œë˜íŠ¼ ì¸µ(Flatten Layer)ì€ 2ì°¨ì› íŠ¹ì§• ë§µì„ 1ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. ì´ëŠ” ì™„ì „ ì—°ê²° ì¸µì— ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ í•„ìš”í•©ë‹ˆë‹¤.4) CNN êµ¬ì¡°ì™€ ì‘ìš© ë‹¤ì–‘í•œ CNN ì•„í‚¤í…ì²˜LeNet:ìµœì´ˆì˜ CNN ì•„í‚¤í…ì²˜ ì¤‘ í•˜ë‚˜ë¡œ, ì†ê¸€ì”¨ ìˆ«ì ì¸ì‹ì— ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.í•©ì„±ê³± ì¸µê³¼ í’€ë§ ì¸µì„ ë°˜ë³µí•œ í›„, ì™„ì „ ì—°ê²° ì¸µì„ ì‚¬ìš©í•©ë‹ˆë‹¤.AlexNet:2012ë…„ ì´ë¯¸ì§€ë„· ëŒ€íšŒì—ì„œ ìš°ìŠ¹í•œ ì•„í‚¤í…ì²˜ë¡œ, ë”¥ëŸ¬ë‹ì˜ ê°€ëŠ¥ì„±ì„ ì…ì¦í–ˆìŠµë‹ˆë‹¤.ReLU í™œì„±í™” í•¨ìˆ˜ì™€ ë“œë¡­ì•„ì›ƒ(dropout)ì„ ë„ì…í•˜ì—¬ ì„±ëŠ¥ì„ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤.VGG:ê¹Šê³  ê·œì¹™ì ì¸ êµ¬ì¡°ë¥¼ ê°€ì§„ ì•„í‚¤í…ì²˜ë¡œ, ì‘ì€ 3x3 í•„í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ê¹Šì´ë¥¼ ì¦ê°€ì‹œì¼°ìŠµë‹ˆë‹¤.VGG16ê³¼ VGG19ê°€ ëŒ€í‘œì ì¸ ëª¨ë¸ì…ë‹ˆë‹¤.02. ì‹¤ìŠµ: CNNì„ ì´ìš©í•œ ì´ë¯¸ì§€ ë¶„ë¥˜ (PyTorch) ì´ì œ PyTorchë¥¼ ì‚¬ìš©í•˜ì—¬ ê°„ë‹¨í•œ CNN ëª¨ë¸ì„ êµ¬ì¶•í•˜ê³ , CIFAR-10 ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•´ë³´ê² ìŠµë‹ˆë‹¤1)  ê°„ë‹¨í•œ CNN ëª¨ë¸ì„ ì´ìš©í•œ ì´ë¯¸ì§€ ë¶„ë¥˜ ì‹¤ìŠµ PyTorch ë° í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸PyTorch ë° í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ {5px}PyTorch ë° í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ï»¿â€‹Pythonë³µì‚¬import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
â€‹ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬ {5px}ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬ ï»¿â€‹Pythonë³µì‚¬# ë°ì´í„°ì…‹ ì „ì²˜ë¦¬
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])
# CIFAR-10 ë°ì´í„°ì…‹ ë¡œë“œ
trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)

testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)
â€‹ ê°„ë‹¨í•œ CNN ëª¨ë¸ ì •ì˜ê°„ë‹¨í•œ CNN ëª¨ë¸ ì •ì˜ {5px}ê°„ë‹¨í•œ CNN ëª¨ë¸ ì •ì˜ ï»¿â€‹Pythonë³µì‚¬class SimpleCNN(nn.Module):
def __init__(self):
super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1) # ì…ë ¥ ì±„ë„ 3, ì¶œë ¥ ì±„ë„ 32, ì»¤ë„ í¬ê¸° 3x3
        self.pool = nn.MaxPool2d(2, 2) # í’€ë§ í¬ê¸° 2x2
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1) # ì…ë ¥ ì±„ë„ 32, ì¶œë ¥ ì±„ë„ 64, ì»¤ë„ í¬ê¸° 3x3
        self.fc1 = nn.Linear(64 * 8 * 8, 512) # ì™„ì „ ì—°ê²° ì¸µ
        self.fc2 = nn.Linear(512, 10) # ì¶œë ¥ ì¸µ (10ê°œì˜ í´ë˜ìŠ¤)
def forward(self, x):
        x = self.pool(torch.relu(self.conv1(x)))
        x = self.pool(torch.relu(self.conv2(x)))
        x = x.view(-1, 64 * 8 * 8) # í”Œë˜íŠ¼
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
return x
â€‹nn.Conv2d: 2ì°¨ì› í•©ì„±ê³± ì¸µì„ ì •ì˜í•©ë‹ˆë‹¤. nn.Conv2d(in_channels, out_channels, kernel_size, padding)ì€ ì…ë ¥ ì±„ë„ ìˆ˜, ì¶œë ¥ ì±„ë„ ìˆ˜, ì»¤ë„ í¬ê¸°, íŒ¨ë”©ì„ ì§€ì •.Conv2d(in_channels, out_channels, kernel_size, padding)ì€ ì…ë ¥ ì±„ë„ ìˆ˜, ì¶œë ¥ ì±„ë„ ìˆ˜, ì»¤ë„ í¬ê¸°, íŒ¨ë”©ì„ ì§€ì •ï»¿â€‹nn.MaxPool2d: 2ì°¨ì› ìµœëŒ€ í’€ë§ ì¸µì„ ì •ì˜í•©ë‹ˆë‹¤.nn.MaxPool2d(kernel_size, stride)ì€ í’€ë§ í¬ê¸°ì™€ ìŠ¤íŠ¸ë¼ì´ë“œë¥¼ ì§€ì •í•©ë‹ˆë‹¤..MaxPool2d(kernel_size, stride)ì€ í’€ë§ í¬ê¸°ì™€ ìŠ¤íŠ¸ë¼ì´ë“œë¥¼ ì§€ì •í•©ë‹ˆë‹¤.ï»¿â€‹view: í…ì„œì˜ í¬ê¸°ë¥¼ ë³€ê²½í•©ë‹ˆë‹¤.x.view(-1, 64 * 8 * 8)ì€ íŠ¹ì§• ë§µì„ 1ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤..view(-1, 64 * 8 * 8)ì€ íŠ¹ì§• ë§µì„ 1ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.ï»¿â€‹ ëª¨ë¸ í•™ìŠµëª¨ë¸ í•™ìŠµ {5px}ëª¨ë¸ í•™ìŠµ ï»¿â€‹Pythonë³µì‚¬# ëª¨ë¸ ì´ˆê¸°í™”
model = SimpleCNN()
# ì†ì‹¤ í•¨ìˆ˜ì™€ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ ì •ì˜
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
# ëª¨ë¸ í•™ìŠµ
for epoch in range(10): # 10 ì—í¬í¬ ë™ì•ˆ í•™ìŠµ
    running_loss = 0.0
for i, data in enumerate(trainloader, 0):
        inputs, labels = data

        # ê¸°ìš¸ê¸° ì´ˆê¸°í™”
        optimizer.zero_grad()
# ìˆœì „íŒŒ + ì—­ì „íŒŒ + ìµœì í™”
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
# ì†ì‹¤ ì¶œë ¥
        running_loss += loss.item()
if i % 100 == 99: # ë§¤ 100 ë¯¸ë‹ˆë°°ì¹˜ë§ˆë‹¤ ì¶œë ¥
print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 100:.3f}')
            running_loss = 0.0
print('Finished Training')
â€‹nn.CrossEntropyLoss: ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ ë¬¸ì œì—ì„œ ì£¼ë¡œ ì‚¬ìš©ë˜ëŠ” ì†ì‹¤ í•¨ìˆ˜ì…ë‹ˆë‹¤. ì˜ˆì¸¡ ê°’ê³¼ ì‹¤ì œ ê°’ ì‚¬ì´ì˜ êµì°¨ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤ì„ ê³„ì‚°í•©ë‹ˆë‹¤.optim.SGD: í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²•(Stochastic Gradient Descent) ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì„ ì •ì˜í•©ë‹ˆë‹¤.  lrì€ í•™ìŠµë¥ , momentumì€ ëª¨ë©˜í…€ ê°’ì„ ì§€ì •í•©ë‹ˆë‹¤.ì€ í•™ìŠµë¥ , momentumì€ ëª¨ë©˜í…€ ê°’ì„ ì§€ì •í•©ë‹ˆë‹¤.ï»¿â€‹optimizer.zero_grad(): ì´ì „ ë‹¨ê³„ì—ì„œ ê³„ì‚°ëœ ê¸°ìš¸ê¸°ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.loss.backward(): ì—­ì „íŒŒë¥¼ í†µí•´ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.optimizer.step(): ê³„ì‚°ëœ ê¸°ìš¸ê¸°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. ëª¨ë¸  í‰ê°€ëª¨ë¸ í‰ê°€ {5px}ëª¨ë¸ í‰ê°€ ï»¿â€‹Pythonë³µì‚¬correct = 0
total = 0
with torch.no_grad():
for data in testloader:
        images, labels = data
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')

â€‹torch.no_grad(): í‰ê°€ ë‹¨ê³„ì—ì„œëŠ” ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•  í•„ìš”ê°€ ì—†ìœ¼ë¯€ë¡œ, ì´ë¥¼ ë¹„í™œì„±í™”í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ì„ ì¤„ì…ë‹ˆë‹¤.torch.max: í…ì„œì˜ ìµœëŒ€ ê°’ì„ ì°¾ìŠµë‹ˆë‹¤. torch.max(outputs.data, 1)ì€ ê° ìƒ˜í”Œì— ëŒ€í•´ ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ í´ë˜ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤..max(outputs.data, 1)ì€ ê° ìƒ˜í”Œì— ëŒ€í•´ ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ í´ë˜ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.ï»¿â€‹labels.size(0): ë°°ì¹˜ í¬ê¸°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.(predicted == labels).sum().item(): ì˜ˆì¸¡ ê°’ê³¼ ì‹¤ì œ ê°’ì´ ì¼ì¹˜í•˜ëŠ” ìƒ˜í”Œì˜ ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤..
Python LangChainê³¼ FAISS LLM & RAGë¥¼ í™œìš©í•œ AI ì„œë¹„ìŠ¤ ë§Œë“¤ê¸°/ LLM & RAGë¥¼ í™œìš©í•œ AI ì„œë¹„ìŠ¤ ë§Œë“¤ê¸° - 5ì£¼ì°¨/Python LangChainê³¼ FAISSì œì‘:Python LangChainê³¼ FAISSìˆ˜ì—… ëª©í‘œLangChain ì‚¬ìš©ì„ ìœ„í•œ í™˜ê²½ ì„¤ì •ì„ ì§„í–‰í•©ë‹ˆë‹¤.FAISSë¥¼ í™œìš©í•œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ë¥¼ êµ¬ì„±í•˜ëŠ” ì‹¤ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤.ëª©ì°¨ì„¤ì¹˜ ë° ê¸°ë³¸ ì„¤ì •LangChain ê¸°ë³¸ ê°œë…ì–¸ì–´ ëª¨ë¸ ì´ˆê¸°í™”í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‚¬ìš©í•˜ê¸°LangChain Expression Language (LCEL)ë¡œ ì²´ì¸ ì—°ê²°FAISSë¥¼ í™œìš©í•œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì„± ë° ì¿¼ë¦¬Step 1: OpenAI ì„ë² ë”© ëª¨ë¸ë¡œ ë²¡í„° ì„ë² ë”© ìƒì„±Step 2: FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™”Step 3: ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ë¬¸ì„œ ì¶”ê°€Step 4: ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬RAG ì²´ì¸ì— FAISS í†µí•©Step 1: Retrieverë¡œ ë³€í™˜Step 2: RAG ì²´ì¸ ìƒì„±FAISS ì¸ë±ìŠ¤ì˜ ì €ì¥ ë° ë¡œë“œFAISS ë°ì´í„°ë² ì´ìŠ¤ ë³‘í•©ì´ë²ˆì—ëŠ” LangChainê³¼ FAISSë¥¼ ì´ìš©í•œ ì‹¤ìŠµì„ ì§„í–‰í•  ì˜ˆì •ì¸ë°ìš”,
ì‹¤ìŠµì€ ì•„ë˜ì™€ ê°™ì€ ìˆœì„œë¡œ ì§„í–‰ë©ë‹ˆë‹¤!

ì„¤ì¹˜ ë° ê¸°ë³¸ ì„¤ì •LangChain ê¸°ë³¸ ê°œë… - ì–¸ì–´ ëª¨ë¸, í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿LangChain Expression Language (LCEL)ë¡œ ì²´ì¸ ì—°ê²°FAISSë¥¼ í™œìš©í•œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì„± ë° ì¿¼ë¦¬RAG ì²´ì¸ì— FAISS í†µí•©FAISS ì¸ë±ìŠ¤ì˜ ì €ì¥ ë° ë¡œë“œ, ë³‘í•©ì„¤ì¹˜ ë° ê¸°ë³¸ ì„¤ì •LangChain, OpenAI, ê·¸ë¦¬ê³  FAISS íŒ¨í‚¤ì§€ë¥¼ ì„¤ì¹˜í•©ë‹ˆë‹¤. GPUë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš° faiss-gpuë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Shellë³µì‚¬pip install langchain langchain-openai faiss-cpu

â€‹
ì„¤ì¹˜ í›„, OpenAI API í‚¤ë¥¼ ì„¤ì •í•´ ì‚¬ìš© í™˜ê²½ì„ ì¤€ë¹„í•©ë‹ˆë‹¤.Pythonë³µì‚¬import os
from getpass import getpass

os.environ["OPENAI_API_KEY"] = getpass("OpenAI API key ì…ë ¥: ")

â€‹LangChain ê¸°ë³¸ ê°œë…ì–¸ì–´ ëª¨ë¸ ì´ˆê¸°í™”OpenAIì˜ GPT-4 ëª¨ë¸ì„ LangChainì„ í†µí•´ ì‚¬ìš©í•´ ë´…ë‹ˆë‹¤. ChatOpenAIë¥¼ ì´ìš©í•´ ì´ˆê¸°í™”í•˜ê³  invoke ë©”ì„œë“œë¥¼ í†µí•´ ë©”ì‹œì§€ë¥¼ ì „ë‹¬í•˜ì—¬ ì‘ë‹µì„ ë°›ì•„ì˜µë‹ˆë‹¤.Pythonë³µì‚¬from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

# ëª¨ë¸ ì´ˆê¸°í™”
model = ChatOpenAI(model="gpt-4")
# ëª¨ë¸ì— ë©”ì‹œì§€ ì „ë‹¬
response = model.invoke([HumanMessage(content="ì•ˆë…•í•˜ì„¸ìš”, ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")])
print(response.content)

â€‹í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì‚¬ìš©í•˜ê¸°í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì€ ë‹¤ì–‘í•œ ì…ë ¥ì„ ë°›ì•„ ë©”ì‹œì§€ë¥¼ ìƒì„±í•˜ëŠ”ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì˜ì–´ ë¬¸ì¥ì„ ë‹¤ë¥¸ ì–¸ì–´ë¡œ ë²ˆì—­í•˜ëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ ì •ì˜í•´ ë´…ì‹œë‹¤.Pythonë³µì‚¬from langchain_core.prompts import ChatPromptTemplate

# ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì„¤ì •
system_template = "Translate the following sentence from English to {language}:"
# ì‚¬ìš©ì í…ìŠ¤íŠ¸ ì…ë ¥
prompt_template = ChatPromptTemplate.from_messages([
("system", system_template),
("user", "{text}")
])
# í”„ë¡¬í”„íŠ¸ ìƒì„±
result = prompt_template.invoke({"language": "French", "text": "How are you?"})
print(result.to_messages())

â€‹LangChain Expression Language (LCEL)ë¡œ ì²´ì¸ ì—°ê²°ì—¬ëŸ¬ ì»´í¬ë„ŒíŠ¸ë¥¼ ì²´ì¸ìœ¼ë¡œ ì—°ê²°í•˜ì—¬ ë°ì´í„° íë¦„ì„ í†µì œí•˜ëŠ” LCELì„ ì‚¬ìš©í•©ë‹ˆë‹¤.Pythonë³µì‚¬from langchain_core.output_parsers import StrOutputParser

# ì‘ë‹µì„ íŒŒì‹±í•˜ëŠ” íŒŒì„œ ì´ˆê¸°í™”
parser = StrOutputParser()
# í…œí”Œë¦¿, ëª¨ë¸, íŒŒì„œë¥¼ ì²´ì¸ìœ¼ë¡œ ì—°ê²°
chain = prompt_template | model | parser

# ì²´ì¸ ì‹¤í–‰
response = chain.invoke({"language": "Spanish", "text": "Where is the library?"})
print(response)

â€‹FAISSë¥¼ í™œìš©í•œ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì„± ë° ì¿¼ë¦¬FAISSëŠ” ë²¡í„° ìœ ì‚¬ì„± ê²€ìƒ‰ì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤. OpenAIEmbeddingsë¡œ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•´ FAISS ì¸ë±ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤.Step 1: OpenAI ì„ë² ë”© ëª¨ë¸ë¡œ ë²¡í„° ì„ë² ë”© ìƒì„±Pythonë³µì‚¬from langchain_openai import OpenAIEmbeddings

# OpenAI ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
embeddings = OpenAIEmbeddings(model="text-embedding-ada-002")

â€‹Step 2: FAISS ì¸ë±ìŠ¤ ì´ˆê¸°í™”Pythonë³µì‚¬import faiss
from langchain_community.vectorstores import FAISS
from langchain_community.docstore.in_memory import InMemoryDocstore

# FAISS ì¸ë±ìŠ¤ ìƒì„±
index = faiss.IndexFlatL2(len(embeddings.embed_query("hello world")))
vector_store = FAISS(
    embedding_function=embeddings,
    index=index,
    docstore=InMemoryDocstore(),
    index_to_docstore_id={}
)

â€‹Step 3: ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ë¬¸ì„œ ì¶”ê°€Pythonë³µì‚¬from langchain_core.documents import Document
from uuid import uuid4

# ë¬¸ì„œ ìƒì„±
documents = [
    Document(page_content="LangChainì„ ì‚¬ìš©í•´ í”„ë¡œì íŠ¸ë¥¼ êµ¬ì¶•í•˜ê³  ìˆìŠµë‹ˆë‹¤!", metadata={"source": "tweet"}),
    Document(page_content="ë‚´ì¼ ë‚ ì”¨ëŠ” ë§‘ê³  ë”°ëœ»í•  ì˜ˆì •ì…ë‹ˆë‹¤.", metadata={"source": "news"}),
    Document(page_content="ì˜¤ëŠ˜ ì•„ì¹¨ì—ëŠ” íŒ¬ì¼€ì´í¬ì™€ ê³„ë€ì„ ë¨¹ì—ˆì–´ìš”.", metadata={"source": "personal"}),
    Document(page_content="ì£¼ì‹ ì‹œì¥ì´ ê²½ê¸° ì¹¨ì²´ ìš°ë ¤ë¡œ í•˜ë½ ì¤‘ì…ë‹ˆë‹¤.", metadata={"source": "news"}),
]
# ê³ ìœ  ID ìƒì„± ë° ë¬¸ì„œ ì¶”ê°€
uuids = [str(uuid4()) for _ in range(len(documents))]
vector_store.add_documents(documents=documents, ids=uuids)

â€‹Step 4: ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ìœ ì‚¬ì„± ê²€ìƒ‰ì„ í†µí•´ íŠ¹ì • ì¿¼ë¦¬ì™€ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•´ë³´ê² ìŠµë‹ˆë‹¤.Pythonë³µì‚¬# ê¸°ë³¸ ìœ ì‚¬ì„± ê²€ìƒ‰
results = vector_store.similarity_search("ë‚´ì¼ ë‚ ì”¨ëŠ” ì–´ë–¨ê¹Œìš”?", k=2, filter={"source": "news"})
for res in results:
print(f"* {res.page_content} [{res.metadata}]")
# ì ìˆ˜ì™€ í•¨ê»˜ ìœ ì‚¬ì„± ê²€ìƒ‰
results_with_scores = vector_store.similarity_search_with_score("LangChainì— ëŒ€í•´ ì´ì•¼ê¸°í•´ì£¼ì„¸ìš”.", k=2, filter={"source": "tweet"})
for res, score in results_with_scores:
print(f"* [SIM={score:.3f}] {res.page_content} [{res.metadata}]")

â€‹RAG ì²´ì¸ì— FAISS í†µí•©RAG (Retrieval-Augmented Generation) ì²´ì¸ì„ êµ¬ì„±í•˜ì—¬ ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ì‘ë‹µí•  ìˆ˜ ìˆë„ë¡ êµ¬ì„±í•©ë‹ˆë‹¤.Step 1: Retrieverë¡œ ë³€í™˜FAISSë¥¼ retrieverë¡œ ë³€í™˜í•´ RAG ì²´ì¸ì—ì„œ ì‚¬ìš©í•©ë‹ˆë‹¤.Pythonë³µì‚¬retriever = vector_store.as_retriever(search_type="similarity", search_kwargs={"k": 1})

â€‹Step 2: RAG ì²´ì¸ ìƒì„±LangChainì˜ ëª¨ë¸ê³¼ í”„ë¡¬í”„íŠ¸ë¥¼ ì—°ê²°í•˜ì—¬ RAG ì²´ì¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤.Pythonë³µì‚¬
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
contextual_prompt = ChatPromptTemplate.from_messages([
("system", "Answer the question using only the following context."),
("user", "Context: {context}\\: {question}")
])
class DebugPassThrough(RunnablePassthrough):
def invoke(self, *args, **kwargs):
        output = super().invoke(*args, **kwargs)
print("Debug Output:", output)
return output
# ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” ë‹¨ê³„ ì¶”ê°€
class ContextToText(RunnablePassthrough):
def invoke(self, inputs, config=None, **kwargs): # config ì¸ìˆ˜ ì¶”ê°€
# contextì˜ ê° ë¬¸ì„œë¥¼ ë¬¸ìì—´ë¡œ ê²°í•©
        context_text = "".join([doc.page_content for doc in inputs["context"]])
return {"context": context_text, "question": inputs["question"]}
# RAG ì²´ì¸ì—ì„œ ê° ë‹¨ê³„ë§ˆë‹¤ DebugPassThrough ì¶”ê°€
rag_chain_debug = {
"context": retriever, # ì»¨í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜¤ëŠ” retriever
"question": DebugPassThrough() # ì‚¬ìš©ì ì§ˆë¬¸ì´ ê·¸ëŒ€ë¡œ ì „ë‹¬ë˜ëŠ”ì§€ í™•ì¸í•˜ëŠ” passthrough
} | DebugPassThrough() | ContextToText()|   contextual_prompt | model

# ì§ˆë¬¸ ì‹¤í–‰ ë° ê° ë‹¨ê³„ ì¶œë ¥ í™•ì¸
response = rag_chain_debug.invoke("ê°•ì‚¬ì´ë¦„ì€?")
print("Final Response:")
print(response.content)
â€‹FAISS ì¸ë±ìŠ¤ì˜ ì €ì¥ ë° ë¡œë“œFAISS ì¸ë±ìŠ¤ë¥¼ ì €ì¥í•´ ë‹¤ì‹œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Pythonë³µì‚¬# ì¸ë±ìŠ¤ ì €ì¥
vector_store.save_local("faiss_index")
# ì €ì¥ëœ ì¸ë±ìŠ¤ ë¡œë“œ
new_vector_store = FAISS.load_local("faiss_index", embeddings)

â€‹FAISS ë°ì´í„°ë² ì´ìŠ¤ ë³‘í•©ë‘ ê°œì˜ FAISS ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë³‘í•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Pythonë³µì‚¬db1 = FAISS.from_texts(["ë¬¸ì„œ 1 ë‚´ìš©"], embeddings)
db2 = FAISS.from_texts(["ë¬¸ì„œ 2 ë‚´ìš©"], embeddings)
# ë³‘í•©
db1.merge_from(db2)

â€‹
íŒë‹¤ìŠ¤ ì‚¬ìš©ì„ ìœ„í•´ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ê¸°ì™€ ì €ì¥í•˜ê¸°ì¸ê³µì§€ëŠ¥ì„ ìœ„í•œ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬/ì¸ê³µì§€ëŠ¥ì„ ìœ„í•œ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ - 3ì£¼ì°¨/íŒë‹¤ìŠ¤ ì‚¬ìš©ì„ ìœ„í•´ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ê¸°ì™€ ì €ì¥í•˜ê¸°ì œì‘:íŒë‹¤ìŠ¤ ì‚¬ìš©ì„ ìœ„í•´ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ê¸°ì™€ ì €ì¥í•˜ê¸°ìˆ˜ì—… ëª©í‘œíŒë‹¤ìŠ¤ë¥¼ ì´ìš©í•œ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° ë° ì €ì¥ ë°©ë²•ì— ëŒ€í•´ì„œ í•™ìŠµí•©ë‹ˆë‹¤.ëª©ì°¨CSV, Excel, JSON ë“± ë‹¤ì–‘í•œ í˜•ì‹ì—ì„œ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°CSV íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°Excel íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°JSON íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°CSV, Excel, JSON, SQL ë“± ë‹¤ì–‘í•œ í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ì €ì¥í•˜ê¸°CSV íŒŒì¼ë¡œ ì €ì¥í•˜ê¸°Excel íŒŒì¼ë¡œ ì €ì¥í•˜ê¸°JSON íŒŒì¼ë¡œ ì €ì¥í•˜ê¸° SQL ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ê¸°ìš”ì•½CSV, Excel, JSON ë“± ë‹¤ì–‘í•œ í˜•ì‹ì—ì„œ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°CSV íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°CSV íŒŒì¼ì€ ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ëŠ” ë°ì´í„° í˜•ì‹ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. íŒë‹¤ìŠ¤ì—ì„œëŠ” read_csv í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ ì‰½ê²Œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆì–´ìš”.Pythonë³µì‚¬import pandas as pd

# CSV íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
df_csv = pd.read_csv('data.csv')
print(df_csv.head())

â€‹ ì£¼ìš” ì˜µì…˜sep : êµ¬ë¶„ìë¥¼ ì§€ì •í•©ë‹ˆë‹¤. ê¸°ë³¸ê°’ì€ ,ì…ë‹ˆë‹¤.header : í—¤ë” í–‰ì„ ì§€ì •í•©ë‹ˆë‹¤. ê¸°ë³¸ê°’ì€ ì²« ë²ˆì§¸ í–‰(0)ì…ë‹ˆë‹¤.ì˜ˆì‹œ: ë§Œì•½ ë°ì´í„°ê°€ íƒ­ìœ¼ë¡œ êµ¬ë¶„ë˜ì–´ ìˆë‹¤ë©´?Pythonë³µì‚¬df_csv = pd.read_csv('data.csv', sep='\')

â€‹URLì—ì„œ CSV íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°: ì¸í„°ë„·ì— ìˆëŠ” CSV íŒŒì¼ì„ ë°”ë¡œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.Pythonë³µì‚¬url = '<https://example.com/data.csv>'
df_csv_url = pd.read_csv(url)
print(df_csv_url.head())
â€‹Excel íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°Excel íŒŒì¼ë„ íŒë‹¤ìŠ¤ë¡œ ì‰½ê²Œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. read_excel í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.Pythonë³µì‚¬# Excel íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
df_excel = pd.read_excel('data.xlsx')
print(df_excel.head())

â€‹ ì£¼ìš” ì˜µì…˜sheet_name : ì½ê³ ì í•˜ëŠ” ì‹œíŠ¸ ì´ë¦„ ë˜ëŠ” ë²ˆí˜¸ë¥¼ ì§€ì •í•©ë‹ˆë‹¤. ê¸°ë³¸ê°’ì€ ì²« ë²ˆì§¸ ì‹œíŠ¸(0)ì…ë‹ˆë‹¤.header : í—¤ë” í–‰ì„ ì§€ì •í•©ë‹ˆë‹¤. ê¸°ë³¸ê°’ì€ ì²« ë²ˆì§¸ í–‰(0)ì…ë‹ˆë‹¤.ì˜ˆì‹œ: íŠ¹ì • ì‹œíŠ¸ì˜ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ê³  ì‹¶ë‹¤ë©´?Pythonë³µì‚¬df_excel = pd.read_excel('data.xlsx', sheet_name='Sheet2')

â€‹URLì—ì„œ Excel íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°: URLì„ í†µí•´ Excel íŒŒì¼ë„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.Pythonë³µì‚¬url = '<https://example.com/data.xlsx>'
df_excel_url = pd.read_excel(url)
print(df_excel_url.head())

â€‹JSON íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°JSON íŒŒì¼ì€ ì›¹ì—ì„œ ìì£¼ ì‚¬ìš©í•˜ëŠ” ë°ì´í„° í˜•ì‹ì…ë‹ˆë‹¤. íŒë‹¤ìŠ¤ì—ì„œëŠ” read_json í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.Pythonë³µì‚¬# JSON íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
df_json = pd.read_json('data.json')
print(df_json.head())

â€‹ ì£¼ìš” ì˜µì…˜orient : JSON ë°ì´í„°ì˜ í˜•ì‹ì„ ì§€ì •í•©ë‹ˆë‹¤. ê¸°ë³¸ê°’ì€ columnsì…ë‹ˆë‹¤.URLì—ì„œ JSON íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°: ì›¹ì—ì„œ JSON ë°ì´í„°ë¥¼ ì§ì ‘ ë¶ˆëŸ¬ì˜¬ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.Pythonë³µì‚¬url = '<https://example.com/data.json>'
df_json_url = pd.read_json(url)
print(df_json_url.head())

â€‹ìš”ì•½íŒë‹¤ìŠ¤ëŠ” CSV, Excel, JSON ë“± ë‹¤ì–‘í•œ í˜•ì‹ì˜ ë°ì´í„°ë¥¼ ì‰½ê²Œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.URLì„ í†µí•´ ì›¹ì—ì„œ ì§ì ‘ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ê²ƒë„ ê°€ëŠ¥í•˜ë©°, ì´ ê¸°ëŠ¥ì„ í†µí•´ ë°ì´í„° ìˆ˜ì§‘ì´ ë§¤ìš° ê°„í¸í•´ì§‘ë‹ˆë‹¤.ë‹¤ì–‘í•œ íŒŒì¼ í˜•ì‹ì˜ ë°ì´í„° ë¡œë“œë¥¼ ìµíˆë©´, ë‹¤ì–‘í•œ ì†ŒìŠ¤ì˜ ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬ ë°ì´í„° ë¶„ì„ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ì´ì œ ë‹¤ì–‘í•œ ë°ì´í„° ì†ŒìŠ¤ë¥¼ ììœ ë¡­ê²Œ ë¶ˆëŸ¬ì™€ì„œ ë¶„ì„ì„ ì‹œì‘í•´ë³´ì„¸ìš”! â€‹CSV, Excel, JSON, SQL ë“± ë‹¤ì–‘í•œ í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ì €ì¥í•˜ê¸°CSV íŒŒì¼ë¡œ ì €ì¥í•˜ê¸°CSV íŒŒì¼ì€ ë°ì´í„°ë¥¼ ì €ì¥í•  ë•Œ ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” í˜•ì‹ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. íŒë‹¤ìŠ¤ì—ì„œëŠ” to_csv í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ ë°ì´í„°ë¥¼ ì‰½ê²Œ CSV íŒŒì¼ë¡œ ì €ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Pythonë³µì‚¬import pandas as pd

# ì˜ˆì‹œ ë°ì´í„°í”„ë ˆì„ ìƒì„±
df = pd.DataFrame({
'ì´ë¦„': ['ì² ìˆ˜', 'ì˜í¬', 'ë¯¼ìˆ˜'],
'ë‚˜ì´': [25, 30, 35],
'ì§ì—…': ['í•™ìƒ', 'íšŒì‚¬ì›', 'í”„ë¦¬ëœì„œ']
})
# CSV íŒŒì¼ë¡œ ì €ì¥í•˜ê¸°
df.to_csv('data.csv', index=False)

â€‹ ì£¼ìš” ì˜µì…˜index=False : ì¸ë±ìŠ¤ë¥¼ ì œì™¸í•˜ê³  ì €ì¥í•©ë‹ˆë‹¤.sep : êµ¬ë¶„ìë¥¼ ì§€ì •í•©ë‹ˆë‹¤. ê¸°ë³¸ê°’ì€ ,ì…ë‹ˆë‹¤.URLë¡œ ì €ì¥í•˜ê¸°: URL ê²½ë¡œê°€ ì•„ë‹Œ, ë¡œì»¬ íŒŒì¼ ì‹œìŠ¤í…œì— ì €ì¥í•˜ëŠ” ë°©ë²•ì´ë¯€ë¡œ, URLì„ ì§€ì •í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.Excel íŒŒì¼ë¡œ ì €ì¥í•˜ê¸°Excel íŒŒì¼ í˜•ì‹ìœ¼ë¡œ ì €ì¥í•˜ë ¤ë©´ to_excel í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.Pythonë³µì‚¬# Excel íŒŒì¼ë¡œ ì €ì¥í•˜ê¸°
df.to_excel('data.xlsx', index=False)

â€‹ ì£¼ìš” ì˜µì…˜sheet_name : ì €ì¥í•  ì‹œíŠ¸ ì´ë¦„ì„ ì§€ì •í•©ë‹ˆë‹¤. ê¸°ë³¸ê°’ì€ Sheet1ì…ë‹ˆë‹¤.index=False : ì¸ë±ìŠ¤ë¥¼ ì œì™¸í•˜ê³  ì €ì¥í•©ë‹ˆë‹¤.JSON íŒŒì¼ë¡œ ì €ì¥í•˜ê¸° JSON íŒŒì¼ í˜•ì‹ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì €ì¥í•˜ë ¤ë©´ to_json í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.Pythonë³µì‚¬# JSON íŒŒì¼ë¡œ ì €ì¥í•˜ê¸°
df.to_json('data.json')

â€‹ ì£¼ìš” ì˜µì…˜orient : JSON í˜•ì‹ì„ ì§€ì •í•©ë‹ˆë‹¤. ê¸°ë³¸ê°’ì€ columnsì…ë‹ˆë‹¤. records, index ë“± ë‹¤ì–‘í•œ í˜•ì‹ìœ¼ë¡œ ì €ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Pythonë³µì‚¬# ì˜ˆì‹œ: ë ˆì½”ë“œ í˜•ì‹ìœ¼ë¡œ ì €ì¥í•˜ê¸°
df.to_json('data_records.json', orient='records')

â€‹SQL ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ê¸°ë°ì´í„°ë¥¼ SQL ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ë ¤ë©´ to_sql í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ê³ , ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ í•„ìš”í•©ë‹ˆë‹¤.Pythonë³µì‚¬import sqlite3

# SQLite ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
conn = sqlite3.connect('database.db')
# ë°ì´í„°í”„ë ˆì„ì„ SQL í…Œì´ë¸”ë¡œ ì €ì¥í•˜ê¸°
df.to_sql('table_name', conn, if_exists='replace', index=False)
# ì—°ê²° ì¢…ë£Œ
conn.close()

â€‹ ì£¼ìš” ì˜µì…˜name : ì €ì¥í•  í…Œì´ë¸” ì´ë¦„ì„ ì§€ì •í•©ë‹ˆë‹¤.con : ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ê°ì²´ë¥¼ ì§€ì •í•©ë‹ˆë‹¤.if_exists : í…Œì´ë¸”ì´ ì´ë¯¸ ì¡´ì¬í•  ê²½ìš° ë™ì‘ì„ ì§€ì •í•©ë‹ˆë‹¤. replace, append, fail ì¤‘ ì„ íƒí•©ë‹ˆë‹¤.index=False : ì¸ë±ìŠ¤ë¥¼ ì œì™¸í•˜ê³  ì €ì¥í•©ë‹ˆë‹¤.ìš”ì•½ìš”ì•½CSV, Excel, JSON, SQL ë“± ë‹¤ì–‘í•œ í˜•ì‹ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì‰½ê²Œ ì €ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ê° ì €ì¥ í˜•ì‹ì— ë§ëŠ” ì˜µì…˜ì„ í™œìš©í•˜ë©´, ì›í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì €ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ë°ì´í„°ë¥¼ ì €ì¥í•˜ëŠ” ê³¼ì •ì€ ë¶„ì„ ê²°ê³¼ë¥¼ ê³µìœ í•˜ê±°ë‚˜, ë‚˜ì¤‘ì— ë‹¤ì‹œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ë° ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤.ì´ì œ í•„ìš”í•œ í˜•ì‹ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì €ì¥í•˜ê³ , ì–¸ì œë“ ì§€ ë‹¤ì‹œ ë¶ˆëŸ¬ì™€ì„œ í™œìš©í•´ë³´ì„¸ìš”! â€‹Son minsuck9ì›” 25ì¼ (í¸ì§‘ë¨)[í”¼ë“œë°±]
ì£¼ì„ìœ¼ë¡œ íƒ€ì´íƒ€ë‹‰ ë°ì´í„°ì…‹ URL ë§í¬ ì‘ì„±í•´ì£¼ë©´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤. 3-2 ê°•ì˜ì—ì„œ ë°‘ì— URLê°€ì§€ê³  ì‹¤ìŠµí•˜ê³  ìˆìŠµë‹ˆë‹¤.
(ë§í¬: https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv)â€¦ ë” ë³´ê¸°
[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 6ê°•. ë°ì´í„° ì „ì²˜ë¦¬[SCC] ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ - 2ì£¼ì°¨/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 6ê°•. ë°ì´í„° ì „ì²˜ë¦¬ì œì‘:[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 6ê°•. ë°ì´í„° ì „ì²˜ë¦¬[ìˆ˜ì—… ëª©í‘œ]ë°ì´í„° ì „ì²˜ë¦¬ ê°œë…ì„ ì•Œì•„ë´…ì‹œë‹¤ë°ì´í„° ì „ì²˜ë¦¬ ë°©ë²•ì„ ì•Œì•„ ë´…ì‹œë‹¤[ëª©ì°¨]01. ë°ì´í„° ì „ì²˜ë¦¬ ê°œë… ë° API ì†Œê°œ
 
 01. ë°ì´í„° ì „ì²˜ë¦¬ ê°œë… ë° API ì†Œê°œë°ì´í„° ì „ì²˜ë¦¬ê°€ ë¬´ì—‡ì¸ì§€ ì•Œì•„ë³´ê³ , ì‚¬ìš©ë˜ëŠ” API ë¥¼ ë°°ì›Œë´…ì‹œë‹¤1) ë°ì´í„° ì „ì²˜ë¦¬ ê°œë…  ë°ì´í„° ì „ì²˜ë¦¬(Data Cleaning)ë€?ë°ì´í„° ë¶„ì„ ë° ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë§ì„ ìœ„í•´ ë°ì´í„°ë¥¼ ì¤€ë¹„í•˜ëŠ” ê³¼ì •ì›ì‹œ ë°ì´í„°(raw data)ëŠ” ì¢…ì¢… ë¶ˆì™„ì „í•˜ê±°ë‚˜, ë…¸ì´ì¦ˆê°€ ë§ê±°ë‚˜, í˜•ì‹ì´ ì¼ê´€ë˜ì§€ ì•Šì•„ ì§ì ‘ ëª¨ë¸ë§ì— ì‚¬ìš©í•˜ê¸° ì–´ë ¤ì›€ë°ì´í„° ì „ì²˜ë¦¬ëŠ” ë°ì´í„°ì˜ í’ˆì§ˆì„ ë†’ì´ê³ , ë¶„ì„ ê²°ê³¼ì˜ ì‹ ë¢°ì„±ì„ í™•ë³´í•˜ê¸° ìœ„í•œ í•„ìˆ˜ì ì¸ ê³¼ì •ë°ì´í„° ì „ì²˜ë¦¬ALTë°ì´í„° í’ˆì§ˆ í–¥ìƒ: ê²°ì¸¡ì¹˜, ì´ìƒì¹˜, ì¤‘ë³µ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ì—¬ ë°ì´í„°ì˜ í’ˆì§ˆì„ ë†’ì…ë‹ˆë‹¤.ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒ: ì ì ˆí•œ ìŠ¤ì¼€ì¼ë§, ì •ê·œí™”ë¥¼ í†µí•´ ëª¨ë¸ì˜ í•™ìŠµ ì†ë„ì™€ ì„±ëŠ¥ì„ ê°œì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ë°ì´í„° ì¼ê´€ì„± í™•ë³´: ì„œë¡œ ë‹¤ë¥¸ ì¶œì²˜ì—ì„œ ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ì¼ê´€ëœ í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.íŠ¹ì„± ê³µí•™(Feature Engineering): ìœ ìš©í•œ íŠ¹ì„±(Feature)ì„ ìƒì„±í•˜ê±°ë‚˜ ë³€í™˜í•˜ì—¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.2) ë°ì´í„° ì „ì²˜ë¦¬ì˜ ì£¼ìš” ê¸°ë²• ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (Handling Missing Data)ê²°ì¸¡ì¹˜ëŠ” ë°ì´í„°ì…‹ì— ëˆ„ë½ëœ ê°’ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ê²°ì¸¡ì¹˜ë¥¼ ì²˜ë¦¬í•˜ì§€ ì•Šìœ¼ë©´ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ì €í•˜ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ì‚­ì œ: ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” í–‰ì´ë‚˜ ì—´ì„ ì‚­ì œí•©ë‹ˆë‹¤. ê²°ì¸¡ì¹˜ê°€ ì ì„ ë•Œ ìœ ìš©í•˜ì§€ë§Œ, ë°ì´í„° ì†ì‹¤ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ëŒ€ì²´: í‰ê· , ì¤‘ì•™ê°’, ìµœë¹ˆê°’ ë“±ìœ¼ë¡œ ê²°ì¸¡ì¹˜ë¥¼ ëŒ€ì²´í•©ë‹ˆë‹¤.ì˜ˆì¸¡: ë‹¤ë¥¸ íŠ¹ì„±ì„ ì‚¬ìš©í•˜ì—¬ ê²°ì¸¡ì¹˜ë¥¼ ì˜ˆì¸¡í•˜ê³  ì±„ì›ë‹ˆë‹¤. ì´ìƒì¹˜ ì²˜ë¦¬ (Handling Outliers)ì´ìƒì¹˜ëŠ” ë°ì´í„°ì—ì„œ ë¹„ì •ìƒì ìœ¼ë¡œ í¬ê±°ë‚˜ ì‘ì€ ê°’ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì´ìƒì¹˜ëŠ” ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì €í•˜ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.ì œê±°: ì´ìƒì¹˜ë¥¼ ë°ì´í„°ì…‹ì—ì„œ ì œê±°í•©ë‹ˆë‹¤.ë³€í™˜: ì´ìƒì¹˜ë¥¼ ë‹¤ë¥¸ ê°’ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤ (ì˜ˆ: ìƒí•œì„ ì´ë‚˜ í•˜í•œì„ ìœ¼ë¡œ ëŒ€ì²´).IQR ë°©ë²•: IQR(Interquartile Range)ì„ ì‚¬ìš©í•˜ì—¬ ì´ìƒì¹˜ë¥¼ íƒì§€í•˜ê³  ì²˜ë¦¬í•©ë‹ˆë‹¤.  ë°ì´í„° ì •ê·œí™” (Normalization)ì •ê·œí™”ëŠ” ë°ì´í„°ë¥¼ ì¼ì •í•œ ë²”ìœ„ë¡œ ìŠ¤ì¼€ì¼ë§í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ [0, 1] ë²”ìœ„ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.Min-Max ì •ê·œí™”: ìµœì†Œê°’ì„ 0, ìµœëŒ€ê°’ì„ 1ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
Xnorm=Xmaxâˆ’XminXâˆ’XminXnorm=Xmaxâˆ’XminXâˆ’XminXnorm=Xmaxâˆ’XminXâˆ’Xminï»¿â€‹Xnorm=Xâˆ’XminXmaxâˆ’XminXnorm=Xâˆ’XminXmaxâˆ’XminXnorm=Xâˆ’XminXmaxâˆ’XminX_{norm} = }{X_{max} - X_{min}}Xnorm=Xâˆ’XminXmaxâˆ’XminXnormâ€‹=Xmaxâ€‹âˆ’Xminâ€‹Xâˆ’Xminâ€‹â€‹ï»¿â€‹ ë°ì´í„° í‘œì¤€í™” (Standardization)í‘œì¤€í™”ëŠ” ë°ì´í„°ë¥¼ í‰ê·  0, ë¶„ì‚° 1ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.Z-ì ìˆ˜ í‘œì¤€í™”: Xstd=ÏƒXâˆ’Î¼Xstd=ÏƒXâˆ’Î¼Xstd=ÏƒXâˆ’Î¼ï»¿
ì—¬ê¸°ì„œ Î¼Î¼ï»¿ëŠ” í‰ê· , ÏƒÏƒï»¿ëŠ” í‘œì¤€í¸ì°¨ì…ë‹ˆë‹¤.Xstd=Xâˆ’Î¼ÏƒXstd=Xâˆ’Î¼ÏƒXstd=Xâˆ’Î¼ÏƒX_{std} = {}Xstd=Xâˆ’Î¼ÏƒXstdâ€‹=ÏƒXâˆ’Î¼â€‹ï»¿â€‹  íŠ¹ì„± ê³µí•™ (Feature Engineering)íŠ¹ì„± ê³µí•™ì€ ë°ì´í„°ë¡œë¶€í„° ìƒˆë¡œìš´ ìœ ìš©í•œ íŠ¹ì„±ì„ ìƒì„±í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.íŠ¹ì„± ìƒì„±: ê¸°ì¡´ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒˆë¡œìš´ íŠ¹ì„±ì„ ìƒì„±í•©ë‹ˆë‹¤ (ì˜ˆ: ë‚ ì§œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ìš”ì¼ íŠ¹ì„± ìƒì„±).íŠ¹ì„± ì„ íƒ: ëª¨ë¸ ì„±ëŠ¥ì— ì¤‘ìš”í•œ íŠ¹ì„±ì„ ì„ íƒí•˜ê³ , ì¤‘ìš”í•˜ì§€ ì•Šì€ íŠ¹ì„±ì„ ì œê±°í•©ë‹ˆë‹¤.  ë°ì´í„° ì¸ì½”ë”© (Data Encoding)ë¹„ì •í˜• ë°ì´í„°ë¥¼ ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•íƒœë¡œ ë³€í™˜í•©ë‹ˆë‹¤.ë ˆì´ë¸” ì¸ì½”ë”© (Label Encoding): ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ìˆ«ìë¡œ ë³€í™˜í•©ë‹ˆë‹¤.ì›-í•« ì¸ì½”ë”© (One-Hot Encoding): ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ì´ì§„ ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.  ë°ì´í„° ë¶„í•  (Data Splitting)ë°ì´í„°ë¥¼ í•™ìŠµìš©(train), ê²€ì¦ìš©(validation), í…ŒìŠ¤íŠ¸ìš©(test)ìœ¼ë¡œ ë¶„í• í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.í•™ìŠµ ë°ì´í„° (Training Data): ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©ë˜ëŠ” ë°ì´í„°.ê²€ì¦ ë°ì´í„° (Validation Data): ëª¨ë¸ íŠœë‹ ë° ì„±ëŠ¥ ê²€ì¦ì— ì‚¬ìš©ë˜ëŠ” ë°ì´í„°.í…ŒìŠ¤íŠ¸ ë°ì´í„° (Test Data): ìµœì¢… ëª¨ë¸ í‰ê°€ì— ì‚¬ìš©ë˜ëŠ” ë°ì´í„°..
[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 6. ìˆœí™˜ ì‹ ê²½ë§(RNN)[SCC] ê¸°ì´ˆê°€ íƒ„íƒ„í•œ ë”¥ëŸ¬ë‹/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] ê¸°ì´ˆê°€ íƒ„íƒ„í•œ ë”¥ëŸ¬ë‹ - 2ì£¼ì°¨/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 6. ìˆœí™˜ ì‹ ê²½ë§(RNN)ì œì‘:[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 6. ìˆœí™˜ ì‹ ê²½ë§(RNN)[ìˆ˜ì—… ëª©í‘œ]ìˆœí™˜ ì‹ ê²½ë§(RNN) ê°œë…ì— ëŒ€í•´ì„œ ë°°ì›Œë³´ê³  ì–´ë–¤ ì›ë¦¬ë¡œ ë™ì‘í•˜ëŠ”ì§€ ì•Œì•„ë´…ì‹œë‹¤Pytorchë¡œ ê°„ë‹¨í•œ RNN ëª¨ë¸ êµ¬í˜„ ì‹¤ìŠµì„ ì§„í–‰í•´ ë´…ì‹œë‹¤[ëª©ì°¨]01. RNNì˜ ê¸°ë³¸ êµ¬ì¡°ì™€ ë™ì‘ ì›ë¦¬02. RNNê³¼ LSTMì„ ì´ìš©í•œ ì‹œê³„ì—´ ë°ì´í„° ì˜ˆì¸¡ (PyTorch)
 
 01. RNNì˜ ê¸°ë³¸ êµ¬ì¡°ì™€ ë™ì‘ ì›ë¦¬RNNì˜ ê¸°ë³¸ êµ¬ì„±ìš”ì†Œì™€ ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ë™ì‘í•˜ëŠ”ì§€ ë°°ì›Œë³´ê³ , LSTM ê³¼ GRUì— ëŒ€í•´ ì•Œì•„ë³´ê³  ë¹„êµí•´ ë´…ì‹œë‹¤.1) RNNì˜ ê¸°ë³¸ êµ¬ì¡°ì™€ ì‘ë™ ë°©ì‹ RNNì˜ ê¸°ë³¸ êµ¬ì¡°ìˆœí™˜ ì‹ ê²½ë§(Recurrent Neural Network, RNN)ì€ ì‹œê³„ì—´ ë°ì´í„°ë‚˜ ìˆœì°¨ì ì¸ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ì„¤ê³„ëœ ì‹ ê²½ë§ì…ë‹ˆë‹¤RNNì€ ì´ì „ ì‹œê°„ ë‹¨ê³„ì˜ ì •ë³´ë¥¼ í˜„ì¬ ì‹œê°„ ë‹¨ê³„ë¡œ ì „ë‹¬í•´, ì‹œí€€ìŠ¤ ë°ì´í„°ì˜ íŒ¨í„´ì„ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ALT RNNì˜ ë™ì‘ ì›ë¦¬ìˆœí™˜ êµ¬ì¡°RNNì€ ì…ë ¥ ë°ì´í„°ì™€ ì´ì „ ì‹œê°„ ë‹¨ê³„ì˜ ì€ë‹‰ ìƒíƒœ(hidden state)ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„, í˜„ì¬ ì‹œê°„ ë‹¨ê³„ì˜ ì€ë‹‰ ìƒíƒœë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.ì€ë‹‰ ìƒíƒœëŠ” ì‹œí€€ìŠ¤ì˜ ì •ë³´ë¥¼ ì €ì¥í•˜ê³ , ë‹¤ìŒ ì‹œê°„ ë‹¨ê³„ë¡œ ì „ë‹¬ë©ë‹ˆë‹¤.ë™ì‘ ì›ë¦¬RNNì€ ì‹œí€€ìŠ¤ì˜ ê° ì‹œê°„ ë‹¨ê³„ì—ì„œ ë™ì¼í•œ ê°€ì¤‘ì¹˜ë¥¼ ê³µìœ í•˜ì—¬, ì‹œí€€ìŠ¤ì˜ íŒ¨í„´ì„ í•™ìŠµí•©ë‹ˆë‹¤.ìˆœì „íŒŒ(Forward Propagation)ì™€ ì—­ì „íŒŒ(Backpropagation Through Time, BPTT)ë¥¼ í†µí•´ ê°€ì¤‘ì¹˜ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.2) LSTM ë° GRU LSTM & GRURNNì€ ì¥ê¸° ì˜ì¡´ì„± ë¬¸ì œ(long-term dependency problem)ë¥¼ ê²ªì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ LSTMê³¼ GRUê°€ ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤. LSTM(Long Short-Term Memory)LSTMì€ ì…€ ìƒíƒœ(cell state)ì™€ ê²Œì´íŠ¸(gate) êµ¬ì¡°ë¥¼ ë„ì…, ì¥ê¸° ì˜ì¡´ì„±ì„ íš¨ê³¼ì ìœ¼ë¡œ í•™ìŠµê°€ëŠ¥ í•©ë‹ˆë‹¤.LSTMì€ ì…ë ¥ ê²Œì´íŠ¸(input gate), ì¶œë ¥ ê²Œì´íŠ¸(output gate), ë§ê° ê²Œì´íŠ¸(forget gate)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ë³´ë¥¼ ì¡°ì ˆí•©ë‹ˆë‹¤. GRU (Gated Recurrent Unit)GRUëŠ” LSTMì˜ ë³€í˜•ìœ¼ë¡œ, ì…€ ìƒíƒœ ëŒ€ì‹  ì€ë‹‰ ìƒíƒœ(hidden state)ë§Œì„ ì‚¬ìš©í•˜ì—¬ êµ¬ì¡°ë¥¼ ë‹¨ìˆœí™”í•©ë‹ˆë‹¤.GRUëŠ” ì—…ë°ì´íŠ¸ ê²Œì´íŠ¸(update gate)ì™€ ë¦¬ì…‹ ê²Œì´íŠ¸(reset gate)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ë³´ë¥¼ ì¡°ì ˆí•©ë‹ˆë‹¤. ì°¨ì´ì LSTMì€ ì…€ ìƒíƒœì™€ ì€ë‹‰ ìƒíƒœë¥¼ ëª¨ë‘ ì‚¬ìš©í•˜ë©°, ë” ë³µì¡í•œ ê²Œì´íŠ¸ êµ¬ì¡°ë¥¼ ê°€ì§‘ë‹ˆë‹¤.GRUëŠ” ì€ë‹‰ ìƒíƒœë§Œì„ ì‚¬ìš©í•˜ë©°, ë” ê°„ë‹¨í•œ ê²Œì´íŠ¸ êµ¬ì¡°ë¥¼ ê°€ì§‘ë‹ˆë‹¤. ë”°ë¼ì„œ ê³„ì‚° ë¹„ìš©ì´ ì ê³ , í•™ìŠµì´ ë¹ ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.ALT3) ì‹œê³„ì—´ ë°ì´í„° ì²˜ë¦¬ RNNì„ ì´ìš©í•œ ì‹œê³„ì—´ ë°ì´í„° ì²˜ë¦¬ ë°©ë²•RNNì€ ì‹œê³„ì—´ ë°ì´í„°ë‚˜ ìˆœì°¨ì ì¸ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë° ì í•©í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì£¼ì‹ ê°€ê²© ì˜ˆì¸¡, ë‚ ì”¨ ì˜ˆì¸¡, í…ìŠ¤íŠ¸ ìƒì„± ë“±ì´ ìˆìŠµë‹ˆë‹¤.ë°ì´í„° ì „ì²˜ë¦¬:ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ì ì ˆí•œ í˜•íƒœë¡œ ë³€í™˜í•˜ê³ , ì •ê·œí™”(normalization)í•©ë‹ˆë‹¤.ì…ë ¥ ì‹œí€€ìŠ¤ì™€ ì¶œë ¥ ì‹œí€€ìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.ëª¨ë¸ êµ¬ì¶•:RNN, LSTM, GRU ë“±ì˜ ëª¨ë¸ì„ ì •ì˜í•©ë‹ˆë‹¤.ì…ë ¥ í¬ê¸°, ì€ë‹‰ ìƒíƒœ í¬ê¸°, ì¶œë ¥ í¬ê¸° ë“±ì„ ì„¤ì •í•©ë‹ˆë‹¤.ëª¨ë¸ í•™ìŠµ:ì†ì‹¤ í•¨ìˆ˜ì™€ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì„ ì •ì˜í•©ë‹ˆë‹¤.ìˆœì „íŒŒì™€ ì—­ì „íŒŒë¥¼ í†µí•´ ëª¨ë¸ì„ í•™ìŠµì‹œí‚µë‹ˆë‹¤.ëª¨ë¸ í‰ê°€:í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.02. RNNê³¼ LSTMì„ ì´ìš©í•œ ì‹œê³„ì—´ ë°ì´í„° ì˜ˆì¸¡ (PyTorch) ì´ì œ PyTorchë¥¼ ì‚¬ìš©í•˜ì—¬ ê°„ë‹¨í•œ RNNê³¼ LSTM ëª¨ë¸ì„ êµ¬ì¶•í•˜ê³ , ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ì˜ˆì¸¡í•´ë³´ê² ìŠµë‹ˆë‹¤. ì˜ˆì œë¡œëŠ” Sine íŒŒí˜• ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤.1)  ê°„ë‹¨í•œ RNN/LSTM ëª¨ë¸ì„ ì´ìš©í•œ ì‹œê³„ì—´ ë°ì´í„° ì˜ˆì¸¡ ì‹¤ìŠµ PyTorch ë° í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸PyTorch ë° í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ {5px}PyTorch ë° í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ï»¿â€‹Pythonë³µì‚¬import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
â€‹ë°ì´í„°ì…‹ ìƒì„± ë° ì „ì²˜ë¦¬ë°ì´í„°ì…‹ ìƒì„± ë° ì „ì²˜ë¦¬ {5px}ë°ì´í„°ì…‹ ìƒì„± ë° ì „ì²˜ë¦¬ ï»¿â€‹Pythonë³µì‚¬# Sine íŒŒí˜• ë°ì´í„° ìƒì„±
def create_sine_wave_data(seq_length, num_samples):
    X = []
    y = []
for _ in range(num_samples):
        start = np.random.rand()
        x = np.linspace(start, start + 2 * np.pi, seq_length)
        X.append(np.sin(x))
        y.append(np.sin(x + 0.1))
return np.array(X), np.array(y)

seq_length = 50
num_samples = 1000
X, y = create_sine_wave_data(seq_length, num_samples)
# ë°ì´í„°ì…‹ì„ PyTorch í…ì„œë¡œ ë³€í™˜
X = torch.tensor(X, dtype=torch.float32).unsqueeze(-1)
y = torch.tensor(y, dtype=torch.float32).unsqueeze(-1)
â€‹ ê°„ë‹¨í•œ RNN ëª¨ë¸ ì •ì˜ê°„ë‹¨í•œ RNN ëª¨ë¸ ì •ì˜ {5px}ê°„ë‹¨í•œ RNN ëª¨ë¸ ì •ì˜ ï»¿â€‹Pythonë³µì‚¬class SimpleRNN(nn.Module):
def __init__(self, input_size, hidden_size, output_size):
super(SimpleRNN, self).__init__()
        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)
def forward(self, x):
        h0 = torch.zeros(1, x.size(0), hidden_size) # ì´ˆê¸° ì€ë‹‰ ìƒíƒœ
        out, _ = self.rnn(x, h0)
        out = self.fc(out[:, -1, :]) # ë§ˆì§€ë§‰ ì‹œê°„ ë‹¨ê³„ì˜ ì¶œë ¥
return out

input_size = 1
hidden_size = 32
output_size = 1
model = SimpleRNN(input_size, hidden_size, output_size)
â€‹nn.RNN: ìˆœí™˜ ì‹ ê²½ë§(RNN) ì¸µì„ ì •ì˜í•©ë‹ˆë‹¤.nn.RNN(input_size, hidden_size, batch_first)ëŠ” ì…ë ¥ í¬ê¸°, ì€ë‹‰ ìƒíƒœ í¬ê¸°, ë°°ì¹˜ ì°¨ì›ì„ ì²« ë²ˆì§¸ë¡œ ì„¤ì •í•©ë‹ˆë‹¤..RNN(input_size, hidden_size, batch_first)ëŠ” ì…ë ¥ í¬ê¸°, ì€ë‹‰ ìƒíƒœ í¬ê¸°, ë°°ì¹˜ ì°¨ì›ì„ ì²« ë²ˆì§¸ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.ï»¿â€‹nn.Linear: ì„ í˜• ë³€í™˜ì„ ì ìš©í•˜ëŠ” ì™„ì „ ì—°ê²°(fully connected) ë ˆì´ì–´ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.nn.Linear(in_features, out_features)ëŠ” ì…ë ¥ íŠ¹ì§•ì˜ ìˆ˜ì™€ ì¶œë ¥ íŠ¹ì§•ì˜ ìˆ˜ë¥¼ ì§€ì •í•©ë‹ˆë‹¤..Linear(in_features, out_features)ëŠ” ì…ë ¥ íŠ¹ì§•ì˜ ìˆ˜ì™€ ì¶œë ¥ íŠ¹ì§•ì˜ ìˆ˜ë¥¼ ì§€ì •í•©ë‹ˆë‹¤.ï»¿â€‹ ê°„ë‹¨í•œ LSTM ëª¨ë¸ ì •ì˜ê°„ë‹¨í•œ LSTM ëª¨ë¸ ì •ì˜ {5px}ê°„ë‹¨í•œ LSTM ëª¨ë¸ ì •ì˜ ï»¿â€‹Pythonë³µì‚¬class SimpleLSTM(nn.Module):
def __init__(self, input_size, hidden_size, output_size):
super(SimpleLSTM, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)
def forward(self, x):
        h0 = torch.zeros(1, x.size(0), hidden_size) # ì´ˆê¸° ì€ë‹‰ ìƒíƒœ
        c0 = torch.zeros(1, x.size(0), hidden_size) # ì´ˆê¸° ì…€ ìƒíƒœ
        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :]) # ë§ˆì§€ë§‰ ì‹œê°„ ë‹¨ê³„ì˜ ì¶œë ¥
return out

model = SimpleLSTM(input_size, hidden_size, output_size)
â€‹nn.LSTM: ì¥ë‹¨ê¸° ë©”ëª¨ë¦¬(LSTM) ì¸µì„ ì •ì˜í•©ë‹ˆë‹¤.nn.LSTM(input_size, hidden_size, batch_first)ëŠ” ì…ë ¥ í¬ê¸°, ì€ë‹‰ ìƒíƒœ í¬ê¸°, ë°°ì¹˜ ì°¨ì›ì„ ì²« ë²ˆì§¸ë¡œ ì„¤ì •í•©ë‹ˆë‹¤..LSTM(input_size, hidden_size, batch_first)ëŠ” ì…ë ¥ í¬ê¸°, ì€ë‹‰ ìƒíƒœ í¬ê¸°, ë°°ì¹˜ ì°¨ì›ì„ ì²« ë²ˆì§¸ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.ï»¿â€‹ ëª¨ë¸ í•™ìŠµëª¨ë¸ í•™ìŠµ {5px}ëª¨ë¸ í•™ìŠµ ï»¿â€‹Pythonë³µì‚¬# ì†ì‹¤ í•¨ìˆ˜ì™€ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ ì •ì˜
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)
# ëª¨ë¸ í•™ìŠµ
num_epochs = 100
for epoch in range(num_epochs):
    outputs = model(X)
    optimizer.zero_grad()
    loss = criterion(outputs, y)
    loss.backward()
    optimizer.step()
if (epoch + 1) % 10 == 0:
print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')
print('Finished Training')
â€‹nn.MSELoss: í‰ê·  ì œê³± ì˜¤ì°¨(MSE) ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.optim.Adam: Adam ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì„ ì •ì˜í•©ë‹ˆë‹¤. lrì€ í•™ìŠµë¥ ì„ ì§€ì •í•©ë‹ˆë‹¤.optimizer.zero_grad(): ì´ì „ ë‹¨ê³„ì—ì„œ ê³„ì‚°ëœ ê¸°ìš¸ê¸°ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.loss.backward(): ì—­ì „íŒŒë¥¼ í†µí•´ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.optimizer.step(): ê³„ì‚°ëœ ê¸°ìš¸ê¸°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. ëª¨ë¸ í‰ê°€ ë° ì‹œê°í™”ëª¨ë¸ í‰ê°€ ë° ì‹œê°í™” {5px}ëª¨ë¸ í‰ê°€ ë° ì‹œê°í™” ï»¿â€‹Pythonë³µì‚¬# ëª¨ë¸ í‰ê°€
model.eval()
with torch.no_grad():
    predicted = model(X).detach().numpy()
# ì‹œê°í™”
plt.figure(figsize=(10, 5))
plt.plot(y.numpy().flatten(), label='True')
plt.plot(predicted.flatten(), label='Predicted')
plt.legend()
plt.show()
â€‹model.eval(): ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.torch.no_grad(): í‰ê°€ ë‹¨ê³„ì—ì„œëŠ” ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•  í•„ìš”ê°€ ì—†ìœ¼ë¯€ë¡œ, ì´ë¥¼ ë¹„í™œì„±í™”í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ì„ ì¤„ì…ë‹ˆë‹¤.detach(): í…ì„œë¥¼ ê³„ì‚° ê·¸ë˜í”„ì—ì„œ ë¶„ë¦¬í•©ë‹ˆë‹¤..
Sentence-Transformer, Word2Vec, ê·¸ë¦¬ê³  Transformer ê¸°ë°˜ ì„ë² ë”© LLM & RAGë¥¼ í™œìš©í•œ AI ì„œë¹„ìŠ¤ ë§Œë“¤ê¸°/ LLM & RAGë¥¼ í™œìš©í•œ AI ì„œë¹„ìŠ¤ ë§Œë“¤ê¸° - 5ì£¼ì°¨/Sentence-Transformer, Word2Vec, ê·¸ë¦¬ê³  Transformer ê¸°ë°˜ ì„ë² ë”©ì œì‘:Sentence-Transformer, Word2Vec, ê·¸ë¦¬ê³  Transformer ê¸°ë°˜ ì„ë² ë”©ìˆ˜ì—… ëª©í‘œí•œêµ­ì–´ ì„ë² ë”©ì˜ íŠ¹ì§•ê³¼ ì„ë² ë”© ì‹¤ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤.ëª©ì°¨Word2Vecì„ ì‚¬ìš©í•œ í•œêµ­ì–´ ì„ë² ë”©Word2Vec í•œêµ­ì–´ ì„ë² ë”© ì‹¤ìŠµSentence-Transformerë¡œ í•œêµ­ì–´ ë¬¸ì¥ ì„ë² ë”©Sentence-Transformer í•œêµ­ì–´ ì„ë² ë”© ì‹¤ìŠµTransformer ê¸°ë°˜ ìµœì‹  í•œêµ­ì–´ ì„ë² ë”©Transformer í•œêµ­ì–´ ì„ë² ë”© ì‹¤ìŠµ: KoBERT ì‚¬ìš©í•œêµ­ì–´ ì„ë² ë”©ì˜ íŠ¹ì§•ê³¼ ë„ì „ ê³¼ì œì™œ í•œêµ­ì–´ ì„ë² ë”©ì´ ì–´ë ¤ìš´ê°€?ìµœì‹  ëª¨ë¸ê³¼ ì ‘ê·¼ ë°©ì‹ë°°ìš´ ë‚´ìš© ì •ë¦¬í•˜ê¸°!â—ì´ë²ˆ ì‹¤ìŠµì—ì„œëŠ” í•œêµ­ì–´ ì„ë² ë”©ì„ ìœ„í•œ ì„¸ ê°€ì§€ ëŒ€í‘œì ì¸ ê¸°ë²•ì¸ Sentence-Transformer, Word2Vec, ê·¸ë¦¬ê³  Transformer ê¸°ë°˜ ì„ë² ë”©ì„ ì‚´í´ë³´ê³  ì‹¤ìŠµí•´ë³¼ ê±°ì˜ˆìš”. íŠ¹íˆ ìµœì‹  Transformer ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í•œêµ­ì–´ë¥¼ ì˜ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ì•Œì•„ë³¼ê²Œìš”.Word2Vecì„ ì‚¬ìš©í•œ í•œêµ­ì–´ ì„ë² ë”©â—Word2Vecì€ ë‹¨ì–´ë¥¼ ê³ ì°¨ì› ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ì˜ë¯¸ì  ìœ ì‚¬ì„±ì„ ì¸¡ì •í•˜ëŠ” ì„ë² ë”© ê¸°ë²•ì´ì—ìš”. ë‹¨ì–´ ê°„ì˜ ë¬¸ë§¥ì  ê´€ê³„ë¥¼ ë°˜ì˜í•˜ì—¬, ë¹„ìŠ·í•œ ì˜ë¯¸ë¥¼ ê°€ì§„ ë‹¨ì–´ë“¤ì´ ìœ ì‚¬í•œ ë²¡í„° ê°’ì„ ê°–ë„ë¡ í•™ìŠµí•©ë‹ˆë‹¤.Word2Vec í•œêµ­ì–´ ì„ë² ë”© ì‹¤ìŠµPythonë³µì‚¬from gensim.models import Word2Vec

# ìƒ˜í”Œ í•œêµ­ì–´ ë¬¸ì¥ ë°ì´í„°
sentences = [
"ë‚˜ëŠ” ì˜¤ëŠ˜ ì±…ì„ ì½ì—ˆë‹¤",
"ê³ ì–‘ì´ê°€ ì•¼ì˜¹í•˜ê³  ìš¸ì—ˆë‹¤",
"ì¸ê³µì§€ëŠ¥ì€ ì •ë§ í¥ë¯¸ë¡œìš´ ì£¼ì œë‹¤",
"í•œêµ­ì–´ ì„ë² ë”©ì„ í•™ìŠµí•˜ëŠ” ì¤‘ì´ë‹¤"
]
# Python ê¸°ë³¸ split() ì‚¬ìš©í•´ ê°„ë‹¨í•˜ê²Œ í† í°í™”
tokenized_sentences = [sentence.split() for sentence in sentences]
# Word2Vec ëª¨ë¸ í•™ìŠµ
word2vec_model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)
# ë‹¨ì–´ 'ê³ ì–‘ì´'ì™€ ìœ ì‚¬í•œ ë‹¨ì–´ ì°¾ê¸°
similar_words = word2vec_model.wv.most_similar("ê³ ì–‘ì´")
print(similar_words)

â€‹Word2Vec íŠ¹ì§•ì¥ì : ë‹¨ì–´ ê°„ì˜ ì˜ë¯¸ì  ê´€ê³„ë¥¼ íŒŒì•…í•˜ëŠ” ë° ì í•©ë‹¨ì : ë‹¨ì–´ ìì²´ë§Œ í•™ìŠµí•˜ë¯€ë¡œ, ë¬¸ì¥ ë‹¨ìœ„ì—ì„œëŠ” ìœ ì—°ì„±ì´ ë¶€ì¡±Sentence-Transformerë¡œ í•œêµ­ì–´ ë¬¸ì¥ ì„ë² ë”©ğŸ’¡Sentence-TransformerëŠ” ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì„ë² ë”©ì„ ìƒì„±í•  ìˆ˜ ìˆëŠ” BERT ê¸°ë°˜ì˜ ëª¨ë¸ì´ì—ìš”. íŠ¹íˆ í•œêµ­ì–´ì— íŠ¹í™”ëœ Ko-Sentence-BERT ê°™ì€ ëª¨ë¸ì„ ì‚¬ìš©í•˜ë©´, ë¬¸ì¥ ê°„ì˜ ì˜ë¯¸ì  ìœ ì‚¬ì„±ì„ ì •í™•í•˜ê²Œ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Sentence-Transformer í•œêµ­ì–´ ì„ë² ë”© ì‹¤ìŠµPythonë³µì‚¬from sentence_transformers import SentenceTransformer

# ìµœì‹  Ko-Sentence-BERT ëª¨ë¸ ë¡œë“œ
model = SentenceTransformer('sentence-transformers/kr-sentence_bert-base')
# ìƒ˜í”Œ ë¬¸ì¥ë“¤
sentences = [
"ë‚˜ëŠ” ì˜¤ëŠ˜ ì±…ì„ ì½ì—ˆë‹¤.",
"ê³ ì–‘ì´ê°€ ì•¼ì˜¹í•˜ê³  ìš¸ì—ˆë‹¤.",
"ì¸ê³µì§€ëŠ¥ì€ í¥ë¯¸ë¡œìš´ ì£¼ì œë‹¤.",
"í•œêµ­ì–´ ì„ë² ë”©ì„ í•™ìŠµí•˜ëŠ” ì¤‘ì´ë‹¤."
]
# ë¬¸ì¥ ì„ë² ë”© ìƒì„±
embeddings = model.encode(sentences)
# ì²« ë²ˆì§¸ ë¬¸ì¥ ì„ë² ë”© í™•ì¸
print(embeddings[0])

â€‹Sentence-Transformer íŠ¹ì§•ì¥ì : ë¬¸ì¥ ê°„ì˜ ìœ ì‚¬ì„±ì„ ì •í™•í•˜ê²Œ ì¸¡ì •, ë¬¸ë§¥ì„ ê³ ë ¤í•œ ì„ë² ë”© ìƒì„±ë‹¨ì : í•™ìŠµ ì†ë„ê°€ ìƒëŒ€ì ìœ¼ë¡œ ëŠë¦¼Transformer ê¸°ë°˜ ìµœì‹  í•œêµ­ì–´ ì„ë² ë”©ğŸ’¡Transformer ê¸°ë°˜ ëª¨ë¸ë“¤ì€ BERT, GPT ê°™ì€ ëª¨ë¸ë“¤ë¡œ ë°œì „í•´ì™”ê³ , ìµœê·¼ì—ëŠ” KoBERT, KoGPTì™€ ê°™ì€ í•œêµ­ì–´ íŠ¹í™” ëª¨ë¸ì´ ë“±ì¥í–ˆì–´ìš”. ì´ ëª¨ë¸ë“¤ì€ ë¬¸ë§¥ì„ ê³ ë ¤í•˜ì—¬ ì •êµí•œ ì„ë² ë”©ì„ ìƒì„±í•´ì¤ë‹ˆë‹¤.Transformer í•œêµ­ì–´ ì„ë² ë”© ì‹¤ìŠµ: KoBERT ì‚¬ìš©Pythonë³µì‚¬from transformers import BertTokenizer, BertModel
import torch

# KLUE-BERT ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
tokenizer = BertTokenizer.from_pretrained('klue/bert-base')
model = BertModel.from_pretrained('klue/bert-base')
# ì…ë ¥ ë¬¸ì¥
sentence = "í•œêµ­ì–´ ì„ë² ë”©ì„ í•™ìŠµí•˜ê³  ìˆìŠµë‹ˆë‹¤."
# í† í°í™” ë° í…ì„œ ë³€í™˜
inputs = tokenizer(sentence, return_tensors='pt')
# ì„ë² ë”© ìƒì„±
with torch.no_grad():
    outputs = model(**inputs)
# ì„ë² ë”© ë²¡í„° ì¶”ì¶œ (í‰ê· ê°’ìœ¼ë¡œ ê³„ì‚°)
embedding = outputs.last_hidden_state.mean(dim=1)
print(embedding)
â€‹Transformer íŠ¹ì§•ì¥ì : ë¬¸ë§¥ì„ ì–‘ë°©í–¥ìœ¼ë¡œ ì´í•´, ë¬¸ì¥ ì „ì²´ì˜ ì˜ë¯¸ë¥¼ ê¹Šì´ ë°˜ì˜ë‹¨ì : ê³„ì‚° ë¹„ìš©ì´ í¬ê³ , ëª¨ë¸ í¬ê¸°ê°€ í¼í•œêµ­ì–´ ì„ë² ë”©ì˜ íŠ¹ì§•ê³¼ ë„ì „ ê³¼ì œì™œ í•œêµ­ì–´ ì„ë² ë”©ì´ ì–´ë ¤ìš´ê°€?1ï¸âƒ£êµì°©ì–´ íŠ¹ì„±: í•œêµ­ì–´ëŠ” ì¡°ì‚¬ì™€ ì–´ë¯¸ë¥¼ ë§ì´ ì‚¬ìš©í•˜ëŠ” êµì°©ì–´ë¡œ, ë‹¨ì–´ ë³€í˜•ì´ ë§ì•„ ì •í™•í•œ í˜•íƒœì†Œ ë¶„ì„ì´ ì¤‘ìš”í•´ìš”.2ï¸âƒ£ì–´ìˆœì˜ ìœ ì—°ì„±: í•œêµ­ì–´ëŠ” ì–´ìˆœì´ ììœ ë¡­ê¸° ë•Œë¬¸ì—, ë™ì¼í•œ ì˜ë¯¸ë¼ë„ ë‹¤ì–‘í•œ í˜•íƒœë¡œ í‘œí˜„ë  ìˆ˜ ìˆì–´ìš”.3ï¸âƒ£ë°ì´í„° ë¶€ì¡±: ì˜ì–´ì— ë¹„í•´ í•œêµ­ì–´ë¡œ í•™ìŠµëœ ë°ì´í„°ê°€ ìƒëŒ€ì ìœ¼ë¡œ ì ì–´, ì„ë² ë”© ëª¨ë¸ì´ ì¶©ë¶„íˆ í•™ìŠµë˜ì§€ ì•Šì€ ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤.ìµœì‹  ëª¨ë¸ê³¼ ì ‘ê·¼ ë°©ì‹KoBERT, KoGPT ë“± í•œêµ­ì–´ ì „ìš© Transformer ëª¨ë¸ë“¤ì€ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ í•œêµ­ì–´ì— íŠ¹í™”ëœ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤.íŠ¹íˆ Sentence-TransformerëŠ” ë¬¸ì¥ ê°„ ì˜ë¯¸ì  ìœ ì‚¬ì„±ì„ ì •í™•í•˜ê²Œ íŒŒì•…í•˜ëŠ” ë° ê°•ì ì„ ê°€ì§€ë©°, í•œêµ­ì–´ ë¬¸ì¥ ì²˜ë¦¬ì— íƒì›”í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.ë°°ìš´ ë‚´ìš© ì •ë¦¬í•˜ê¸°!ìš”ì•½Word2Vec: ë‹¨ì–´ ë‹¨ìœ„ì˜ ì˜ë¯¸ì  ìœ ì‚¬ì„±ì„ ë²¡í„°ë¡œ í‘œí˜„.Sentence-Transformer: ë¬¸ì¥ ê°„ ìœ ì‚¬ì„±ì„ ë²¡í„°ë¡œ í‘œí˜„, Ko-Sentence-BERT ëª¨ë¸ë¡œ í•œêµ­ì–´ ë¬¸ì¥ ì²˜ë¦¬.Transformer ëª¨ë¸ (íŠ¹íˆ? KoBERT): ë¬¸ë§¥ì„ ê³ ë ¤í•œ ê³ ì°¨ì› ì„ë² ë”©, í•œêµ­ì–´ ì „ìš© ëª¨ë¸ë¡œ ê°•ë ¥í•œ ì„±ëŠ¥.ğŸ˜€ì´ëŸ¬í•œ ë‹¤ì–‘í•œ ì„ë² ë”© ê¸°ë²•ì„ í†µí•´ í•œêµ­ì–´ í…ìŠ¤íŠ¸ ë¶„ì„ì˜ ì •í™•ì„±ê³¼ íš¨ìœ¨ì„±ì„ ë†’ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•œêµ­ì–´ ì„ë² ë”©ì˜ íŠ¹ì„±ê³¼ ìµœì‹  ê¸°ë²•ì„ ì˜ í™œìš©í•˜ì—¬ LLM ì‹œìŠ¤í…œì— ì ìš©í•´ë³´ì„¸ìš”!
ë¶ˆëŸ¬ì˜¨ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° ë° ê¸°ë³¸ ì •ë³´ í™•ì¸ì¸ê³µì§€ëŠ¥ì„ ìœ„í•œ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬/ì¸ê³µì§€ëŠ¥ì„ ìœ„í•œ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ - 3ì£¼ì°¨/ë¶ˆëŸ¬ì˜¨ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° ë° ê¸°ë³¸ ì •ë³´ í™•ì¸ì œì‘:ë¶ˆëŸ¬ì˜¨ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° ë° ê¸°ë³¸ ì •ë³´ í™•ì¸ìˆ˜ì—… ëª©í‘œíŒë‹¤ìŠ¤ë¥¼ í†µí•´ ë¶ˆëŸ¬ì˜¨ ë°ì´í„°ë¥¼ í™•ì¸í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ì„œ í•™ìŠµí•©ë‹ˆë‹¤.ëª©ì°¨ê¸°ì´ˆ ì •ë³´ë¥¼ í™•ì¸í•˜ëŠ” ë‹¤ì–‘í•œ ê¸°ëŠ¥ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°ë°ì´í„°í”„ë ˆì„ì˜ ê¸°ë³¸ ì •ë³´ í™•ì¸í•˜ê¸°
ë°ì´í„°í”„ë ˆì„ì˜ ê°œë³„ ë°ì´í„° í™•ì¸í•˜ê¸°ìš”ì•½ê¸°ì´ˆ ì •ë³´ë¥¼ í™•ì¸í•˜ëŠ” ë‹¤ì–‘í•œ ê¸°ëŠ¥ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°ğŸ“šë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¨ í›„, ì „ì²´ ë°ì´í„°ë¥¼ ë‹¤ í™•ì¸í•˜ê¸°ì—ëŠ” ë¶€ë‹´ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ íŒë‹¤ìŠ¤ëŠ” ë°ì´í„°ë¥¼ ë¯¸ë¦¬ë³´ê¸° í•  ìˆ˜ ìˆëŠ” ë‹¤ì–‘í•œ ë°©ë²•ì„ ì œê³µí•©ë‹ˆë‹¤. head()ë¡œ ìƒìœ„ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°head() í•¨ìˆ˜ëŠ” ë°ì´í„°í”„ë ˆì„ì˜ ìƒìœ„ ëª‡ ê°œì˜ í–‰ì„ ë¯¸ë¦¬ë³´ê¸° í•©ë‹ˆë‹¤. ê¸°ë³¸ì ìœ¼ë¡œ 5ê°œ í–‰ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.Pythonë³µì‚¬import pandas as pd

# CSV íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
df = pd.read_csv('data.csv')
# ìƒìœ„ 5ê°œ í–‰ ë¯¸ë¦¬ë³´ê¸°
print(df.head())

â€‹íŠ¹ì • ê°œìˆ˜ì˜ í–‰ì„ ë³´ê³  ì‹¶ë‹¤ë©´?Pythonë³µì‚¬print(df.head(10)) # ìƒìœ„ 10ê°œ í–‰ ë¯¸ë¦¬ë³´ê¸°
â€‹tail()ë¡œ í•˜ìœ„ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°tail() í•¨ìˆ˜ëŠ” í•˜ìœ„ ëª‡ ê°œì˜ í–‰ì„ ë¯¸ë¦¬ë³´ê¸° í•©ë‹ˆë‹¤. ì—­ì‹œ ê¸°ë³¸ê°’ì€ 5ê°œ í–‰ì…ë‹ˆë‹¤.Pythonë³µì‚¬print(df.tail())

â€‹íŠ¹ì • ê°œìˆ˜ì˜ í•˜ìœ„ í–‰ì„ ë³´ê³  ì‹¶ë‹¤ë©´?Pythonë³µì‚¬print(df.tail(3)) # í•˜ìœ„ 3ê°œ í–‰ ë¯¸ë¦¬ë³´ê¸°
â€‹ë°ì´í„°í”„ë ˆì„ì˜ ê¸°ë³¸ ì •ë³´ í™•ì¸í•˜ê¸°ğŸ“šë°ì´í„°í”„ë ˆì„ì˜ ê¸°ë³¸ ì •ë³´ë¥¼ í™•ì¸í•˜ë©´ ë°ì´í„°ì˜ êµ¬ì¡°ì™€ ë‚´ìš©ì„ ì‰½ê²Œ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.info()ë¡œ ë°ì´í„°í”„ë ˆì„ì˜ ìš”ì•½ ì •ë³´ í™•ì¸info() í•¨ìˆ˜ëŠ” ë°ì´í„°í”„ë ˆì„ì˜ ì „ì²´ êµ¬ì¡°ë¥¼ í•œëˆˆì— ë³´ì—¬ì¤ë‹ˆë‹¤. ê° ì—´ì˜ ë°ì´í„° íƒ€ì…ê³¼ ë„ ê°’ ì—¬ë¶€ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Pythonë³µì‚¬print(df.info())

â€‹ì¶œë ¥ ì˜ˆì‹œ:Plain Textë³µì‚¬<class 'pandas.core.frame.DataFrame'>
RangeIndex: 100 entries, 0 to 99
Data columns (total 4 columns):
 #   Column   Non-Null Count  Dtype
---  ------   --------------  -----
 0   ì´ë¦„      100 non-null    object
 1   ë‚˜ì´      100 non-null    int64
 2   ì§ì—…      100 non-null    object
 3   ì§€ì—­      95 non-null     object
dtypes: int64(1), object(3)
memory usage: 3.2+ KB

â€‹describe()ë¡œ ì£¼ìš” í†µê³„ ì •ë³´ í™•ì¸describe() í•¨ìˆ˜ëŠ” ìˆ«ìí˜• ë°ì´í„°ì— ëŒ€í•œ ì£¼ìš” í†µê³„ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤. í‰ê· , í‘œì¤€í¸ì°¨, ìµœì†Œê°’, ìµœëŒ€ê°’ ë“±ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Pythonë³µì‚¬print(df.describe())

â€‹íŠ¹ì • ì—´ì— ëŒ€í•œ í†µê³„ë¥¼ ë³´ê³  ì‹¶ë‹¤ë©´?Pythonë³µì‚¬print(df['ë‚˜ì´'].describe())

â€‹ì¶œë ¥ ì˜ˆì‹œ:Plain Textë³µì‚¬              ë‚˜ì´
count  100.000000
mean    30.500000
std      4.5
min     25.000000
25%     27.000000
50%     30.000000
75%     34.000000
max     35.000000

â€‹columnsì™€ indexë¡œ ì—´ê³¼ í–‰ ì •ë³´ í™•ì¸ë°ì´í„°í”„ë ˆì„ì˜ ì—´ ì´ë¦„ê³¼ í–‰ ì¸ë±ìŠ¤ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Pythonë³µì‚¬print(df.columns) # ì—´ ì´ë¦„ í™•ì¸
print(df.index) # í–‰ ì¸ë±ìŠ¤ í™•ì¸

â€‹
ë°ì´í„°í”„ë ˆì„ì˜ ê°œë³„ ë°ì´í„° í™•ì¸í•˜ê¸° loc[]ë¡œ í–‰ê³¼ ì—´ì˜ ìœ„ì¹˜ë¥¼ ì§€ì •í•˜ì—¬ ì ‘ê·¼í•˜ê¸°loc[]ì€ ë¼ë²¨(ì¸ë±ìŠ¤, ì»¬ëŸ¼ëª…)ì„ ì‚¬ìš©í•´ íŠ¹ì • ë°ì´í„°ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.Pythonë³µì‚¬# íŠ¹ì • í–‰ ì¡°íšŒ
print(df.loc[0]) # ì²« ë²ˆì§¸ í–‰
# íŠ¹ì • í–‰ê³¼ ì—´ ì¡°íšŒ
print(df.loc[0, 'ì´ë¦„']) # ì²« ë²ˆì§¸ í–‰ì˜ 'ì´ë¦„' ì—´ ë°ì´í„°

â€‹iloc[]ë¡œ í–‰ê³¼ ì—´ì˜ ìœ„ì¹˜ ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•´ ì ‘ê·¼í•˜ê¸°iloc[]ì€ ì •ìˆ˜ ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•´ ë°ì´í„°ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.Pythonë³µì‚¬# ì²« ë²ˆì§¸ í–‰ ì¡°íšŒ
print(df.iloc[0])
# ì²« ë²ˆì§¸ í–‰ì˜ ë‘ ë²ˆì§¸ ì—´ ë°ì´í„° ì¡°íšŒ
print(df.iloc[0, 1])

â€‹íŠ¹ì • ì—´ì´ë‚˜ í–‰ ì „ì²´ë¥¼ ì„ íƒí•˜ê¸°íŠ¹ì • ì—´ ì „ì²´ë¥¼ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Pythonë³µì‚¬print(df['ì´ë¦„']) # 'ì´ë¦„' ì—´ ì „ì²´

â€‹ì—¬ëŸ¬ ì—´ì„ ì„ íƒí•˜ê³  ì‹¶ë‹¤ë©´?Pythonë³µì‚¬print(df[['ì´ë¦„', 'ë‚˜ì´']]) # 'ì´ë¦„'ê³¼ 'ë‚˜ì´' ì—´ ì „ì²´

â€‹íŠ¹ì • í–‰ ì „ì²´ë¥¼ ì„ íƒí•˜ë ¤ë©´ loc[] ë˜ëŠ” iloc[]ì„ ì‚¬ìš©í•˜ì„¸ìš”.ìš”ì•½ğŸ“šìš”ì•½ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¨ í›„, head()ì™€ tail()ë¡œ ë°ì´í„°ì˜ ìƒìœ„ ë° í•˜ìœ„ í–‰ì„ ë¯¸ë¦¬ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.info()ì™€ describe()ë¥¼ ì‚¬ìš©í•´ ë°ì´í„°í”„ë ˆì„ì˜ êµ¬ì¡°ì™€ ì£¼ìš” í†µê³„ ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.loc[], iloc[]ì„ ì‚¬ìš©í•´ ê°œë³„ ë°ì´í„°ë‚˜ íŠ¹ì • í–‰ê³¼ ì—´ì˜ ê°’ì„ ì‰½ê²Œ ì¡°íšŒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ì´ì œ ë°ì´í„°ë¥¼ ë¯¸ë¦¬ íŒŒì•…í•˜ê³  ë¶„ì„ì„ ì‹œì‘í•´ë³´ì„¸ìš”! â€‹
[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 7ê°•. ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤ìŠµ [SCC] ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ - 2ì£¼ì°¨/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 7ê°•. ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤ìŠµ ì œì‘:[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 7ê°•. ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤ìŠµ [ìˆ˜ì—… ëª©í‘œ]ë°ì´í„° ì „ì²˜ë¦¬ ê°œë… ë° Pandasì—ì„œ ì‚¬ìš©í•˜ëŠ” APIë¥¼ ì•Œì•„ë´…ì‹œë‹¤ë°ì´í„° ì „ì²˜ë¦¬ ë°©ë²•ì„ ì•Œì•„ ë´…ì‹œë‹¤[ëª©ì°¨]01. ë°ì´í„° ì „ì²˜ë¦¬DatağŸ’¡
 
 01. ë°ì´í„° ì „ì²˜ë¦¬âœ”ï¸ë°ì´í„° ì „ì²˜ë¦¬ ë°©ë²•ì„ ì•Œì•„ ë´…ì‹œë‹¤DataPythonë³µì‚¬data = {
'A': [1, 2, np.nan, 4, 5, 100, 1, 2, 2, 4, '1', '2', '3', '4', '5', 10, 20, 30, 40, 50],
'B': [5, np.nan, np.nan, 8, 10, 60, 10, 20, 20, 40, '10', '20', '30', '40', '50', 5, 4, 3, 2, 1],
'C': [1, 2, 3, 4, 5, 5, 100, 200, 200, 400, 100, 200, 300, 400, 500, 1, 2, 3, 4, 5],
'D': [np.nan, np.nan, 3, 3, 3, 5, 5, 5, 5, 5, np.nan, np.nan, np.nan, np.nan, np.nan, 2, 3, 4, 5, 6],
'category_column': [np.nan]*10 + ['A', 'B', 'A', 'C', 'B'] + [np.nan]*5,
'value_column': [np.nan]*10 + [1, 2, 3, 4, 5] + [np.nan]*5,
'target': [np.nan]*15 + [1, 0, 1, 0, 1]
}
â€‹1)ê²°ì¸¡ê°’ ì²˜ë¦¬ ê²°ì¸¡ê°’ ì²˜ë¦¬ ë°©ë²•ì œê±°: ê²°ì¸¡ê°’ì´ í¬í•¨ëœ í–‰ ë˜ëŠ” ì—´ì„ ì œê±°í•©ë‹ˆë‹¤.ëŒ€ì²´: ê²°ì¸¡ê°’ì„ íŠ¹ì • ê°’ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.ì˜ˆì¸¡: ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ê²°ì¸¡ê°’ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.ê²°ì¸¡ê°’ ì œê±° {5px}ê²°ì¸¡ê°’ ì œê±° ï»¿â€‹Pythonë³µì‚¬# ê²°ì¸¡ê°’ì´ í¬í•¨ëœ í–‰ ì œê±°
df_dropped_rows = df.dropna()
# ê²°ì¸¡ê°’ì´ í¬í•¨ëœ ì—´ ì œê±°
df_dropped_cols = df.dropna(axis=1)
â€‹dropna()ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²°ì¸¡ê°’ ì œê±°()ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²°ì¸¡ê°’ ì œê±°ï»¿â€‹ê²°ì¸¡ê°’ ëŒ€ì²´ {5px}ê²°ì¸¡ê°’ ëŒ€ì²´ ï»¿â€‹Pythonë³µì‚¬# ê²°ì¸¡ê°’ì„ 0ìœ¼ë¡œ ëŒ€ì²´
df_filled = df.fillna(0)
# ê²°ì¸¡ê°’ì„ ê° ì—´ì˜ í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´
df_filled_mean = df.fillna(df.mean())
# ê²°ì¸¡ê°’ì„ ê° ì—´ì˜ ì¤‘ê°„ê°’ìœ¼ë¡œ ëŒ€ì²´
df_filled_median = df.fillna(df.median())
# ê²°ì¸¡ê°’ì„ ê° ì—´ì˜ ìµœë¹ˆê°’ìœ¼ë¡œ ëŒ€ì²´
df_filled_mode = df.fillna(df.mode().iloc[0])
â€‹fillna()ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²°ì¸¡ê°’ ëŒ€ì²´()ë¥¼ ì‚¬ìš©í•˜ì—¬ ê²°ì¸¡ê°’ ëŒ€ì²´ï»¿â€‹ê²°ì¸¡ê°’ ì˜ˆì¸¡ {5px}ê²°ì¸¡ê°’ ì˜ˆì¸¡ ï»¿â€‹Pythonë³µì‚¬from sklearn.linear_model import LinearRegression

# ê²°ì¸¡ê°’ì´ ìˆëŠ” ì—´ê³¼ ì—†ëŠ” ì—´ ë¶„ë¦¬
df_with_na = df[df['column_with_na'].isnull()]
df_without_na = df[df['column_with_na'].notnull()]
# íšŒê·€ ëª¨ë¸ í•™ìŠµ
model = LinearRegression()
model.fit(df_without_na[['feature1', 'feature2']], df_without_na['column_with_na'])
# ê²°ì¸¡ê°’ ì˜ˆì¸¡
predicted_values = model.predict(df_with_na[['feature1', 'feature2']])
# ì˜ˆì¸¡ëœ ê°’ìœ¼ë¡œ ê²°ì¸¡ê°’ ëŒ€ì²´
df.loc[df['column_with_na'].isnull(), 'column_with_na'] = predicted_values
â€‹2) ì´ìƒê°’ ì²˜ë¦¬ ì´ìƒì¹˜ë€?ë°ì´í„°ì…‹ì—ì„œ ë¹„ì •ìƒì ìœ¼ë¡œ í° ê°’ì´ë‚˜ ì‘ì€ ê°’ì´ìƒì¹˜ëŠ” ë¶„ì„ ê²°ê³¼ì— í° ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ì´ë¥¼ ì ì ˆíˆ ì²˜ë¦¬í•˜ëŠ” ê²ƒì´ ì¤‘ìš”ì´ìƒì¹˜ í™•ì¸ ë°©ë²• {5px}ì´ìƒì¹˜ í™•ì¸ ë°©ë²• ï»¿â€‹Pythonë³µì‚¬# íŠ¹ì • ì—´ì˜ ì´ìƒì¹˜ í™•ì¸ (IQR ë°©ë²•)
Q1 = df['column_name'].quantile(0.25)
Q3 = df['column_name'].quantile(0.75)
IQR = Q3 - Q1

# ì´ìƒì¹˜ ë²”ìœ„ ì„¤ì •
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# ì´ìƒì¹˜ í™•ì¸
outliers = df[(df['column_name'] < lower_bound) | (df['column_name'] > upper_bound)]
print(outliers)
â€‹ ì´ìƒì¹˜ ì²˜ë¦¬ ë°©ë²•ì œê±°: ì´ìƒì¹˜ë¥¼ ë°ì´í„°ì…‹ì—ì„œ ì œê±°í•©ë‹ˆë‹¤.ëŒ€ì²´: ì´ìƒì¹˜ë¥¼ íŠ¹ì • ê°’ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.ë³€í™˜: ì´ìƒì¹˜ë¥¼ ë³€í™˜í•˜ì—¬ ë°ì´í„°ì˜ ë¶„í¬ë¥¼ ì¡°ì •í•©ë‹ˆë‹¤.ì´ìƒì¹˜ ì²˜ë¦¬ ë°©ë²• {5px}ì´ìƒì¹˜ ì²˜ë¦¬ ë°©ë²• ï»¿â€‹Pythonë³µì‚¬# ì´ìƒì¹˜ ì œê±°
df_no_outliers = df[(df['column_name'] >= lower_bound) & (df['column_name'] <= upper_bound)]
# ì´ìƒì¹˜ë¥¼ í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´
mean_value = df['column_name'].mean()
df['column_name'] = df['column_name'].apply(lambda x: mean_value if x < lower_bound or x > upper_bound else x)
â€‹3) ì¤‘ë³µê°’ ì œê±° ì¤‘ë³µ ë°ì´í„° ì œê±°ì¤‘ë³µ ë°ì´í„° ì œê±° {5px}ì¤‘ë³µ ë°ì´í„° ì œê±° ï»¿â€‹Pythonë³µì‚¬# ì¤‘ë³µëœ í–‰ í™•ì¸
print(df.duplicated().sum())
# ì¤‘ë³µëœ í–‰ ì œê±°
df_no_duplicates = df.drop_duplicates()
â€‹4) ë°ì´í„° íƒ€ì… ë³€í™˜ ë°ì´í„° íƒ€ì… ë³€í™˜ì˜ í•„ìš”ì„±ì˜ëª»ëœ ë°ì´í„° íƒ€ì…ì€ ë¶„ì„ ê²°ê³¼ì— ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆìœ¼ë©°, ëª¨ë¸ í•™ìŠµì— ì˜¤ë¥˜ë¥¼ ë°œìƒì‹œí‚¬ ìˆ˜ìˆì–´ ì ì ˆí•œ ë°ì´í„° íƒ€ì… ë³€í™˜ì´ í•„ìš”í•©ë‹ˆë‹¤ë°ì´í„° íƒ€ì… ë³€í™˜ ë°©ë²• {5px}ë°ì´í„° íƒ€ì… ë³€í™˜ ë°©ë²• ï»¿â€‹Pythonë³µì‚¬# íŠ¹ì • ì—´ì˜ ë°ì´í„° íƒ€ì…ì„ ì •ìˆ˜í˜•ìœ¼ë¡œ ë³€í™˜
df['column_name'] = df['column_name'].astype(int)
# íŠ¹ì • ì—´ì˜ ë°ì´í„° íƒ€ì…ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
df['column_name'] = df['column_name'].astype(str)
# íŠ¹ì • ì—´ì˜ ë°ì´í„° íƒ€ì…ì„ ë¶€ë™ ì†Œìˆ˜ì ìœ¼ë¡œ ë³€í™˜
df['column_name'] = df['column_name'].astype(float)
â€‹Pandasì˜ astype() ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° íƒ€ì…ì„ ë³€í™˜ì˜ astype() ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° íƒ€ì…ì„ ë³€í™˜ï»¿â€‹5) ì¸ì½”ë”© ì¸ì½”ë”©ì´ë€?ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ìˆ˜ì¹˜í˜• ë°ì´í„°ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì€ ìˆ˜ì¹˜í˜• ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ê¸°ë•Œë¬¸ì—, ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ìˆ˜ì¹˜í˜•ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ê²ƒì´ í•„ìš”ì¸ì½”ë”© ë°©ë²• {5px}ì¸ì½”ë”© ë°©ë²• ï»¿â€‹Pythonë³µì‚¬# ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ë”ë¯¸ ë³€ìˆ˜ë¡œ ë³€í™˜
df_encoded = pd.get_dummies(df, columns=['category_column'])
# ê²°ê³¼ ì¶œë ¥
print(df_encoded.head())
â€‹Pandasì˜ get_dummies() ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ë”ë¯¸ ë³€ìˆ˜ë¡œ ë³€í™˜ì˜ get_dummies() ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ë”ë¯¸ ë³€ìˆ˜ë¡œ ë³€í™˜ï»¿â€‹6) ìƒ˜í”Œë§ ìƒ˜í”Œë§ì´ë€?ë°ì´í„°ì…‹ì˜ í¬ê¸°ë¥¼ ì¤„ì´ê±°ë‚˜ ëŠ˜ë¦¬ëŠ” ê³¼ì •ë°ì´í„°ì…‹ì˜ ëŒ€í‘œì„±ì„ ìœ ì§€í•˜ë©´ì„œ ë°ì´í„°ì˜ í¬ê¸°ë¥¼ ì¡°ì ˆí•˜ëŠ” ë° ì‚¬ìš©ìƒ˜í”Œë§ ë°©ë²• {5px}ìƒ˜í”Œë§ ë°©ë²• ï»¿â€‹Pythonë³µì‚¬# ë°ì´í„°ì…‹ì—ì„œ 50% ìƒ˜í”Œ ì¶”ì¶œ
df_sampled = df.sample(frac=0.5)
# ë°ì´í„°ì…‹ì—ì„œ 100ê°œì˜ ìƒ˜í”Œ ì¶”ì¶œ
df_sampled_n = df.sample(n=100)
â€‹Pandasì˜ sample() ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ì…‹ì—ì„œ ìƒ˜í”Œì„ ì¶”ì¶œì˜ sample() ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ì…‹ì—ì„œ ìƒ˜í”Œì„ ì¶”ì¶œï»¿â€‹7) íŠ¹ì§• ì„ íƒ ë° ì¶”ì¶œ íŠ¹ì§• ì„ íƒ ë° ì¶”ì¶œì´ë€?íŠ¹ì§• ì„ íƒ(Feature Selection) ë° ì¶”ì¶œ(Feature Extraction)ì€ ëª¨ë¸ ì„±ëŠ¥ì„ ë†’ì´ê¸° ìœ„í•´ ì¤‘ìš”í•œ íŠ¹ì§•ì„ ì„ íƒí•˜ê±°ë‚˜ ìƒˆë¡œìš´ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ëŠ” ê³¼ì •íŠ¹ì§• ì„ íƒ ë°©ë²• {5px}íŠ¹ì§• ì„ íƒ ë°©ë²• ï»¿â€‹Pythonë³µì‚¬from sklearn.feature_selection import SelectKBest, f_classif

# íŠ¹ì§• ì„ íƒ (ìƒìœ„ 5ê°œì˜ íŠ¹ì§• ì„ íƒ)
selector = SelectKBest(score_func=f_classif, k=5)
X_new = selector.fit_transform(X, y)
# ì„ íƒëœ íŠ¹ì§•ì˜ ì¸ë±ìŠ¤
selected_features = selector.get_support(indices=True)
print(selected_features)
â€‹Pandasì™€ Scikit-learnì„ ì‚¬ìš©í•˜ì—¬ íŠ¹ì§• ì„ íƒì„ ìˆ˜í–‰ì™€ Scikit-learnì„ ì‚¬ìš©í•˜ì—¬ íŠ¹ì§• ì„ íƒì„ ìˆ˜í–‰ï»¿â€‹íŠ¹ì§• ì¶”ì¶œ ë°©ë²• {5px}íŠ¹ì§• ì¶”ì¶œ ë°©ë²• ï»¿â€‹Pythonë³µì‚¬# ë‘ ì—´ì˜ ê³±ì„ ìƒˆë¡œìš´ íŠ¹ì§•ìœ¼ë¡œ ì¶”ê°€
df['new_feature'] = df['feature1'] * df['feature2']
# ë‘ ì—´ì˜ í•©ì„ ìƒˆë¡œìš´ íŠ¹ì§•ìœ¼ë¡œ ì¶”ê°€
df['new_feature_sum'] = df['feature1'] + df['feature2']
â€‹.
[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 7. ì–´í…ì…˜ (Attention) ë©”ì»¤ë‹ˆì¦˜[SCC] ê¸°ì´ˆê°€ íƒ„íƒ„í•œ ë”¥ëŸ¬ë‹/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] ê¸°ì´ˆê°€ íƒ„íƒ„í•œ ë”¥ëŸ¬ë‹ - 3ì£¼ì°¨/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 7. ì–´í…ì…˜ (Attention) ë©”ì»¤ë‹ˆì¦˜ì œì‘:[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 7. ì–´í…ì…˜ (Attention) ë©”ì»¤ë‹ˆì¦˜[ìˆ˜ì—… ëª©í‘œ]ìµœê·¼ ê°€ì¥ ì„±ëŠ¥ ì¢‹ì€ ë§¤ì»¤ë‹ˆì¦˜! ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì— ëŒ€í•´ ì•Œì•„ë´…ì‹œë‹¤Pytorchì˜ êµ¬í˜„ ì˜ˆì‹œë¥¼ ì‚´í´ë´…ì‹œë‹¤[ëª©ì°¨]01. ê°œë…02. ì‹¤ìŠµ:  Attention ë©”ì»¤ë‹ˆì¦˜ì˜ êµ¬í˜„ğŸ’¡
 
 01. ê°œë…âœ”ï¸ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì˜ ê¸°ë³¸ ê°œë…ê³¼ ë™ì‘ ë°©ì‹ì— ëŒ€í•´ ì•Œì•„ë´…ì‹œë‹¤1) ì–´í…ì…˜ì˜ ê¸°ë³¸ êµ¬ì„± ìš”ì†Œì™€ ë™ì‘ ë°©ì‹ Attention ë©”ì»¤ë‹ˆì¦˜ì´ë€?Attention ë©”ì»¤ë‹ˆì¦˜ì€ ì‹œí€€ìŠ¤ ë°ì´í„°ì—ì„œ ì¤‘ìš”í•œ ë¶€ë¶„ì— ë” ë§ì€ ê°€ì¤‘ì¹˜ë¥¼ í• ë‹¹í•˜ì—¬ ì •ë³´ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” ê¸°ë²•ì£¼ë¡œ ìì—°ì–´ ì²˜ë¦¬(NLP)ì™€ ì‹œê³„ì—´ ë°ì´í„°ì—ì„œ ì‚¬ìš©ë˜ë©°, ê¸°ê³„ ë²ˆì—­, ìš”ì•½, ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë°œíœ˜ ë™ì‘ ë°©ì‹ê°œìš”Attention ë©”ì»¤ë‹ˆì¦˜ì€ ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ê° ìš”ì†Œì— ëŒ€í•´ ì¤‘ìš”ë„ë¥¼ ê³„ì‚°í•˜ì—¬ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì¤‘ìš”í•œ ì •ë³´ì— ì§‘ì¤‘í•˜ê³ , ë¶ˆí•„ìš”í•œ ì •ë³´ë¥¼ ë¬´ì‹œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Attention ë©”ì»¤ë‹ˆì¦˜ì€ ì£¼ë¡œ ì„¸ ê°€ì§€ ì£¼ìš” êµ¬ì„± ìš”ì†Œë¡œ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤: Query, Key, Value.Attention ìŠ¤ì½”ì–´ ê³„ì‚°Attention ìŠ¤ì½”ì–´ëŠ” Queryì™€ Key ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ì¸¡ì •í•˜ì—¬ ì¤‘ìš”ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. ì´ ìœ ì‚¬ë„ëŠ” ë‚´ì (dot product) ë“±ì„ ì‚¬ìš©í•˜ì—¬ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.score(Q,K)=Qâ‹…KT(Q, K) = Q  K^Tscore(Q,K)=Qâ‹…KTï»¿â€‹Softmaxë¥¼ í†µí•œ ê°€ì¤‘ì¹˜ ê³„ì‚°ê³„ì‚°ëœ Attention ìŠ¤ì½”ì–´ëŠ” Softmax í•¨ìˆ˜ë¥¼ í†µí•´ í™•ë¥  ë¶„í¬ë¡œ ë³€í™˜ë©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ê°€ì¤‘ì¹˜ì˜ í•©ì´ 1ì´ ë˜ë„ë¡ í•©ë‹ˆë‹¤.Î±i=expâ¡(score(Q,Ki))âˆ‘jexpâ¡(score(Q,Kj))_i = (Q, K_i))}{_{j} ((Q, K_j))}Î±iâ€‹=âˆ‘jâ€‹exp(score(Q,Kjâ€‹))exp(score(Q,Kiâ€‹))â€‹ï»¿â€‹Softmaxë¥¼ í†µí•œ ê°€ì¤‘ì¹˜ ê³„ì‚°Softmaxë¥¼ í†µí•´ ì–»ì–´ì§„ ê°€ì¤‘ì¹˜ë¥¼ Valueì— ê³±í•˜ì—¬ ìµœì¢… Attention ì¶œë ¥ì„ ê³„ì‚°í•©ë‹ˆë‹¤.Attention(Q,K,V)=âˆ‘iÎ±iVi(Q, K, V) = _{i} _i V_iAttention(Q,K,V)=âˆ‘iâ€‹Î±iâ€‹Viâ€‹ï»¿â€‹2)  Self-Attentionê³¼ Multi-Head Attention Self-AttentionSelf-Attentionì€ ì‹œí€€ìŠ¤ ë‚´ì˜ ê° ìš”ì†Œê°€ ì„œë¡œë¥¼ ì°¸ì¡°í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜ì…ë‹ˆë‹¤. ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ëª¨ë“  ìš”ì†Œê°€ Query, Key, Valueë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ê° ìš”ì†Œê°€ ì‹œí€€ìŠ¤ ë‚´ ë‹¤ë¥¸ ìš”ì†Œë“¤ê³¼ì˜ ê´€ê³„ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ì˜ˆë¥¼ ë“¤ì–´, ë¬¸ì¥ ë‚´ì—ì„œ ë‹¨ì–´ ê°„ì˜ ê´€ê³„ë¥¼ í•™ìŠµí•˜ì—¬ ë²ˆì—­ì´ë‚˜ ìš”ì•½ì— í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Multi-Head AttentionMulti-Head Attentionì€ ì—¬ëŸ¬ ê°œì˜ Self-Attentionì„ ë³‘ë ¬ë¡œ ìˆ˜í–‰í•˜ëŠ” ë©”ì»¤ë‹ˆì¦˜ì…ë‹ˆë‹¤. ê° í—¤ë“œëŠ” ì„œë¡œ ë‹¤ë¥¸ ë¶€ë¶„ì˜ ì •ë³´ë¥¼ í•™ìŠµí•˜ë©°, ì´ë¥¼ í†µí•´ ëª¨ë¸ì´ ë‹¤ì–‘í•œ ê´€ì ì—ì„œ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.02. ì‹¤ìŠµ:  Attention ë©”ì»¤ë‹ˆì¦˜ì˜ êµ¬í˜„âœ”ï¸ ì–´í…ì…˜ì˜ êµ¬í˜„ì— ëŒ€í•œ ì½”ë“œì…ë‹ˆë‹¤! ì‚¬ì‹¤ ì–´í…ì…˜ì„ ì§ì ‘ êµ¬í˜„í•˜ëŠ” ì¼ì€ êµ‰ì¥íˆ ì ì–´ìš”.
í•œë²ˆ í›‘ëŠ” ì •ë„ë¡œ ë„˜ì–´ê°‘ì‹œë‹¤!1)  Attention Scaled Dot-Product AttentionScaled Dot-Product attention ë©”ì»¤ë‹ˆì¦˜ êµ¬í˜„{5px}Scaled Dot-Product attention ë©”ì»¤ë‹ˆì¦˜ êµ¬í˜„ï»¿â€‹Pythonë³µì‚¬import torch
import torch.nn.functional as F

def scaled_dot_product_attention(Q, K, V):
    d_k = Q.size(-1) # Keyì˜ ì°¨ì› ìˆ˜
    scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32)) # ìœ ì‚¬ë„ ê³„ì‚° ë° ìŠ¤ì¼€ì¼ë§
    attn_weights = F.softmax(scores, dim=-1) # Softmaxë¥¼ í†µí•œ ê°€ì¤‘ì¹˜ ê³„ì‚°
    output = torch.matmul(attn_weights, V) # ê°€ì¤‘í•©ì„ í†µí•œ ìµœì¢… ì¶œë ¥ ê³„ì‚°
return output, attn_weights

â€‹ Multi-Head Attention Multi-Head Attention ë©”ì»¤ë‹ˆì¦˜ êµ¬í˜„{5px} Multi-Head Attention ë©”ì»¤ë‹ˆì¦˜ êµ¬í˜„ï»¿â€‹Pythonë³µì‚¬class MultiHeadAttention(nn.Module):
def __init__(self, embed_size, heads):
super(MultiHeadAttention, self).__init__()
        self.embed_size = embed_size
        self.heads = heads
        self.head_dim = embed_size // heads

        assert (
            self.head_dim * heads == embed_size
        ), "Embedding size needs to be divisible by heads"

        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)
        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)
        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)
        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)
def forward(self, values, keys, query, mask=None):
        N = query.shape[0]
        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]
# Linear transformations
        values = self.values(values).view(N, value_len, self.heads, self.head_dim)
        keys = self.keys(keys).view(N, key_len, self.heads, self.head_dim)
        queries = self.queries(query).view(N, query_len, self.heads, self.head_dim)
# Scaled dot-product attention
        out, _ = scaled_dot_product_attention(queries, keys, values)

        out = out.view(N, query_len, self.heads * self.head_dim)
        out = self.fc_out(out)
return out

â€‹.
ë¬¸ì„œ ì„ë² ë”© ì‹¤ìŠµí•˜ê¸° LLM & RAGë¥¼ í™œìš©í•œ AI ì„œë¹„ìŠ¤ ë§Œë“¤ê¸°/ LLM & RAGë¥¼ í™œìš©í•œ AI ì„œë¹„ìŠ¤ ë§Œë“¤ê¸° - 5ì£¼ì°¨/ë¬¸ì„œ ì„ë² ë”© ì‹¤ìŠµí•˜ê¸°ì œì‘:ë¬¸ì„œ ì„ë² ë”© ì‹¤ìŠµí•˜ê¸°ìˆ˜ì—… ëª©í‘œë¬¸ì„œ ì„ë² ë”©ì˜ ê¸°ë³¸ ê°œë…ê³¼ Faissë¡œ ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹¤ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤.ëª©ì°¨ì‹¤ìŠµ ì¤€ë¹„ë¬¼ ë¬¸ì„œ ì„ë² ë”©ì˜ ê¸°ë³¸ ê°œë…ë¬¸ì„œ ë‹¨ìœ„ ì„ë² ë”© ì „ì²´ ë¬¸ì„œ ì„ë² ë”©í•˜ê¸°ë¬¸ì¥ ë‹¨ìœ„ ì„ë² ë”©í•˜ê¸°ë¬¸ë‹¨ ë‹¨ìœ„ ì„ë² ë”©í•˜ê¸°Faissë¡œ ìœ ì‚¬ë„ ê²€ìƒ‰í•˜ê¸°Faiss ì´ˆê¸°í™” ë° ì¸ë±ì‹±ìœ ì‚¬ ë¬¸ì¥ ê²€ìƒ‰í•˜ê¸°ìœ ì‚¬ë„ì— ëŒ€í•œ ì‹¤í—˜ê¸¸ì´ ë‹¨ìœ„ ì„ íƒ ê°€ì´ë“œì¶”ê°€ ì‹¤í—˜: ì„ë² ë”© ë²¡í„° ì‹œê°í™”í•˜ê¸°ì¶”ê°€ ì‹¤í—˜: t-SNEë¥¼ ì´ìš©í•œ ê³ ì°¨ì› ì„ë² ë”© ì‹œê°í™”ğŸ’¡ì—¬ëŸ¬ë¶„ ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ì€ input.txtë¼ëŠ” txt íŒŒì¼ì„ ê°€ì§€ê³  ë¬¸ì„œ ì„ë² ë”©ì„ ì‹¤ìŠµí•´ë³´ë ¤ê³  í•©ë‹ˆë‹¤.  íŠ¹íˆ Faissë¥¼ í™œìš©í•´ ìœ ì‚¬ ë‹¨ì–´ ê²€ìƒ‰ê¹Œì§€ í•´ë³¼ ê±°ì˜ˆìš”! ë¬¸ì„œë¥¼ ì„ë² ë”©í•˜ëŠ” ì—¬ëŸ¬ ë°©ë²•ì— ëŒ€í•´ì„œë„ ì‹¤í—˜í•´ë³¼ í…Œë‹ˆ ê¸°ëŒ€í•´ ì£¼ì„¸ìš”. â€‹ì‹¤ìŠµ ì¤€ë¹„ë¬¼ Python ì½”ë“œ ì‹¤í–‰ í™˜ê²½ (Jupyter Notebook ë˜ëŠ” IDE ì¶”ì²œ)input.txt íŒŒì¼ (ì´ íŒŒì¼ì—ëŠ” ìš°ë¦¬ê°€ ë¶„ì„í•  í…ìŠ¤íŠ¸ ë°ì´í„°ê°€ ë“¤ì–´ìˆì–´ìš”!)sentence-transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜faiss ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜Plain Textë³µì‚¬pip install sentence-transformers faiss-cpuâ€‹ë¬¸ì„œ ì„ë² ë”©ì˜ ê¸°ë³¸ ê°œë…â—ë¬¸ì„œë¥¼ ì„ë² ë”©í•  ë•Œ, í…ìŠ¤íŠ¸ë¥¼ ë²¡í„° í˜•íƒœë¡œ ë³€í™˜í•˜ì—¬ ì»´í“¨í„°ê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ ë§Œë“¤ì–´ìš”. ë³€í™˜ëœ ë²¡í„°ë“¤ì€ ë¬¸ì„œ ê°„ì˜ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ê±°ë‚˜, ê²€ìƒ‰ì„ ìœ„í•´ í™œìš©ë©ë‹ˆë‹¤. ì´ë²ˆ ì‹¤ìŠµì—ì„œëŠ” ìµœì‹  í•œêµ­ì–´ ëª¨ë¸ì„ ì‚¬ìš©í•´ ë¬¸ì„œë¥¼ ì„ë² ë”©í•  ê±°ì˜ˆìš”! ìµœì‹  ëª¨ë¸ì€ sentence-transformersë¥¼ í†µí•´ ë¶ˆëŸ¬ì˜¬ ì˜ˆì •ì…ë‹ˆë‹¤. â€‹ì°¸ê³ : ì„ë² ë”©ì´ë€ í…ìŠ¤íŠ¸, ì´ë¯¸ì§€ ë“± ë‹¤ì–‘í•œ ë°ì´í„°ë¥¼ ê³ ì°¨ì›ì˜ ë²¡í„° ê³µê°„ì— í‘œí˜„í•˜ëŠ” ê²ƒì„ ë§í•©ë‹ˆë‹¤. 
ì´ë ‡ê²Œ í‘œí˜„ëœ ë²¡í„°ë“¤ì€ ê¸°ê³„ í•™ìŠµ ëª¨ë¸ë“¤ì´ ë°ì´í„°ë¥¼ ë” ì‰½ê²Œ ì´í•´í•˜ê³  ë¶„ì„í•  ìˆ˜ ìˆê²Œ ë„ì™€ì¤˜ìš”!ë¬¸ì„œ ë‹¨ìœ„ ì„ë² ë”© ğŸ’¡ë¬¸ì„œë¥¼ ì„ë² ë”©í•  ë•Œ ì¤‘ìš”í•œ ê²°ì • ì¤‘ í•˜ë‚˜ëŠ” ì„ë² ë”©í•  í…ìŠ¤íŠ¸ì˜ ê¸¸ì´ë¥¼ ì–´ë–»ê²Œ ì„¤ì •í• ì§€ì…ë‹ˆë‹¤. í•œ ë²ˆ ì „ì²´ ë¬¸ì„œë¥¼ í†µì§¸ë¡œ ì„ë² ë”©í•  ìˆ˜ë„ ìˆê³ , ë¬¸ì¥ì„ ë‹¨ìœ„ë¡œ í•˜ê±°ë‚˜ ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ìª¼ê°œì„œ ì„ë² ë”©í•  ìˆ˜ë„ ìˆì–´ìš”. ê° ë°©ë²•ì˜ ì¥ë‹¨ì ì„ ë¹„êµí•´ ë³´ì£ !ì „ì²´ ë¬¸ì„œ ì„ë² ë”©í•˜ê¸°ë¨¼ì € ì „ì²´ ë¬¸ì„œë¥¼ í•˜ë‚˜ì˜ ë²¡í„°ë¡œ ì„ë² ë”©í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.
ì´ ë°©ë²•ì€ ë¬¸ì„œ ì „ì²´ì˜ ë§¥ë½ì„ ì´í•´í•˜ëŠ” ë° ìœ ë¦¬í•˜ì§€ë§Œ, ê¸¸ì´ê°€ ë„ˆë¬´ ê¸¸ ê²½ìš° ì¤‘ìš”í•œ ì •ë³´ê°€ ì†ì‹¤ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Pythonë³µì‚¬from sentence_transformers import SentenceTransformer
import numpy as np

# ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
model = SentenceTransformer('sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens')
# input.txt íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°
with open('input.txt', 'r', encoding='utf-8') as file:
    document = file.read()
# ì „ì²´ ë¬¸ì„œ ì„ë² ë”©
doc_embedding = model.encode(document)
â€‹ì‹¤ìŠµ ì‹¤í—˜: ì „ì²´ ë¬¸ì„œ ì„ë² ë”©ì˜ ì„±ëŠ¥ì„ í™•ì¸í•˜ê¸° ìœ„í•´ doc_embeddingì„ ì‚¬ìš©í•´ ë¬¸ì„œ ì „ì²´ì˜ ìš”ì•½ë³¸ì„ ë‹¤ë¥¸ ë¬¸ì„œì™€ ë¹„êµí•´ ë³´ì„¸ìš”. ì „ì²´ ë¬¸ì„œë¥¼ í•˜ë‚˜ì˜ ë²¡í„°ë¡œ í‘œí˜„í•˜ëŠ” ê²ƒì´ ì‹¤ì œ ë¬¸ì„œì˜ ì˜ë¯¸ë¥¼ ì–¼ë§ˆë‚˜ ì˜ ë°˜ì˜í•˜ëŠ”ì§€ í™•ì¸í•  ìˆ˜ ìˆì–´ìš”.ë¬¸ì¥ ë‹¨ìœ„ ì„ë² ë”©í•˜ê¸°ë¬¸ì„œë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ì–´ ì„ë² ë”©í•´ ë³¼ê¹Œìš”? ì´ë ‡ê²Œ í•˜ë©´ ë¬¸ì¥ë³„ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ê¸°ì— ì¢‹ìŠµë‹ˆë‹¤. íŠ¹íˆ ê²€ìƒ‰ ì‹œ íŠ¹ì • ë¬¸ì¥ê³¼ ìœ ì‚¬í•œ ë¬¸ì¥ì„ ì°¾ëŠ” ë° ìœ ìš©í•´ìš”.Pythonë³µì‚¬from nltk.tokenize import sent_tokenize

# ë¬¸ì¥ìœ¼ë¡œ ë‚˜ëˆ„ê¸°
sentences = sent_tokenize(document)
# ê° ë¬¸ì¥ì„ ì„ë² ë”©
sentence_embeddings = model.encode(sentences)
â€‹ì‹¤ìŠµ ì‹¤í—˜: ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì„ë² ë”©í•œ í›„, íŠ¹ì • ë¬¸ì¥ì„ ì…ë ¥í•´ ìœ ì‚¬ ë¬¸ì¥ì„ ì°¾ì•„ë³´ì„¸ìš”. ì˜ˆë¥¼ ë“¤ì–´, "ì´ ë¬¸ì„œì˜ í•µì‹¬ì€ ë¬´ì—‡ì¸ê°€ìš”?" ê°™ì€ ì§ˆë¬¸ì„ ë˜ì§€ê³  ê·¸ì™€ ìœ ì‚¬í•œ ë¬¸ì¥ì„ ê²€ìƒ‰í•´ ë´…ì‹œë‹¤.ë¬¸ë‹¨ ë‹¨ìœ„ ì„ë² ë”©í•˜ê¸°ë¬¸ì„œë¥¼ ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ìª¼ê°œ ì„ë² ë”©í•˜ëŠ” ë°©ë²•ë„ ìˆì–´ìš”. ì´ ë°©ë²•ì€ ë¬¸ì„œ ë‚´ ì£¼ì œë³„ ì •ë³´ë¥¼ ë¶„ì„í•  ë•Œ ìœ ë¦¬í•©ë‹ˆë‹¤.Pythonë³µì‚¬paragraphs = document.split('') # ë‘ ì¤„ ê°œí–‰ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ë‹¨ ë‚˜ëˆ„ê¸°
# ê° ë¬¸ë‹¨ì„ ì„ë² ë”©
paragraph_embeddings = model.encode(paragraphs)
â€‹ì‹¤ìŠµ ì‹¤í—˜: ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ ë‚˜ëˆˆ í›„ ê° ë¬¸ë‹¨ì˜ ë²¡í„°ë¥¼ ì‹œê°í™”í•´ì„œ ë¬¸ë‹¨ë³„ ìœ ì‚¬ë„ë¥¼ ë¹„êµí•´ ë³´ì„¸ìš”. ì´ ê³¼ì •ì—ì„œ PCA ë˜ëŠ” t-SNE ê°™ì€ ì°¨ì› ì¶•ì†Œ ë°©ë²•ì„ ì‚¬ìš©í•˜ë©´ ë²¡í„° ê°„ì˜ ê´€ê³„ë¥¼ ì‰½ê²Œ í™•ì¸í•  ìˆ˜ ìˆì–´ìš”!Faissë¡œ ìœ ì‚¬ë„ ê²€ìƒ‰í•˜ê¸°ğŸ’¡ì„ë² ë”©ì´ ëë‚¬ë‹¤ë©´, ì´ì œ Faissë¥¼ ì‚¬ìš©í•´ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•´ ë´…ì‹œë‹¤. FaissëŠ” ë²¡í„° ê°„ ìœ ì‚¬ë„ë¥¼ ë¹ ë¥´ê²Œ ê³„ì‚°í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.Faiss ì´ˆê¸°í™” ë° ì¸ë±ì‹±ë¨¼ì €, ì„ë² ë”©ëœ ë²¡í„°ë“¤ì„ ì¸ë±ì‹±í•´ì„œ ê²€ìƒ‰í•  ì¤€ë¹„ë¥¼ í•´ë³¼ê²Œìš”!Pythonë³µì‚¬import faiss

# ë¬¸ì¥ ì„ë² ë”©ì„ ì‚¬ìš©í•´ ì¸ë±ìŠ¤ ìƒì„±
dimension = sentence_embeddings.shape[1]
index = faiss.IndexFlatL2(dimension) # L2 ê±°ë¦¬ ê¸°ë°˜ ì¸ë±ìŠ¤
# ì¸ë±ìŠ¤ì— ë²¡í„° ì¶”ê°€
index.add(np.array(sentence_embeddings))
â€‹ìœ ì‚¬ ë¬¸ì¥ ê²€ìƒ‰í•˜ê¸°íŠ¹ì • ë¬¸ì¥ê³¼ ìœ ì‚¬í•œ ë¬¸ì¥ì„ ì°¾ì•„ë´…ì‹œë‹¤! ê²€ìƒ‰í•˜ê³  ì‹¶ì€ ë¬¸ì¥ì„ ì…ë ¥í•˜ê³ , ì¸ë±ìŠ¤ë¥¼ í™œìš©í•´ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.Pythonë³µì‚¬query = "ê²€ìƒ‰í•  ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš”."
query_vec = model.encode(query)
# ìœ ì‚¬í•œ ë¬¸ì¥ 3ê°œ ì°¾ê¸°
D, I = index.search(np.array([query_vec]), k=3)
# ê²°ê³¼ ì¶œë ¥
for idx in I[0]:
print(f"ìœ ì‚¬ ë¬¸ì¥: {sentences[idx]}")
â€‹ì‹¤ìŠµ ì‹¤í—˜: ë‹¤ë¥¸ ë¬¸ì¥ë“¤ì„ ì…ë ¥í•´ì„œ ì–´ë–¤ ë¬¸ì¥ì´ ê°€ì¥ ìœ ì‚¬í•˜ê²Œ ê²€ìƒ‰ë˜ëŠ”ì§€ ì‹¤í—˜í•´ ë³´ì„¸ìš”. ì˜ˆë¥¼ ë“¤ì–´, "ì˜¤ëŠ˜ ë‚ ì”¨ëŠ” ì–´ë•Œ?"ì™€ ê°™ì€ ë¬¸ì¥ì„ ë„£ì–´ë³´ë©´ ìœ ì‚¬í•œ ì£¼ì œë¥¼ ê°€ì§„ ë¬¸ì¥ë“¤ì´ ì˜ ê²€ìƒ‰ë˜ëŠ”ì§€ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ìœ ì‚¬ë„ì— ëŒ€í•œ ì‹¤í—˜ìœ ì‚¬ë„ ê²€ìƒ‰ ê²°ê³¼ì˜ ì •í™•ë„ë¥¼ í‰ê°€í•´ë´…ì‹œë‹¤. ë‹¤ìŒê³¼ ê°™ì€ ì‹¤í—˜ì„ í†µí•´ ìœ ì‚¬ë„ ê²°ê³¼ë¥¼ ë¶„ì„í•´ ë³¼ ìˆ˜ ìˆì–´ìš”

1ï¸âƒ£ë‹¤ì–‘í•œ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸
ë‹¤ì–‘í•œ ì¿¼ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ìœ ì‚¬ ë¬¸ì¥ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë¹„êµí•´ ë³´ì„¸ìš”. ì˜ˆë¥¼ ë“¤ì–´, ê°„ë‹¨í•œ ë¬¸ì¥ê³¼ ë³µì¡í•œ ë¬¸ì¥ì„ ê°ê° ì…ë ¥í•´ ë³´ê³  ê²°ê³¼ê°€ ì–´ë–»ê²Œ ë‹¬ë¼ì§€ëŠ”ì§€ ê´€ì°°í•©ë‹ˆë‹¤.2ï¸âƒ£ìœ ì‚¬ë„ ì ìˆ˜ ë¶„ì„
ê²€ìƒ‰ ê²°ê³¼ë¡œ ë°˜í™˜ëœ ìœ ì‚¬ë„ ì ìˆ˜(D)ë¥¼ ë¶„ì„í•˜ì—¬, ë†’ì€ ìœ ì‚¬ë„ì™€ ë‚®ì€ ìœ ì‚¬ë„ ê°„ì˜ ì°¨ì´ë¥¼ í™•ì¸í•´ ë³´ì„¸ìš”. ì˜ˆë¥¼ ë“¤ì–´, D ê°’ì„ ì¶œë ¥í•˜ì—¬ ì–¼ë§ˆë‚˜ ìœ ì‚¬í•œì§€ ì •ëŸ‰ì ìœ¼ë¡œ í‰ê°€í•´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.Pythonë³µì‚¬# ìœ ì‚¬ë„ ì ìˆ˜ì™€ í•¨ê»˜ ê²°ê³¼ ì¶œë ¥
for idx, score in zip(I[0], D[0]):
print(f"ìœ ì‚¬ ë¬¸ì¥: {sentences[idx]}, ìœ ì‚¬ë„ ì ìˆ˜: {score}")
â€‹ì‹¤ìŠµ ì‹¤í—˜: ìœ ì‚¬ë„ ì ìˆ˜ê°€ ë†’ì€ ë¬¸ì¥ê³¼ ë‚®ì€ ë¬¸ì¥ì„ ë¹„êµí•´ ë³´ê³ , ê·¸ ì°¨ì´ê°€ ë¬¸ì¥ì˜ ì˜ë¯¸ë‚˜ í‘œí˜„ ë°©ì‹ì—ì„œ ì–´ë–»ê²Œ ë“œëŸ¬ë‚˜ëŠ”ì§€ ë¶„ì„í•´ ë³´ì„¸ìš”. ì´ë ‡ê²Œ í•˜ë©´ ëª¨ë¸ì´ ìœ ì‚¬ë„ë¥¼ ì–´ë–»ê²Œ íŒë‹¨í•˜ëŠ”ì§€ ë” ê¹Šì´ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ê¸¸ì´ ë‹¨ìœ„ ì„ íƒ ê°€ì´ë“œì „ì²´ ë¬¸ì„œ ì„ë² ë”©: ë¬¸ì„œ ì „ì²´ ë§¥ë½ì„ ë¶„ì„í•˜ê³  ì‹¶ì„ ë•Œ.ë¬¸ì¥ ë‹¨ìœ„ ì„ë² ë”©: íŠ¹ì • ë¬¸ì¥ê³¼ì˜ ìœ ì‚¬ë„ ê³„ì‚°ì´ë‚˜ ì„¸ë°€í•œ ê²€ìƒ‰ì´ í•„ìš”í•  ë•Œ.ë¬¸ë‹¨ ë‹¨ìœ„ ì„ë² ë”©: ì£¼ì œë³„ ìœ ì‚¬ë„ë¥¼ ë¶„ì„í•˜ê³  ì‹¶ì„ ë•Œ.Tip: ê° ë‹¨ìœ„ë¡œ ì„ë² ë”©í–ˆì„ ë•Œì˜ ì°¨ì´ë¥¼ ë¹„êµí•´ ë³´ëŠ” ê²ƒë„ ì¢‹ì€ ì‹¤ìŠµì´ ë  ìˆ˜ ìˆì–´ìš”. ì˜ˆë¥¼ ë“¤ì–´, ë¬¸ì¥ ë‹¨ìœ„ë¡œ í–ˆì„ ë•Œì™€ ë¬¸ë‹¨ ë‹¨ìœ„ë¡œ í–ˆì„ ë•Œì˜ ê²€ìƒ‰ ê²°ê³¼ê°€ ì–´ë–»ê²Œ ë‹¬ë¼ì§€ëŠ”ì§€ í™•ì¸í•´ ë³´ì„¸ìš”. â€‹ì¶”ê°€ ì‹¤í—˜: ì„ë² ë”© ë²¡í„° ì‹œê°í™”í•˜ê¸°â—ì„ë² ë”©ëœ ë²¡í„°ë“¤ì€ ê³ ì°¨ì› ê³µê°„ì— ìœ„ì¹˜í•˜ê²Œ ë©ë‹ˆë‹¤. ì´ë¥¼ ì‹œê°í™”í•˜ë©´ ë²¡í„° ê°„ì˜ ê´€ê³„ë¥¼ ë” ì§ê´€ì ìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆì–´ìš”. PCAë‚˜ t-SNE ê°™ì€ ì°¨ì› ì¶•ì†Œ ê¸°ë²•ì„ ì‚¬ìš©í•´ 2Dë‚˜ 3Dë¡œ ì‹œê°í™”í•´ ë´…ì‹œë‹¤!Pythonë³µì‚¬from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# ë¬¸ì¥ ì„ë² ë”© ë²¡í„°ì˜ ì°¨ì›ì„ ì¶•ì†Œí•˜ì—¬ ì‹œê°í™”
pca = PCA(n_components=2)
reduced_embeddings = pca.fit_transform(sentence_embeddings)
# ì‹œê°í™”
plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1])
plt.title('Sentence Embeddings Visualization')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.show()
â€‹ì‹¤ìŠµ ì‹¤í—˜: ê° ìƒ‰ê¹”ë¡œ ë‹¤ë¥¸ ë¬¸ë‹¨ì´ë‚˜ ë¬¸ì„œì˜ ë²¡í„°ë¥¼ ì‹œê°í™”í•´ì„œ ë²¡í„°ë“¤ì´ ì–´ë–»ê²Œ êµ°ì§‘ë˜ëŠ”ì§€ í™•ì¸í•´ ë³´ì„¸ìš”. ì´ëŠ” ìœ ì‚¬í•œ ë¬¸ì¥ë“¤ì´ ë²¡í„° ê³µê°„ì—ì„œ ì–¼ë§ˆë‚˜ ê°€ê¹ê²Œ ìœ„ì¹˜í•˜ëŠ”ì§€ ì§ê´€ì ìœ¼ë¡œ ì´í•´í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤. â€‹ì¶”ê°€ ì‹¤í—˜: t-SNEë¥¼ ì´ìš©í•œ ê³ ì°¨ì› ì„ë² ë”© ì‹œê°í™”â—t-SNEëŠ” ê³ ì°¨ì› ë°ì´í„°ë¥¼ ì €ì°¨ì›ìœ¼ë¡œ ì‹œê°í™”í•˜ëŠ” ë° ìœ ìš©í•œ ê¸°ë²•ì´ì—ìš”. ì„ë² ë”©ëœ ë²¡í„°ë“¤ì„ t-SNEë¥¼ ì‚¬ìš©í•´ ì‹œê°í™”í•´ ë³´ë©´, ë²¡í„° ê°„ì˜ ê´€ê³„ë¥¼ ë” ëª…í™•í•˜ê²Œ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Pythonë³µì‚¬from sklearn.manifold import TSNE

# t-SNEë¡œ ì°¨ì› ì¶•ì†Œ
tsne = TSNE(n_components=2, perplexity=30, n_iter=300)
tsne_results = tsne.fit_transform(sentence_embeddings)
# ì‹œê°í™”
plt.scatter(tsne_results[:, 0], tsne_results[:, 1])
plt.title('t-SNE Sentence Embeddings Visualization')
plt.xlabel('t-SNE Component 1')
plt.ylabel('t-SNE Component 2')
plt.show()
â€‹Tip: t-SNEëŠ” ê³„ì‚° ë¹„ìš©ì´ ë§ì´ ë“¤ê¸° ë•Œë¬¸ì—, ë°ì´í„°ì˜ í¬ê¸°ê°€ í´ ê²½ìš° ì¼ë¶€ë§Œ ìƒ˜í”Œë§í•´ì„œ ì‹œê°í™”í•´ ë³´ì„¸ìš”. t-SNEëŠ” íŠ¹íˆ ë°ì´í„° ê°„ì˜ ì§€ì—­ì  êµ¬ì¡°ë¥¼ ì˜ ë“œëŸ¬ë‚´ëŠ” ë° ìœ ë¦¬í•´ìš”.
ë°ì´í„°ë¥¼ ì„ íƒí•˜ëŠ” ê¸°ë³¸ ë°©ë²•ì¸ê³µì§€ëŠ¥ì„ ìœ„í•œ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬/ì¸ê³µì§€ëŠ¥ì„ ìœ„í•œ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ - 3ì£¼ì°¨/ë°ì´í„°ë¥¼ ì„ íƒí•˜ëŠ” ê¸°ë³¸ ë°©ë²•ì œì‘:ë°ì´í„°ë¥¼ ì„ íƒí•˜ëŠ” ê¸°ë³¸ ë°©ë²•ìˆ˜ì—… ëª©í‘œíŒë‹¤ìŠ¤ì—ì„œ ì¸ë±ìŠ¤, ì—´, í–‰ì„ ì§€ì •í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ì„œ í•™ìŠµí•©ë‹ˆë‹¤.ëª©ì°¨ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•œ ë°ì´í„° ì„ íƒì—´(Column) ì§€ì •í•˜ì—¬ ë°ì´í„° ì„ íƒí•˜ê¸°í–‰(Row) ì§€ì •í•˜ì—¬ ë°ì´í„° ì„ íƒí•˜ê¸°ìš”ì•½ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•œ ë°ì´í„° ì„ íƒë°ì´í„°í”„ë ˆì„ì—ì„œ ì¸ë±ìŠ¤ëŠ” ê° í–‰ì„ ê³ ìœ í•˜ê²Œ ì‹ë³„í•  ìˆ˜ ìˆëŠ” ë¼ë²¨ì…ë‹ˆë‹¤. ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•´ íŠ¹ì • í–‰ì´ë‚˜ ê·¸ë£¹ì„ ì‰½ê²Œ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.set_index()ë¡œ ì¸ë±ìŠ¤ ì„¤ì •í•˜ê¸°íŠ¹ì • ì—´ì„ ì¸ë±ìŠ¤ë¡œ ì„¤ì •í•˜ë©´, ë°ì´í„° ì„ íƒì´ ë” ì§ê´€ì ì´ ë©ë‹ˆë‹¤.Pythonë³µì‚¬import pandas as pd

# ì˜ˆì‹œ ë°ì´í„°í”„ë ˆì„ ìƒì„±
data = {
'ì´ë¦„': ['ì² ìˆ˜', 'ì˜í¬', 'ë¯¼ìˆ˜'],
'ë‚˜ì´': [25, 30, 35],
'ì§ì—…': ['í•™ìƒ', 'íšŒì‚¬ì›', 'í”„ë¦¬ëœì„œ']
}
df = pd.DataFrame(data)
# 'ì´ë¦„' ì—´ì„ ì¸ë±ìŠ¤ë¡œ ì„¤ì •
df = df.set_index('ì´ë¦„')
print(df)

â€‹ê²°ê³¼:Plain Textë³µì‚¬      ë‚˜ì´       ì§ì—…
ì´ë¦„
ì² ìˆ˜   25      í•™ìƒ
ì˜í¬   30    íšŒì‚¬ì›
ë¯¼ìˆ˜   35  í”„ë¦¬ëœì„œ

â€‹loc[]ì„ ì‚¬ìš©í•´ ì¸ë±ìŠ¤ë¡œ ë°ì´í„° ì„ íƒì¸ë±ìŠ¤ë¥¼ ì„¤ì •í•œ í›„, loc[]ì„ ì‚¬ìš©í•´ ì‰½ê²Œ ë°ì´í„°ì— ì ‘ê·¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Pythonë³µì‚¬# 'ì˜í¬'ì˜ ë°ì´í„° ì„ íƒ
print(df.loc['ì˜í¬'])

â€‹ê²°ê³¼:Plain Textë³µì‚¬ë‚˜ì´       30
ì§ì—…    íšŒì‚¬ì›
Name: ì˜í¬, dtype: objectâ€‹ì—´(Column) ì§€ì •í•˜ì—¬ ë°ì´í„° ì„ íƒí•˜ê¸°ë°ì´í„°í”„ë ˆì„ì˜ ì—´ì„ ì§€ì •í•˜ì—¬ íŠ¹ì • ë°ì´í„°ì— ì ‘ê·¼í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.ë‹¨ì¼ ì—´ ì„ íƒì—´ ì´ë¦„ì„ ì‚¬ìš©í•´ ë‹¨ì¼ ì—´ì„ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Pythonë³µì‚¬# 'ë‚˜ì´' ì—´ ì„ íƒ
print(df['ë‚˜ì´'])

â€‹ê²°ê³¼:Plain Textë³µì‚¬ì´ë¦„
ì² ìˆ˜    25
ì˜í¬    30
ë¯¼ìˆ˜    35
Name: ë‚˜ì´, dtype: int64â€‹ì—¬ëŸ¬ ì—´ ì„ íƒì—¬ëŸ¬ ì—´ì„ ì„ íƒí•˜ê³  ì‹¶ë‹¤ë©´, ì—´ ì´ë¦„ì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.Pythonë³µì‚¬# 'ë‚˜ì´'ì™€ 'ì§ì—…' ì—´ ì„ íƒ
print(df[['ë‚˜ì´', 'ì§ì—…']])

â€‹ê²°ê³¼:Plain Textë³µì‚¬      ë‚˜ì´       ì§ì—…
ì´ë¦„
ì² ìˆ˜   25      í•™ìƒ
ì˜í¬   30    íšŒì‚¬ì›
ë¯¼ìˆ˜   35  í”„ë¦¬ëœì„œ

â€‹í–‰(Row) ì§€ì •í•˜ì—¬ ë°ì´í„° ì„ íƒí•˜ê¸°í–‰ì„ ì§€ì •í•˜ì—¬ ë°ì´í„°í”„ë ˆì„ì˜ íŠ¹ì • ë¶€ë¶„ì„ ì„ íƒí•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.iloc[]ì„ ì‚¬ìš©í•œ í–‰ ì„ íƒiloc[]ì„ ì‚¬ìš©í•˜ë©´ ì •ìˆ˜ ì¸ë±ìŠ¤ë¡œ í–‰ì„ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Pythonë³µì‚¬# ì²« ë²ˆì§¸ í–‰ ì„ íƒ
print(df.iloc[0])

â€‹ê²°ê³¼:Plain Textë³µì‚¬ë‚˜ì´     25
ì§ì—…     í•™ìƒ
Name: ì² ìˆ˜, dtype: object

â€‹loc[]ì„ ì‚¬ìš©í•œ íŠ¹ì • í–‰ ì„ íƒloc[]ì„ ì‚¬ìš©í•´ ì¸ë±ìŠ¤ ë¼ë²¨ë¡œ íŠ¹ì • í–‰ì„ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Pythonë³µì‚¬# 'ë¯¼ìˆ˜'ì˜ ë°ì´í„° ì„ íƒ
print(df.loc['ë¯¼ìˆ˜'])

â€‹ê²°ê³¼:Plain Textë³µì‚¬ë‚˜ì´       35
ì§ì—…    í”„ë¦¬ëœì„œ
Name: ë¯¼ìˆ˜, dtype: object

â€‹ì—¬ëŸ¬ í–‰ ì„ íƒiloc[]ì´ë‚˜ loc[]ì„ ì‚¬ìš©í•´ ì—¬ëŸ¬ í–‰ì„ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Pythonë³µì‚¬# ì²« ë²ˆì§¸ì™€ ë‘ ë²ˆì§¸ í–‰ ì„ íƒ
print(df.iloc[0:2])

â€‹ê²°ê³¼:Plain Textë³µì‚¬      ë‚˜ì´    ì§ì—…
ì´ë¦„
ì² ìˆ˜   25   í•™ìƒ
ì˜í¬   30  íšŒì‚¬ì›

â€‹ìš”ì•½ìš”ì•½ì¸ë±ìŠ¤ëŠ” ë°ì´í„°í”„ë ˆì„ì˜ íŠ¹ì • í–‰ì„ ê³ ìœ í•˜ê²Œ ì‹ë³„í•˜ëŠ” ë° ì‚¬ìš©ë˜ë©°, set_index()ë¡œ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.loc[]ì€ ì¸ë±ìŠ¤ì™€ ì—´ ë¼ë²¨ì„ ì‚¬ìš©í•´ ë°ì´í„°ì— ì ‘ê·¼í•˜ë©°, iloc[]ì€ ì •ìˆ˜ ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.ë‹¨ì¼ ì—´ ë˜ëŠ” ì—¬ëŸ¬ ì—´ì„ ì„ íƒí•˜ì—¬ íŠ¹ì • ë°ì´í„°ë¥¼ ì¶”ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.í–‰ê³¼ ì—´ì„ í•¨ê»˜ ì§€ì •í•´ ì •êµí•œ ë°ì´í„° ì„ íƒì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.ì´ì œ ë°ì´í„°í”„ë ˆì„ì—ì„œ ì›í•˜ëŠ” ë°ì´í„°ë¥¼ ììœ ë¡­ê²Œ ì„ íƒí•´ë³´ì„¸ìš”! â€‹
[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 8ê°•. ì§€ë„í•™ìŠµ : íšŒê·€ëª¨ë¸ [SCC] ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ - 3ì£¼ì°¨/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 8ê°•. ì§€ë„í•™ìŠµ : íšŒê·€ëª¨ë¸ ì œì‘:[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 8ê°•. ì§€ë„í•™ìŠµ : íšŒê·€ëª¨ë¸ [ìˆ˜ì—… ëª©í‘œ]ë‹¤ì–‘í•œ íšŒê·€ ëª¨ë¸ì— ëŒ€í•´ì„œ ë°°ì›Œë´…ì‹œë‹¤íšŒê·€(Regression)ëª¨ë¸ì€ ì—°ì†ì ì¸ ê°’ì„ ì˜ˆì¸¡í•˜ëŠ” ë¬¸ì œì…ë‹ˆë‹¤.íšŒê·€(Regression)ëª¨ë¸ì€ ì—°ì†ì ì¸ ê°’ì„ ì˜ˆì¸¡í•˜ëŠ” ë¬¸ì œì…ë‹ˆë‹¤.ï»¿
ì˜¤ëŠ˜ì€ ì„ í˜•íšŒê·€/ë‹¤í•­íšŒê·€/ë¦¬ì§€íšŒê·€/ë¼ì˜íšŒê·€ ë¥¼ ë‹¤ë¤„ë³¼ ì˜ˆì •ì…ë‹ˆë‹¤ì˜¤ëŠ˜ì€ ì„ í˜•íšŒê·€/ë‹¤í•­íšŒê·€/ë¦¬ì§€íšŒê·€/ë¼ì˜íšŒê·€ ë¥¼ ë‹¤ë¤„ë³¼ ì˜ˆì •ì…ë‹ˆë‹¤ï»¿â€‹[ëª©ì°¨]01. íšŒê·€ëª¨ë¸
 
 01. íšŒê·€ëª¨ë¸ë‹¤ì–‘í•œ íšŒê·€ ëª¨ë¸(ì„ í˜•íšŒê·€/ë‹¤í•­íšŒê·€/ë¦¬ì§€íšŒê·€/ë¼ì˜íšŒê·€)ì— ëŒ€í•´ì„œ ì•Œì•„ë³´ê³  ì‹¤ìŠµí•´ ë´…ì‹œë‹¤1) ì„ í˜• íšŒê·€ ì„ í˜• íšŒê·€ì„ í˜•íšŒê·€ëŠ” ì¢…ì† ë³€ìˆ˜ì™€ í•˜ë‚˜ ì´ìƒì˜ ë…ë¦½ ë³€ìˆ˜ ê°„ì˜ ì„ í˜• ê´€ê³„ë¥¼ ëª¨ë¸ë§ í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤ë…ë¦½ë³€ìˆ˜ì˜ ìˆ˜ì— ë”°ë¼ ë‹¨ìˆœ ì„ í˜•íšŒê·€ì™€ ë‹¤ì¤‘ ì„ í˜•íšŒê·€ë¡œ ë‚˜ë‰©ë‹ˆë‹¤ ë‹¨ìˆœ ì„ í˜• íšŒê·€ VS ë‹¤ì¤‘ ì„ í˜• íšŒê·€ALTë‹¨ìˆœ ì„ í˜• íšŒê·€ : í•˜ë‚˜ì˜ ë…ë¦½ ë³€ìˆ˜ì™€ í•˜ë‚˜ì˜ ì¢…ì† ë³€ìˆ˜ ê°„ì˜ ê´€ê³„ë¥¼ ëª¨ë¸ë§ë‹¤ì¤‘ ì„ í˜• íšŒê·€ : ì—¬ëŸ¬ ë…ë¦½ ë³€ìˆ˜ì™€ í•˜ë‚˜ì˜ ì¢…ì† ë³€ìˆ˜ ê°„ì˜ ê´€ê³„ë¥¼ ëª¨ë¸ë§ì„ í˜• íšŒê·€ì˜ ê¸°ë³¸ ìˆ˜ì‹ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.ì„ í˜• íšŒê·€ì˜ ê¸°ë³¸ ìˆ˜ì‹ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.ï»¿
y=Î²0â€‹+Î²1â€‹x1â€‹+Î²2â€‹x2â€‹+â‹¯+Î²nâ€‹xnâ€‹+Ïµy=Î²0â€‹+Î²1â€‹x1â€‹+Î²2â€‹x2â€‹+â‹¯+Î²nâ€‹xnâ€‹+Ïµy=Î²0â€‹+Î²1â€‹x1â€‹+Î²2â€‹x2â€‹+â‹¯+Î²nâ€‹xnâ€‹+Ïµï»¿
ì—¬ê¸°ì„œ yëŠ” ì¢…ì†ë³€ìˆ˜, x1,x2,â€¦,xn ì€ ë…ë¦½ë³€ìˆ˜, b0 ëŠ” ì ˆí¸, b1,b2,â€¦,bnì€ íšŒê·€ê³„ìˆ˜, eëŠ” ì˜¤ì°¨ì…ë‹ˆë‹¤ì—¬ê¸°ì„œ yëŠ” ì¢…ì†ë³€ìˆ˜, x1,x2,â€¦,xn ì€ ë…ë¦½ë³€ìˆ˜, b0 ëŠ” ì ˆí¸, b1,b2,â€¦,bnì€ íšŒê·€ê³„ìˆ˜, eëŠ” ì˜¤ì°¨ì…ë‹ˆë‹¤ï»¿
â‹„ ë‹¨ìˆœ ì„ í˜• íšŒê·€ì¼ê²½ìš° â‹„ ë‹¨ìˆœ ì„ í˜• íšŒê·€ì¼ê²½ìš° ï»¿
y=Î²0â€‹+Î²1â€‹x+Ïµy=Î²0â€‹+Î²1â€‹x+Ïµï»¿
 Scikit-learn ì„ ì‚¬ìš©í•œ ì„ í˜• íšŒê·€ ëª¨ë¸ êµ¬í˜„ ë° í‰ê°€ì„ í˜• íšŒê·€ ëª¨ë¸ êµ¬í˜„ ë° í‰ê°€ {5px}ì„ í˜• íšŒê·€ ëª¨ë¸ êµ¬í˜„ ë° í‰ê°€ ï»¿â€‹Pythonë³µì‚¬import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# ë°ì´í„° ìƒì„±
X = np.array([[1, 1], [2, 2], [3, 3], [4, 4], [5, 5],[6,6]])
y = np.array([1, 2, 3, 4, 5, 6])
# ë°ì´í„° ë¶„í•  (í›ˆë ¨ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# ì„ í˜• íšŒê·€ ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
model = LinearRegression()
model.fit(X_train, y_train)
# ì˜ˆì¸¡
y_pred = model.predict(X_test)
# ëª¨ë¸ í‰ê°€
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f'Mean Squared Error: {mse}')
print(f'R^2 Score: {r2}')
â€‹2) ë‹¤í•­ íšŒê·€ ë‹¤í•­ íšŒê·€ë‹¤í•­ íšŒê·€(Polynomial Regression)ëŠ” ì¢…ì† ë³€ìˆ˜ì™€ ë…ë¦½ ë³€ìˆ˜ ê°„ì˜ ë¹„ì„ í˜• ê´€ê³„ë¥¼ ëª¨ë¸ë§í•˜ëŠ” ë°©ë²•ë…ë¦½ë³€ìˆ˜ì˜ ë‹¤í•­ì‹ì„ ì‚¬ìš©í•˜ì—¬ ê´€ê³„ë¥¼ ëª¨ë¸ë§ í•©ë‹ˆë‹¤.ë‹¤í•­ íšŒê·€ì˜ ê¸°ë³¸ ìˆ˜ì‹ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤ë‹¤í•­ íšŒê·€ì˜ ê¸°ë³¸ ìˆ˜ì‹ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤ï»¿
y=Î²0â€‹+Î²1â€‹x+Î²2â€‹x2+â‹¯+Î²nâ€‹xn+Ïµy=Î²0â€‹+Î²1â€‹x+Î²2â€‹x^2+â‹¯+Î²nâ€‹x^n+Ïµy=Î²0â€‹+Î²1â€‹x+Î²2â€‹x2+â‹¯+Î²nâ€‹xn+Ïµï»¿
ì—¬ê¸°ì„œ yëŠ” ì¢…ì†ë³€ìˆ˜, x1,x2,â€¦,xn ì€ ë…ë¦½ë³€ìˆ˜, b0 ëŠ” ì ˆí¸, b1,b2,â€¦,bnì€ íšŒê·€ê³„ìˆ˜, eëŠ” ì˜¤ì°¨ì…ë‹ˆë‹¤ì—¬ê¸°ì„œ yëŠ” ì¢…ì†ë³€ìˆ˜, x1,x2,â€¦,xn ì€ ë…ë¦½ë³€ìˆ˜, b0 ëŠ” ì ˆí¸, b1,b2,â€¦,bnì€ íšŒê·€ê³„ìˆ˜, eëŠ” ì˜¤ì°¨ì…ë‹ˆë‹¤ï»¿â€‹ ë‹¤í•­ íšŒê·€ ì°¨ìˆ˜ ì„ íƒë‹¤í•­íšŒê·€ ì°¨ìˆ˜(degree) : ë…ë¦½ ë³€ìˆ˜ì˜ ìµœëŒ€ ì°¨ìˆ˜ì°¨ìˆ˜ê°€ ë†’ì„ìˆ˜ë¡ ëª¨ë¸ì´ ë” ë³µì¡í•´ì§€ë©° ê³¼ì í•©(overfitting)ì˜ ìœ„í—˜ ì¡´ì¬ â†’ ì ì ˆí•œ ì°¨ìˆ˜ ì„ íƒ í•„ìš”ê³¼ì í•©ì´ë€ í•™ìŠµë°ì´í„°ì— ëª¨ë¸ì´ ê³¼ë„í•˜ê²Œ ì í•©(fitting)ë˜ëŠ” í˜„ìƒì…ë‹ˆë‹¤ê³¼ì í•©ì´ë€ í•™ìŠµë°ì´í„°ì— ëª¨ë¸ì´ ê³¼ë„í•˜ê²Œ ì í•©(fitting)ë˜ëŠ” í˜„ìƒì…ë‹ˆë‹¤ï»¿â€‹ Scikit-learnì„ ì‚¬ìš©í•œ ë‹¤í•­ íšŒê·€ ëª¨ë¸ êµ¬í˜„ ë° í‰ê°€ë‹¤í•­ íšŒê·€ ëª¨ë¸ êµ¬í˜„ ë° í‰ê°€ {5px}ë‹¤í•­ íšŒê·€ ëª¨ë¸ êµ¬í˜„ ë° í‰ê°€ ï»¿â€‹Pythonë³µì‚¬import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# ë°ì´í„° ìƒì„±
X = np.array([[1], [2], [3], [4], [5], [6]])
y = np.array([1, 4, 9, 16, 25, 36])
# ë‹¤í•­ íŠ¹ì§• ìƒì„± (ì°¨ìˆ˜ 2)
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)
# ë°ì´í„° ë¶„í•  (í›ˆë ¨ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°)
X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=42)
# ë‹¤í•­ íšŒê·€ ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
model = LinearRegression()
model.fit(X_train, y_train)
# ì˜ˆì¸¡
y_pred = model.predict(X_test)
# ëª¨ë¸ í‰ê°€
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f'Mean Squared Error: {mse}')
print(f'R^2 Score: {r2}')
â€‹3) ë¦¬ì§€ íšŒê·€ ë¦¬ì§€ íšŒê·€ë¦¬ì§€ íšŒê·€(Ridge Regression)ëŠ” ì„ í˜• íšŒê·€ì˜ ì¼ì¢…íšŒê·€ ê³„ìˆ˜ì˜ í¬ê¸°ë¥¼ ì œì–´í•˜ì—¬ ê³¼ì í•©ì„ ë°©ì§€í•˜ëŠ” ì •ê·œí™” ê¸°ë²•L2 ì •ê·œí™”(regularization)ë¥¼ ì‚¬ìš©í•˜ì—¬ íšŒê·€ ê³„ìˆ˜ì˜ ì œê³±í•©ì„ ìµœì†Œí™” í•©ë‹ˆë‹¤ë¦¬ì§€ íšŒê·€ì˜ ê¸°ë³¸ ìˆ˜ì‹ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤ë¦¬ì§€ íšŒê·€ì˜ ê¸°ë³¸ ìˆ˜ì‹ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤ï»¿
J(Î²)=âˆ‘i=1n(yiâˆ’y^i)2+Î»âˆ‘j=1pÎ²j2J() = _{i=1}^{n} (y_i - _i)^2 +  _{j=1}^{p} _j^2J(Î²)=âˆ‘i=1nâ€‹(yiâ€‹âˆ’y^â€‹iâ€‹)2+Î»âˆ‘j=1pâ€‹Î²j2â€‹ï»¿
ì—¬ê¸°ì„œÎ»ëŠ” ì •ê·œí™” ê°•ë„ë¥¼ ì¡°ì ˆí•˜ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„° ì…ë‹ˆë‹¤. ì—¬ê¸°ì„œÎ»ëŠ” ì •ê·œí™” ê°•ë„ë¥¼ ì¡°ì ˆí•˜ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„° ì…ë‹ˆë‹¤.ï»¿â€‹ L2 ì •ê·œí™” L2 ì •ê·œí™”ëŠ” ëª¨ë“  ê°€ì¤‘ì¹˜ë¥¼ ì‘ê²Œ ë§Œë“¤ì–´ ëª¨ë¸ì˜ ë³µì¡ë„ë¥¼ ì¤„ì…ë‹ˆë‹¤.ì†ì‹¤ í•¨ìˆ˜ì— ì œê³±í•­ì„ ì¶”ê°€í•˜ì—¬ ë§¤ë„ëŸ¬ìš´ ìµœì í™”ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.ì •ê·œí™”ëŠ” ëª¨ë¸ì˜ ë³µì¡ë„ë¥¼ ì œì–´í•˜ì—¬ ê³¼ì í•©ì„ ë°©ì§€í•˜ëŠ” ë° í•„ìš”í•©ë‹ˆë‹¤. Scikit-learnì„ ì‚¬ìš©í•œ ë¦¬ì§€ íšŒê·€ ëª¨ë¸ êµ¬í˜„ ë° í‰ê°€ë¦¬ì§€ íšŒê·€ ëª¨ë¸ êµ¬í˜„ ë° í‰ê°€ {5px}ë¦¬ì§€ íšŒê·€ ëª¨ë¸ êµ¬í˜„ ë° í‰ê°€ ï»¿â€‹Pythonë³µì‚¬import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error, r2_score

# ë°ì´í„° ìƒì„±
X = np.array([[1, 1], [2, 2], [3, 3], [4, 4], [5, 5], [6, 6]])
y = np.array([1, 2, 3, 4, 5, 6])
# ë°ì´í„° ë¶„í•  (í›ˆë ¨ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# ë¦¬ì§€ íšŒê·€ ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
model = Ridge(alpha=1.0)
model.fit(X_train, y_train)
# ì˜ˆì¸¡
y_pred = model.predict(X_test)
# ëª¨ë¸ í‰ê°€
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f'Mean Squared Error: {mse}')
print(f'R^2 Score: {r2}')
â€‹4) ë¼ì˜ íšŒê·€ ë¼ì˜ íšŒê·€ë¼ì˜ íšŒê·€(Lasso Regression)ëŠ” ì„ í˜• íšŒê·€ì˜ ì¼ì¢…íšŒê·€ ê³„ìˆ˜ì˜ í¬ê¸°ë¥¼ ì œì–´í•˜ì—¬ ê³¼ì í•©ì„ ë°©ì§€í•˜ëŠ” ì •ê·œí™” ê¸°ë²•L1 ì •ê·œí™”(regularization)ë¥¼ ì‚¬ìš©í•˜ì—¬ íšŒê·€ ê³„ìˆ˜ì˜ ì ˆëŒ€ê°’ í•©ì„ ìµœì†Œí™” í•©ë‹ˆë‹¤ë¼ì˜ íšŒê·€ì˜ ê¸°ë³¸ ìˆ˜ì‹ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤ë¼ì˜ íšŒê·€ì˜ ê¸°ë³¸ ìˆ˜ì‹ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤ï»¿
J(Î²)=âˆ‘i=1n(yiâˆ’y^i)2+Î»âˆ‘j=1pâˆ£Î²jâˆ£J() = _{i=1}^{n} (y_i - _i)^2 +  _{j=1}^{p} |_j|J(Î²)=âˆ‘i=1nâ€‹(yiâ€‹âˆ’y^â€‹iâ€‹)2+Î»âˆ‘j=1pâ€‹âˆ£Î²jâ€‹âˆ£ï»¿
ì—¬ê¸°ì„œÎ»ëŠ” ì •ê·œí™” ê°•ë„ë¥¼ ì¡°ì ˆí•˜ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„° ì…ë‹ˆë‹¤. ì—¬ê¸°ì„œÎ»ëŠ” ì •ê·œí™” ê°•ë„ë¥¼ ì¡°ì ˆí•˜ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„° ì…ë‹ˆë‹¤.ï»¿â€‹ L1 ì •ê·œí™”ì™€ íŠ¹ì§• ì„ íƒL1 ì •ê·œí™”ëŠ” ì¼ë¶€ íšŒê·€ ê³„ìˆ˜ë¥¼ 0ìœ¼ë¡œ ë§Œë“¤ì–´ íŠ¹ì§• ì„ íƒ(feature selection)ì„ ìˆ˜í–‰ëª¨ë¸ì˜ í•´ì„ ê°€ëŠ¥ì„±ì„ ë†’ì´ê³ , ë¶ˆí•„ìš”í•œ íŠ¹ì§•ì„ ì œê±°í•˜ëŠ” ë° ìœ ìš©í•©ë‹ˆë‹¤ Scikit-learnì„ ì‚¬ìš©í•œ ë¼ì˜ íšŒê·€ ëª¨ë¸ êµ¬í˜„ ë° í‰ê°€ë¼ì˜ íšŒê·€ ëª¨ë¸ êµ¬í˜„ ë° í‰ê°€ {5px}ë¼ì˜ íšŒê·€ ëª¨ë¸ êµ¬í˜„ ë° í‰ê°€ ï»¿â€‹Pythonë³µì‚¬import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Lasso
from sklearn.metrics import mean_squared_error, r2_score

# ë°ì´í„° ìƒì„±
X = np.array([[1, 1], [2, 2], [3, 3], [4, 4], [5, 5], [6,6]])
y = np.array([1, 2, 3, 4, 5, 6])
# ë°ì´í„° ë¶„í•  (í›ˆë ¨ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ ë°ì´í„°)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# ë¼ì˜ íšŒê·€ ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
model = Lasso(alpha=1.0)
model.fit(X_train, y_train)
# ì˜ˆì¸¡
y_pred = model.predict(X_test)
# ëª¨ë¸ í‰ê°€
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f'Mean Squared Error: {mse}')
print(f'R^2 Score: {r2}')
â€‹.
[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 8. ìì—°ì–´ ì²˜ë¦¬(NLP) ëª¨ë¸[SCC] ê¸°ì´ˆê°€ íƒ„íƒ„í•œ ë”¥ëŸ¬ë‹/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] ê¸°ì´ˆê°€ íƒ„íƒ„í•œ ë”¥ëŸ¬ë‹ - 3ì£¼ì°¨/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 8. ìì—°ì–´ ì²˜ë¦¬(NLP) ëª¨ë¸ì œì‘:[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 8. ìì—°ì–´ ì²˜ë¦¬(NLP) ëª¨ë¸[ìˆ˜ì—… ëª©í‘œ]ìì—°ì–´ ì²˜ë¦¬ ëª¨ë¸ì— ëŒ€í•´ì„œ ì•Œì•„ë³´ê³  ë™ì‘ ì›ë¦¬ì— ëŒ€í•´ì„œ í•™ìŠµí•´ ë´…ì‹œë‹¤Pytorchë¡œ ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ë¶„ë¥˜ ë° ìƒì„± ëª¨ë¸ êµ¬í˜„ ì‹¤ìŠµì„ ì§„í–‰í•´ ë´…ì‹œë‹¤[ëª©ì°¨]01. ì›Œë“œ ì„ë² ë”©ê³¼ ì‹œí€€ìŠ¤ ëª¨ë¸ë§02. Transformerì™€ BERTğŸ’¡
 
 01. ì›Œë“œ ì„ë² ë”©ê³¼ ì‹œí€€ìŠ¤ ëª¨ë¸ë§âœ”ï¸ì›Œë“œì„ë² ë”© ê¸°ë²•ì´ ë¬´ì—‡ì¸ì§€ ì•Œì•„ë³´ê³  ì‹œí€€ìŠ¤ ëª¨ë¸ë§ì´ ë¬´ì—‡ì¸ì§€ í•™ìŠµí•´ ë´…ì‹œë‹¤1) ì›Œë“œ ì„ë² ë”© ê¸°ë²• ì›Œë“œ ì„ë² ë”© ê¸°ë²•ì›Œë“œ ì„ë² ë”©(Word Embedding)ì€ ë‹¨ì–´ë¥¼ ê³ ì •ëœ í¬ê¸°ì˜ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ê¸°ë²•ìœ¼ë¡œ, ë‹¨ì–´ ê°„ì˜ ì˜ë¯¸ì  ìœ ì‚¬ì„±ì„ ë°˜ì˜í•©ë‹ˆë‹¤.ëŒ€í‘œì ì¸ ì›Œë“œ ì„ë² ë”© ê¸°ë²•ìœ¼ë¡œëŠ” Word2Vecê³¼ GloVeê°€ ìˆìŠµë‹ˆë‹¤.ALT Word2VecWord2Vecì€ ë‹¨ì–´ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ë‘ ê°€ì§€ ëª¨ë¸(CBOWì™€ Skip-gram)ì„ ì œê³µí•©ë‹ˆë‹¤.CBOW (Continuous Bag of Words): ì£¼ë³€ ë‹¨ì–´(context)ë¡œ ì¤‘ì‹¬ ë‹¨ì–´(target)ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.Skip-gram: ì¤‘ì‹¬ ë‹¨ì–´(target)ë¡œ ì£¼ë³€ ë‹¨ì–´(context)ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤. GloVe (Global Vectors for Word Representation)GloVeëŠ” ë‹¨ì–´-ë‹¨ì–´ ê³µê¸°í–‰ë ¬(word-word co-occurrence matrix)ì„ ì‚¬ìš©, ë‹¨ì–´ ë²¡í„°ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.ì „ì—­ì ì¸ í†µê³„ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ë‹¨ì–´ ê°„ì˜ ì˜ë¯¸ì  ìœ ì‚¬ì„±ì„ ë°˜ì˜í•©ë‹ˆë‹¤.2) ì‹œí€€ìŠ¤ ëª¨ë¸ë§ ì‹œí€€ìŠ¤ ëª¨ë¸ë§ì˜ ê¸°ë³¸ ê°œë…ì‹œí€€ìŠ¤ ëª¨ë¸ë§(Sequence Modeling)ì€ ìˆœì°¨ì ì¸ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê³  ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ë§ ê¸°ë²•ì…ë‹ˆë‹¤. ì‹œí€€ìŠ¤ ëª¨ë¸ë§ì€ ì£¼ë¡œ RNN, LSTM, GRUì™€ ê°™ì€ ìˆœí™˜ ì‹ ê²½ë§ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.ALT ì…ë ¥ ì‹œí€€ìŠ¤ì‹œí€€ìŠ¤ ëª¨ë¸ë§ì—ì„œëŠ” ì…ë ¥ ë°ì´í„°ê°€ ìˆœì°¨ì ì¸ í˜•íƒœë¡œ ì œê³µë©ë‹ˆë‹¤.ì˜ˆë¥¼ ë“¤ì–´, í…ìŠ¤íŠ¸ ë°ì´í„°ëŠ” ë‹¨ì–´ì˜ ì‹œí€€ìŠ¤ë¡œ í‘œí˜„ë©ë‹ˆë‹¤. ì€ë‹‰ ìƒíƒœìˆœí™˜ ì‹ ê²½ë§ì€ ì´ì „ ì‹œê°„ ë‹¨ê³„ì˜ ì€ë‹‰ ìƒíƒœë¥¼ í˜„ì¬ ì‹œê°„ ë‹¨ê³„ë¡œ ì „ë‹¬í•˜ì—¬, ì‹œí€€ìŠ¤ì˜ íŒ¨í„´ì„ í•™ìŠµí•©ë‹ˆë‹¤. ì¶œë ¥ ì‹œí€€ìŠ¤ì‹œí€€ìŠ¤ ëª¨ë¸ë§ì˜ ì¶œë ¥ì€ ì…ë ¥ ì‹œí€€ìŠ¤ì™€ ë™ì¼í•œ ê¸¸ì´ì˜ ì‹œí€€ìŠ¤ì¼ ìˆ˜ë„ ìˆê³ , ë‹¨ì¼ ê°’ì¼ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.02. Transformerì™€ BERTâœ”ï¸Transformerì˜ êµ¬ì¡°ì— ëŒ€í•´ ì•Œì•„ë³´ê³  ì´ë¥¼ ì´ìš©í•œ BERT ëª¨ë¸ì— ëŒ€í•´ì„œ ë°°ì›Œë´…ì‹œë‹¤1) Transformerì˜ êµ¬ì¡°ì™€ ì›ë¦¬ Transformerì˜ êµ¬ì¡°ì™€ ì›ë¦¬TransformerëŠ” ìˆœì°¨ì ì¸ ë°ì´í„°ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ë¡œ, ìì—°ì–´ ì²˜ë¦¬ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.TransformerëŠ” ì¸ì½”ë”-ë””ì½”ë”(Encoder-Decoder) êµ¬ì¡°ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.ALT ì¸ì½”ë” (Encoder)ì…ë ¥ ì‹œí€€ìŠ¤ë¥¼ ì²˜ë¦¬í•˜ì—¬ ì¸ì½”ë”©ëœ í‘œí˜„ì„ ìƒì„±í•©ë‹ˆë‹¤.ê° ì¸ì½”ë” ì¸µì€ ì…€í”„ ì–´í…ì…˜(Self-Attention)ê³¼ í”¼ë“œí¬ì›Œë“œ ì‹ ê²½ë§(Feed-Forward Neural Network)ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. ë””ì½”ë” (Decoder)ì¸ì½”ë”©ëœ í‘œí˜„ì„ ë°”íƒ•ìœ¼ë¡œ ì¶œë ¥ ì‹œí€€ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.ê° ë””ì½”ë” ì¸µì€ ì…€í”„ ì–´í…ì…˜, ì¸ì½”ë”-ë””ì½”ë” ì–´í…ì…˜, í”¼ë“œí¬ì›Œë“œ ì‹ ê²½ë§ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ (Attention Mechanism)ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ì€ ì…ë ¥ ì‹œí€€ìŠ¤ì˜ ê° ìœ„ì¹˜ì— ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ì—¬, ì¤‘ìš”í•œ ì •ë³´ë¥¼ ê°•ì¡°í•©ë‹ˆë‹¤.ì…€í”„ ì–´í…ì…˜ì€ ì…ë ¥ ì‹œí€€ìŠ¤ ë‚´ì˜ ë‹¨ì–´ ê°„ì˜ ê´€ê³„ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.2) BERTì˜ ê°œë…ê³¼ ì‘ìš© BERTë€?BERT(Bidirectional Encoder Representations from Transformers)ëŠ” Transformer ì¸ì½”ë”ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ì‚¬ì „ í•™ìŠµëœ ì–¸ì–´ ëª¨ë¸ì…ë‹ˆë‹¤.BERTëŠ” ì–‘ë°©í–¥ìœ¼ë¡œ ë¬¸ë§¥ì„ ì´í•´í•  ìˆ˜ ìˆì–´, ë‹¤ì–‘í•œ ìì—°ì–´ ì²˜ë¦¬ ì‘ì—…ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. ì‚¬ì „ í•™ìŠµ(Pre-training)BERTëŠ” ëŒ€ê·œëª¨ í…ìŠ¤íŠ¸ ì½”í¼ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ì „ í•™ìŠµë©ë‹ˆë‹¤.ë§ˆìŠ¤í‚¹ ì–¸ì–´ ëª¨ë¸(Masked Language Model)ê³¼ ë‹¤ìŒ ë¬¸ì¥ ì˜ˆì¸¡(Next Sentence Prediction) ì‘ì—…ì„ í†µí•´ í•™ìŠµë©ë‹ˆë‹¤. íŒŒì¸íŠœë‹ (Fine-tuning)ì‚¬ì „ í•™ìŠµëœ BERT ëª¨ë¸ì„ íŠ¹ì • ì‘ì—…ì— ë§ê²Œ íŒŒì¸íŠœë‹í•©ë‹ˆë‹¤.í…ìŠ¤íŠ¸ ë¶„ë¥˜, ì§ˆì˜ ì‘ë‹µ, í…ìŠ¤íŠ¸ ìƒì„± ë“± ë‹¤ì–‘í•œ ìì—°ì–´ ì²˜ë¦¬ ì‘ì—…ì— ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤..
ì¡°ê±´ë¶€ í•„í„°ë§ê³¼ ë°ì´í„° íƒ€ì… ë³€í™˜ì¸ê³µì§€ëŠ¥ì„ ìœ„í•œ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬/ì¸ê³µì§€ëŠ¥ì„ ìœ„í•œ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ - 3ì£¼ì°¨/ì¡°ê±´ë¶€ í•„í„°ë§ê³¼ ë°ì´í„° íƒ€ì… ë³€í™˜ì œì‘:ì¡°ê±´ë¶€ í•„í„°ë§ê³¼ ë°ì´í„° íƒ€ì… ë³€í™˜ìˆ˜ì—… ëª©í‘œíŒë‹¤ìŠ¤ì—ì„œ í•„í„°ë§ê³¼ ë°ì´í„° íƒ€ì…ì„ ë³€í™˜í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ì„œ í•™ìŠµí•©ë‹ˆë‹¤.ëª©ì°¨ì¡°ê±´ë¶€ í•„í„°ë§ë°ì´í„° íƒ€ì… ë³€í™˜ìš”ì•½ì¡°ê±´ë¶€ í•„í„°ë§ì¡°ê±´ë¶€ í•„í„°ë§ì„ í†µí•´ ë°ì´í„°í”„ë ˆì„ì—ì„œ íŠ¹ì • ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ë°ì´í„°ë§Œ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ê¸°ëŠ¥ì€ ë°ì´í„° ë¶„ì„ì—ì„œ ë§¤ìš° ìœ ìš©í•©ë‹ˆë‹¤.ê¸°ë³¸ ì¡°ê±´ë¶€ í•„í„°ë§ì¡°ê±´ì„ ì‚¬ìš©í•´ í•„í„°ë§ëœ ë°ì´í„°í”„ë ˆì„ì„ ë°˜í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Pythonë³µì‚¬import pandas as pd

# ì˜ˆì‹œ ë°ì´í„°í”„ë ˆì„ ìƒì„±
data = {
'ì´ë¦„': ['ì² ìˆ˜', 'ì˜í¬', 'ë¯¼ìˆ˜', 'ì§€ìˆ˜'],
'ë‚˜ì´': [25, 30, 22, 35],
'ì§ì—…': ['í•™ìƒ', 'íšŒì‚¬ì›', 'í•™ìƒ', 'í”„ë¦¬ëœì„œ']
}
df = pd.DataFrame(data)
# ë‚˜ì´ê°€ 25 ì´ìƒì¸ í–‰ë§Œ ì„ íƒ
filtered_df = df[df['ë‚˜ì´'] >= 25]
print(filtered_df)

â€‹ê²°ê³¼:Plain Textë³µì‚¬   ì´ë¦„  ë‚˜ì´       ì§ì—…
0  ì² ìˆ˜   25      í•™ìƒ
1  ì˜í¬   30    íšŒì‚¬ì›
3  ì§€ìˆ˜   35  í”„ë¦¬ëœì„œ

â€‹ì—¬ëŸ¬ ì¡°ê±´ì„ ì‚¬ìš©í•œ í•„í„°ë§ì—¬ëŸ¬ ì¡°ê±´ì„ ê²°í•©í•˜ì—¬ í•„í„°ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. AND(&), OR(|) ì—°ì‚°ìë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.Pythonë³µì‚¬# ë‚˜ì´ê°€ 25 ì´ìƒì´ê³ , ì§ì—…ì´ 'í•™ìƒ'ì¸ í–‰ë§Œ ì„ íƒ
filtered_df = df[(df['ë‚˜ì´'] >= 25) & (df['ì§ì—…'] == 'í•™ìƒ')]
print(filtered_df)

â€‹ê²°ê³¼:Plain Textë³µì‚¬   ì´ë¦„  ë‚˜ì´  ì§ì—…
0  ì² ìˆ˜   25  í•™ìƒ

â€‹isin()ì„ ì‚¬ìš©í•œ í•„í„°ë§íŠ¹ì • ê°’ë“¤ì´ í¬í•¨ëœ í–‰ì„ í•„í„°ë§í•  ë•Œ ìœ ìš©í•©ë‹ˆë‹¤.Pythonë³µì‚¬# ì§ì—…ì´ 'í•™ìƒ' ë˜ëŠ” 'í”„ë¦¬ëœì„œ'ì¸ í–‰ ì„ íƒ
filtered_df = df[df['ì§ì—…'].isin(['í•™ìƒ', 'í”„ë¦¬ëœì„œ'])]
print(filtered_df)

â€‹ê²°ê³¼:Plain Textë³µì‚¬   ì´ë¦„  ë‚˜ì´       ì§ì—…
0  ì² ìˆ˜   25      í•™ìƒ
2  ë¯¼ìˆ˜   22      í•™ìƒ
3  ì§€ìˆ˜   35  í”„ë¦¬ëœì„œ

â€‹~ ì—°ì‚°ìë¥¼ ì‚¬ìš©í•œ ë°˜ëŒ€ ì¡°ê±´ í•„í„°ë§íŠ¹ì • ì¡°ê±´ì„ ë¶€ì •í•˜ëŠ” ë°ì´í„°ë¥¼ ì„ íƒí•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.Pythonë³µì‚¬# ì§ì—…ì´ 'í•™ìƒ'ì´ ì•„ë‹Œ í–‰ ì„ íƒ
filtered_df = df[~(df['ì§ì—…'] == 'í•™ìƒ')]
print(filtered_df)

â€‹ê²°ê³¼:Plain Textë³µì‚¬   ì´ë¦„  ë‚˜ì´       ì§ì—…
1  ì˜í¬   30    íšŒì‚¬ì›
3  ì§€ìˆ˜   35  í”„ë¦¬ëœì„œ

â€‹ë°ì´í„° íƒ€ì… ë³€í™˜ë°ì´í„°í”„ë ˆì„ì˜ ë°ì´í„° íƒ€ì…ì„ í™•ì¸í•˜ê³ , í•„ìš”ì— ë”°ë¼ ë³€í™˜í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ë°ì´í„° ë¶„ì„ ë° ì²˜ë¦¬ ì‹œ, ì˜¬ë°”ë¥¸ ë°ì´í„° íƒ€ì…ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.dtypeìœ¼ë¡œ ë°ì´í„° íƒ€ì… í™•ì¸ê° ì—´ì˜ ë°ì´í„° íƒ€ì…ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Pythonë³µì‚¬print(df.dtypes)

â€‹ê²°ê³¼:Plain Textë³µì‚¬ì´ë¦„    object
ë‚˜ì´     int64
ì§ì—…    object
dtype: object

â€‹astype()ì„ ì‚¬ìš©í•œ ë°ì´í„° íƒ€ì… ë³€í™˜astype() í•¨ìˆ˜ë¡œ íŠ¹ì • ì—´ì˜ ë°ì´í„° íƒ€ì…ì„ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Pythonë³µì‚¬# 'ë‚˜ì´' ì—´ì„ ì •ìˆ˜í˜•(int)ì—ì„œ ì‹¤ìˆ˜í˜•(float)ìœ¼ë¡œ ë³€í™˜
df['ë‚˜ì´'] = df['ë‚˜ì´'].astype(float)
print(df.dtypes)

â€‹ê²°ê³¼:Plain Textë³µì‚¬ì´ë¦„    object
ë‚˜ì´    float64
ì§ì—…    object
dtype: object

â€‹ë‚ ì§œ íƒ€ì…ìœ¼ë¡œ ë³€í™˜ë‚ ì§œ ë°ì´í„°ëŠ” pd.to_datetime() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ ë‚ ì§œ íƒ€ì…ìœ¼ë¡œ ë³€í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Pythonë³µì‚¬# ì˜ˆì‹œ ë°ì´í„°í”„ë ˆì„ ìƒì„±
data = {
'ì´ë¦„': ['ì² ìˆ˜', 'ì˜í¬', 'ë¯¼ìˆ˜'],
'ê°€ì…ì¼': ['2023-01-01', '2022-12-15', '2023-05-22']
}
df = pd.DataFrame(data)
# 'ê°€ì…ì¼'ì„ ë‚ ì§œ íƒ€ì…ìœ¼ë¡œ ë³€í™˜
df['ê°€ì…ì¼'] = pd.to_datetime(df['ê°€ì…ì¼'])
print(df.dtypes)

â€‹ê²°ê³¼:Plain Textë³µì‚¬ì´ë¦„            object
ê°€ì…ì¼    datetime64[ns]
dtype: object

â€‹ì¹´í…Œê³ ë¦¬ ë°ì´í„°ë¡œ ë³€í™˜ì¹´í…Œê³ ë¦¬ëŠ” ë©”ëª¨ë¦¬ë¥¼ ì ˆì•½í•˜ê³ , ì„±ëŠ¥ì„ ë†’ì´ê¸° ìœ„í•´ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Pythonë³µì‚¬# 'ì§ì—…' ì—´ì„ ì¹´í…Œê³ ë¦¬í˜•ìœ¼ë¡œ ë³€í™˜
df['ì§ì—…'] = df['ì§ì—…'].astype('category')
print(df.dtypes)

â€‹ê²°ê³¼:Plain Textë³µì‚¬ì´ë¦„     object
ë‚˜ì´    float64
ì§ì—…   category
dtype: object

â€‹ìš”ì•½ìš”ì•½ì¡°ê±´ë¶€ í•„í„°ë§ì€ íŠ¹ì • ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ë°ì´í„°ë§Œ ì„ íƒí•˜ëŠ” ê°•ë ¥í•œ ë„êµ¬ì…ë‹ˆë‹¤. &, |, ~ ë“±ì˜ ì—°ì‚°ìë¥¼ ê²°í•©í•˜ì—¬ ë³µì¡í•œ ì¡°ê±´ì„ êµ¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ë°ì´í„° íƒ€ì… ë³€í™˜ì€ ë°ì´í„°ë¥¼ ì˜¬ë°”ë¥´ê²Œ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ í•„ìˆ˜ì ì´ë©°, astype(), pd.to_datetime() ë“±ì„ ì‚¬ìš©í•´ ë³€í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ë°ì´í„°ì˜ ì •í™•í•œ íƒ€ì…ì„ ì§€ì •í•¨ìœ¼ë¡œì¨ ë©”ëª¨ë¦¬ ì ˆì•½ê³¼ ì„±ëŠ¥ í–¥ìƒì„ ê¸°ëŒ€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ì´ì œ ë‹¤ì–‘í•œ ì¡°ê±´ìœ¼ë¡œ ë°ì´í„°ë¥¼ í•„í„°ë§í•˜ê³ , ì ì ˆí•œ ë°ì´í„° íƒ€ì…ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë°ì´í„° ë¶„ì„ì˜ íš¨ìœ¨ì„ ë†’ì—¬ë³´ì„¸ìš”! â€‹
[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 9ê°•. ì§€ë„í•™ìŠµ : ë¶„ë¥˜ëª¨ë¸ - ë¡œì§€ìŠ¤í‹± íšŒê·€[SCC] ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ - 3ì£¼ì°¨/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 9ê°•. ì§€ë„í•™ìŠµ : ë¶„ë¥˜ëª¨ë¸ - ë¡œì§€ìŠ¤í‹± íšŒê·€ì œì‘:[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 9ê°•. ì§€ë„í•™ìŠµ : ë¶„ë¥˜ëª¨ë¸ - ë¡œì§€ìŠ¤í‹± íšŒê·€[ìˆ˜ì—… ëª©í‘œ]ì§€ë„í•™ìŠµ : ë¶„ë¥˜ëª¨ë¸ì˜ ë¡œì§€ìŠ¤í‹± íšŒê·€ì— ëŒ€í•´ ì•Œì•„ë³´ê³  ì‹¤ìŠµì„ í†µí•´ ë°°ì›Œë´…ì‹œë‹¤[ëª©ì°¨]01. ë¡œì§€ìŠ¤í‹± íšŒê·€ ê°œë…02. ë¡œì§€ìŠ¤í‹± íšŒê·€ë¶„ì„ ì‹¤ìŠµğŸ’¡
 
 01. ë¡œì§€ìŠ¤í‹± íšŒê·€ ê°œë…âœ”ï¸ë¡œì§€ìŠ¤í‹± íšŒê·€ë€ ë¬´ì—‡ì¸ì§€, ì™œ ì‚¬ìš©í•˜ëŠ”ì§€ ë°°ì›Œë´…ì‹œë‹¤1) ë¡œì§€ìŠ¤í‹± íšŒê·€ ë¡œì§€ìŠ¤í‹± íšŒê·€ë€?ì¢…ì† ë³€ìˆ˜ê°€ ì´ì§„í˜•ì¼ ë•Œ(ì¦‰, ê²°ê³¼ê°€ ë‘ ê°€ì§€ ì¤‘ í•˜ë‚˜ì¼ ë•Œ) ì‚¬ìš©ë˜ëŠ” í†µê³„ ê¸°ë²•ë¡œì§€ìŠ¤í‹± íšŒê·€ëŠ” ì„ í˜• íšŒê·€ì™€ ë‹¬ë¦¬ ê²°ê³¼ê°’ì´ 0ê³¼ 1 ì‚¬ì´ì— ìœ„ì¹˜í•˜ê²Œ í•˜ê¸° ìœ„í•´ ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜(Sigmoid Function)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.ë¡œì§€ìŠ¤í‹± íšŒê·€ëŠ” íšŒê·€ ë¶„ì„ì´ì§€ë§Œ, ì‹¤ì œë¡œëŠ” ë¶„ë¥˜ ì‘ì—…ì— ì‚¬ìš©ë©ë‹ˆë‹¤!!ë¡œì§€ìŠ¤í‹± íšŒê·€ëŠ” íšŒê·€ ë¶„ì„ì´ì§€ë§Œ, ì‹¤ì œë¡œëŠ” ë¶„ë¥˜ ì‘ì—…ì— ì‚¬ìš©ë©ë‹ˆë‹¤!!ï»¿â€‹ ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ëŠ” ë¡œì§€ìŠ¤í‹± íšŒê·€ì˜ í•µì‹¬ìœ¼ë¡œ ì…ë ¥ê°’ì„ 0ê³¼ 1ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤Ïƒ(z)=11+eâˆ’z(z) = {1 + e^{-z}} Ïƒ(z)=1+eâˆ’z1â€‹ì—¬ê¸°ì„œ z ëŠ” ì„ í˜• íšŒê·€ ë°©ì •ì‹z=Î²0+Î²1x1+Î²2x2+â€¦+Î²nxnì…ë‹ˆë‹¤.{z = _0 + _1x_1 + _2x_2 +  + _nx_n }ì—¬ê¸°ì„œ z ëŠ” ì„ í˜• íšŒê·€ ë°©ì •ì‹z=Î²0â€‹+Î²1â€‹x1â€‹+Î²2â€‹x2â€‹+â€¦+Î²nâ€‹xnâ€‹ì…ë‹ˆë‹¤.ï»¿â€‹ ë¡œì§€ìŠ¤í‹± íšŒê·€ì˜ ëª©ì ë¡œì§€ìŠ¤í‹± íšŒê·€ëŠ” ë°ì´í„°ë¥¼ í•™ìŠµí•˜ì—¬ ê° ë°ì´í„° í¬ì¸íŠ¸ê°€ íŠ¹ì • í´ë˜ìŠ¤ì— ì†í•  í™•ë¥ ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.ì˜ˆë¥¼ ë“¤ì–´, ìœ ë°©ì•” ë°ì´í„°ì—ì„œëŠ” í™˜ìê°€ ì•”ì— ê±¸ë ¸ì„ í™•ë¥ , íƒ€ì´íƒ€ë‹‰ ë°ì´í„° ì—ì„œëŠ” ìŠ¹ê°ì´ ìƒì¡´í•  í™•ë¥ ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤ ë¹„ìš© í•¨ìˆ˜ë¡œì§€ìŠ¤í‹± íšŒê·€ì˜ ë¹„ìš© í•¨ìˆ˜ëŠ” ëª¨ë¸ì˜ ì˜ˆì¸¡ í™•ë¥ ê³¼ ì‹¤ì œ ë ˆì´ë¸” ì‚¬ì´ì˜ ì°¨ì´ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤.ë¡œê·¸ ì†ì‹¤ í•¨ìˆ˜(Log Loss) ë˜ëŠ” í¬ë¡œìŠ¤ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤ í•¨ìˆ˜(Cross-Entropy loss)ë¼ê³  ë¶ˆë¦½ë‹ˆë‹¤.J(Î¸)=âˆ’1mâˆ‘i=1m[y(i)logâ¡(hÎ¸(x(i)))+(1âˆ’y(i))logâ¡(1âˆ’hÎ¸(x(i)))]J() = -{m} _{i=1}^{m} [ y^{(i)} (h_(x^{(i)})) + (1 - y^{(i)}) (1 - h_(x^{(i)})) ]J(Î¸)=âˆ’m1â€‹i=1âˆ‘mâ€‹[y(i)log(hÎ¸â€‹(x(i)))+(1âˆ’y(i))log(1âˆ’hÎ¸â€‹(x(i)))]02. ë¡œì§€ìŠ¤í‹± íšŒê·€ë¶„ì„ ì‹¤ìŠµâœ”ï¸Scikit-learnì˜ ìœ ë°©ì•”ë°ì´í„°ì™€ Seabornì˜ íƒ€ì´íƒ€ë‹‰ ë°ì´í„°ë¡œ ë¡œì§€ìŠ¤í‹± íšŒê·€ë¶„ì„ ì‹¤ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤1) ìœ ë°©ì•” ë°ì´í„° ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ìœ ë°©ì•” ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ {5px}ìœ ë°©ì•” ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ï»¿â€‹Pythonë³µì‚¬import numpy as np
import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# ë°ì´í„° ë¡œë“œ
data = load_breast_cancer()
X = data.data
y = data.target

# ë°ì´í„° ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# ë°ì´í„° ìŠ¤ì¼€ì¼ë§
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
â€‹sklearn.datasets.load_breast_cancer: ìœ ë°©ì•” ë°ì´í„°ì…‹ ë¡œë“œreturn_X_y=False: ë°ì´í„°ì™€ íƒ€ê²Ÿì„ í•¨ê»˜ ë°˜í™˜í• ì§€ ì—¬ë¶€. ê¸°ë³¸ê°’ì€ Falseì…ë‹ˆë‹¤._X_y=False: ë°ì´í„°ì™€ íƒ€ê²Ÿì„ í•¨ê»˜ ë°˜í™˜í• ì§€ ì—¬ë¶€. ê¸°ë³¸ê°’ì€ Falseì…ë‹ˆë‹¤.ï»¿â€‹sklearn.model_selection.train_test_split: ë°ì´í„°ë¥¼ í›ˆë ¨ ì„¸íŠ¸/ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ ë¶„í• test_size=0.2: í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì˜ ë¹„ìœ¨ì„ 0.2ë¡œ ì„¤ì •í•©ë‹ˆë‹¤._size=0.2: í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì˜ ë¹„ìœ¨ì„ 0.2ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.ï»¿â€‹random_state=42: ëœë¤ ì‹œë“œ ê°’ìœ¼ë¡œ, ë°ì´í„° ë¶„í• ì˜ ì¬í˜„ì„±ì„ ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤._state=42: ëœë¤ ì‹œë“œ ê°’ìœ¼ë¡œ, ë°ì´í„° ë¶„í• ì˜ ì¬í˜„ì„±ì„ ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.ï»¿â€‹sklearn.preprocessing.StandardScaler: ë°ì´í„°ì˜ í‰ê· ì„ 0, ë¶„ì‚°ì„ 1ë¡œ ìŠ¤ì¼€ì¼ë§fit_transform(X_train): í›ˆë ¨ ì„¸íŠ¸ë¥¼ ìŠ¤ì¼€ì¼ë§í•˜ê³  ë³€í™˜í•©ë‹ˆë‹¤._transform(X_train): í›ˆë ¨ ì„¸íŠ¸ë¥¼ ìŠ¤ì¼€ì¼ë§í•˜ê³  ë³€í™˜í•©ë‹ˆë‹¤.ï»¿â€‹transform(X_test): í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¥¼ ë³€í™˜í•©ë‹ˆë‹¤.(X_test): í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¥¼ ë³€í™˜í•©ë‹ˆë‹¤.ï»¿â€‹ ëª¨ë¸ í•™ìŠµëª¨ë¸ í•™ìŠµ {5px}ëª¨ë¸ í•™ìŠµ ï»¿â€‹Pythonë³µì‚¬from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
model = LogisticRegression()
model.fit(X_train, y_train)
# ì˜ˆì¸¡
y_pred = model.predict(X_test)
# í‰ê°€
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
print(f"Classification Report:")
print(f"Confusion Matrix:")
â€‹sklearn.linear_model.LogisticRegression: ë¡œì§€ìŠ¤í‹± íšŒê·€ ëª¨ë¸ ìƒì„±fit(X_train, y_train): ëª¨ë¸ì„ í›ˆë ¨ ì„¸íŠ¸ì— ë§ì¶”ì–´ í•™ìŠµì‹œí‚µë‹ˆë‹¤.(X_train, y_train): ëª¨ë¸ì„ í›ˆë ¨ ì„¸íŠ¸ì— ë§ì¶”ì–´ í•™ìŠµì‹œí‚µë‹ˆë‹¤.ï»¿â€‹predict(X_test): í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì— ëŒ€í•´ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.(X_test): í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì— ëŒ€í•´ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.ï»¿â€‹sklearn.metrics.accuracy_score: ì •í™•ë„ ê³„ì‚°accuracy_score(y_test, y_pred): ì‹¤ì œ ê°’ê³¼ ì˜ˆì¸¡ ê°’ì„ ë¹„êµí•˜ì—¬ ì •í™•ë„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤._score(y_test, y_pred): ì‹¤ì œ ê°’ê³¼ ì˜ˆì¸¡ ê°’ì„ ë¹„êµí•˜ì—¬ ì •í™•ë„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.ï»¿â€‹sklearn.metrics.classification_report: ë¶„ë¥˜ ë³´ê³ ì„œ ìƒì„±classification_report(y_test, y_pred): ì •í™•ë„, ì •ë°€ë„, ì¬í˜„ìœ¨ ë“±ì˜ ë©”íŠ¸ë¦­ì„ í¬í•¨í•œ ë³´ê³ ì„œë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤._report(y_test, y_pred): ì •í™•ë„, ì •ë°€ë„, ì¬í˜„ìœ¨ ë“±ì˜ ë©”íŠ¸ë¦­ì„ í¬í•¨í•œ ë³´ê³ ì„œë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.ï»¿â€‹sklearn.metrics.confusion_matrix: í˜¼ë™ í–‰ë ¬ ìƒì„±confusion_matrix(y_test, y_pred): ì‹¤ì œ ê°’ê³¼ ì˜ˆì¸¡ ê°’ì˜ í˜¼ë™ í–‰ë ¬ì„ ë°˜í™˜í•©ë‹ˆë‹¤._matrix(y_test, y_pred): ì‹¤ì œ ê°’ê³¼ ì˜ˆì¸¡ ê°’ì˜ í˜¼ë™ í–‰ë ¬ì„ ë°˜í™˜í•©ë‹ˆë‹¤.ï»¿â€‹2) íƒ€ì´íƒ€ë‹‰ ë°ì´í„° ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬íƒ€ì´íƒ€ë‹‰ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ {5px}íƒ€ì´íƒ€ë‹‰ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ï»¿â€‹Pythonë³µì‚¬import seaborn as sns

# ë°ì´í„° ë¡œë“œ
titanic = sns.load_dataset('titanic')
# í•„ìš”í•œ ì—´ ì„ íƒ ë° ê²°ì¸¡ê°’ ì²˜ë¦¬
titanic = titanic[['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']].dropna()
# ì„±ë³„ê³¼ íƒ‘ìŠ¹í•œ ê³³ ì¸ì½”ë”©
titanic['sex'] = titanic['sex'].map({'male': 0, 'female': 1})
titanic['embarked'] = titanic['embarked'].map({'C': 0, 'Q': 1, 'S': 2})
# íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
X = titanic.drop('survived', axis=1)
y = titanic['survived']
# ë°ì´í„° ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# ë°ì´í„° ìŠ¤ì¼€ì¼ë§
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
â€‹seaborn.load_dataset: seabornì˜ ë‚´ì¥ ë°ì´í„°ì…‹ ë¡œë“œâ€™titanicâ€™: íƒ€ì´íƒ€ë‹‰ ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤.â€™titanicâ€™: íƒ€ì´íƒ€ë‹‰ ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤.ï»¿â€‹ pandas.DataFrame.dropna: ê²°ì¸¡ê°’ì´ ìˆëŠ” í–‰ ì œê±°pandas.DataFrame.map: ë°ì´í„° ê°’ì„ ë‹¤ë¥¸ ê°’ìœ¼ë¡œ ë§¤í•‘â€™maleâ€™: 0, â€™femaleâ€™: 1: ì„±ë³„ì„ ìˆ«ìë¡œ ë§¤í•‘í•©ë‹ˆë‹¤.: ì„±ë³„ì„ ìˆ«ìë¡œ ë§¤í•‘í•©ë‹ˆë‹¤.}â€™maleâ€™: 0, â€™femaleâ€™: 1: ì„±ë³„ì„ ìˆ«ìë¡œ ë§¤í•‘í•©ë‹ˆë‹¤.ï»¿â€‹â€™Câ€™: 0, â€™Qâ€™: 1, â€™Sâ€™: 2: íƒ‘ìŠ¹í•œ ê³³ì„ ìˆ«ìë¡œ ë§¤í•‘í•©ë‹ˆë‹¤.: íƒ‘ìŠ¹í•œ ê³³ì„ ìˆ«ìë¡œ ë§¤í•‘í•©ë‹ˆë‹¤.}â€™Câ€™: 0, â€™Qâ€™: 1, â€™Sâ€™: 2: íƒ‘ìŠ¹í•œ ê³³ì„ ìˆ«ìë¡œ ë§¤í•‘í•©ë‹ˆë‹¤.ï»¿â€‹ ëª¨ë¸ í•™ìŠµëª¨ë¸ í•™ìŠµ {5px}ëª¨ë¸ í•™ìŠµ ï»¿â€‹Pythonë³µì‚¬# ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
model = LogisticRegression()
model.fit(X_train, y_train)
# ì˜ˆì¸¡
y_pred = model.predict(X_test)
# í‰ê°€
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
print(f"Classification Report:")
print(f"Confusion Matrix:")
â€‹.
[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 9. ResNet[SCC] ê¸°ì´ˆê°€ íƒ„íƒ„í•œ ë”¥ëŸ¬ë‹/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] ê¸°ì´ˆê°€ íƒ„íƒ„í•œ ë”¥ëŸ¬ë‹ - 4ì£¼ì°¨/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 9. ResNetì œì‘:[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 9. ResNet[ìˆ˜ì—… ëª©í‘œ]ë¹„ì „ ëª¨ë¸ì˜ ê¸¸ì„ ì—´ì–´ì¤€ ResNet!ì™œ ì¢‹ì€ ì§€ í•œë²ˆ ì•Œì•„ë´…ì‹œë‹¤[ëª©ì°¨]01. ê°œë…ğŸ’¡
 
 01. ê°œë…âœ”ï¸ResNet ê¸°ë³¸ ê°œë…ê³¼ ë™ì‘ ë°©ì‹ì— ëŒ€í•´ ì•Œì•„ë´…ì‹œë‹¤1) ResNet ê¸°ë³¸ ê°œë…ê³¼ ë™ì‘ ë°©ì‹ ResNetì´ë€?ResNet(Residual Network)ì€ ê¹Šì€ ì‹ ê²½ë§ì„ í•™ìŠµí•˜ê¸° ìœ„í•´ ê°œë°œëœ ëª¨ë¸ë¡œ, ì”ì°¨ í•™ìŠµ(Residual Learning) ê°œë…ì„ ë„ì…í•˜ì—¬ ë§¤ìš° ê¹Šì€ ë„¤íŠ¸ì›Œí¬ì—ì„œë„ íš¨ìœ¨ì ì¸ í•™ìŠµì´ ê°€ëŠ¥í•˜ë„ë¡ í•©ë‹ˆë‹¤. ResNetì€ 2015ë…„ Microsoft Researchì—ì„œ ê°œë°œë˜ì—ˆìœ¼ë©°, ë”¥ëŸ¬ë‹ ëª¨ë¸ì´ ë„ˆë¬´ ê¹Šì–´ì§ˆ ë•Œ ë°œìƒí•˜ëŠ” ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤.  ResNetì˜ ê¸°ë³¸ ê°œë…ê¹Šì€ ì‹ ê²½ë§ì˜ ë¬¸ì œê¹Šì€ ì‹ ê²½ë§ì€ ë” ë§ì€ ê³„ì¸µì„ ìŒ“ì•„ ë³µì¡í•œ íŒ¨í„´ì„ í•™ìŠµí•  ìˆ˜ ìˆì§€ë§Œ, ë„ˆë¬´ ê¹Šì–´ì§€ë©´ í•™ìŠµì´ ì–´ë ¤ì›Œì§€ëŠ” ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ì£¼ë¡œ ê¸°ìš¸ê¸° ì†Œì‹¤(Vanishing Gradient)ì´ë‚˜ ê¸°ìš¸ê¸° í­ë°œ(Exploding Gradient) ê°™ì€ í˜„ìƒ ë•Œë¬¸ì— ë°œìƒí•©ë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì´ ë” ì´ìƒ ê¹Šì–´ì§€ì§€ ëª»í•˜ê³  ì„±ëŠ¥ì´ ì €í•˜ë˜ëŠ” ê²°ê³¼ë¥¼ ì´ˆë˜í•©ë‹ˆë‹¤.ì”ì°¨ í•™ìŠµ(Residual Learning)ResNetì€ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì”ì°¨ í•™ìŠµ(Residual Learning)ì„ ë„ì…í•©ë‹ˆë‹¤. ì”ì°¨ í•™ìŠµì€ ê° ì¸µì˜ ì¶œë ¥ì´ ë°”ë¡œ ë‹¤ìŒ ì¸µì˜ ì…ë ¥ìœ¼ë¡œ ì „ë‹¬ë˜ì§€ ì•Šê³ , ì´ì „ ì¸µì˜ ì…ë ¥ì„ ë”í•´ì¤Œìœ¼ë¡œì¨ í•™ìŠµì„ ë•ìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œë¥¼ ì™„í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.2)  ResNetì˜ ì£¼ìš” íŠ¹ì§• ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œ í•´ê²°ResNetì€ ì”ì°¨ í•™ìŠµì„ í†µí•´ ê¹Šì€ ë„¤íŠ¸ì›Œí¬ì—ì„œë„ ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤.ì…ë ¥ì„ ì¶œë ¥ì— ë”í•´ì¤Œìœ¼ë¡œì¨ ì‹ í˜¸ê°€ ë”ìš± ì‰½ê²Œ ì „ë‹¬ë˜ì–´ í•™ìŠµì´ ì›í™œí•˜ê²Œ ì´ë£¨ì–´ì§‘ë‹ˆë‹¤. ê°„ë‹¨í•œ ë¸”ë¡ êµ¬ì¡°ResNetì€ ê°„ë‹¨í•œ ë¸”ë¡ êµ¬ì¡°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë„¤íŠ¸ì›Œí¬ë¥¼ ì‰½ê²Œ í™•ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  ë†’ì€ ì„±ëŠ¥ResNetì€ ì´ë¯¸ì§€ ë¶„ë¥˜, ê°ì²´ ê²€ì¶œ ë“± ë‹¤ì–‘í•œ ì»´í“¨í„° ë¹„ì „ ì‘ì—…ì—ì„œ ë†’ì€ ì„±ëŠ¥ì„ ë°œíœ˜í•©ë‹ˆë‹¤. ê¹Šì€ ë„¤íŠ¸ì›Œí¬ì—ì„œë„ ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆì–´, ë³µì¡í•œ íŒ¨í„´ì„ ì˜ í•™ìŠµí•©ë‹ˆë‹¤.3) ResNet ì‹¤ìŠµ ì½”ë“œPythonë³µì‚¬import torch
import torch.nn as nn
import torch.nn.functional as F

class Block(nn.Module):
def __init__(self, in_ch, out_ch, stride=1):
super(Block, self).__init__()
# ì²« ë²ˆì§¸ ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´
        self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_ch) # ë°°ì¹˜ ì •ê·œí™”
# ë‘ ë²ˆì§¸ ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´
        self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_ch) # ë°°ì¹˜ ì •ê·œí™”
# ì…ë ¥ê³¼ ì¶œë ¥ì˜ ì°¨ì›ì´ ë‹¤ë¥¼ ê²½ìš° shortcut ê²½ë¡œ ì •ì˜
        self.skip_connection = nn.Sequential()
if stride != 1 or in_ch != out_ch:
            self.skip_connection = nn.Sequential(
                nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=stride, bias=False), # ì°¨ì› ë§ì¶”ê¸° ìœ„í•œ 1x1 ì»¨ë³¼ë£¨ì…˜
                nn.BatchNorm2d(out_ch) # ë°°ì¹˜ ì •ê·œí™”
)
def forward(self, x):
# ì²« ë²ˆì§¸ ì»¨ë³¼ë£¨ì…˜ + ReLU í™œì„±í™” í•¨ìˆ˜
        output = F.relu(self.bn1(self.conv1(x)))
# ë‘ ë²ˆì§¸ ì»¨ë³¼ë£¨ì…˜ í›„ ë°°ì¹˜ ì •ê·œí™”
        output = self.bn2(self.conv2(output))
# shortcut ê²½ë¡œ ì¶œë ¥ê³¼ í˜„ì¬ ë¸”ë¡ì˜ ì¶œë ¥ ë”í•˜ê¸°
        output += self.skip_connection(x)
# ìµœì¢… ReLU í™œì„±í™” í•¨ìˆ˜ ì ìš©
        output = F.relu(output)
return output

# ResNet ëª¨ë¸ ì •ì˜
class CustomResNet(nn.Module):
def __init__(self, block, layers, num_classes=10):
super(CustomResNet, self).__init__()
        self.initial_channels = 64 # ì²« ë²ˆì§¸ ë ˆì´ì–´ì˜ ì…ë ¥ ì±„ë„ ìˆ˜ ì •ì˜
# ì²« ë²ˆì§¸ ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64) # ë°°ì¹˜ ì •ê·œí™”
# ResNetì˜ ê° ë ˆì´ì–´ ìƒì„±
        self.layer1 = self._create_layer(block, 64, layers[0], stride=1)
        self.layer2 = self._create_layer(block, 128, layers[1], stride=2)
        self.layer3 = self._create_layer(block, 256, layers[2], stride=2)
        self.layer4 = self._create_layer(block, 512, layers[3], stride=2)
# í‰ê·  í’€ë§ ë ˆì´ì–´
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
# ìµœì¢… ì™„ì „ ì—°ê²° ë ˆì´ì–´
        self.fc = nn.Linear(512, num_classes)
# ResNetì˜ ê° ë ˆì´ì–´ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜
def _create_layer(self, block, out_ch, num_layers, stride):
        layer_list = []
# ì²« ë²ˆì§¸ ë¸”ë¡ì€ strideë¥¼ ë°›ì„ ìˆ˜ ìˆìŒ
        layer_list.append(block(self.initial_channels, out_ch, stride))
        self.initial_channels = out_ch  # ë‹¤ìŒ ë¸”ë¡ì„ ìœ„í•´ ì±„ë„ ìˆ˜ ì—…ë°ì´íŠ¸
# ë‚˜ë¨¸ì§€ ë¸”ë¡ë“¤ì€ ê¸°ë³¸ strideë¥¼ ì‚¬ìš©
for _ in range(1, num_layers):
            layer_list.append(block(out_ch, out_ch))
return nn.Sequential(*layer_list)
def forward(self, x):
# ì²« ë²ˆì§¸ ì»¨ë³¼ë£¨ì…˜ + ReLU í™œì„±í™” í•¨ìˆ˜
        x = F.relu(self.bn1(self.conv1(x)))
# ê° ë ˆì´ì–´ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ í†µê³¼
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
# í‰ê·  í’€ë§ ë° í…ì„œì˜ ì°¨ì› ì¶•ì†Œ
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
# ìµœì¢… ì™„ì „ ì—°ê²° ë ˆì´ì–´ë¥¼ í†µí•´ í´ë˜ìŠ¤ë³„ ì˜ˆì¸¡ê°’ ì¶œë ¥
        x = self.fc(x)
return x

# Custom ResNet-18 ëª¨ë¸ ìƒì„± (ê° ë ˆì´ì–´ì˜ ë¸”ë¡ ìˆ˜ëŠ” 2ê°œì”©)
model = CustomResNet(Block, [2, 2, 2, 2], num_classes=10)

â€‹.
ë°ì´í„° ë³€í˜•í•´ë³´ê¸°: ë°ì´í„° ì •ë ¬ê³¼ ë³‘í•©ì¸ê³µì§€ëŠ¥ì„ ìœ„í•œ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬/ì¸ê³µì§€ëŠ¥ì„ ìœ„í•œ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ - 4ì£¼ì°¨/ë°ì´í„° ë³€í˜•í•´ë³´ê¸°: ë°ì´í„° ì •ë ¬ê³¼ ë³‘í•©ì œì‘:ë°ì´í„° ë³€í˜•í•´ë³´ê¸°: ë°ì´í„° ì •ë ¬ê³¼ ë³‘í•©ìˆ˜ì—… ëª©í‘œíŒë‹¤ìŠ¤ë¥¼ í†µí•´ ë°ì´í„°ë¥¼ ì •ë ¬í•˜ê³  ë³‘í•©í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ í•™ìŠµí•©ë‹ˆë‹¤.ëª©ì°¨ë°ì´í„° ì •ë ¬ë°ì´í„° ë³‘í•©ìš”ì•½ë°ì´í„° ì •ë ¬ë°ì´í„°ë¥¼ ì •ë ¬í•˜ëŠ” ê²ƒì€ ë°ì´í„° ë¶„ì„ì˜ ê¸°ë³¸ì…ë‹ˆë‹¤. íŒë‹¤ìŠ¤ì—ì„œëŠ” sort_values()ì™€ sort_index()ë¥¼ ì‚¬ìš©í•´ ì‰½ê²Œ ë°ì´í„°ë¥¼ ì •ë ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.sort_values()ë¥¼ ì‚¬ìš©í•œ ê°’(Value) ê¸°ì¤€ ì •ë ¬íŠ¹ì • ì—´ì˜ ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì˜¤ë¦„ì°¨ìˆœ ë˜ëŠ” ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Pythonë³µì‚¬import pandas as pd

# ì˜ˆì‹œ ë°ì´í„°í”„ë ˆì„ ìƒì„±
data = {
'ì´ë¦„': ['ì² ìˆ˜', 'ì˜í¬', 'ë¯¼ìˆ˜', 'ì§€ìˆ˜'],
'ë‚˜ì´': [25, 30, 22, 35],
'ì§ì—…': ['í•™ìƒ', 'íšŒì‚¬ì›', 'í•™ìƒ', 'í”„ë¦¬ëœì„œ']
}
df = pd.DataFrame(data)
# 'ë‚˜ì´' ê¸°ì¤€ìœ¼ë¡œ ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬
sorted_df = df.sort_values(by='ë‚˜ì´')
print(sorted_df)

â€‹ê²°ê³¼:Plain Textë³µì‚¬   ì´ë¦„  ë‚˜ì´       ì§ì—…
2  ë¯¼ìˆ˜   22      í•™ìƒ
0  ì² ìˆ˜   25      í•™ìƒ
1  ì˜í¬   30    íšŒì‚¬ì›
3  ì§€ìˆ˜   35  í”„ë¦¬ëœì„œ

â€‹ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬í•˜ë ¤ë©´ ascending=Falseë¥¼ ì§€ì •í•˜ì„¸ìš”.Pythonë³µì‚¬# 'ë‚˜ì´' ê¸°ì¤€ìœ¼ë¡œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
sorted_df_desc = df.sort_values(by='ë‚˜ì´', ascending=False)
print(sorted_df_desc)

â€‹sort_values()ë¥¼ ì‚¬ìš©í•œ ì—¬ëŸ¬ ì—´ ê¸°ì¤€ ì •ë ¬ì—¬ëŸ¬ ì—´ì„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìš°ì„ ìˆœìœ„ì— ë”°ë¼ ì²« ë²ˆì§¸ ì—´ë¶€í„° ì •ë ¬ë©ë‹ˆë‹¤.Pythonë³µì‚¬# 'ì§ì—…'ì„ ê¸°ì¤€ìœ¼ë¡œ, ê°™ì€ ì§ì—… ë‚´ì—ì„œ 'ë‚˜ì´' ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬
sorted_df_multi = df.sort_values(by=['ì§ì—…', 'ë‚˜ì´'])
print(sorted_df_multi)

â€‹sort_index()ë¥¼ ì‚¬ìš©í•œ ì¸ë±ìŠ¤ ê¸°ì¤€ ì •ë ¬ì¸ë±ìŠ¤ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì •ë ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Pythonë³µì‚¬# ì¸ë±ìŠ¤ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
sorted_index_df = df.sort_index()
print(sorted_index_df)

â€‹ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì¸ë±ìŠ¤ë¥¼ ì •ë ¬í•˜ë ¤ë©´ ascending=Falseë¥¼ ì§€ì •í•©ë‹ˆë‹¤.Pythonë³µì‚¬# ì¸ë±ìŠ¤ë¥¼ ë‚´ë¦¼ì°¨ìˆœìœ¼ë¡œ ì •ë ¬
sorted_index_df_desc = df.sort_index(ascending=False)
print(sorted_index_df_desc)

â€‹ë°ì´í„° ë³‘í•©ë°ì´í„° ë¶„ì„ ì‹œ ì—¬ëŸ¬ ê°œì˜ ë°ì´í„°ë¥¼ ë³‘í•©í•˜ëŠ” ì‘ì—…ì´ í•„ìš”í•  ë•Œê°€ ë§ìŠµë‹ˆë‹¤. íŒë‹¤ìŠ¤ì—ì„œëŠ” merge(), concat(), join()ì„ ì‚¬ìš©í•´ ë°ì´í„°ë¥¼ ë³‘í•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.merge()ë¥¼ ì‚¬ìš©í•œ ë°ì´í„°í”„ë ˆì„ ë³‘í•©SQLì˜ JOINê³¼ ìœ ì‚¬í•˜ê²Œ ë‘ ë°ì´í„°í”„ë ˆì„ì„ ê³µí†µ ì—´ì„ ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©í•©ë‹ˆë‹¤.Pythonë³µì‚¬# ì˜ˆì‹œ ë°ì´í„°í”„ë ˆì„ ìƒì„±
df1 = pd.DataFrame({
'ì´ë¦„': ['ì² ìˆ˜', 'ì˜í¬', 'ë¯¼ìˆ˜'],
'ë‚˜ì´': [25, 30, 22]
})

df2 = pd.DataFrame({
'ì´ë¦„': ['ì² ìˆ˜', 'ì˜í¬', 'ì§€ìˆ˜'],
'ì§ì—…': ['í•™ìƒ', 'íšŒì‚¬ì›', 'í”„ë¦¬ëœì„œ']
})
# 'ì´ë¦„'ì„ ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©
merged_df = pd.merge(df1, df2, on='ì´ë¦„')
print(merged_df)

â€‹ê²°ê³¼:Plain Textë³µì‚¬   ì´ë¦„  ë‚˜ì´    ì§ì—…
0  ì² ìˆ˜   25   í•™ìƒ
1  ì˜í¬   30  íšŒì‚¬ì›

â€‹ë‹¤ì–‘í•œ merge() ë°©ì‹inner (ê¸°ë³¸ê°’): ê³µí†µëœ ë°ì´í„°ë§Œ ë³‘í•©.outer: ê³µí†µë˜ì§€ ì•Šì€ ë°ì´í„°ë„ í¬í•¨í•˜ì—¬ ë³‘í•©, ì—†ëŠ” ê°’ì€ NaNìœ¼ë¡œ ì±„ì›€.left: ì™¼ìª½ ë°ì´í„°í”„ë ˆì„ ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©.right: ì˜¤ë¥¸ìª½ ë°ì´í„°í”„ë ˆì„ ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©.Pythonë³µì‚¬# outer joinì„ ì‚¬ìš©í•œ ë³‘í•©
merged_df_outer = pd.merge(df1, df2, on='ì´ë¦„', how='outer')
print(merged_df_outer)

â€‹ê²°ê³¼:Plain Textë³µì‚¬   ì´ë¦„   ë‚˜ì´       ì§ì—…
0  ì² ìˆ˜  25.0      í•™ìƒ
1  ì˜í¬  30.0    íšŒì‚¬ì›
2  ë¯¼ìˆ˜  22.0      NaN
3  ì§€ìˆ˜   NaN  í”„ë¦¬ëœì„œ

â€‹concat()ì„ ì‚¬ìš©í•œ ë°ì´í„°í”„ë ˆì„ ì—°ê²°í–‰(row) ë˜ëŠ” ì—´(column) ë‹¨ìœ„ë¡œ ë°ì´í„°í”„ë ˆì„ì„ ì—°ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Pythonë³µì‚¬# í–‰ ë‹¨ìœ„ë¡œ ë°ì´í„°í”„ë ˆì„ ì—°ê²°
concat_df = pd.concat([df1, df2], axis=0)
print(concat_df)

â€‹ê²°ê³¼:Plain Textë³µì‚¬    ì´ë¦„    ë‚˜ì´       ì§ì—…
0   ì² ìˆ˜  25.0       NaN
1   ì˜í¬  30.0       NaN
2   ë¯¼ìˆ˜  22.0       NaN
0   ì² ìˆ˜   NaN      í•™ìƒ
1   ì˜í¬   NaN    íšŒì‚¬ì›
2   ì§€ìˆ˜   NaN  í”„ë¦¬ëœì„œ

â€‹ì—´ ë‹¨ìœ„ë¡œ ì—°ê²°í•˜ê³  ì‹¶ë‹¤ë©´ axis=1ì„ ì§€ì •í•˜ì„¸ìš”.Pythonë³µì‚¬# ì—´ ë‹¨ìœ„ë¡œ ë°ì´í„°í”„ë ˆì„ ì—°ê²°
concat_df_axis1 = pd.concat([df1, df2], axis=1)
print(concat_df_axis1)

â€‹join()ì„ ì‚¬ìš©í•œ ë°ì´í„°í”„ë ˆì„ ë³‘í•©ì¸ë±ìŠ¤ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„°í”„ë ˆì„ì„ ë³‘í•©í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤.Pythonë³µì‚¬# ì˜ˆì‹œ ë°ì´í„°í”„ë ˆì„ ìƒì„±
df3 = pd.DataFrame({
'ì§ì—…': ['í•™ìƒ', 'íšŒì‚¬ì›', 'í”„ë¦¬ëœì„œ'],
'ì—°ë´‰': [2000, 3000, 4000]
}, index=['ì² ìˆ˜', 'ì˜í¬', 'ì§€ìˆ˜'])
# ì¸ë±ìŠ¤ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©
joined_df = df1.set_index('ì´ë¦„').join(df3)
print(joined_df)

â€‹ê²°ê³¼:Plain Textë³µì‚¬      ë‚˜ì´       ì§ì—…     ì—°ë´‰
ì´ë¦„
ì² ìˆ˜     25      í•™ìƒ  2000.0
ì˜í¬     30    íšŒì‚¬ì›  3000.0
ë¯¼ìˆ˜     22      NaN     NaN

â€‹ìš”ì•½ìš”ì•½ë°ì´í„° ì •ë ¬ì€ sort_values()ë¡œ íŠ¹ì • ì—´ ê¸°ì¤€ìœ¼ë¡œ, sort_index()ë¡œ ì¸ë±ìŠ¤ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ë°ì´í„° ë³‘í•©ì€ merge()ë¥¼ ì‚¬ìš©í•´ ê³µí†µ ì—´ì„ ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©í•˜ê±°ë‚˜, concat()ì„ ì‚¬ìš©í•´ í–‰ ë˜ëŠ” ì—´ ë‹¨ìœ„ë¡œ ì—°ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ì¸ë±ìŠ¤ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë³‘í•©í•  ë•ŒëŠ” join()ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìœ¼ë©°, ë‹¤ì–‘í•œ ë³‘í•© ë°©ì‹(inner, outer, left, right)ì´ ì§€ì›ë©ë‹ˆë‹¤.ì´ì œ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ì •ë ¬í•˜ê³ , í•„ìš”ì— ë”°ë¼ ë³‘í•©í•˜ì—¬ ë” ê¹Šì´ ìˆëŠ” ë¶„ì„ì„ ì§„í–‰í•´ë³´ì„¸ìš”! â€‹
[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 10ê°•. ì§€ë„í•™ìŠµ : ë¶„ë¥˜ëª¨ë¸ - SVM[SCC] ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ - 3ì£¼ì°¨/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 10ê°•. ì§€ë„í•™ìŠµ : ë¶„ë¥˜ëª¨ë¸ - SVMì œì‘:[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 10ê°•. ì§€ë„í•™ìŠµ : ë¶„ë¥˜ëª¨ë¸ - SVM[ìˆ˜ì—… ëª©í‘œ]SVM(Support Vector Machine)ì— ëŒ€í•œ ê°œë…ì„ ë°°ìš°ê³ , ë°ì´í„°ë¥¼ ì´ìš©í•´ ì‹¤ìŠµí•´ ë´…ë‹ˆë‹¤[ëª©ì°¨]01. SVM ê°œë…02. SVM ì‹¤ìŠµğŸ’¡
 
 01. SVM ê°œë…âœ”ï¸SVMì´ ë¬´ì—‡ì¸ì§€ ì•Œì•„ë´…ì‹œë‹¤1) SVM SVMì´ë€?ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹ (SVM)ì€ ë¶„ë¥˜ì™€ íšŒê·€ ë¶„ì„ì— ì‚¬ìš©ë˜ëŠ” ê°•ë ¥í•œ ì§€ë„í•™ìŠµ ëª¨ë¸ë°ì´í„°ë¥¼ ë¶„ë¥˜í•˜ê¸° ìœ„í•´ ê²°ì • ê²½ê³„(ê²°ì • ì´ˆí‰ë©´, hyperplane)ë¥¼ ì°¾ì•„ ë¶„ë¥˜í•©ë‹ˆë‹¤.ì´ˆí‰ë©´ì€ ë‘ í´ë˜ìŠ¤ ì‚¬ì´ì˜ ìµœëŒ€ ë§ˆì§„ì„ ë³´ì¥í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì„ íƒí•©ë‹ˆë‹¤.ALTë§ˆì§„ : ë‘ í´ë˜ìŠ¤ ê°„ì˜ ê°€ì¥ ê°€ê¹Œìš´ ë°ì´í„° í¬ì¸íŠ¸ ì‚¬ì´ì˜ ê±°ë¦¬ë§ˆì§„ : ë‘ í´ë˜ìŠ¤ ê°„ì˜ ê°€ì¥ ê°€ê¹Œìš´ ë°ì´í„° í¬ì¸íŠ¸ ì‚¬ì´ì˜ ê±°ë¦¬ï»¿
ì„œí¬íŠ¸ ë²¡í„° : ê²°ì • ì´ˆí‰ë©´ì— ê°€ì¥ ê°€ê¹Œì´ ìœ„ì¹˜í•œ ë°ì´í„° í¬ì¸íŠ¸ - ê²°ì • ì´ˆí‰ë©´ì„ ì •ì˜í•©ë‹ˆë‹¤ì„œí¬íŠ¸ ë²¡í„° : ê²°ì • ì´ˆí‰ë©´ì— ê°€ì¥ ê°€ê¹Œì´ ìœ„ì¹˜í•œ ë°ì´í„° í¬ì¸íŠ¸ - ê²°ì • ì´ˆí‰ë©´ì„ ì •ì˜í•©ë‹ˆë‹¤ï»¿
ì»¤ë„ í•¨ìˆ˜ : ë°ì´í„°ë¥¼ ë” ë†’ì€ ì°¨ì›ìœ¼ë¡œ ë§¤í•‘í•˜ì—¬ ì„ í˜•ì ìœ¼ë¡œ ë¶„ë¦¬ í•  ìˆ˜ ì—†ëŠ” ë°ì´í„°ë¥¼ ë¶„ë¦¬í•˜ê²Œ í•©ë‹ˆë‹¤. ì»¤ë„ í•¨ìˆ˜ : ë°ì´í„°ë¥¼ ë” ë†’ì€ ì°¨ì›ìœ¼ë¡œ ë§¤í•‘í•˜ì—¬ ì„ í˜•ì ìœ¼ë¡œ ë¶„ë¦¬ í•  ìˆ˜ ì—†ëŠ” ë°ì´í„°ë¥¼ ë¶„ë¦¬í•˜ê²Œ í•©ë‹ˆë‹¤. ï»¿â€‹ SVMì˜ ëª©ì SVMì˜ ëª©í‘œëŠ” ë§ˆì§„ì„ ìµœëŒ€í™”í•˜ë©´ì„œ ê²°ì • ì´ˆí‰ë©´ì„ ì°¾ì•„ ë°ì´í„° í¬ì¸íŠ¸ë¥¼ ì •í™•í•˜ê²Œ ë¶„ë¥˜í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ëŠ” ì¼ë°˜í™” ì„±ëŠ¥ì„ ë†’ì´ëŠ” ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤.wâ‹…xâˆ’b=0   - b = 0 wâ‹…xâˆ’b=0ì—¬ê¸°ì„œ wëŠ” ê°€ì¤‘ì¹˜ ë²¡í„°, xëŠ” ì…ë ¥ ë²¡í„°, bëŠ” ì ˆí¸ì…ë‹ˆë‹¤.\)ëŠ” ê°€ì¤‘ì¹˜ ë²¡í„°, \(\)ëŠ” ì…ë ¥ ë²¡í„°, \(b\)ëŠ” ì ˆí¸ì…ë‹ˆë‹¤.}ì—¬ê¸°ì„œ wëŠ” ê°€ì¤‘ì¹˜ ë²¡í„°, xëŠ” ì…ë ¥ ë²¡í„°, bëŠ” ì ˆí¸ì…ë‹ˆë‹¤.ï»¿â€‹02. SVM ì‹¤ìŠµâœ”ï¸Scikit-learnì˜ ìœ ë°©ì•”ë°ì´í„°ì™€ Seabornì˜ íƒ€ì´íƒ€ë‹‰ ë°ì´í„°ë¡œ SVM ì‹¤ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤1) ìœ ë°©ì•” ë°ì´í„° ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ìœ ë°©ì•” ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ {5px}ìœ ë°©ì•” ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ï»¿â€‹Pythonë³µì‚¬import numpy as np
import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# ë°ì´í„° ë¡œë“œ
data = load_breast_cancer()
X = data.data
y = data.target

# ë°ì´í„° ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# ë°ì´í„° ìŠ¤ì¼€ì¼ë§
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
â€‹sklearn.datasets.load_breast_cancer: ìœ ë°©ì•” ë°ì´í„°ì…‹ ë¡œë“œreturn_X_y=False: ë°ì´í„°ì™€ íƒ€ê²Ÿì„ í•¨ê»˜ ë°˜í™˜í• ì§€ ì—¬ë¶€. ê¸°ë³¸ê°’ì€ Falseì…ë‹ˆë‹¤._X_y=False: ë°ì´í„°ì™€ íƒ€ê²Ÿì„ í•¨ê»˜ ë°˜í™˜í• ì§€ ì—¬ë¶€. ê¸°ë³¸ê°’ì€ Falseì…ë‹ˆë‹¤.ï»¿â€‹sklearn.model_selection.train_test_split: ë°ì´í„°ë¥¼ í›ˆë ¨ ì„¸íŠ¸/ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ ë¶„í• test_size=0.2: í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì˜ ë¹„ìœ¨ì„ 0.2ë¡œ ì„¤ì •í•©ë‹ˆë‹¤._size=0.2: í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì˜ ë¹„ìœ¨ì„ 0.2ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.ï»¿â€‹random_state=42: ëœë¤ ì‹œë“œ ê°’ìœ¼ë¡œ, ë°ì´í„° ë¶„í• ì˜ ì¬í˜„ì„±ì„ ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤._state=42: ëœë¤ ì‹œë“œ ê°’ìœ¼ë¡œ, ë°ì´í„° ë¶„í• ì˜ ì¬í˜„ì„±ì„ ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.ï»¿â€‹sklearn.preprocessing.StandardScaler: ë°ì´í„°ì˜ í‰ê· ì„ 0, ë¶„ì‚°ì„ 1ë¡œ ìŠ¤ì¼€ì¼ë§fit_transform(X_train): í›ˆë ¨ ì„¸íŠ¸ë¥¼ ìŠ¤ì¼€ì¼ë§í•˜ê³  ë³€í™˜í•©ë‹ˆë‹¤._transform(X_train): í›ˆë ¨ ì„¸íŠ¸ë¥¼ ìŠ¤ì¼€ì¼ë§í•˜ê³  ë³€í™˜í•©ë‹ˆë‹¤.ï»¿â€‹transform(X_test): í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¥¼ ë³€í™˜í•©ë‹ˆë‹¤.(X_test): í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¥¼ ë³€í™˜í•©ë‹ˆë‹¤.ï»¿â€‹ ëª¨ë¸ í•™ìŠµëª¨ë¸ í•™ìŠµ {5px}ëª¨ë¸ í•™ìŠµ ï»¿â€‹Pythonë³µì‚¬from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
model = SVC(kernel='linear')
model.fit(X_train, y_train)
# ì˜ˆì¸¡
y_pred = model.predict(X_test)
# í‰ê°€
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
print(f"Classification Report:")
print(f"Confusion Matrix:")
â€‹sklearn.svm.SVC: ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹  ë¶„ë¥˜ ëª¨ë¸ ìƒì„±kernel=â€™linearâ€™: ì„ í˜• ì»¤ë„ì„ ì‚¬ìš©í•˜ì—¬ SVMì„ í•™ìŠµí•©ë‹ˆë‹¤.=â€™linearâ€™: ì„ í˜• ì»¤ë„ì„ ì‚¬ìš©í•˜ì—¬ SVMì„ í•™ìŠµí•©ë‹ˆë‹¤.ï»¿â€‹fit(X_train, y_train): ëª¨ë¸ì„ í›ˆë ¨ ì„¸íŠ¸ì— ë§ì¶”ì–´ í•™ìŠµì‹œí‚µë‹ˆë‹¤(X_train, y_train): ëª¨ë¸ì„ í›ˆë ¨ ì„¸íŠ¸ì— ë§ì¶”ì–´ í•™ìŠµì‹œí‚µë‹ˆë‹¤ï»¿â€‹predict(X_test): í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì— ëŒ€í•´ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.(X_test): í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì— ëŒ€í•´ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.ï»¿â€‹sklearn.metrics.accuracy_score: ì •í™•ë„ ê³„ì‚°accuracy_score(y_test, y_pred): ì‹¤ì œ ê°’ê³¼ ì˜ˆì¸¡ ê°’ì„ ë¹„êµí•˜ì—¬ ì •í™•ë„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤._score(y_test, y_pred): ì‹¤ì œ ê°’ê³¼ ì˜ˆì¸¡ ê°’ì„ ë¹„êµí•˜ì—¬ ì •í™•ë„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.ï»¿â€‹sklearn.metrics.classification_report: ë¶„ë¥˜ ë³´ê³ ì„œ ìƒì„±classification_report(y_test, y_pred): ì •í™•ë„, ì •ë°€ë„, ì¬í˜„ìœ¨ ë“±ì˜ ë©”íŠ¸ë¦­ì„ í¬í•¨í•œ ë³´ê³ ì„œë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤._report(y_test, y_pred): ì •í™•ë„, ì •ë°€ë„, ì¬í˜„ìœ¨ ë“±ì˜ ë©”íŠ¸ë¦­ì„ í¬í•¨í•œ ë³´ê³ ì„œë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.ï»¿â€‹sklearn.metrics.confusion_matrix: í˜¼ë™ í–‰ë ¬ ìƒì„±confusion_matrix(y_test, y_pred): ì‹¤ì œ ê°’ê³¼ ì˜ˆì¸¡ ê°’ì˜ í˜¼ë™ í–‰ë ¬ì„ ë°˜í™˜í•©ë‹ˆë‹¤._matrix(y_test, y_pred): ì‹¤ì œ ê°’ê³¼ ì˜ˆì¸¡ ê°’ì˜ í˜¼ë™ í–‰ë ¬ì„ ë°˜í™˜í•©ë‹ˆë‹¤.ï»¿â€‹2) íƒ€ì´íƒ€ë‹‰ ë°ì´í„° ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬íƒ€ì´íƒ€ë‹‰ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ {5px}íƒ€ì´íƒ€ë‹‰ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ï»¿â€‹Pythonë³µì‚¬import seaborn as sns

# ë°ì´í„° ë¡œë“œ
titanic = sns.load_dataset('titanic')
# í•„ìš”í•œ ì—´ ì„ íƒ ë° ê²°ì¸¡ê°’ ì²˜ë¦¬
titanic = titanic[['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']].dropna()
# ì„±ë³„ê³¼ íƒ‘ìŠ¹í•œ ê³³ ì¸ì½”ë”©
titanic['sex'] = titanic['sex'].map({'male': 0, 'female': 1})
titanic['embarked'] = titanic['embarked'].map({'C': 0, 'Q': 1, 'S': 2})
# íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
X = titanic.drop('survived', axis=1)
y = titanic['survived']
# ë°ì´í„° ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# ë°ì´í„° ìŠ¤ì¼€ì¼ë§
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
â€‹seaborn.load_dataset: seabornì˜ ë‚´ì¥ ë°ì´í„°ì…‹ ë¡œë“œâ€™titanicâ€™: íƒ€ì´íƒ€ë‹‰ ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤.â€™titanicâ€™: íƒ€ì´íƒ€ë‹‰ ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤.ï»¿â€‹ pandas.DataFrame.dropna: ê²°ì¸¡ê°’ì´ ìˆëŠ” í–‰ ì œê±°pandas.DataFrame.map: ë°ì´í„° ê°’ì„ ë‹¤ë¥¸ ê°’ìœ¼ë¡œ ë§¤í•‘â€™maleâ€™: 0, â€™femaleâ€™: 1: ì„±ë³„ì„ ìˆ«ìë¡œ ë§¤í•‘í•©ë‹ˆë‹¤.: ì„±ë³„ì„ ìˆ«ìë¡œ ë§¤í•‘í•©ë‹ˆë‹¤.}â€™maleâ€™: 0, â€™femaleâ€™: 1: ì„±ë³„ì„ ìˆ«ìë¡œ ë§¤í•‘í•©ë‹ˆë‹¤.ï»¿â€‹â€™Câ€™: 0, â€™Qâ€™: 1, â€™Sâ€™: 2: íƒ‘ìŠ¹í•œ ê³³ì„ ìˆ«ìë¡œ ë§¤í•‘í•©ë‹ˆë‹¤.: íƒ‘ìŠ¹í•œ ê³³ì„ ìˆ«ìë¡œ ë§¤í•‘í•©ë‹ˆë‹¤.}â€™Câ€™: 0, â€™Qâ€™: 1, â€™Sâ€™: 2: íƒ‘ìŠ¹í•œ ê³³ì„ ìˆ«ìë¡œ ë§¤í•‘í•©ë‹ˆë‹¤.ï»¿â€‹ ëª¨ë¸ í•™ìŠµëª¨ë¸ í•™ìŠµ {5px}ëª¨ë¸ í•™ìŠµ ï»¿â€‹Pythonë³µì‚¬# ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
model = SVC(kernel='linear')
model.fit(X_train, y_train)
# ì˜ˆì¸¡
y_pred = model.predict(X_test)
# í‰ê°€
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
print(f"Classification Report:")
print(f"Confusion Matrix:")
â€‹.
[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 10. ì´ë¯¸ì§€ ì²˜ë¦¬ ëª¨ë¸[SCC] ê¸°ì´ˆê°€ íƒ„íƒ„í•œ ë”¥ëŸ¬ë‹/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] ê¸°ì´ˆê°€ íƒ„íƒ„í•œ ë”¥ëŸ¬ë‹ - 4ì£¼ì°¨/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 10. ì´ë¯¸ì§€ ì²˜ë¦¬ ëª¨ë¸ì œì‘:[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 10. ì´ë¯¸ì§€ ì²˜ë¦¬ ëª¨ë¸[ìˆ˜ì—… ëª©í‘œ]ì´ë¯¸ì§€ ì²˜ë¦¬ ëª¨ë¸ì— ëŒ€í•´ ë°°ì›Œë´…ì‹œë‹¤Pytorchë¡œ ê°„ë‹¨í•œ YOLO ëª¨ë¸ êµ¬í˜„ ì‹¤ìŠµì„ ì§„í–‰í•´ ë´…ì‹œë‹¤[ëª©ì°¨]01. CNNê¸°ë°˜ ì´ë¯¸ì§€ ë¶„ë¥˜ğŸ’¡
 
 01. CNNê¸°ë°˜ ì´ë¯¸ì§€ ë¶„ë¥˜âœ”ï¸CNNê¸°ë°˜ì˜ ì´ë¯¸ì§€ë¶„ë¥˜ ì•„í‚¤í…ì³ ì†Œê°œì™€ YOLO, ì´ë¯¸ì§€ ì„¸ê·¸ë©˜í…Œì´ì…˜ì— ëŒ€í•´ ë°°ì›Œë´…ì‹œë‹¤1) ResNet ë“± ì£¼ìš” CNN ì•„í‚¤í…ì³ ì†Œê°œ ResNet (Residual Network)ResNetì€ ë§¤ìš° ê¹Šì€ ì‹ ê²½ë§ì„ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ëœ ì•„í‚¤í…ì²˜ì…ë‹ˆë‹¤.ì”ì°¨ ì—°ê²°(Residual Connection)ì„ ë„ì…í•˜ì—¬, ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤.ResNet-50, ResNet-101, ResNet-152 ë“±ì˜ ë³€í˜•ì´ ìˆìŠµë‹ˆë‹¤. VGGVGGëŠ” ì‘ì€ 3x3 í•„í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ê¹Šì´ë¥¼ ì¦ê°€ì‹œí‚¨ ì•„í‚¤í…ì²˜ì…ë‹ˆë‹¤.VGG16ê³¼ VGG19ê°€ ëŒ€í‘œì ì¸ ëª¨ë¸ì…ë‹ˆë‹¤.ë‹¨ìˆœí•˜ê³  ê·œì¹™ì ì¸ êµ¬ì¡°ë¡œ ì¸í•´, ë‹¤ì–‘í•œ ë³€í˜•ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. InceptionInceptionì€ ë‹¤ì–‘í•œ í¬ê¸°ì˜ í•„í„°ë¥¼ ë³‘ë ¬ë¡œ ì ìš©í•˜ì—¬, ì—¬ëŸ¬ ìˆ˜ì¤€ì˜ íŠ¹ì§•ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.Inception ëª¨ë“ˆì„ ì‚¬ìš©í•˜ì—¬, ë„¤íŠ¸ì›Œí¬ì˜ ê¹Šì´ì™€ ë„ˆë¹„ë¥¼ ë™ì‹œì— í™•ì¥í•©ë‹ˆë‹¤.GoogLeNet(Inception v1), Inception v2, Inception v3 ë“±ì´ ìˆìŠµë‹ˆë‹¤.2) ê°ì²´ íƒì§€(YOLO) YOLO(You Only Look Once) YOLO(You Only Look Once)ëŠ” ê°ì²´ íƒì§€(Object Detection) ëª¨ë¸ë¡œ, ì´ë¯¸ì§€ì—ì„œ ê°ì²´ì˜ ìœ„ì¹˜ì™€ í´ë˜ìŠ¤ë¥¼ ë™ì‹œì— ì˜ˆì¸¡í•©ë‹ˆë‹¤.YOLOëŠ” ì´ë¯¸ì§€ ì „ì²´ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬í•˜ì—¬, ë¹ ë¥´ê³  ì •í™•í•œ ê°ì²´ íƒì§€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.ALT YOLOì˜ ê°œë…YOLOëŠ” ì´ë¯¸ì§€ë¥¼ SxS ê·¸ë¦¬ë“œë¡œ ë‚˜ëˆ„ê³ , ê° ê·¸ë¦¬ë“œ ì…€ì—ì„œ ê°ì²´ì˜ ì¡´ì¬ ì—¬ë¶€ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤.ê° ê·¸ë¦¬ë“œ ì…€ì€ Bê°œì˜ ë°”ìš´ë”© ë°•ìŠ¤ì™€ Cê°œì˜ í´ë˜ìŠ¤ í™•ë¥ ì„ ì¶œë ¥í•©ë‹ˆë‹¤. YOLOì˜ ë™ì‘ ì›ë¦¬ì…ë ¥ ì´ë¯¸ì§€ë¥¼ CNNì„ í†µí•´ íŠ¹ì§• ë§µìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.íŠ¹ì§• ë§µì„ SxS ê·¸ë¦¬ë“œë¡œ ë‚˜ëˆ„ê³ , ê° ê·¸ë¦¬ë“œ ì…€ì—ì„œ ë°”ìš´ë”© ë°•ìŠ¤ì™€ í´ë˜ìŠ¤ í™•ë¥ ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.ì˜ˆì¸¡ëœ ë°”ìš´ë”© ë°•ìŠ¤ì™€ í´ë˜ìŠ¤ í™•ë¥ ì„ ë°”íƒ•ìœ¼ë¡œ, ê°ì²´ì˜ ìœ„ì¹˜ì™€ í´ë˜ìŠ¤ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.3) ì´ë¯¸ì§€ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì´ë¯¸ì§€ ì„¸ê·¸ë©˜í…Œì´ì…˜ ê¸°ë²•ê³¼ ì‘ìš©ì´ë¯¸ì§€ ì„¸ê·¸ë©˜í…Œì´ì…˜(Image Segmentation)ì€ ì´ë¯¸ì§€ì˜ ê° í”½ì…€ì„ í´ë˜ìŠ¤ ë ˆì´ë¸”ë¡œ ë¶„ë¥˜í•˜ëŠ” ì‘ì—…ì…ë‹ˆë‹¤. ì´ë¯¸ì§€ ì„¸ê·¸ë©˜í…Œì´ì…˜ì€ ì£¼ë¡œ ì‹œë§¨í‹± ì„¸ê·¸ë©˜í…Œì´ì…˜ê³¼ ì¸ìŠ¤í„´ìŠ¤ ì„¸ê·¸ë©˜í…Œì´ì…˜ ë‘ê°€ì§€ë¡œ ë‚˜ë‰©ë‹ˆë‹¤ALTì‹œë§¨í‹± ì„¸ê·¸ë©˜í…Œì´ì…˜ (Semantic Segmentation)
ì´ë¯¸ì§€ì˜ ê° í”½ì…€ì„ í´ë˜ìŠ¤ ë ˆì´ë¸”ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤ì´ë¯¸ì§€ì˜ ê° í”½ì…€ì„ í´ë˜ìŠ¤ ë ˆì´ë¸”ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤ï»¿
ì¸ìŠ¤í„´ìŠ¤ ì„¸ê·¸ë©˜í…Œì´ì…˜ (Instance Segmentation)
ì‹œë§¨í‹± ì„¸ê·¸ë©˜í…Œì´ì…˜ê³¼ ë‹¬ë¦¬, ê°™ì€ í´ë˜ìŠ¤ ë‚´ì—ì„œë„ ê°œë³„ ê°ì²´ë¥¼ êµ¬ë¶„í•©ë‹ˆë‹¤.ì‹œë§¨í‹± ì„¸ê·¸ë©˜í…Œì´ì…˜ê³¼ ë‹¬ë¦¬, ê°™ì€ í´ë˜ìŠ¤ ë‚´ì—ì„œë„ ê°œë³„ ê°ì²´ë¥¼ êµ¬ë¶„í•©ë‹ˆë‹¤.ï»¿â€‹ ì£¼ìš” ì„¸ê·¸ë©˜í…Œì´ì…˜ ëª¨ë¸FCN (Fully Convolutional Network): ëª¨ë“  ë ˆì´ì–´ë¥¼ í•©ì„±ê³± ë ˆì´ì–´ë¡œ êµ¬ì„±í•˜ì—¬, í”½ì…€ ë‹¨ìœ„ì˜ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.U-Net: Uìí˜• êµ¬ì¡°ë¥¼ ê°€ì§€ë©°, ì¸ì½”ë”-ë””ì½”ë” ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„¸ê·¸ë©˜í…Œì´ì…˜ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.Mask R-CNN: ê°ì²´ íƒì§€ì™€ ì¸ìŠ¤í„´ìŠ¤ ì„¸ê·¸ë©˜í…Œì´ì…˜ì„ ë™ì‹œì— ìˆ˜í–‰í•˜ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤..
ë°ì´í„° ë³€í˜•í•´ë³´ê¸°: ê·¸ë£¹í™” ë° ì§‘ê³„, í”¼ë²—í…Œì´ë¸”ì¸ê³µì§€ëŠ¥ì„ ìœ„í•œ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬/ì¸ê³µì§€ëŠ¥ì„ ìœ„í•œ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ - 4ì£¼ì°¨/ë°ì´í„° ë³€í˜•í•´ë³´ê¸°: ê·¸ë£¹í™” ë° ì§‘ê³„, í”¼ë²—í…Œì´ë¸”ì œì‘:ë°ì´í„° ë³€í˜•í•´ë³´ê¸°: ê·¸ë£¹í™” ë° ì§‘ê³„, í”¼ë²—í…Œì´ë¸”ìˆ˜ì—… ëª©í‘œíŒë‹¤ìŠ¤ì—ì„œ ê·¸ë£¹í™” ë° í”¼ë²—í…Œì´ë¸”ì„ ìƒì„±í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ì„œ ì•Œì•„ë´…ë‹ˆë‹¤.ëª©ì°¨ë°ì´í„° ê·¸ë£¹í™” ë° ì§‘ê³„ í”¼ë²—í…Œì´ë¸” ì‚¬ìš©í•˜ê¸°ìš”ì•½ë°ì´í„° ê·¸ë£¹í™” ë° ì§‘ê³„ ğŸ“šë°ì´í„° ê·¸ë£¹í™”ëŠ” ë°ì´í„°ë¥¼ íŠ¹ì • ê¸°ì¤€ì— ë”°ë¼ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆ„ê³ , ì§‘ê³„ëŠ” ê° ê·¸ë£¹ì— ëŒ€í•´ ìš”ì•½ í†µê³„ë¥¼ ê³„ì‚°í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. íŒë‹¤ìŠ¤ì—ì„œëŠ” groupby()ì™€ ì§‘ê³„ í•¨ìˆ˜ë“¤ì„ ì‚¬ìš©í•´ ì´ë¥¼ ì†ì‰½ê²Œ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.groupby()ë¥¼ ì‚¬ìš©í•œ ê·¸ë£¹í™”groupby() í•¨ìˆ˜ëŠ” ë°ì´í„°ë¥¼ íŠ¹ì • ì—´ì„ ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í™”í•©ë‹ˆë‹¤.Pythonë³µì‚¬import pandas as pd

# ì˜ˆì‹œ ë°ì´í„°í”„ë ˆì„ ìƒì„±
data = {
'ì´ë¦„': ['ì² ìˆ˜', 'ì˜í¬', 'ë¯¼ìˆ˜', 'ì§€ìˆ˜', 'ì² ìˆ˜', 'ì˜í¬'],
'ê³¼ëª©': ['ìˆ˜í•™', 'ìˆ˜í•™', 'ê³¼í•™', 'ê³¼í•™', 'ì˜ì–´', 'ì˜ì–´'],
'ì ìˆ˜': [90, 85, 95, 80, 75, 88]
}
df = pd.DataFrame(data)
# 'ì´ë¦„'ì„ ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í™”
grouped = df.groupby('ì´ë¦„')

â€‹ì§‘ê³„ í•¨ìˆ˜ ì‚¬ìš©í•˜ê¸°ê·¸ë£¹í™”í•œ ë°ì´í„°ì— ëŒ€í•´ ë‹¤ì–‘í•œ ì§‘ê³„ í•¨ìˆ˜(mean, sum, count ë“±)ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Pythonë³µì‚¬# ê° í•™ìƒì˜ í‰ê·  ì ìˆ˜ ê³„ì‚°
mean_scores = grouped['ì ìˆ˜'].mean()
print(mean_scores)

â€‹ê²°ê³¼:Plain Textë³µì‚¬ì´ë¦„
ë¯¼ìˆ˜    95.0
ì˜í¬    86.5
ì² ìˆ˜    82.5
ì§€ìˆ˜    80.0
Name: ì ìˆ˜, dtype: float64

â€‹ì—¬ëŸ¬ ì§‘ê³„ í•¨ìˆ˜ë¥¼ ë™ì‹œì— ì‚¬ìš©í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.Pythonë³µì‚¬# ê° í•™ìƒì˜ ì ìˆ˜ í•©ê³„ì™€ í‰ê·  ê³„ì‚°
agg_scores = grouped['ì ìˆ˜'].agg(['sum', 'mean'])
print(agg_scores)

â€‹ê²°ê³¼:Plain Textë³µì‚¬      sum  mean
ì´ë¦„
ë¯¼ìˆ˜    95  95.0
ì˜í¬   173  86.5
ì² ìˆ˜   165  82.5
ì§€ìˆ˜    80  80.0

â€‹ì—¬ëŸ¬ ì—´ì„ ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í™”í•˜ê¸°ì—¬ëŸ¬ ì—´ì„ ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í™”í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.Pythonë³µì‚¬# 'ì´ë¦„'ê³¼ 'ê³¼ëª©'ì„ ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ì ìˆ˜ í•©ê³„ ê³„ì‚°
grouped_multi = df.groupby(['ì´ë¦„', 'ê³¼ëª©'])['ì ìˆ˜'].sum()
print(grouped_multi)

â€‹ê²°ê³¼:Plain Textë³µì‚¬ì´ë¦„  ê³¼ëª©
ë¯¼ìˆ˜  ê³¼í•™    95
ì˜í¬  ìˆ˜í•™    85
      ì˜ì–´    88
ì² ìˆ˜  ìˆ˜í•™    90
      ì˜ì–´    75
ì§€ìˆ˜  ê³¼í•™    80
Name: ì ìˆ˜, dtype: int64

â€‹í”¼ë²—í…Œì´ë¸” ì‚¬ìš©í•˜ê¸°ğŸ“ší”¼ë²—í…Œì´ë¸”ì€ ë°ì´í„°ë¥¼ ìš”ì•½í•˜ëŠ” ê°•ë ¥í•œ ë„êµ¬ì…ë‹ˆë‹¤. ì—‘ì…€ì˜ í”¼ë²—í…Œì´ë¸”ê³¼ ìœ ì‚¬í•˜ë©°, íŠ¹ì • ê¸°ì¤€ì— ë”°ë¼ ë°ì´í„°ë¥¼ ì¬êµ¬ì¡°í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.pivot_table() ê¸°ë³¸ ì‚¬ìš©ë²•pivot_table() í•¨ìˆ˜ëŠ” ë°ì´í„°ë¥¼ ìš”ì•½í•˜ê³ , íŠ¹ì • ê¸°ì¤€ì— ë”°ë¼ ì¬êµ¬ì¡°í™”í•©ë‹ˆë‹¤.Pythonë³µì‚¬# í”¼ë²—í…Œì´ë¸” ìƒì„±: 'ì´ë¦„'ì„ ì¸ë±ìŠ¤ë¡œ, 'ê³¼ëª©'ì„ ì»¬ëŸ¼ìœ¼ë¡œ í•˜ì—¬ ì ìˆ˜ì˜ í‰ê·  ê³„ì‚°
pivot = pd.pivot_table(df, index='ì´ë¦„', columns='ê³¼ëª©', values='ì ìˆ˜', aggfunc='mean')
print(pivot)

â€‹ê²°ê³¼:Plain Textë³µì‚¬ê³¼ëª©   ê³¼í•™    ìˆ˜í•™    ì˜ì–´
ì´ë¦„
ë¯¼ìˆ˜  95.0   NaN   NaN
ì˜í¬   NaN  85.0  88.0
ì² ìˆ˜   NaN  90.0  75.0
ì§€ìˆ˜  80.0   NaN   NaN

â€‹ì—¬ëŸ¬ ì§‘ê³„ í•¨ìˆ˜ ì‚¬ìš©í•˜ê¸°aggfuncì— ì—¬ëŸ¬ ì§‘ê³„ í•¨ìˆ˜ë¥¼ ì§€ì •í•˜ë©´, ë‹¤ì–‘í•œ ìš”ì•½ í†µê³„ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.Pythonë³µì‚¬# ì ìˆ˜ì˜ í•©ê³„ì™€ í‰ê· ì„ ê³„ì‚°í•˜ëŠ” í”¼ë²—í…Œì´ë¸” ìƒì„±
pivot_multi = pd.pivot_table(df, index='ì´ë¦„', columns='ê³¼ëª©', values='ì ìˆ˜', aggfunc=['sum', 'mean'])
print(pivot_multi)

â€‹ê²°ê³¼:Plain Textë³µì‚¬        sum          mean
ê³¼ëª©     ê³¼í•™   ìˆ˜í•™   ì˜ì–´   ê³¼í•™   ìˆ˜í•™   ì˜ì–´
ì´ë¦„
ë¯¼ìˆ˜   95.0   NaN   NaN  95.0   NaN   NaN
ì˜í¬    NaN  85.0  88.0   NaN  85.0  88.0
ì² ìˆ˜    NaN  90.0  75.0   NaN  90.0  75.0
ì§€ìˆ˜   80.0   NaN   NaN  80.0   NaN   NaN

â€‹margins ì˜µì…˜ì„ ì‚¬ìš©í•´ ì „ì²´ í•©ê³„ ì¶”ê°€í•˜ê¸°margins=True ì˜µì…˜ì„ ì‚¬ìš©í•˜ë©´, ê° í–‰ê³¼ ì—´ì˜ í•©ê³„ê°€ ì¶”ê°€ëœ í”¼ë²—í…Œì´ë¸”ì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Pythonë³µì‚¬# ê° ì´ë¦„ë³„, ê³¼ëª©ë³„ ì´í•©ì„ í¬í•¨í•œ í”¼ë²—í…Œì´ë¸” ìƒì„±
pivot_with_totals = pd.pivot_table(df, index='ì´ë¦„', columns='ê³¼ëª©', values='ì ìˆ˜', aggfunc='sum', margins=True)
print(pivot_with_totals)

â€‹ê²°ê³¼:Plain Textë³µì‚¬ê³¼ëª©      ê³¼í•™    ìˆ˜í•™    ì˜ì–´     All
ì´ë¦„
ë¯¼ìˆ˜    95.0   NaN   NaN    95.0
ì˜í¬     NaN  85.0  88.0   173.0
ì² ìˆ˜     NaN  90.0  75.0   165.0
ì§€ìˆ˜    80.0   NaN   NaN    80.0
All   175.0  175.0  163.0  513.0

â€‹ìš”ì•½ğŸ“šìš”ì•½ê·¸ë£¹í™”ëŠ” groupby()ë¥¼ ì‚¬ìš©í•´ ë°ì´í„°ë¥¼ íŠ¹ì • ê¸°ì¤€ì— ë”°ë¼ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆ„ê³ , ì§‘ê³„ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ ê° ê·¸ë£¹ì— ëŒ€í•œ í†µê³„ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.í”¼ë²—í…Œì´ë¸”ì€ pivot_table()ì„ ì‚¬ìš©í•´ ë°ì´í„°ë¥¼ ìš”ì•½í•˜ê³  ì¬êµ¬ì¡°í™”í•  ìˆ˜ ìˆìœ¼ë©°, ì—¬ëŸ¬ ì§‘ê³„ í•¨ìˆ˜ì™€ margins ì˜µì…˜ì„ í†µí•´ ì „ì²´ í•©ê³„ë¥¼ í¬í•¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ê·¸ë£¹í™”ì™€ í”¼ë²—í…Œì´ë¸”ì€ ë°ì´í„° ë¶„ì„ì—ì„œ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ìš”ì•½í•˜ê³  íŒ¨í„´ì„ ë°œê²¬í•˜ëŠ” ë° ë§¤ìš° ìœ ìš©í•©ë‹ˆë‹¤.ì´ì œ ë°ì´í„°ë¥¼ ê·¸ë£¹í™”í•˜ê³  í”¼ë²—í…Œì´ë¸”ì„ ì‚¬ìš©í•˜ì—¬ ë¶„ì„ì„ í•œ ë‹¨ê³„ ë” ê¹Šì´ ìˆê²Œ ì§„í–‰í•´ë³´ì„¸ìš”! â€‹ìµëª…9ì›” 25ì¼í…ìŠ¤íŠ¸ ëˆ„ë½ ëœ ë¶€ë¶„ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤
[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 11ê°•. ì§€ë„í•™ìŠµ : ë¶„ë¥˜ëª¨ë¸ - KNN[SCC] ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ - 3ì£¼ì°¨/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 11ê°•. ì§€ë„í•™ìŠµ : ë¶„ë¥˜ëª¨ë¸ - KNNì œì‘:[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 11ê°•. ì§€ë„í•™ìŠµ : ë¶„ë¥˜ëª¨ë¸ - KNN[ìˆ˜ì—… ëª©í‘œ]KNN(K-Nearest Neighbors)ì— ëŒ€í•œ ê°œë…ì„ ë°°ìš°ê³ , ë°ì´í„°ë¥¼ ì´ìš©í•´ ì‹¤ìŠµí•´ ë´…ë‹ˆë‹¤[ëª©ì°¨]01. KNN ê°œë…02. KNN ì‹¤ìŠµğŸ’¡
 
 01. KNN ê°œë…âœ”ï¸KNN(K ìµœê·¼ì ‘ ì´ì›ƒ , K-Nearest Neighbors)ì´ ë¬´ì—‡ì¸ì§€ ì•Œì•„ë´…ì‹œë‹¤1) KNN KNNì´ë€?KNN ì•Œê³ ë¦¬ì¦˜ì€ ë¶„ë¥˜ì™€ íšŒê·€ ë¶„ì„ì— ì‚¬ìš©ë˜ëŠ” ë¹„ëª¨ìˆ˜ì  ë°©ë²• ì…ë‹ˆë‹¤.ìƒˆë¡œìš´ ë°ì´í„° í¬ì¸íŠ¸ë¥¼ ê¸°ì¡´ ë°ì´í„° í¬ì¸íŠ¸ ì¤‘ ê°€ì¥ ê°€ê¹Œìš´ Kê°œì˜ ì´ì›ƒê³¼ ë¹„êµí•˜ì—¬ ë¶„ë¥˜í•©ë‹ˆë‹¤ë°ì´í„° í¬ì¸íŠ¸ì˜ íŠ¹ì„±ì„ ê¸°ì¤€ìœ¼ë¡œ ê±°ë¦¬ ê³„ì‚°ì„ í†µí•´ ê°€ì¥ ê°€ê¹Œìš´ ì´ì›ƒì„ ì°¾ìŠµë‹ˆë‹¤.ALTê±°ë¦¬ ì¸¡ì • : KNN ì•Œê³ ë¦¬ì¦˜ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ìš”ì†Œì…ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ìœ í´ë¦¬ë“œ ê±°ë¦¬ê°€ ì‚¬ìš©ë©ë‹ˆë‹¤.ê±°ë¦¬ ì¸¡ì • : KNN ì•Œê³ ë¦¬ì¦˜ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ìš”ì†Œì…ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ìœ í´ë¦¬ë“œ ê±°ë¦¬ê°€ ì‚¬ìš©ë©ë‹ˆë‹¤.ï»¿
Kê°’ : KëŠ” ê²°ì • ê²½ê³„ì˜ ë§¤ë„ëŸ¬ì›€ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°ì…ë‹ˆë‹¤. ê°’ : KëŠ” ê²°ì • ê²½ê³„ì˜ ë§¤ë„ëŸ¬ì›€ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„°ì…ë‹ˆë‹¤. ï»¿
 ì‘ì€ Kê°’ì€ ë” ë…¸ì´ì¦ˆì— ë¯¼ê°í•˜ê³ ,í° Kê°’ì€ ë” ë§¤ë„ëŸ¬ìš´ ê²½ê³„ë¥¼ ë§Œë“­ë‹ˆë‹¤. ì‘ì€ Kê°’ì€ ë” ë…¸ì´ì¦ˆì— ë¯¼ê°í•˜ê³ ,í° Kê°’ì€ ë” ë§¤ë„ëŸ¬ìš´ ê²½ê³„ë¥¼ ë§Œë“­ë‹ˆë‹¤.ï»¿
ë‹¤ìˆ˜ê²° íˆ¬í‘œ : Kê°œì˜ ê°€ì¥ ê°€ê¹Œìš´ ì´ì›ƒì˜ í´ë˜ìŠ¤ì¤‘ ê°€ì¥ ë¹ˆë²ˆí•œ í´ë˜ìŠ¤ë¡œ ìƒˆë¡œìš´ ë°ì´í„° í¬ì¸íŠ¸ë¥¼ ë¶„ë¥˜í•©ë‹ˆë‹¤ë‹¤ìˆ˜ê²° íˆ¬í‘œ : Kê°œì˜ ê°€ì¥ ê°€ê¹Œìš´ ì´ì›ƒì˜ í´ë˜ìŠ¤ì¤‘ ê°€ì¥ ë¹ˆë²ˆí•œ í´ë˜ìŠ¤ë¡œ ìƒˆë¡œìš´ ë°ì´í„° í¬ì¸íŠ¸ë¥¼ ë¶„ë¥˜í•©ë‹ˆë‹¤ï»¿â€‹ KNNì˜ ëª©ì KNNì˜ ëª©í‘œëŠ” í•™ìŠµ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒˆë¡œìš´ ë°ì´í„° í¬ì¸íŠ¸ì˜ í´ë˜ìŠ¤ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤ì´ëŠ” ë¶„ë¥˜ ë¬¸ì œì—ì„œ ì£¼ë¡œ ì‚¬ìš©ë˜ë©° ë‹¤ì–‘í•œ ì‘ìš© ë¶„ì•¼ì— í™œìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤02. KNN ì‹¤ìŠµâœ”ï¸Scikit-learnì˜ ìœ ë°©ì•”ë°ì´í„°ì™€ Seabornì˜ íƒ€ì´íƒ€ë‹‰ ë°ì´í„°ë¡œ KNN ì‹¤ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤1) ìœ ë°©ì•” ë°ì´í„° ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ìœ ë°©ì•” ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ {5px}ìœ ë°©ì•” ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ï»¿â€‹Pythonë³µì‚¬import numpy as np
import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# ë°ì´í„° ë¡œë“œ
data = load_breast_cancer()
X = data.data
y = data.target

# ë°ì´í„° ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# ë°ì´í„° ìŠ¤ì¼€ì¼ë§
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
â€‹sklearn.datasets.load_breast_cancer: ìœ ë°©ì•” ë°ì´í„°ì…‹ ë¡œë“œreturn_X_y=False: ë°ì´í„°ì™€ íƒ€ê²Ÿì„ í•¨ê»˜ ë°˜í™˜í• ì§€ ì—¬ë¶€. ê¸°ë³¸ê°’ì€ Falseì…ë‹ˆë‹¤._X_y=False: ë°ì´í„°ì™€ íƒ€ê²Ÿì„ í•¨ê»˜ ë°˜í™˜í• ì§€ ì—¬ë¶€. ê¸°ë³¸ê°’ì€ Falseì…ë‹ˆë‹¤.ï»¿â€‹sklearn.model_selection.train_test_split: ë°ì´í„°ë¥¼ í›ˆë ¨ ì„¸íŠ¸/ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ ë¶„í• test_size=0.2: í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì˜ ë¹„ìœ¨ì„ 0.2ë¡œ ì„¤ì •í•©ë‹ˆë‹¤._size=0.2: í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì˜ ë¹„ìœ¨ì„ 0.2ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.ï»¿â€‹random_state=42: ëœë¤ ì‹œë“œ ê°’ìœ¼ë¡œ, ë°ì´í„° ë¶„í• ì˜ ì¬í˜„ì„±ì„ ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤._state=42: ëœë¤ ì‹œë“œ ê°’ìœ¼ë¡œ, ë°ì´í„° ë¶„í• ì˜ ì¬í˜„ì„±ì„ ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.ï»¿â€‹sklearn.preprocessing.StandardScaler: ë°ì´í„°ì˜ í‰ê· ì„ 0, ë¶„ì‚°ì„ 1ë¡œ ìŠ¤ì¼€ì¼ë§fit_transform(X_train): í›ˆë ¨ ì„¸íŠ¸ë¥¼ ìŠ¤ì¼€ì¼ë§í•˜ê³  ë³€í™˜í•©ë‹ˆë‹¤._transform(X_train): í›ˆë ¨ ì„¸íŠ¸ë¥¼ ìŠ¤ì¼€ì¼ë§í•˜ê³  ë³€í™˜í•©ë‹ˆë‹¤.ï»¿â€‹transform(X_test): í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¥¼ ë³€í™˜í•©ë‹ˆë‹¤.(X_test): í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¥¼ ë³€í™˜í•©ë‹ˆë‹¤.ï»¿â€‹ ëª¨ë¸ í•™ìŠµëª¨ë¸ í•™ìŠµ {5px}ëª¨ë¸ í•™ìŠµ ï»¿â€‹Pythonë³µì‚¬from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
model = KNeighborsClassifier(n_neighbors=5)
model.fit(X_train, y_train)
# ì˜ˆì¸¡
y_pred = model.predict(X_test)
# í‰ê°€
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
print(f"Classification Report:")
print(f"Confusion Matrix:")
â€‹sklearn.neighbors.KNeighborsClassifier: KNN ëª¨ë¸ ìƒì„±n_neighbors=5: ì‚¬ìš©í•  ì´ì›ƒì˜ ìˆ˜(K)ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤._neighbors=5: ì‚¬ìš©í•  ì´ì›ƒì˜ ìˆ˜(K)ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.ï»¿â€‹fit(X_train, y_train): ëª¨ë¸ì„ í›ˆë ¨ ì„¸íŠ¸ì— ë§ì¶”ì–´ í•™ìŠµì‹œí‚µë‹ˆë‹¤(X_train, y_train): ëª¨ë¸ì„ í›ˆë ¨ ì„¸íŠ¸ì— ë§ì¶”ì–´ í•™ìŠµì‹œí‚µë‹ˆë‹¤ï»¿â€‹predict(X_test): í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì— ëŒ€í•´ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.(X_test): í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì— ëŒ€í•´ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.ï»¿â€‹sklearn.metrics.accuracy_score: ì •í™•ë„ ê³„ì‚°accuracy_score(y_test, y_pred): ì‹¤ì œ ê°’ê³¼ ì˜ˆì¸¡ ê°’ì„ ë¹„êµí•˜ì—¬ ì •í™•ë„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤._score(y_test, y_pred): ì‹¤ì œ ê°’ê³¼ ì˜ˆì¸¡ ê°’ì„ ë¹„êµí•˜ì—¬ ì •í™•ë„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.ï»¿â€‹sklearn.metrics.classification_report: ë¶„ë¥˜ ë³´ê³ ì„œ ìƒì„±classification_report(y_test, y_pred): ì •í™•ë„, ì •ë°€ë„, ì¬í˜„ìœ¨ ë“±ì˜ ë©”íŠ¸ë¦­ì„ í¬í•¨í•œ ë³´ê³ ì„œë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤._report(y_test, y_pred): ì •í™•ë„, ì •ë°€ë„, ì¬í˜„ìœ¨ ë“±ì˜ ë©”íŠ¸ë¦­ì„ í¬í•¨í•œ ë³´ê³ ì„œë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.ï»¿â€‹sklearn.metrics.confusion_matrix: í˜¼ë™ í–‰ë ¬ ìƒì„±confusion_matrix(y_test, y_pred): ì‹¤ì œ ê°’ê³¼ ì˜ˆì¸¡ ê°’ì˜ í˜¼ë™ í–‰ë ¬ì„ ë°˜í™˜í•©ë‹ˆë‹¤._matrix(y_test, y_pred): ì‹¤ì œ ê°’ê³¼ ì˜ˆì¸¡ ê°’ì˜ í˜¼ë™ í–‰ë ¬ì„ ë°˜í™˜í•©ë‹ˆë‹¤.ï»¿â€‹2) íƒ€ì´íƒ€ë‹‰ ë°ì´í„° ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬íƒ€ì´íƒ€ë‹‰ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ {5px}íƒ€ì´íƒ€ë‹‰ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ï»¿â€‹Pythonë³µì‚¬import seaborn as sns

# ë°ì´í„° ë¡œë“œ
titanic = sns.load_dataset('titanic')
# í•„ìš”í•œ ì—´ ì„ íƒ ë° ê²°ì¸¡ê°’ ì²˜ë¦¬
titanic = titanic[['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']].dropna()
# ì„±ë³„ê³¼ íƒ‘ìŠ¹í•œ ê³³ ì¸ì½”ë”©
titanic['sex'] = titanic['sex'].map({'male': 0, 'female': 1})
titanic['embarked'] = titanic['embarked'].map({'C': 0, 'Q': 1, 'S': 2})
# íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
X = titanic.drop('survived', axis=1)
y = titanic['survived']
# ë°ì´í„° ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# ë°ì´í„° ìŠ¤ì¼€ì¼ë§
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
â€‹seaborn.load_dataset: seabornì˜ ë‚´ì¥ ë°ì´í„°ì…‹ ë¡œë“œâ€™titanicâ€™: íƒ€ì´íƒ€ë‹‰ ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤.â€™titanicâ€™: íƒ€ì´íƒ€ë‹‰ ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤.ï»¿â€‹pandas.DataFrame.dropna: ê²°ì¸¡ê°’ì´ ìˆëŠ” í–‰ ì œê±°pandas.DataFrame.map: ë°ì´í„° ê°’ì„ ë‹¤ë¥¸ ê°’ìœ¼ë¡œ ë§¤í•‘â€™maleâ€™: 0, â€™femaleâ€™: 1: ì„±ë³„ì„ ìˆ«ìë¡œ ë§¤í•‘í•©ë‹ˆë‹¤.: ì„±ë³„ì„ ìˆ«ìë¡œ ë§¤í•‘í•©ë‹ˆë‹¤.}â€™maleâ€™: 0, â€™femaleâ€™: 1: ì„±ë³„ì„ ìˆ«ìë¡œ ë§¤í•‘í•©ë‹ˆë‹¤.ï»¿â€‹â€™Câ€™: 0, â€™Qâ€™: 1, â€™Sâ€™: 2: íƒ‘ìŠ¹í•œ ê³³ì„ ìˆ«ìë¡œ ë§¤í•‘í•©ë‹ˆë‹¤.: íƒ‘ìŠ¹í•œ ê³³ì„ ìˆ«ìë¡œ ë§¤í•‘í•©ë‹ˆë‹¤.}â€™Câ€™: 0, â€™Qâ€™: 1, â€™Sâ€™: 2: íƒ‘ìŠ¹í•œ ê³³ì„ ìˆ«ìë¡œ ë§¤í•‘í•©ë‹ˆë‹¤.ï»¿â€‹ ëª¨ë¸ í•™ìŠµëª¨ë¸ í•™ìŠµ {5px}ëª¨ë¸ í•™ìŠµ ï»¿â€‹Pythonë³µì‚¬# ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
model = KNeighborsClassifier(n_neighbors=5)
model.fit(X_train, y_train)
# ì˜ˆì¸¡
y_pred = model.predict(X_test)
# í‰ê°€
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
print(f"Classification Report:")
print(f"Confusion Matrix:")
â€‹.
[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 11. ì˜¤í† ì¸ì½”ë”[SCC] ê¸°ì´ˆê°€ íƒ„íƒ„í•œ ë”¥ëŸ¬ë‹/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] ê¸°ì´ˆê°€ íƒ„íƒ„í•œ ë”¥ëŸ¬ë‹ - 5ì£¼ì°¨/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 11. ì˜¤í† ì¸ì½”ë”ì œì‘:[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 11. ì˜¤í† ì¸ì½”ë”[ìˆ˜ì—… ëª©í‘œ]ì˜¤í† ì¸ì½”ë”ì˜ ê°œë…ì— ëŒ€í•´ ì•Œì•„ë´…ì‹œë‹¤[ëª©ì°¨]01. ì˜¤í† ì¸ì½”ë”02. ì˜¤í† ì¸ì½”ë”ì˜ êµ¬ì¡°ğŸ’¡
 
 01. ì˜¤í† ì¸ì½”ë”1) ì˜¤í† ì¸ì½”ë”ì˜ ê¸°ë³¸ ê°œë… ì˜¤í† ì¸ì½”ë”ë€?ì˜¤í† ì¸ì½”ë”(Autoencoder)ëŠ” ì…ë ¥ ë°ì´í„°ë¥¼ ì••ì¶•í•˜ê³ , ì´ë¥¼ ë‹¤ì‹œ ë³µì›í•˜ëŠ” ê³¼ì •ì„ í†µí•´ ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ í‘œí˜„í•˜ëŠ” ë¹„ì§€ë„ í•™ìŠµ ëª¨ë¸ì…ë‹ˆë‹¤. ì£¼ë¡œ ì°¨ì› ì¶•ì†Œ, ì¡ìŒ ì œê±°, ìƒì„± ëª¨ë¸ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í™œìš©ë©ë‹ˆë‹¤.2) ë™ì‘ ì›ë¦¬ ì¸ì½”ë”(Encoder)ì¸ì½”ë”ëŠ” ì…ë ¥ ë°ì´í„°ë¥¼ ì €ì°¨ì›(latent space) í‘œí˜„ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. ì¸ì½”ë”ì˜ ëª©ì ì€ ì¤‘ìš”í•œ íŠ¹ì§•ì„ ì¶”ì¶œí•˜ê³ , ì…ë ¥ ë°ì´í„°ë¥¼ ì••ì¶•í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ë””ì½”ë”(Decoder)ë””ì½”ë”ëŠ” ì¸ì½”ë”ì— ì˜í•´ ìƒì„±ëœ ì €ì°¨ì› í‘œí˜„ì„ ë‹¤ì‹œ ì›ë˜ì˜ ê³ ì°¨ì› ë°ì´í„°ë¡œ ë³µì›í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. ë””ì½”ë”ì˜ ëª©ì ì€ ì…ë ¥ ë°ì´í„°ë¥¼ ìµœëŒ€í•œ ì›ë³¸ê³¼ ê°€ê¹ê²Œ ë³µì›í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì ì¬ ê³µê°„(Latent Space)ì ì¬ ê³µê°„ì€ ì¸ì½”ë”ì— ì˜í•´ ìƒì„±ëœ ì €ì°¨ì› í‘œí˜„ ê³µê°„ì…ë‹ˆë‹¤. ì´ ê³µê°„ì—ì„œëŠ” ì…ë ¥ ë°ì´í„°ì˜ ì¤‘ìš”í•œ íŠ¹ì§•ë§Œì„ í¬í•¨í•˜ê³  ìˆìœ¼ë©°, ë””ì½”ë”ëŠ” ì´ë¥¼ ì´ìš©í•´ ì›ë˜ ë°ì´í„°ë¥¼ ë³µì›í•©ë‹ˆë‹¤.02. ì˜¤í† ì¸ì½”ë”ì˜ êµ¬ì¡°âœ”ï¸ì˜¤í† ì¸ì½”ë”ì˜ ë‹¤ì–‘í•œ êµ¬ì¡° ë° ì¢…ë¥˜ì— ëŒ€í•´ ì•Œì•„ë´…ì‹œë‹¤1) ì˜¤í† ì¸ì½”ë”ì˜ ì¢…ë¥˜ ê¸°ë³¸ ì˜¤í† ì¸ì½”ë”ALT ë³€í˜•ëœ ì˜¤í† ì¸ì½”ë”ì˜¤í† ì¸ì½”ë”ëŠ” ë‹¤ì–‘í•œ ë³€í˜• ëª¨ë¸ë“¤ì´ ì¡´ì¬í•©ë‹ˆë‹¤. ëŒ€í‘œì ì¸ ì˜ˆë¡œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:ë”¥ ì˜¤í† ì¸ì½”ë”(Deep Autoencoder): ë” ê¹Šì€ ì¸ì½”ë”ì™€ ë””ì½”ë” êµ¬ì¡°ë¥¼ ê°€ì§€ë©°, ë³µì¡í•œ ë°ì´í„° í‘œí˜„ì„ í•™ìŠµí•©ë‹ˆë‹¤.ë³€ë¶„ ì˜¤í† ì¸ì½”ë”(Variational Autoencoder, VAE): í™•ë¥ ì  ì ì¬ ê³µê°„ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ì˜ ë¶„í¬ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.í¬ì†Œ ì˜¤í† ì¸ì½”ë”(Sparse Autoencoder): ì ì¬ ê³µê°„ì˜ í‘œí˜„ì„ í¬ì†Œí•˜ê²Œ ìœ ì§€í•˜ì—¬ ì¤‘ìš”í•œ íŠ¹ì§•ë§Œì„ í•™ìŠµí•©ë‹ˆë‹¤.ì¡ìŒ ì œê±° ì˜¤í† ì¸ì½”ë”(Denoising Autoencoder): ì…ë ¥ ë°ì´í„°ì— ì¡ìŒì„ ì¶”ê°€í•˜ê³ , ì´ë¥¼ ì œê±°í•˜ëŠ” í•™ìŠµì„ í†µí•´ ë°ì´í„° ë³µì› ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤..
ë°ì´í„° ì „ì²˜ë¦¬: ê²°ì¸¡ì¹˜ íƒì§€ì™€ ë‹¤ì–‘í•œ ì²˜ë¦¬ ë°©ë²•ì¸ê³µì§€ëŠ¥ì„ ìœ„í•œ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬/ì¸ê³µì§€ëŠ¥ì„ ìœ„í•œ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ - 5ì£¼ì°¨/ë°ì´í„° ì „ì²˜ë¦¬: ê²°ì¸¡ì¹˜ íƒì§€ì™€ ë‹¤ì–‘í•œ ì²˜ë¦¬ ë°©ë²•ì œì‘:ë°ì´í„° ì „ì²˜ë¦¬: ê²°ì¸¡ì¹˜ íƒì§€ì™€ ë‹¤ì–‘í•œ ì²˜ë¦¬ ë°©ë²•ìˆ˜ì—… ëª©í‘œë¶ˆëŸ¬ì˜¨ ë°ì´í„°ì—ì„œ ê²°ì¸¡ì¹˜ë¥¼ ì°¾ê³  ëŒ€ì²´í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ì„œ í•™ìŠµí•©ë‹ˆë‹¤.ëª©ì°¨ê²°ì¸¡ì¹˜ íƒì§€ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ë°©ë²•ê³ ê¸‰ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ë°©ë²•ìš”ì•½ê²°ì¸¡ì¹˜ íƒì§€ğŸ“šë°ì´í„° ë¶„ì„ì—ì„œ ê²°ì¸¡ì¹˜(Missing Values)ëŠ” í”íˆ ë°œìƒí•˜ë©°, ì´ë¥¼ ì˜¬ë°”ë¥´ê²Œ ì²˜ë¦¬í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. íŒë‹¤ìŠ¤ëŠ” ê²°ì¸¡ì¹˜ë¥¼ ì‰½ê²Œ íƒì§€í•˜ê³  ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.isna()ì™€ isnull()ë¡œ ê²°ì¸¡ì¹˜ íƒì§€isna()ì™€ isnull() í•¨ìˆ˜ëŠ” ë°ì´í„°í”„ë ˆì„ì˜ ê° ìš”ì†Œê°€ ê²°ì¸¡ì¹˜ì¸ì§€ ì—¬ë¶€ë¥¼ í™•ì¸í•©ë‹ˆë‹¤. ë‘ í•¨ìˆ˜ëŠ” ë™ì¼í•œ ê¸°ëŠ¥ì„ í•©ë‹ˆë‹¤.Pythonë³µì‚¬import pandas as pd

# ì˜ˆì‹œ ë°ì´í„°í”„ë ˆì„ ìƒì„±
data = {
'ì´ë¦„': ['ì² ìˆ˜', 'ì˜í¬', 'ë¯¼ìˆ˜', 'ì§€ìˆ˜'],
'ë‚˜ì´': [25, 30, None, 35],
'ì§ì—…': ['í•™ìƒ', 'íšŒì‚¬ì›', 'í•™ìƒ', None]
}
df = pd.DataFrame(data)
# ê²°ì¸¡ì¹˜ ì—¬ë¶€ í™•ì¸
print(df.isna())

â€‹ê²°ê³¼:Plain Textë³µì‚¬     ì´ë¦„     ë‚˜ì´    ì§ì—…
0  False  False  False
1  False  False  False
2  False   True  False
3  False  False   True

â€‹sum()ì„ ì‚¬ìš©í•œ ê²°ì¸¡ì¹˜ ê°œìˆ˜ í™•ì¸ê²°ì¸¡ì¹˜ì˜ ì´ ê°œìˆ˜ë¥¼ í™•ì¸í•˜ë ¤ë©´ isna() ë˜ëŠ” isnull()ê³¼ sum()ì„ ì¡°í•©í•´ ì‚¬ìš©í•©ë‹ˆë‹¤.Pythonë³µì‚¬# ê° ì—´ë³„ ê²°ì¸¡ì¹˜ ê°œìˆ˜ í™•ì¸
print(df.isna().sum())

â€‹ê²°ê³¼:Plain Textë³µì‚¬ì´ë¦„    0
ë‚˜ì´    1
ì§ì—…    1
dtype: int64

â€‹ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ë°©ë²•ğŸ“šê²°ì¸¡ì¹˜ë¥¼ ì ì ˆí•˜ê²Œ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•ì€ ë°ì´í„°ì˜ íŠ¹ì„±ê³¼ ë¶„ì„ ëª©ì ì— ë”°ë¼ ë‹¬ë¼ì§‘ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ëŒ€í‘œì ì¸ ì²˜ë¦¬ ë°©ë²•ë“¤ì„ ì†Œê°œí•©ë‹ˆë‹¤.ê²°ì¸¡ì¹˜ ì œê±°ê²°ì¸¡ì¹˜ê°€ í¬í•¨ëœ í–‰ì´ë‚˜ ì—´ì„ ì‚­ì œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. dropna() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.Pythonë³µì‚¬# ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” í–‰ ì œê±°
df_dropped_rows = df.dropna()
print(df_dropped_rows)

â€‹ê²°ê³¼:Plain Textë³µì‚¬   ì´ë¦„   ë‚˜ì´    ì§ì—…
0  ì² ìˆ˜  25.0   í•™ìƒ
1  ì˜í¬  30.0  íšŒì‚¬ì›

â€‹ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” ì—´ì„ ì œê±°í•˜ë ¤ë©´ axis=1ì„ ì§€ì •í•©ë‹ˆë‹¤.Pythonë³µì‚¬# ê²°ì¸¡ì¹˜ê°€ ìˆëŠ” ì—´ ì œê±°
df_dropped_columns = df.dropna(axis=1)
print(df_dropped_columns)

â€‹ê²°ê³¼:Plain Textë³µì‚¬   ì´ë¦„
0  ì² ìˆ˜
1  ì˜í¬
2  ë¯¼ìˆ˜
3  ì§€ìˆ˜

â€‹ê²°ì¸¡ì¹˜ ëŒ€ì²´(ì±„ìš°ê¸°)ê²°ì¸¡ì¹˜ë¥¼ íŠ¹ì • ê°’ìœ¼ë¡œ ëŒ€ì²´(ì±„ìš°ê¸°)í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. fillna() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.Pythonë³µì‚¬# ê²°ì¸¡ì¹˜ë¥¼ 'ì—†ìŒ'ìœ¼ë¡œ ëŒ€ì²´
df_filled = df.fillna('ì—†ìŒ')
print(df_filled)

â€‹ê²°ê³¼:Plain Textë³µì‚¬   ì´ë¦„    ë‚˜ì´    ì§ì—…
0  ì² ìˆ˜   25.0    í•™ìƒ
1  ì˜í¬   30.0  íšŒì‚¬ì›
2  ë¯¼ìˆ˜  ì—†ìŒ    í•™ìƒ
3  ì§€ìˆ˜   35.0   ì—†ìŒ

â€‹í‰ê· , ì¤‘ì•™ê°’, ìµœë¹ˆê°’ ë“±ìœ¼ë¡œ ê²°ì¸¡ì¹˜ë¥¼ ì±„ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.Pythonë³µì‚¬# 'ë‚˜ì´' ì—´ì˜ ê²°ì¸¡ì¹˜ë¥¼ í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´
df['ë‚˜ì´'] = df['ë‚˜ì´'].fillna(df['ë‚˜ì´'].mean())
print(df)

â€‹ê²°ê³¼:Plain Textë³µì‚¬   ì´ë¦„    ë‚˜ì´    ì§ì—…
0  ì² ìˆ˜  25.0    í•™ìƒ
1  ì˜í¬  30.0  íšŒì‚¬ì›
2  ë¯¼ìˆ˜  30.0    í•™ìƒ
3  ì§€ìˆ˜  35.0   ì—†ìŒ

â€‹ê²°ì¸¡ì¹˜ ë³´ê°„(Interpolation)ê²°ì¸¡ì¹˜ë¥¼ ì£¼ë³€ ê°’ë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ ë³´ê°„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. interpolate() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. íŠ¹íˆ ì‹œê°„ ë°ì´í„°ë¥¼ ë‹¤ë£° ë•Œ ìœ ìš©í•©ë‹ˆë‹¤.Pythonë³µì‚¬# ì˜ˆì‹œ ë°ì´í„°í”„ë ˆì„ ìƒì„±
data = {
'ë‚ ì§œ': pd.date_range('2023-01-01', periods=5),
'ì˜¨ë„': [20, 22, None, 24, 25]
}
df2 = pd.DataFrame(data)
# ì„ í˜• ë³´ê°„ë²•ìœ¼ë¡œ ê²°ì¸¡ì¹˜ ì±„ìš°ê¸°
df2['ì˜¨ë„'] = df2['ì˜¨ë„'].interpolate()
print(df2)

â€‹ê²°ê³¼:Plain Textë³µì‚¬         ë‚ ì§œ    ì˜¨ë„
0 2023-01-01  20.0
1 2023-01-02  22.0
2 2023-01-03  23.0
3 2023-01-04  24.0
4 2023-01-05  25.0

â€‹ê³ ê¸‰ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ë°©ë²•íŠ¹ì • ì¡°ê±´ì„ ê¸°ë°˜ìœ¼ë¡œ ê²°ì¸¡ì¹˜ ì²˜ë¦¬íŠ¹ì • ì¡°ê±´ì„ ê¸°ë°˜ìœ¼ë¡œ ê²°ì¸¡ì¹˜ë¥¼ ì²˜ë¦¬í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ë‹¤ë¥¸ ì—´ì˜ ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ê²°ì¸¡ì¹˜ë¥¼ ì±„ìš°ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.Pythonë³µì‚¬# 'ì§ì—…'ì´ 'í•™ìƒ'ì¸ ê²½ìš° 'ë‚˜ì´'ë¥¼ 20ìœ¼ë¡œ ì±„ìš°ê¸°
df.loc[(df['ì§ì—…'] == 'í•™ìƒ') & (df['ë‚˜ì´'].isna()), 'ë‚˜ì´'] = 20
print(df)

â€‹ê²°ê³¼:Plain Textë³µì‚¬   ì´ë¦„    ë‚˜ì´    ì§ì—…
0  ì² ìˆ˜  25.0    í•™ìƒ
1  ì˜í¬  30.0  íšŒì‚¬ì›
2  ë¯¼ìˆ˜  20.0    í•™ìƒ
3  ì§€ìˆ˜  35.0   ì—†ìŒ

â€‹apply()ë¥¼ ì‚¬ìš©í•œ ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜ ì ìš©apply() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ ê²°ì¸¡ì¹˜ë¥¼ ì²˜ë¦¬í•˜ëŠ” ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜ë¥¼ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Pythonë³µì‚¬# ì˜ˆì‹œ: 'ë‚˜ì´'ê°€ ê²°ì¸¡ì¹˜ì¼ ê²½ìš°, ê¸°ë³¸ê°’ìœ¼ë¡œ 18ì„ ì±„ìš°ëŠ” í•¨ìˆ˜
def fill_missing_age(x):
if pd.isna(x):
return 18
return x

df['ë‚˜ì´'] = df['ë‚˜ì´'].apply(fill_missing_age)
print(df)

â€‹ê²°ê³¼:Plain Textë³µì‚¬   ì´ë¦„    ë‚˜ì´    ì§ì—…
0  ì² ìˆ˜  25.0    í•™ìƒ
1  ì˜í¬  30.0  íšŒì‚¬ì›
2  ë¯¼ìˆ˜  20.0    í•™ìƒ
3  ì§€ìˆ˜  35.0   ì—†ìŒ

â€‹ìš”ì•½ğŸ“šìš”ì•½ê²°ì¸¡ì¹˜ íƒì§€ëŠ” isna()ì™€ isnull()ì„ ì‚¬ìš©í•´ ìˆ˜í–‰í•˜ë©°, sum()ì„ í†µí•´ ì—´ë³„ ê²°ì¸¡ì¹˜ ê°œìˆ˜ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ë°©ë²•ìœ¼ë¡œëŠ” í–‰ ë˜ëŠ” ì—´ ì‚­ì œ(dropna()), íŠ¹ì • ê°’ìœ¼ë¡œ ì±„ìš°ê¸°(fillna()), ë³´ê°„ë²•(interpolate()), ì¡°ê±´ ê¸°ë°˜ ì²˜ë¦¬ ë° ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜ ì ìš© ë“±ì´ ìˆìŠµë‹ˆë‹¤.ë°ì´í„°ì˜ íŠ¹ì„±ê³¼ ë¶„ì„ ëª©ì ì— ë”°ë¼ ì ì ˆí•œ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ë°©ë²•ì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.ì´ì œ ê²°ì¸¡ì¹˜ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ë” ì •í™•í•œ ë¶„ì„ ê²°ê³¼ë¥¼ ì–»ì–´ë³´ì„¸ìš”! â€‹
[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 12ê°•. ì§€ë„í•™ìŠµ : ë¶„ë¥˜ëª¨ë¸ - ë‚˜ì´ë¸Œë² ì´ì¦ˆ[SCC] ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ - 3ì£¼ì°¨/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 12ê°•. ì§€ë„í•™ìŠµ : ë¶„ë¥˜ëª¨ë¸ - ë‚˜ì´ë¸Œë² ì´ì¦ˆì œì‘:[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 12ê°•. ì§€ë„í•™ìŠµ : ë¶„ë¥˜ëª¨ë¸ - ë‚˜ì´ë¸Œë² ì´ì¦ˆ[ìˆ˜ì—… ëª©í‘œ]ë¶„ë¥˜ëª¨ë¸ì¤‘ ë‚˜ì´ë¸Œë² ì´ì¦ˆì— ëŒ€í•œ ê°œë…ì„ ë°°ìš°ê³ , ë°ì´í„°ë¥¼ ì´ìš©í•´ ì‹¤ìŠµí•´ ë´…ë‹ˆë‹¤[ëª©ì°¨]01. ë‚˜ì´ë¸Œë² ì´ì¦ˆ ê°œë…02. ë‚˜ì´ë¸Œë² ì´ì¦ˆ ì‹¤ìŠµğŸ’¡
 
 01. ë‚˜ì´ë¸Œë² ì´ì¦ˆ ê°œë…âœ”ï¸1) ë‚˜ì´ë¸Œë² ì´ì¦ˆ ë‚˜ì´ë¸Œë² ì´ì¦ˆë€?ë‚˜ì´ë¸Œë² ì´ì¦ˆ(Naive Bayes) ë¶„ë¥˜ê¸°ëŠ” ë² ì´ì¦ˆ ì •ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” í†µê³„ì  ë¶„ë¥˜ ê¸°ë²•ì…ë‹ˆë‹¤. ë‚˜ì´ë¸Œë¼ëŠ” ì´ë¦„ì´ ë¶™ì€ ì´ìœ ëŠ” ê° íŠ¹ì§•(feature)ì´ ë…ë¦½ì ì´ë¼ê³  ê°€ì •í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.ì£¼ë¡œ í…ìŠ¤íŠ¸ ë¶„ë¥˜ ë¬¸ì œì—ì„œ ë„ë¦¬ ì‚¬ìš©ë©ë‹ˆë‹¤. ë² ì´ì¦ˆì •ë¦¬P(Aâˆ£B)=P(Bâˆ£A)â‹…P(A)P(B)P(A|B) = {P(B)}P(Aâˆ£B)=P(B)P(Bâˆ£A)â‹…P(A)â€‹P(Aâˆ£B): Bê°€ ì£¼ì–´ì¡Œì„ ë•Œ Aì˜ í™•ë¥  (ì‚¬í›„ í™•ë¥ )P(Bâˆ£A): Aê°€ ì£¼ì–´ì¡Œì„ ë•Œ Bì˜ í™•ë¥  (ìš°ë„)P(A): Aì˜ ì‚¬ì „ í™•ë¥ P(B): Bì˜ ì‚¬ì „ í™•ë¥  ë‚˜ì´ë¸Œë² ì´ì¦ˆì˜ ì¢…ë¥˜ê°€ìš°ì‹œì•ˆ ë‚˜ì´ë¸Œë² ì´ì¦ˆ: íŠ¹ì§•ë“¤ì´ ì—°ì†ì ì´ê³  ì •ê·œ ë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.ë² ë¥´ëˆ„ì´ ë‚˜ì´ë¸Œë² ì´ì¦ˆ: íŠ¹ì§•ë“¤ì´ ì´ì§„ìˆ˜(0 ë˜ëŠ” 1)ë¡œ í‘œí˜„ë˜ëŠ” ê²½ìš° ì‚¬ìš©í•©ë‹ˆë‹¤.ë©€í‹°ë…¸ë¯¸ì–¼ ë‚˜ì´ë¸Œë² ì´ì¦ˆ: íŠ¹ì§•ë“¤ì´ ë‹¤í•­ ë¶„í¬ë¥¼ ë”°ë¥´ëŠ” ê²½ìš° ì‚¬ìš©í•©ë‹ˆë‹¤. ë‚˜ì´ë¸Œë² ì´ì¦ˆì˜ ëª©ì ë‚˜ì´ë¸Œë² ì´ì¦ˆì˜ ëª©í‘œëŠ” ì£¼ì–´ì§„ ë°ì´í„° í¬ì¸íŠ¸ê°€ íŠ¹ì • í´ë˜ìŠ¤ì— ì†í•  í™•ë¥ ì„ ê³„ì‚°í•˜ì—¬ ë¶„ë¥˜í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ ëª¨ë¸ì€ ë‹¨ìˆœí•˜ê³  ê³„ì‚°ì´ íš¨ìœ¨ì ì´ë©°, í…ìŠ¤íŠ¸ ë¶„ë¥˜ì™€ ê°™ì€ ë¬¸ì œì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ë°œíœ˜í•©ë‹ˆë‹¤.02. ë‚˜ì´ë¸Œë² ì´ì¦ˆ ì‹¤ìŠµâœ”ï¸Scikit-learnì˜ ìœ ë°©ì•”ë°ì´í„°ì™€ Seabornì˜ íƒ€ì´íƒ€ë‹‰ ë°ì´í„°ë¡œ ë‚˜ì´ë¸Œë² ì´ì¦ˆ ì‹¤ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤1) ìœ ë°©ì•” ë°ì´í„° ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ìœ ë°©ì•” ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ {5px}ìœ ë°©ì•” ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ï»¿â€‹Pythonë³µì‚¬import numpy as np
import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# ë°ì´í„° ë¡œë“œ
data = load_breast_cancer()
X = data.data
y = data.target

# ë°ì´í„° ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# ë°ì´í„° ìŠ¤ì¼€ì¼ë§
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
â€‹sklearn.datasets.load_breast_cancer: ìœ ë°©ì•” ë°ì´í„°ì…‹ ë¡œë“œreturn_X_y=False: ë°ì´í„°ì™€ íƒ€ê²Ÿì„ í•¨ê»˜ ë°˜í™˜í• ì§€ ì—¬ë¶€. ê¸°ë³¸ê°’ì€ Falseì…ë‹ˆë‹¤._X_y=False: ë°ì´í„°ì™€ íƒ€ê²Ÿì„ í•¨ê»˜ ë°˜í™˜í• ì§€ ì—¬ë¶€. ê¸°ë³¸ê°’ì€ Falseì…ë‹ˆë‹¤.ï»¿â€‹sklearn.model_selection.train_test_split: ë°ì´í„°ë¥¼ í›ˆë ¨ ì„¸íŠ¸/ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ ë¶„í• test_size=0.2: í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì˜ ë¹„ìœ¨ì„ 0.2ë¡œ ì„¤ì •í•©ë‹ˆë‹¤._size=0.2: í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì˜ ë¹„ìœ¨ì„ 0.2ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.ï»¿â€‹random_state=42: ëœë¤ ì‹œë“œ ê°’ìœ¼ë¡œ, ë°ì´í„° ë¶„í• ì˜ ì¬í˜„ì„±ì„ ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤._state=42: ëœë¤ ì‹œë“œ ê°’ìœ¼ë¡œ, ë°ì´í„° ë¶„í• ì˜ ì¬í˜„ì„±ì„ ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.ï»¿â€‹sklearn.preprocessing.StandardScaler: ë°ì´í„°ì˜ í‰ê· ì„ 0, ë¶„ì‚°ì„ 1ë¡œ ìŠ¤ì¼€ì¼ë§fit_transform(X_train): í›ˆë ¨ ì„¸íŠ¸ë¥¼ ìŠ¤ì¼€ì¼ë§í•˜ê³  ë³€í™˜í•©ë‹ˆë‹¤._transform(X_train): í›ˆë ¨ ì„¸íŠ¸ë¥¼ ìŠ¤ì¼€ì¼ë§í•˜ê³  ë³€í™˜í•©ë‹ˆë‹¤.ï»¿â€‹transform(X_test): í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¥¼ ë³€í™˜í•©ë‹ˆë‹¤.(X_test): í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¥¼ ë³€í™˜í•©ë‹ˆë‹¤.ï»¿â€‹ ëª¨ë¸ í•™ìŠµëª¨ë¸ í•™ìŠµ {5px}ëª¨ë¸ í•™ìŠµ ï»¿â€‹Pythonë³µì‚¬from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
model = GaussianNB()
model.fit(X_train, y_train)
# ì˜ˆì¸¡
y_pred = model.predict(X_test)
# í‰ê°€
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
print(f"Classification Report:")
print(f"Confusion Matrix:")
â€‹sklearn.naive_bayes.GaussianNB: ê°€ìš°ì‹œì•ˆ ë‚˜ì´ë¸Œë² ì´ì¦ˆ ë¶„ë¥˜ ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤fit(X_train, y_train): ëª¨ë¸ì„ í›ˆë ¨ ì„¸íŠ¸ì— ë§ì¶”ì–´ í•™ìŠµì‹œí‚µë‹ˆë‹¤.(X_train, y_train): ëª¨ë¸ì„ í›ˆë ¨ ì„¸íŠ¸ì— ë§ì¶”ì–´ í•™ìŠµì‹œí‚µë‹ˆë‹¤.ï»¿â€‹predict(X_test): í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì— ëŒ€í•´ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.(X_test): í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì— ëŒ€í•´ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.ï»¿â€‹sklearn.metrics.accuracy_score: ì •í™•ë„ ê³„ì‚°accuracy_score(y_test, y_pred): ì‹¤ì œ ê°’ê³¼ ì˜ˆì¸¡ ê°’ì„ ë¹„êµí•˜ì—¬ ì •í™•ë„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤._score(y_test, y_pred): ì‹¤ì œ ê°’ê³¼ ì˜ˆì¸¡ ê°’ì„ ë¹„êµí•˜ì—¬ ì •í™•ë„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.ï»¿â€‹sklearn.metrics.classification_report: ë¶„ë¥˜ ë³´ê³ ì„œ ìƒì„±classification_report(y_test, y_pred): ì •í™•ë„, ì •ë°€ë„, ì¬í˜„ìœ¨ ë“±ì˜ ë©”íŠ¸ë¦­ì„ í¬í•¨í•œ ë³´ê³ ì„œë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤._report(y_test, y_pred): ì •í™•ë„, ì •ë°€ë„, ì¬í˜„ìœ¨ ë“±ì˜ ë©”íŠ¸ë¦­ì„ í¬í•¨í•œ ë³´ê³ ì„œë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.ï»¿â€‹sklearn.metrics.confusion_matrix: í˜¼ë™ í–‰ë ¬ ìƒì„±confusion_matrix(y_test, y_pred): ì‹¤ì œ ê°’ê³¼ ì˜ˆì¸¡ ê°’ì˜ í˜¼ë™ í–‰ë ¬ì„ ë°˜í™˜í•©ë‹ˆë‹¤._matrix(y_test, y_pred): ì‹¤ì œ ê°’ê³¼ ì˜ˆì¸¡ ê°’ì˜ í˜¼ë™ í–‰ë ¬ì„ ë°˜í™˜í•©ë‹ˆë‹¤.ï»¿â€‹2) íƒ€ì´íƒ€ë‹‰ ë°ì´í„° ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬íƒ€ì´íƒ€ë‹‰ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ {5px}íƒ€ì´íƒ€ë‹‰ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ï»¿â€‹Pythonë³µì‚¬import seaborn as sns

# ë°ì´í„° ë¡œë“œ
titanic = sns.load_dataset('titanic')
# í•„ìš”í•œ ì—´ ì„ íƒ ë° ê²°ì¸¡ê°’ ì²˜ë¦¬
titanic = titanic[['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']].dropna()
# ì„±ë³„ê³¼ íƒ‘ìŠ¹í•œ ê³³ ì¸ì½”ë”©
titanic['sex'] = titanic['sex'].map({'male': 0, 'female': 1})
titanic['embarked'] = titanic['embarked'].map({'C': 0, 'Q': 1, 'S': 2})
# íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
X = titanic.drop('survived', axis=1)
y = titanic['survived']
# ë°ì´í„° ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# ë°ì´í„° ìŠ¤ì¼€ì¼ë§
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
â€‹seaborn.load_dataset: seabornì˜ ë‚´ì¥ ë°ì´í„°ì…‹ ë¡œë“œâ€™titanicâ€™: íƒ€ì´íƒ€ë‹‰ ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤.â€™titanicâ€™: íƒ€ì´íƒ€ë‹‰ ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤.ï»¿â€‹pandas.DataFrame.dropna: ê²°ì¸¡ê°’ì´ ìˆëŠ” í–‰ ì œê±°pandas.DataFrame.map: ë°ì´í„° ê°’ì„ ë‹¤ë¥¸ ê°’ìœ¼ë¡œ ë§¤í•‘â€™maleâ€™: 0, â€™femaleâ€™: 1: ì„±ë³„ì„ ìˆ«ìë¡œ ë§¤í•‘í•©ë‹ˆë‹¤.: ì„±ë³„ì„ ìˆ«ìë¡œ ë§¤í•‘í•©ë‹ˆë‹¤.}â€™maleâ€™: 0, â€™femaleâ€™: 1: ì„±ë³„ì„ ìˆ«ìë¡œ ë§¤í•‘í•©ë‹ˆë‹¤.ï»¿â€‹â€™Câ€™: 0, â€™Qâ€™: 1, â€™Sâ€™: 2: íƒ‘ìŠ¹í•œ ê³³ì„ ìˆ«ìë¡œ ë§¤í•‘í•©ë‹ˆë‹¤.: íƒ‘ìŠ¹í•œ ê³³ì„ ìˆ«ìë¡œ ë§¤í•‘í•©ë‹ˆë‹¤.}â€™Câ€™: 0, â€™Qâ€™: 1, â€™Sâ€™: 2: íƒ‘ìŠ¹í•œ ê³³ì„ ìˆ«ìë¡œ ë§¤í•‘í•©ë‹ˆë‹¤.ï»¿â€‹ ëª¨ë¸ í•™ìŠµëª¨ë¸ í•™ìŠµ {5px}ëª¨ë¸ í•™ìŠµ ï»¿â€‹Pythonë³µì‚¬# ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
model = GaussianNB()
model.fit(X_train, y_train)
# ì˜ˆì¸¡
y_pred = model.predict(X_test)
# í‰ê°€
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
print(f"Classification Report:")
print(f"Confusion Matrix:")
â€‹.âœ”ï¸ë‚˜ì´ë¸Œë² ì´ì¦ˆê°€ ë¬´ì—‡ì¸ì§€ ì•Œì•„ë´…ì‹œë‹¤
[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 12. ìƒì„±í˜• ëª¨ë¸ [SCC] ê¸°ì´ˆê°€ íƒ„íƒ„í•œ ë”¥ëŸ¬ë‹/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] ê¸°ì´ˆê°€ íƒ„íƒ„í•œ ë”¥ëŸ¬ë‹ - 5ì£¼ì°¨/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 12. ìƒì„±í˜• ëª¨ë¸ ì œì‘:[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 12. ìƒì„±í˜• ëª¨ë¸ [ìˆ˜ì—… ëª©í‘œ]ìƒì„±í˜• ëª¨ë¸ì¸ GAN ê³¼ VAE ëª¨ë¸ì— ëŒ€í•´ ë°°ì›Œë´…ì‹œë‹¤[ëª©ì°¨]01. GAN(Generative Adversarial Network)02. VAE (Variational Autoencoder)ğŸ’¡
 
 01. GAN(Generative Adversarial Network)1) GANì˜ ê°œë… GANì´ë€GANì€ 2014ë…„ Ian Goodfellowì™€ ê·¸ì˜ ë™ë£Œë“¤ì— ì˜í•´ ì œì•ˆëœ ìƒì„±í˜• ëª¨ë¸ì…ë‹ˆë‹¤GANì€ ë‘ ê°œì˜ ì‹ ê²½ë§, ì¦‰ ìƒì„±ì(Generator)ì™€ íŒë³„ì(Discriminator)ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.ìƒì„±ìëŠ” ê°€ì§œ ë°ì´í„°ë¥¼ ìƒì„±í•˜ê³ , íŒë³„ìëŠ” ì´ ë°ì´í„°ê°€ ì§„ì§œì¸ì§€ ê°€ì§œì¸ì§€ íŒë³„í•˜ë©°, ì„œë¡œ ê²½ìŸí•˜ì—¬ ë™ì‹œì— í•™ìŠµí•©ë‹ˆë‹¤ALT2) ë™ì‘ ì›ë¦¬ ìƒì„±ì(Generator)ëœë¤ ë…¸ì´ì¦ˆ ë²¡í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ì„œ ì´ë¥¼ í†µí•´ ê°€ì§œ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤ìƒì„±ëœ ë°ì´í„°ëŠ” íŒë³„ìì—ê²Œ ì „ë‹¬ë˜ì–´ ì§„ì§œ ë°ì´í„°ì²˜ëŸ¼ ë³´ì´ë„ë¡ í•™ìŠµë©ë‹ˆë‹¤. íŒë³„ì(Discriminator)ì§„ì§œ ë°ì´í„°ì™€ ìƒì„±ëœ ê°€ì§œ ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ì„œ ì´ë¥¼ êµ¬ë¶„í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤íŒë³„ìëŠ” ì§„ì§œ ë°ì´í„°ë¥¼ 1ë¡œ, ê°€ì§œ ë°ì´í„°ë¥¼ 0ìœ¼ë¡œ ë¶„ë¥˜í•˜ë„ë¡ í•™ìŠµë©ë‹ˆë‹¤. ê²½ìŸ ê³¼ì •ìƒì„±ìëŠ” íŒë³„ìë¥¼ ì†ì´ê¸° ìœ„í•´ ì ì  ë” ì§„ì§œ ê°™ì€ ë°ì´í„°ë¥¼ ìƒì„±í•˜ë ¤ê³  ë…¸ë ¥í•˜ê²Œ ë©ë‹ˆë‹¤.íŒë³„ìëŠ” ìƒì„±ìê°€ ë§Œë“  ê°€ì§œ ë°ì´í„°ë¥¼ ë” ì˜ êµ¬ë¶„í•˜ë ¤ê³  ë…¸ë ¥í•˜ê²Œ ë©ë‹ˆë‹¤ì´ ê³¼ì •ì—ì„œ ë‘ ë„¤íŠ¸ì›Œí¬ëŠ” ì„œë¡œ ê²½ìŸí•˜ë©° ë™ì‹œì— ë°œì „í•˜ê²Œ ë©ë‹ˆë‹¤02. VAE (Variational Autoencoder)âœ”ï¸ìƒì„±í˜• ëª¨ë¸ì¤‘ í•˜ë‚˜ì¸ VAEëª¨ë¸ì´ ë¬´ì—‡ì¸ì§€, ì–´ë””ì— ì‚¬ìš©í•  ìˆ˜ ìˆì„ì§€ ì•Œì•„ ë´…ì‹œë‹¤.1) VAEì˜ ê°œë… VAEì´ë€?VAEëŠ” 2013ë…„ Kingmaì™€ Wellingì— ì˜í•´ ì œì•ˆëœ ìƒì„±í˜• ëª¨ë¸ì…ë‹ˆë‹¤.VAEëŠ” ì¸ì½”ë”(Encoder)ì™€ ë””ì½”ë”(Decoder)ë¡œ êµ¬ì„±ëœ ì˜¤í† ì¸ì½”ë”ì˜ ë³€í˜•ì…ë‹ˆë‹¤.ì¸ì½”ë”ëŠ” ì…ë ¥ ë°ì´í„°ë¥¼ ì ì¬ ê³µê°„(latent space)ìœ¼ë¡œ ë§¤í•‘í•˜ê³ , ë””ì½”ë”ëŠ” ì´ ì ì¬ ê³µê°„ì—ì„œ ë°ì´í„°ë¥¼ ë‹¤ì‹œ ì›ë˜ ê³µê°„ìœ¼ë¡œ ë³µì›í•©ë‹ˆë‹¤.VAEëŠ” ì ì¬ ê³µê°„ì„ í™•ë¥  ë¶„í¬ë¡œ ëª¨ë¸ë§í•˜ì—¬, ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ìƒì„±í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì„ ê°–ì¶”ê²Œ ë©ë‹ˆë‹¤.ALT2) ì‘ìš© ì´ë¯¸ì§€ ìƒì„±VAEëŠ” ìƒˆë¡œìš´ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” ë° ì‚¬ìš©ë  ìˆ˜ ìˆë‹¤.ì˜ˆë¥¼ ë“¤ì–´, ì–¼êµ´ ì´ë¯¸ì§€ ë°ì´í„°ì…‹ì„ í•™ìŠµí•œ VAEëŠ” ìƒˆë¡œìš´ ì–¼êµ´ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•  ìˆ˜ ìˆë‹¤. ë°ì´í„° ì••ì¶•VAEëŠ” ë°ì´í„°ë¥¼ ì ì¬ ê³µê°„ìœ¼ë¡œ ì••ì¶•í•˜ê³ , ì´ë¥¼ í†µí•´ ë°ì´í„° ì••ì¶• ë° ë³µì›ì— ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ë…¸ì´ì¦ˆ ì œê±°VAEëŠ” ë…¸ì´ì¦ˆê°€ ìˆëŠ” ë°ì´í„°ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ì„œ ë…¸ì´ì¦ˆë¥¼ ì œê±°í•œ ê¹¨ë—í•œ ë°ì´í„°ë¥¼ ì¶œë ¥í•  ìˆ˜ ìˆë‹¤..
ë°ì´í„° ì „ì²˜ë¦¬: ì´ìƒì¹˜ íƒì§€ ë° ì²˜ë¦¬ì¸ê³µì§€ëŠ¥ì„ ìœ„í•œ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬/ì¸ê³µì§€ëŠ¥ì„ ìœ„í•œ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ - 5ì£¼ì°¨/ë°ì´í„° ì „ì²˜ë¦¬: ì´ìƒì¹˜ íƒì§€ ë° ì²˜ë¦¬ë°ì´í„° ì „ì²˜ë¦¬: ì´ìƒì¹˜ íƒì§€ ë° ì²˜ë¦¬ìˆ˜ì—… ëª©í‘œë°ì´í„°ì˜ ì´ìƒì¹˜ê°€ ë¬´ì—‡ì¸ì§€ í•™ìŠµí•©ë‹ˆë‹¤.ì „ì²˜ë¦¬ ê³¼ì •ì—ì„œ ì´ìƒì¹˜ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ì„œ í•™ìŠµí•©ë‹ˆë‹¤.ëª©ì°¨ì´ìƒì¹˜(Outlier)ë€ì´ìƒì¹˜ íƒì§€ ë°©ë²•ì´ìƒì¹˜ ì²˜ë¦¬ ë°©ë²•ìš”ì•½ì´ìƒì¹˜(Outlier)ë€â—ì´ìƒì¹˜ëŠ” ë°ì´í„°ì˜ ì¼ë°˜ì ì¸ íŒ¨í„´ì—ì„œ ë²—ì–´ë‚œ ê°’ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

ì´ëŸ¬í•œ ê°’ë“¤ì€ ë°ì´í„° ë¶„ì„ì— ë¶€ì •ì ì¸ ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆê¸° ë•Œë¬¸ì—,
íƒì§€í•˜ê³  ì ì ˆíˆ ì²˜ë¦¬í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤!ì´ìƒì¹˜ íƒì§€ ë°©ë²•ğŸ“šì´ìƒì¹˜ë¥¼ íƒì§€í•˜ëŠ” ë°©ë²•ì—ëŠ” ì—¬ëŸ¬ ê°€ì§€ê°€ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ê¸°ìˆ  í†µê³„, ì‹œê°í™”, IQR(ì‚¬ë¶„ìœ„ìˆ˜ ë²”ìœ„)ë¥¼ ì‚¬ìš©í•œ ë°©ë²•ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.ê¸°ìˆ  í†µê³„ ê¸°ë°˜ ì´ìƒì¹˜ íƒì§€describe() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ì˜ ê¸°ë³¸ í†µê³„ëŸ‰ì„ í™•ì¸í•˜ê³ , ì´ìƒì¹˜ë¥¼ ì˜ì‹¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Pythonë³µì‚¬import pandas as pd

# ì˜ˆì‹œ ë°ì´í„°í”„ë ˆì„ ìƒì„±
data = {
'ì´ë¦„': ['ì² ìˆ˜', 'ì˜í¬', 'ë¯¼ìˆ˜', 'ì§€ìˆ˜', 'ìƒìˆ˜'],
'ë‚˜ì´': [25, 30, 22, 35, 120], # 120ì€ ì´ìƒì¹˜ë¡œ ì˜ì‹¬ë¨
'ì ìˆ˜': [90, 85, 95, 80, 88]
}
df = pd.DataFrame(data)
# ê¸°ìˆ  í†µê³„ëŸ‰ í™•ì¸
print(df['ë‚˜ì´'].describe())

â€‹ê²°ê³¼:Plain Textë³µì‚¬count      5.000000
mean      46.400000
std       41.892848
min       22.000000
25%       25.000000
50%       30.000000
75%       35.000000
max      120.000000
Name: ë‚˜ì´, dtype: float64

â€‹í‰ê· (mean)ê³¼ í‘œì¤€í¸ì°¨(std)ê°€ í° ì°¨ì´ë¥¼ ë³´ì´ëŠ” ê²½ìš°, ë˜ëŠ” ìµœëŒ€ê°’(max)ì´ ë¹„ì •ìƒì ìœ¼ë¡œ ë†’ì€ ê²½ìš° ì´ìƒì¹˜ë¥¼ ì˜ì‹¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ì‹œê°í™”ë¥¼ ì‚¬ìš©í•œ ì´ìƒì¹˜ íƒì§€ë°•ìŠ¤í”Œë¡¯(Box Plot)ê³¼ íˆìŠ¤í† ê·¸ë¨ì„ ì‚¬ìš©í•˜ë©´ ë°ì´í„°ì˜ ë¶„í¬ë¥¼ ì‹œê°ì ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆì–´, ì´ìƒì¹˜ë¥¼ íƒì§€í•˜ê¸° ìš©ì´í•©ë‹ˆë‹¤.Pythonë³µì‚¬import matplotlib.pyplot as plt

# ë°•ìŠ¤í”Œë¡¯ìœ¼ë¡œ ì´ìƒì¹˜ ì‹œê°í™”
plt.boxplot(df['ë‚˜ì´'])
plt.title('ë‚˜ì´ì˜ ë°•ìŠ¤í”Œë¡¯')
plt.show()

â€‹ë°•ìŠ¤í”Œë¡¯ì˜ ì´ìƒì¹˜ëŠ” í†µìƒì ìœ¼ë¡œ ë°•ìŠ¤(ì‚¬ë¶„ìœ„ìˆ˜ ë²”ìœ„)ì˜ ìœ„ì•„ë˜ì— ìœ„ì¹˜í•œ ì ìœ¼ë¡œ í‘œì‹œë©ë‹ˆë‹¤.IQR(Interquartile Range)ì„ ì‚¬ìš©í•œ ì´ìƒì¹˜ íƒì§€IQRì€ 1ì‚¬ë¶„ìœ„ìˆ˜(Q1)ì™€ 3ì‚¬ë¶„ìœ„ìˆ˜(Q3)ì˜ ì°¨ì´ë¡œ, ì´ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ëŠ” ë°ì´í„°ë¥¼ ì´ìƒì¹˜ë¡œ ê°„ì£¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Pythonë³µì‚¬# IQR ê³„ì‚°
Q1 = df['ë‚˜ì´'].quantile(0.25)
Q3 = df['ë‚˜ì´'].quantile(0.75)
IQR = Q3 - Q1

# IQRì„ ì´ìš©í•œ ì´ìƒì¹˜ íƒì§€
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

outliers = df[(df['ë‚˜ì´'] < lower_bound) | (df['ë‚˜ì´'] > upper_bound)]
print(outliers)

â€‹ê²°ê³¼:Plain Textë³µì‚¬    ì´ë¦„   ë‚˜ì´  ì ìˆ˜
4  ìƒìˆ˜  120  88

â€‹120ì´ë¼ëŠ” ê°’ì´ IQR ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ë¯€ë¡œ ì´ìƒì¹˜ë¡œ íƒì§€ë©ë‹ˆë‹¤.ì´ìƒì¹˜ ì²˜ë¦¬ ë°©ë²•ğŸ“šì´ìƒì¹˜ë¥¼ íƒì§€í•œ í›„, ë°ì´í„°ì˜ íŠ¹ì„±ê³¼ ë¶„ì„ ëª©ì ì— ë”°ë¼ ì ì ˆíˆ ì²˜ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤. ëŒ€í‘œì ì¸ ì²˜ë¦¬ ë°©ë²•ë“¤ì„ ì†Œê°œí•©ë‹ˆë‹¤.ì´ìƒì¹˜ ì œê±°ì´ìƒì¹˜ë¥¼ ë°ì´í„°í”„ë ˆì„ì—ì„œ ì œê±°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Pythonë³µì‚¬# ì´ìƒì¹˜ë¥¼ ì œê±°í•œ ë°ì´í„°í”„ë ˆì„
df_without_outliers = df[(df['ë‚˜ì´'] >= lower_bound) & (df['ë‚˜ì´'] <= upper_bound)]
print(df_without_outliers)

â€‹ê²°ê³¼:Plain Textë³µì‚¬   ì´ë¦„   ë‚˜ì´  ì ìˆ˜
0  ì² ìˆ˜  25  90
1  ì˜í¬  30  85
2  ë¯¼ìˆ˜  22  95
3  ì§€ìˆ˜  35  80

â€‹ì´ìƒì¹˜ë¥¼ íŠ¹ì • ê°’ìœ¼ë¡œ ëŒ€ì²´ì´ìƒì¹˜ë¥¼ íŠ¹ì • ê°’ìœ¼ë¡œ ëŒ€ì²´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, í•´ë‹¹ ì—´ì˜ ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Pythonë³µì‚¬# 'ë‚˜ì´'ì˜ ì¤‘ì•™ê°’ìœ¼ë¡œ ì´ìƒì¹˜ ëŒ€ì²´
median_age = df['ë‚˜ì´'].median()
df['ë‚˜ì´'] = df['ë‚˜ì´'].apply(lambda x: median_age if x > upper_bound or x < lower_bound else x)
print(df)

â€‹ê²°ê³¼:Plain Textë³µì‚¬   ì´ë¦„   ë‚˜ì´  ì ìˆ˜
0  ì² ìˆ˜  25.0  90
1  ì˜í¬  30.0  85
2  ë¯¼ìˆ˜  22.0  95
3  ì§€ìˆ˜  35.0  80
4  ìƒìˆ˜  30.0  88

â€‹120ì´ì—ˆë˜ ì´ìƒì¹˜ê°€ 30ìœ¼ë¡œ ëŒ€ì²´ë˜ì—ˆìŠµë‹ˆë‹¤.ì´ìƒì¹˜ë¥¼ ê·¸ëŒ€ë¡œ ìœ ì§€ì´ìƒì¹˜ê°€ ì¤‘ìš”í•œ ë¶„ì„ í¬ì¸íŠ¸ê°€ ë  ìˆ˜ ìˆë‹¤ê³  íŒë‹¨ë˜ë©´, ë³„ë„ì˜ ì²˜ë¦¬ë¥¼ í•˜ì§€ ì•Šê³  ê·¸ëŒ€ë¡œ ìœ ì§€í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.Pythonë³µì‚¬# ì´ìƒì¹˜ë¥¼ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ëŠ” ê²½ìš° (ì²˜ë¦¬í•˜ì§€ ì•ŠìŒ)
print(df)

â€‹ì´ ë°©ë²•ì€ ë¶„ì„ ëª©ì ì— ë”°ë¼ ì„ íƒì ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.ìš”ì•½ğŸ“šìš”ì•½ì´ìƒì¹˜ëŠ” ë°ì´í„°ì˜ ì¼ë°˜ì ì¸ íŒ¨í„´ì—ì„œ ë²—ì–´ë‚œ ê°’ì´ë©°, ë¶„ì„ ê²°ê³¼ì— í° ì˜í–¥ì„ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.íƒì§€ ë°©ë²•ìœ¼ë¡œëŠ” ê¸°ìˆ  í†µê³„ëŸ‰ í™•ì¸, ì‹œê°í™”(ë°•ìŠ¤í”Œë¡¯, íˆìŠ¤í† ê·¸ë¨), IQRì„ í™œìš©í•œ ë°©ë²• ë“±ì´ ìˆìŠµë‹ˆë‹¤.ì²˜ë¦¬ ë°©ë²•ìœ¼ë¡œëŠ” ì´ìƒì¹˜ ì œê±°, íŠ¹ì • ê°’ìœ¼ë¡œ ëŒ€ì²´, ë˜ëŠ” ì´ìƒì¹˜ë¥¼ ê·¸ëŒ€ë¡œ ìœ ì§€í•˜ëŠ” ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤.ì´ì œ ë°ì´í„°ì˜ ì´ìƒì¹˜ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ íƒì§€í•˜ê³ , ì ì ˆíˆ ì²˜ë¦¬í•˜ì—¬ ë¶„ì„ì˜ ì •í™•ë„ë¥¼ ë†’ì—¬ë³´ì„¸ìš”! â€‹ìµëª…9ì›” 25ì¼matplotlib íŒ¨í‚¤ì§€ ì„¤ì¹˜ ëª…ë ¹ì–´ ì¶”ê°€í•˜ë©´ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.

!pip install matplotlib
[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 13ê°•. ì§€ë„í•™ìŠµ : ë¶„ë¥˜ëª¨ë¸ - ì˜ì‚¬ê²°ì •ë‚˜ë¬´[SCC] ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ - 3ì£¼ì°¨/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 13ê°•. ì§€ë„í•™ìŠµ : ë¶„ë¥˜ëª¨ë¸ - ì˜ì‚¬ê²°ì •ë‚˜ë¬´ì œì‘:[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 13ê°•. ì§€ë„í•™ìŠµ : ë¶„ë¥˜ëª¨ë¸ - ì˜ì‚¬ê²°ì •ë‚˜ë¬´[ìˆ˜ì—… ëª©í‘œ]ë¶„ë¥˜ëª¨ë¸ì¤‘ ì˜ì‚¬ê²°ì •ë‚˜ë¬´ì— ëŒ€í•œ ê°œë…ì„ ë°°ìš°ê³ , ë°ì´í„°ë¥¼ ì´ìš©í•´ ì‹¤ìŠµí•´ ë´…ë‹ˆë‹¤[ëª©ì°¨]01. ì˜ì‚¬ê²°ì •ë‚˜ë¬´ ê°œë…02. ì˜ì‚¬ê²°ì •ë‚˜ë¬´ ì‹¤ìŠµğŸ’¡
 
 01. ì˜ì‚¬ê²°ì •ë‚˜ë¬´ ê°œë…âœ”ï¸ì§€ë„í•™ìŠµ:ë¶„ë¥˜ëª¨ë¸ì˜ ì˜ì‚¬ê²°ì •ë‚˜ë¬´ê°€ ë¬´ì—‡ì¸ì§€ ì•Œì•„ë´…ì‹œë‹¤1) ì˜ì‚¬ê²°ì •ë‚˜ë¬´ ì˜ì‚¬ê²°ì •ë‚˜ë¬´ë€?ì˜ì‚¬ê²°ì •ë‚˜ë¬´(Decision Tree)ëŠ” ì˜ˆì¸¡ ëª¨ë¸ ì¤‘ í•˜ë‚˜ë¡œ, ë°ì´í„°ì˜ íŠ¹ì§•(feature)ì„ ê¸°ì¤€ìœ¼ë¡œ ì˜ì‚¬ê²°ì • ê·œì¹™ì„ ë§Œë“¤ê³  ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë¶„ë¥˜í•˜ê±°ë‚˜ íšŒê·€í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤ì˜ì‚¬ê²°ì •ë‚˜ë¬´ëŠ” íŠ¸ë¦¬ êµ¬ì¡°ë¥¼ ê°€ì§€ë©°, ê° ë‚´ë¶€ ë…¸ë“œëŠ” ë°ì´í„°ì˜ íŠ¹ì • íŠ¹ì§•ì— ëŒ€í•œ í…ŒìŠ¤íŠ¸ë¥¼ ë‚˜íƒ€ë‚´ê³ , ê° ê°€ì§€(branch)ëŠ” í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ë‚˜íƒ€ë‚´ë©°, ê° ë¦¬í”„ ë…¸ë“œ(leaf)ëŠ” í´ë˜ìŠ¤ ë ˆì´ë¸”ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.ALTë…¸ë“œ: íŠ¸ë¦¬ì˜ ê° ë¶„ê¸°ì ìœ¼ë¡œ, í•˜ë‚˜ì˜ íŠ¹ì§•(feature)ì— ëŒ€í•œ í…ŒìŠ¤íŠ¸ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.ë…¸ë“œ: íŠ¸ë¦¬ì˜ ê° ë¶„ê¸°ì ìœ¼ë¡œ, í•˜ë‚˜ì˜ íŠ¹ì§•(feature)ì— ëŒ€í•œ í…ŒìŠ¤íŠ¸ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.ï»¿â€‹ë£¨íŠ¸ ë…¸ë“œ: íŠ¸ë¦¬ì˜ ìµœìƒìœ„ ë…¸ë“œë¡œ, ì „ì²´ ë°ì´í„°ì…‹ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.ë£¨íŠ¸ ë…¸ë“œ: íŠ¸ë¦¬ì˜ ìµœìƒìœ„ ë…¸ë“œë¡œ, ì „ì²´ ë°ì´í„°ì…‹ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.ï»¿â€‹ë¦¬í”„ ë…¸ë“œ: íŠ¸ë¦¬ì˜ ë ë…¸ë“œë¡œ, ìµœì¢… í´ë˜ìŠ¤ ë ˆì´ë¸”ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.ë¦¬í”„ ë…¸ë“œ: íŠ¸ë¦¬ì˜ ë ë…¸ë“œë¡œ, ìµœì¢… í´ë˜ìŠ¤ ë ˆì´ë¸”ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.ï»¿â€‹ê¹Šì´: íŠ¸ë¦¬ì˜ ë£¨íŠ¸ ë…¸ë“œë¶€í„° ë¦¬í”„ ë…¸ë“œê¹Œì§€ì˜ ìµœëŒ€ ê±°ë¦¬ì…ë‹ˆë‹¤.ê¹Šì´: íŠ¸ë¦¬ì˜ ë£¨íŠ¸ ë…¸ë“œë¶€í„° ë¦¬í”„ ë…¸ë“œê¹Œì§€ì˜ ìµœëŒ€ ê±°ë¦¬ì…ë‹ˆë‹¤.ï»¿â€‹ë¶„í•  ê¸°ì¤€: ë…¸ë“œë¥¼ ë‚˜ëˆŒ ë•Œ ì‚¬ìš©í•˜ëŠ” ê¸°ì¤€ìœ¼ë¡œ, ì •ë³´ ì´ë“(Information Gain), ì§€ë‹ˆ ê³„ìˆ˜(Gini Index) ë“±ì´ ìˆìŠµë‹ˆë‹¤.ë¶„í•  ê¸°ì¤€: ë…¸ë“œë¥¼ ë‚˜ëˆŒ ë•Œ ì‚¬ìš©í•˜ëŠ” ê¸°ì¤€ìœ¼ë¡œ, ì •ë³´ ì´ë“(Information Gain), ì§€ë‹ˆ ê³„ìˆ˜(Gini Index) ë“±ì´ ìˆìŠµë‹ˆë‹¤.ï»¿â€‹ ë¶„í• ê¸°ì¤€ì •ë³´ ì´ë“(Information Gain) : ì—”íŠ¸ë¡œí”¼(Entropy)ê°’ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë‚˜ëˆ„ëŠ” ê¸°ì¤€ì…ë‹ˆë‹¤. ì—”íŠ¸ë¡œí”¼ëŠ” ë¶ˆí™•ì‹¤ì„±ì„ ë‚˜íƒ€ë‚´ë©°, ì—”íŠ¸ë¡œí”¼ê°€ ë‚®ì„ìˆ˜ë¡ ë¶ˆí™•ì‹¤ì„±ì´ ì ìŠµë‹ˆë‹¤.Information Gain(D,A)=Entropy(D)âˆ’âˆ‘vâˆˆvalues(A)âˆ£Dvâˆ£âˆ£Dâˆ£Entropy(Dv)(D, A) = (D) - _{v  (A)} {|D|} (D_v)Information Gain(D,A)=Entropy(D)âˆ’vâˆˆvalues(A)âˆ‘â€‹âˆ£Dâˆ£âˆ£Dvâ€‹âˆ£â€‹Entropy(Dvâ€‹)ì§€ë‹ˆ ê³„ìˆ˜(Gini Index): ë¶ˆìˆœë„ë¥¼ ì¸¡ì •í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ, ì§€ë‹ˆ ê³„ìˆ˜ê°€ ë‚®ì„ìˆ˜ë¡ ë¶ˆìˆœë„ê°€ ì ìŠµë‹ˆë‹¤Gini(D)=1âˆ’âˆ‘i=1Cpi2(D) = 1 - _{i=1}^{C} p_i^2Gini(D)=1âˆ’i=1âˆ‘Câ€‹pi2â€‹ì—¬ê¸°ì„œ piëŠ” í´ë˜ìŠ¤ iì˜ ë¹„ìœ¨ì…ë‹ˆë‹¤.ì—¬ê¸°ì„œ piâ€‹ëŠ” í´ë˜ìŠ¤ iì˜ ë¹„ìœ¨ì…ë‹ˆë‹¤.ï»¿â€‹02. ì˜ì‚¬ê²°ì •ë‚˜ë¬´ ì‹¤ìŠµâœ”ï¸Scikit-learnì˜ ìœ ë°©ì•”ë°ì´í„°ì™€ Seabornì˜ íƒ€ì´íƒ€ë‹‰ ë°ì´í„°ë¡œ ì˜ì‚¬ê²°ì •ë‚˜ë¬´ ì‹¤ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤1) ìœ ë°©ì•” ë°ì´í„° ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ìœ ë°©ì•” ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ {5px}ìœ ë°©ì•” ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ï»¿â€‹Pythonë³µì‚¬import numpy as np
import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# ë°ì´í„° ë¡œë“œ
data = load_breast_cancer()
X = data.data
y = data.target

# ë°ì´í„° ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# ë°ì´í„° ìŠ¤ì¼€ì¼ë§
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
â€‹sklearn.datasets.load_breast_cancer: ìœ ë°©ì•” ë°ì´í„°ì…‹ ë¡œë“œreturn_X_y=False: ë°ì´í„°ì™€ íƒ€ê²Ÿì„ í•¨ê»˜ ë°˜í™˜í• ì§€ ì—¬ë¶€. ê¸°ë³¸ê°’ì€ Falseì…ë‹ˆë‹¤._X_y=False: ë°ì´í„°ì™€ íƒ€ê²Ÿì„ í•¨ê»˜ ë°˜í™˜í• ì§€ ì—¬ë¶€. ê¸°ë³¸ê°’ì€ Falseì…ë‹ˆë‹¤.ï»¿â€‹sklearn.model_selection.train_test_split: ë°ì´í„°ë¥¼ í›ˆë ¨ ì„¸íŠ¸/ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ ë¶„í• test_size=0.2: í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì˜ ë¹„ìœ¨ì„ 0.2ë¡œ ì„¤ì •í•©ë‹ˆë‹¤._size=0.2: í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì˜ ë¹„ìœ¨ì„ 0.2ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.ï»¿â€‹random_state=42: ëœë¤ ì‹œë“œ ê°’ìœ¼ë¡œ, ë°ì´í„° ë¶„í• ì˜ ì¬í˜„ì„±ì„ ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤._state=42: ëœë¤ ì‹œë“œ ê°’ìœ¼ë¡œ, ë°ì´í„° ë¶„í• ì˜ ì¬í˜„ì„±ì„ ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.ï»¿â€‹sklearn.preprocessing.StandardScaler: ë°ì´í„°ì˜ í‰ê· ì„ 0, ë¶„ì‚°ì„ 1ë¡œ ìŠ¤ì¼€ì¼ë§fit_transform(X_train): í›ˆë ¨ ì„¸íŠ¸ë¥¼ ìŠ¤ì¼€ì¼ë§í•˜ê³  ë³€í™˜í•©ë‹ˆë‹¤._transform(X_train): í›ˆë ¨ ì„¸íŠ¸ë¥¼ ìŠ¤ì¼€ì¼ë§í•˜ê³  ë³€í™˜í•©ë‹ˆë‹¤.ï»¿â€‹transform(X_test): í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¥¼ ë³€í™˜í•©ë‹ˆë‹¤.(X_test): í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¥¼ ë³€í™˜í•©ë‹ˆë‹¤.ï»¿â€‹ ëª¨ë¸ í•™ìŠµëª¨ë¸ í•™ìŠµ {5px}ëª¨ë¸ í•™ìŠµ ï»¿â€‹Pythonë³µì‚¬from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
model = DecisionTreeClassifier(random_state=42)
model.fit(X_train, y_train)
# ì˜ˆì¸¡
y_pred = model.predict(X_test)
# í‰ê°€
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
print(f"Classification Report:")
print(f"Confusion Matrix:")
â€‹sklearn.tree.DecisionTreeClassifier: ì˜ì‚¬ê²°ì •ë‚˜ë¬´ ë¶„ë¥˜ ëª¨ë¸ ìƒì„±random_state=42: ëœë¤ ì‹œë“œ ê°’ìœ¼ë¡œ, íŠ¸ë¦¬ì˜ ì¬í˜„ì„±ì„ ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤._state=42: ëœë¤ ì‹œë“œ ê°’ìœ¼ë¡œ, íŠ¸ë¦¬ì˜ ì¬í˜„ì„±ì„ ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.ï»¿â€‹fit(X_train, y_train): ëª¨ë¸ì„ í›ˆë ¨ ì„¸íŠ¸ì— ë§ì¶”ì–´ í•™ìŠµì‹œí‚µë‹ˆë‹¤.(X_train, y_train): ëª¨ë¸ì„ í›ˆë ¨ ì„¸íŠ¸ì— ë§ì¶”ì–´ í•™ìŠµì‹œí‚µë‹ˆë‹¤.ï»¿â€‹predict(X_test): í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì— ëŒ€í•´ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.(X_test): í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì— ëŒ€í•´ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.ï»¿â€‹sklearn.metrics.accuracy_score: ì •í™•ë„ ê³„ì‚°accuracy_score(y_test, y_pred): ì‹¤ì œ ê°’ê³¼ ì˜ˆì¸¡ ê°’ì„ ë¹„êµí•˜ì—¬ ì •í™•ë„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤._score(y_test, y_pred): ì‹¤ì œ ê°’ê³¼ ì˜ˆì¸¡ ê°’ì„ ë¹„êµí•˜ì—¬ ì •í™•ë„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.ï»¿â€‹sklearn.metrics.classification_report: ë¶„ë¥˜ ë³´ê³ ì„œ ìƒì„±classification_report(y_test, y_pred): ì •í™•ë„, ì •ë°€ë„, ì¬í˜„ìœ¨ ë“±ì˜ ë©”íŠ¸ë¦­ì„ í¬í•¨í•œ ë³´ê³ ì„œë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤._report(y_test, y_pred): ì •í™•ë„, ì •ë°€ë„, ì¬í˜„ìœ¨ ë“±ì˜ ë©”íŠ¸ë¦­ì„ í¬í•¨í•œ ë³´ê³ ì„œë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.ï»¿â€‹sklearn.metrics.confusion_matrix: í˜¼ë™ í–‰ë ¬ ìƒì„±confusion_matrix(y_test, y_pred): ì‹¤ì œ ê°’ê³¼ ì˜ˆì¸¡ ê°’ì˜ í˜¼ë™ í–‰ë ¬ì„ ë°˜í™˜í•©ë‹ˆë‹¤._matrix(y_test, y_pred): ì‹¤ì œ ê°’ê³¼ ì˜ˆì¸¡ ê°’ì˜ í˜¼ë™ í–‰ë ¬ì„ ë°˜í™˜í•©ë‹ˆë‹¤.ï»¿â€‹2) íƒ€ì´íƒ€ë‹‰ ë°ì´í„° ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬íƒ€ì´íƒ€ë‹‰ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ {5px}íƒ€ì´íƒ€ë‹‰ ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ï»¿â€‹Pythonë³µì‚¬import seaborn as sns

# ë°ì´í„° ë¡œë“œ
titanic = sns.load_dataset('titanic')
# í•„ìš”í•œ ì—´ ì„ íƒ ë° ê²°ì¸¡ê°’ ì²˜ë¦¬
titanic = titanic[['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']].dropna()
# ì„±ë³„ê³¼ íƒ‘ìŠ¹í•œ ê³³ ì¸ì½”ë”©
titanic['sex'] = titanic['sex'].map({'male': 0, 'female': 1})
titanic['embarked'] = titanic['embarked'].map({'C': 0, 'Q': 1, 'S': 2})
# íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
X = titanic.drop('survived', axis=1)
y = titanic['survived']
# ë°ì´í„° ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# ë°ì´í„° ìŠ¤ì¼€ì¼ë§
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
â€‹seaborn.load_dataset: seabornì˜ ë‚´ì¥ ë°ì´í„°ì…‹ ë¡œë“œâ€™titanicâ€™: íƒ€ì´íƒ€ë‹‰ ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤.â€™titanicâ€™: íƒ€ì´íƒ€ë‹‰ ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤.ï»¿â€‹pandas.DataFrame.dropna: ê²°ì¸¡ê°’ì´ ìˆëŠ” í–‰ ì œê±°pandas.DataFrame.map: ë°ì´í„° ê°’ì„ ë‹¤ë¥¸ ê°’ìœ¼ë¡œ ë§¤í•‘â€™maleâ€™: 0, â€™femaleâ€™: 1: ì„±ë³„ì„ ìˆ«ìë¡œ ë§¤í•‘í•©ë‹ˆë‹¤.: ì„±ë³„ì„ ìˆ«ìë¡œ ë§¤í•‘í•©ë‹ˆë‹¤.}â€™maleâ€™: 0, â€™femaleâ€™: 1: ì„±ë³„ì„ ìˆ«ìë¡œ ë§¤í•‘í•©ë‹ˆë‹¤.ï»¿â€‹â€™Câ€™: 0, â€™Qâ€™: 1, â€™Sâ€™: 2: íƒ‘ìŠ¹í•œ ê³³ì„ ìˆ«ìë¡œ ë§¤í•‘í•©ë‹ˆë‹¤.: íƒ‘ìŠ¹í•œ ê³³ì„ ìˆ«ìë¡œ ë§¤í•‘í•©ë‹ˆë‹¤.}â€™Câ€™: 0, â€™Qâ€™: 1, â€™Sâ€™: 2: íƒ‘ìŠ¹í•œ ê³³ì„ ìˆ«ìë¡œ ë§¤í•‘í•©ë‹ˆë‹¤.ï»¿â€‹ ëª¨ë¸ í•™ìŠµëª¨ë¸ í•™ìŠµ {5px}ëª¨ë¸ í•™ìŠµ ï»¿â€‹Pythonë³µì‚¬# ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
model = DecisionTreeClassifier(random_state=42)
model.fit(X_train, y_train)
# ì˜ˆì¸¡
y_pred = model.predict(X_test)
# í‰ê°€
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
print(f"Classification Report:")
print(f"Confusion Matrix:")
â€‹.
[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 13. ì „ì´í•™ìŠµ[SCC] ê¸°ì´ˆê°€ íƒ„íƒ„í•œ ë”¥ëŸ¬ë‹/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] ê¸°ì´ˆê°€ íƒ„íƒ„í•œ ë”¥ëŸ¬ë‹ - 5ì£¼ì°¨/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 13. ì „ì´í•™ìŠµì œì‘:[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 13. ì „ì´í•™ìŠµ[ìˆ˜ì—… ëª©í‘œ]ì „ì´í•™ìŠµì— ëŒ€í•´ì„œ ë°°ì›Œë³´ê³ , ì–¸ì œ ì „ì´í•™ìŠµì„ ì‚¬ìš©í•˜ëŠ”ì§€ ì•Œì•„ ë´…ì‹œë‹¤Pytorchë¡œ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ì´ìš©í•´ ì „ì´í•™ìŠµ êµ¬í˜„ ì‹¤ìŠµì„ ì§„í–‰í•´ ë´…ì‹œë‹¤[ëª©ì°¨]01. ì „ì´í•™ìŠµ
 
 01. ì „ì´í•™ìŠµì „ì´í•™ìŠµì´ë€ ë¬´ì—‡ì¸ì§€, ì–´ë–¨ë•Œ ì ìš©í•˜ëŠ”ì§€ ë°°ì›Œë´…ì‹œë‹¤1) ì „ì´í•™ìŠµì˜ í•„ìš”ì„±ê³¼ ì›ë¦¬ ì „ì´ í•™ìŠµì´ë€?ì „ì´í•™ìŠµ(Transfer Learning)ì€ ì´ë¯¸ í•™ìŠµëœ ëª¨ë¸ì˜ ì§€ì‹ì„ ìƒˆë¡œìš´ ë¬¸ì œì— ì ìš©í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì „ì´í•™ìŠµì€ íŠ¹íˆ ë°ì´í„°ê°€ ë¶€ì¡±í•œ ìƒí™©ì—ì„œ ìœ ìš©í•˜ë©°, ëª¨ë¸ í•™ìŠµ ì‹œê°„ì„ ë‹¨ì¶•í•˜ê³  ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.ALT ì „ì´í•™ìŠµì˜ í•„ìš”ì„±ë°ì´í„° ë¶€ì¡±: ìƒˆë¡œìš´ ë¬¸ì œì— ëŒ€í•œ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•Šì„ ë•Œ, ì „ì´í•™ìŠµì„ í†µí•´ ê¸°ì¡´ ëª¨ë¸ì˜ ì§€ì‹ì„ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.í•™ìŠµ ì‹œê°„ ë‹¨ì¶•: ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš©ì‹œ, ì²˜ìŒë¶€í„° ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ”ê²ƒ ë³´ë‹¤ ë¹ ë¥´ê²Œ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ì„±ëŠ¥ í–¥ìƒ: ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì€ ëŒ€ê·œëª¨ ë°ì´í„°ì…‹ì—ì„œ í•™ìŠµë˜ì—ˆê¸° ë•Œë¬¸ì—, í†µìƒ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. ì „ì´í•™ìŠµì˜ ì›ë¦¬íŠ¹ì§• ì¶”ì¶œê¸° (Feature Extractor): ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì˜ ì´ˆê¸° ì¸µì„ ê³ ì •í•˜ê³ , ìƒˆë¡œìš´ ë°ì´í„°ì— ë§ê²Œ ë§ˆì§€ë§‰ ì¸µë§Œ ì¬í•™ìŠµí•©ë‹ˆë‹¤.ë¯¸ì„¸ ì¡°ì • (Fine-Tuning): ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ì „ì²´ë¥¼ ìƒˆë¡œìš´ ë°ì´í„°ì— ë§ê²Œ ì¬í•™ìŠµí•©ë‹ˆë‹¤.2) ì „ì´í•™ìŠµì„ í†µí•´ ëª¨ë¸ ë§Œë“¤ì–´ ë³´ê¸° ì „ì´í•™ìŠµì„ ì ìš©í•œ ëª¨ë¸ êµ¬ì¶• ê³¼ì • ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ ë¡œë“œ:PyTorchì—ì„œ ì œê³µí•˜ëŠ” ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤.ì˜ˆë¥¼ ë“¤ì–´, ResNet, VGG, Inception ë“±ì˜ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ëª¨ë¸ ìˆ˜ì •:ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì˜ ë§ˆì§€ë§‰ ì¸µì„ ìƒˆë¡œìš´ ë¬¸ì œì— ë§ê²Œ ìˆ˜ì •í•©ë‹ˆë‹¤.ì˜ˆë¥¼ ë“¤ì–´, ì´ë¯¸ì§€ ë¶„ë¥˜ ë¬¸ì œì—ì„œ í´ë˜ìŠ¤ ìˆ˜ë¥¼ ë³€ê²½í•©ë‹ˆë‹¤.ëª¨ë¸ í•™ìŠµ:ìˆ˜ì •ëœ ëª¨ë¸ì„ ìƒˆë¡œìš´ ë°ì´í„°ì— ë§ê²Œ í•™ìŠµì‹œí‚µë‹ˆë‹¤.íŠ¹ì§• ì¶”ì¶œê¸° ë°©ì‹ì´ë‚˜ ë¯¸ì„¸ ì¡°ì • ë°©ì‹ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤..
ë°ì´í„° ì „ì²˜ë¦¬: ë°ì´í„° ì •ê·œí™”ì™€ í‘œì¤€í™” (ë¹„ì„ í˜• ë³€í™˜ í¬í•¨)ì¸ê³µì§€ëŠ¥ì„ ìœ„í•œ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬/ì¸ê³µì§€ëŠ¥ì„ ìœ„í•œ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ - 5ì£¼ì°¨/ë°ì´í„° ì „ì²˜ë¦¬: ë°ì´í„° ì •ê·œí™”ì™€ í‘œì¤€í™” (ë¹„ì„ í˜• ë³€í™˜ í¬í•¨)ì œì‘:ë°ì´í„° ì „ì²˜ë¦¬: ë°ì´í„° ì •ê·œí™”ì™€ í‘œì¤€í™” (ë¹„ì„ í˜• ë³€í™˜ í¬í•¨)ìˆ˜ì—… ëª©í‘œë°ì´í„° ì „ì²˜ë¦¬ ê³¼ì •ì—ì„œì˜ ì •ê·œí™”ì™€ í‘œì¤€í™” ë°©ë²•ì— ëŒ€í•´ì„œ ì•Œì•„ë´…ë‹ˆë‹¤.ëª©ì°¨1. ë°ì´í„° ì •ê·œí™”(Normalization) ë°ì´í„° í‘œì¤€í™”(Standardization) 3. ë¹„ì„ í˜• ë³€í™˜(Non-linear Transformation) ìš”ì•½1. ë°ì´í„° ì •ê·œí™”(Normalization) â€‹ì •ê·œí™”(Normalization)ëŠ” ë°ì´í„°ì˜ ë²”ìœ„ë¥¼ 0ê³¼ 1 ì‚¬ì´ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ì´ëŠ” ì„œë¡œ ë‹¤ë¥¸ ë²”ìœ„ë¥¼ ê°€ì§„ ë°ì´í„°ë¥¼ ë™ì¼í•œ ìŠ¤ì¼€ì¼ë¡œ ë§ì¶”ì–´ ë¹„êµí•˜ê¸° ì‰½ê²Œ ë§Œë“­ë‹ˆë‹¤.Min-Max ì •ê·œí™”Min-Max ì •ê·œí™”ëŠ” ê°€ì¥ ì¼ë°˜ì ì¸ ì •ê·œí™” ë°©ë²•ìœ¼ë¡œ, ê° ë°ì´í„°ë¥¼ ìµœì†Œê°’ì„ 0, ìµœëŒ€ê°’ì„ 1ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.Pythonë³µì‚¬import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# ì˜ˆì‹œ ë°ì´í„°í”„ë ˆì„ ìƒì„±
data = {
'íŠ¹ì„±1': [10, 20, 30, 40, 50],
'íŠ¹ì„±2': [1, 2, 3, 4, 5]
}
df = pd.DataFrame(data)
# Min-Max ì •ê·œí™”
scaler = MinMaxScaler()
normalized_df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)
print(normalized_df)

â€‹ê²°ê³¼:Plain Textë³µì‚¬   íŠ¹ì„±1  íŠ¹ì„±2
0   0.0   0.0
1   0.25  0.25
2   0.5   0.5
3   0.75  0.75
4   1.0   1.0

â€‹ì´ ë°©ì‹ìœ¼ë¡œ ê° ì—´ì˜ ëª¨ë“  ë°ì´í„°ê°€ 0ì—ì„œ 1 ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ ë³€í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.ë°ì´í„° í‘œì¤€í™”(Standardization) í‘œì¤€í™”(Standardization)ëŠ” ë°ì´í„°ë¥¼ í‰ê· ì´ 0, í‘œì¤€í¸ì°¨ê°€ 1ì´ ë˜ë„ë¡ ë³€í™˜í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ì´ëŠ” ì •ê·œ ë¶„í¬ë¥¼ ê°€ì •í•œ ë§ì€ ë¶„ì„ ê¸°ë²•ì— ìœ ë¦¬í•©ë‹ˆë‹¤.Z-ì ìˆ˜ í‘œì¤€í™”Z-ì ìˆ˜ í‘œì¤€í™”ëŠ” ë°ì´í„°ì—ì„œ í‰ê· ì„ ë¹¼ê³  í‘œì¤€í¸ì°¨ë¡œ ë‚˜ëˆ„ì–´, ëª¨ë“  ë°ì´í„°ê°€ í‘œì¤€ ì •ê·œë¶„í¬(í‰ê·  0, í‘œì¤€í¸ì°¨ 1)ë¥¼ ë”°ë¥´ë„ë¡ ë§Œë“­ë‹ˆë‹¤.Pythonë³µì‚¬from sklearn.preprocessing import StandardScaler

# Z-ì ìˆ˜ í‘œì¤€í™”
scaler = StandardScaler()
standardized_df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)
print(standardized_df)

â€‹ê²°ê³¼:Plain Textë³µì‚¬      íŠ¹ì„±1      íŠ¹ì„±2
0 -1.414214 -1.414214
1 -0.707107 -0.707107
2  0.000000  0.000000
3  0.707107  0.707107
4  1.414214  1.414214

â€‹ê° ì—´ì˜ ë°ì´í„°ê°€ í‰ê· ì´ 0, í‘œì¤€í¸ì°¨ê°€ 1ì´ ë˜ë„ë¡ ë³€í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.3. ë¹„ì„ í˜• ë³€í™˜(Non-linear Transformation) â€‹ë¹„ì„ í˜• ë³€í™˜ì€ ë°ì´í„°ì˜ ë¹„ì •ìƒì ì¸ ë¶„í¬ë¥¼ ì •ê·œ ë¶„í¬ì— ê°€ê¹ê²Œ ë§Œë“¤ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤. ëŒ€í‘œì ì¸ ë°©ë²•ìœ¼ë¡œ ë¡œê·¸ ë³€í™˜, ì œê³±ê·¼ ë³€í™˜, ë°•ìŠ¤-ì½•ìŠ¤ ë³€í™˜ ë“±ì´ ìˆìŠµë‹ˆë‹¤.ë¡œê·¸(Log) ë³€í™˜ë¡œê·¸ ë³€í™˜ì€ ì–‘ì˜ ë°ì´í„°ì—ì„œ ì£¼ë¡œ ì‚¬ìš©ë˜ë©°, ë°ì´í„°ì˜ ë¶„í¬ë¥¼ ì¢íˆëŠ” ë° ìœ ìš©í•©ë‹ˆë‹¤. íŠ¹íˆ, ì§€ìˆ˜ ë¶„í¬ë¥¼ ê°€ì§„ ë°ì´í„°ë¥¼ ë‹¤ë£° ë•Œ íš¨ê³¼ì ì…ë‹ˆë‹¤.Pythonë³µì‚¬import numpy as np

# ë¡œê·¸ ë³€í™˜
df['íŠ¹ì„±1_log'] = np.log(df['íŠ¹ì„±1'])
print(df)

â€‹ê²°ê³¼:Plain Textë³µì‚¬   íŠ¹ì„±1  íŠ¹ì„±2  íŠ¹ì„±1_log
0    10     1   2.302585
1    20     2   2.995732
2    30     3   3.401197
3    40     4   3.688879
4    50     5   3.912023

â€‹ë¡œê·¸ ë³€í™˜ í›„ ë°ì´í„°ì˜ ë¶„í¬ê°€ í‰íƒ„í•´ì§€ëŠ” íš¨ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.ì œê³±ê·¼(Square Root) ë³€í™˜ì œê³±ê·¼ ë³€í™˜ì€ ë°ì´í„°ì˜ ë¶„í¬ë¥¼ í‰íƒ„í•˜ê²Œ ë§Œë“¤ê¸° ìœ„í•œ ë˜ ë‹¤ë¥¸ ë°©ë²•ì…ë‹ˆë‹¤. íŠ¹íˆ, í¬ì•„ì†¡ ë¶„í¬ë¥¼ ê°€ì§„ ë°ì´í„°ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤.Pythonë³µì‚¬# ì œê³±ê·¼ ë³€í™˜
df['íŠ¹ì„±1_sqrt'] = np.sqrt(df['íŠ¹ì„±1'])
print(df)

â€‹ê²°ê³¼:Plain Textë³µì‚¬   íŠ¹ì„±1  íŠ¹ì„±2  íŠ¹ì„±1_log  íŠ¹ì„±1_sqrt
0    10     1   2.302585    3.162278
1    20     2   2.995732    4.472136
2    30     3   3.401197    5.477226
3    40     4   3.688879    6.324555
4    50     5   3.912023    7.071068

â€‹ì œê³±ê·¼ ë³€í™˜ì„ í†µí•´ ë¶„í¬ê°€ ì¤„ì–´ë“œëŠ” íš¨ê³¼ë¥¼ ì–»ìŠµë‹ˆë‹¤.ë°•ìŠ¤-ì½•ìŠ¤(Box-Cox) ë³€í™˜ë°•ìŠ¤-ì½•ìŠ¤ ë³€í™˜ì€ ë‹¤ì–‘í•œ í˜•íƒœì˜ ë°ì´í„° ë¶„í¬ë¥¼ ì •ê·œë¶„í¬ì— ê°€ê¹ê²Œ ë³€í™˜í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ ì–‘ìˆ˜ ë°ì´í„°ì—ì„œë§Œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.Pythonë³µì‚¬from scipy.stats import boxcox

# ë°•ìŠ¤-ì½•ìŠ¤ ë³€í™˜
df['íŠ¹ì„±1_boxcox'], _ = boxcox(df['íŠ¹ì„±1'])
print(df)

â€‹ê²°ê³¼:Plain Textë³µì‚¬   íŠ¹ì„±1  íŠ¹ì„±2  íŠ¹ì„±1_log  íŠ¹ì„±1_sqrt  íŠ¹ì„±1_boxcox
0    10     1   2.302585    3.162278      2.850350
1    20     2   2.995732    4.472136      3.992679
2    30     3   3.401197    5.477226      4.872105
3    40     4   3.688879    6.324555      5.609646
4    50     5   3.912023    7.071068      6.245548

â€‹ë°•ìŠ¤-ì½•ìŠ¤ ë³€í™˜ì„ í†µí•´ ë°ì´í„°ì˜ ë¶„í¬ê°€ ì •ê·œë¶„í¬ì— ê°€ê¹Œì›Œì§€ëŠ” íš¨ê³¼ë¥¼ ì–»ìŠµë‹ˆë‹¤.ìš”ì•½ìš”ì•½ì •ê·œí™”(Normalization)ëŠ” ë°ì´í„°ì˜ ë²”ìœ„ë¥¼ 0ê³¼ 1 ì‚¬ì´ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ìœ¼ë¡œ, ì£¼ë¡œ Min-Max ìŠ¤ì¼€ì¼ë§ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.í‘œì¤€í™”(Standardization)ëŠ” ë°ì´í„°ë¥¼ í‰ê· ì´ 0, í‘œì¤€í¸ì°¨ê°€ 1ì´ ë˜ë„ë¡ ë³€í™˜í•˜ëŠ” ê³¼ì •ìœ¼ë¡œ, Z-ì ìˆ˜ í‘œì¤€í™”ê°€ ëŒ€í‘œì ì…ë‹ˆë‹¤.ë¹„ì„ í˜• ë³€í™˜ì€ ë¡œê·¸, ì œê³±ê·¼, ë°•ìŠ¤-ì½•ìŠ¤ ë³€í™˜ ë“±ì„ í†µí•´ ë¹„ì •ìƒì ì¸ ë°ì´í„° ë¶„í¬ë¥¼ ì •ê·œ ë¶„í¬ì— ê°€ê¹ê²Œ ë³€í™˜í•©ë‹ˆë‹¤.ì´ì œ ë°ì´í„°ë¥¼ ì •ê·œí™”í•˜ê³  í‘œì¤€í™”í•˜ì—¬ ë¶„ì„ì— ì í•©í•œ ìƒíƒœë¡œ ë§Œë“¤ì–´ ë³´ì„¸ìš”! â€‹ìµëª…9ì›” 25ì¼pip install scikit-learn
[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 14ê°•. ë¹„ì§€ë„í•™ìŠµ : êµ°ì§‘í™”ëª¨ë¸ - k-means clustering[SCC] ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ - 4ì£¼ì°¨ /[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 14ê°•. ë¹„ì§€ë„í•™ìŠµ : êµ°ì§‘í™”ëª¨ë¸ - k-means clusteringì œì‘:[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 14ê°•. ë¹„ì§€ë„í•™ìŠµ : êµ°ì§‘í™”ëª¨ë¸ - k-means clustering[ìˆ˜ì—… ëª©í‘œ]ë¹„ì§€ë„í•™ìŠµ êµ°ì§‘í™”ëª¨ë¸ ì¤‘ k-means clustering ì— ëŒ€í•œ ê°œë…ì„ ë°°ìš°ê³ , ë°ì´í„°ë¥¼ ì´ìš©í•´ ì‹¤ìŠµí•´ ë´…ë‹ˆë‹¤[ëª©ì°¨]01. k-means clustering ê°œë…02. k-means clustering ì‹¤ìŠµğŸ’¡
 
 01. k-means clustering ê°œë…âœ”ï¸k-means clusteringì´ ë¬´ì—‡ì¸ì§€ ì•Œì•„ë´…ì‹œë‹¤1) k-means clustering k-means clustering ì´ë€?ALT ì•Œê³ ë¦¬ì¦˜ì˜ ë‹¨ê³„ì´ˆê¸°í™”: kê°œì˜ êµ°ì§‘ ì¤‘ì‹¬ì„ ëœë¤í•˜ê²Œ ì„¤ì •í•©ë‹ˆë‹¤.í• ë‹¹ ë‹¨ê³„: ê° ë°ì´í„° í¬ì¸íŠ¸ë¥¼ ê°€ì¥ ê°€ê¹Œìš´ êµ°ì§‘ ì¤‘ì‹¬ì— í• ë‹¹í•©ë‹ˆë‹¤.ì—…ë°ì´íŠ¸ ë‹¨ê³„: ê° êµ°ì§‘ì˜ ì¤‘ì‹¬ì„ í•´ë‹¹ êµ°ì§‘ì— ì†í•œ ë°ì´í„° í¬ì¸íŠ¸ë“¤ì˜ í‰ê· ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.ë°˜ë³µ: í• ë‹¹ ë‹¨ê³„ì™€ ì—…ë°ì´íŠ¸ ë‹¨ê³„ë¥¼ êµ°ì§‘ ì¤‘ì‹¬ì´ ë” ì´ìƒ ë³€í™”í•˜ì§€ ì•Šì„ ë•Œê¹Œì§€ ë°˜ë³µí•©ë‹ˆë‹¤ ê±°ë¦¬ ì¸¡ì • ë°©ë²•k-means ì•Œê³ ë¦¬ì¦˜ì€ ì£¼ë¡œ ìœ í´ë¦¬ë“œ ê±°ë¦¬(Euclidean Distance)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° í¬ì¸íŠ¸ì™€ êµ°ì§‘ ì¤‘ì‹¬ ê°„ì˜ ê±°ë¦¬ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.d(p,q)=âˆ‘i=1n(piâˆ’qi)2d(p, q) = ^{n} (p_i - q_i)^2}d(p,q)=i=1âˆ‘nâ€‹(piâ€‹âˆ’qiâ€‹)2â€‹ì—˜ë³´ìš° ë°©ë²•ìµœì ì˜ kë¥¼ ì„ íƒí•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.kë¥¼ ì¦ê°€ì‹œí‚¤ë©´ì„œ ê° kì— ëŒ€í•œ êµ°ì§‘ì˜ ì‘ì§‘ë„(ë˜ëŠ” ê´€ì„±, Inertia)ë¥¼ ê³„ì‚°í•˜ê³ , ì´ë¥¼ ê·¸ë˜í”„ë¡œ ë‚˜íƒ€ë‚´ì–´ ê·¸ë˜í”„ì—ì„œ ì‘ì§‘ë„ê°€ ê¸‰ê²©íˆ ê°ì†Œí•˜ëŠ” ì§€ì ì„ ì°¾ìŠµë‹ˆë‹¤ALT02. k-means clustering ì‹¤ìŠµâœ”ï¸Kaggle ì‡¼í•‘ëª° ë°ì´í„°ë¥¼ ì´ìš©í•˜ì—¬ K-means clustering ì‹¤ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤1) ì‡¼í•‘ëª° ë°ì´í„° ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œKaggleì—ì„œ "Mall_Customers.csv" íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.ë‹¤ìš´ë¡œë“œí•œ íŒŒì¼ì„ ì‘ì—… ë””ë ‰í† ë¦¬ì— ì €ì¥í•©ë‹ˆë‹¤. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ìœ ë°©ì•” ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ {5px}ìœ ë°©ì•” ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ï»¿â€‹Pythonë³µì‚¬import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import seaborn as sns

# ë°ì´í„° ë¡œë“œ
data = pd.read_csv('Mall_Customers.csv')
# í•„ìš”í•œ ì—´ ì„ íƒ ë° ê²°ì¸¡ê°’ ì²˜ë¦¬
data = data[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']]
# ë°ì´í„° ìŠ¤ì¼€ì¼ë§
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data)
â€‹pandas.read_csv: CSV íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ ë°ì´í„°í”„ë ˆì„ ìƒì„±â€™Mall_Customers.csvâ€™: ë¡œë“œí•  íŒŒì¼ì˜ ê²½ë¡œì…ë‹ˆë‹¤.â€™Mall_Customers.csvâ€™: ë¡œë“œí•  íŒŒì¼ì˜ ê²½ë¡œì…ë‹ˆë‹¤.ï»¿â€‹pandas.DataFrame.dropna: ê²°ì¸¡ê°’ì´ ìˆëŠ” í–‰ì„ ì œê±°í•©ë‹ˆë‹¤.pandas.DataFrame.map: ë°ì´í„° ê°’ì„ ë‹¤ë¥¸ ê°’ìœ¼ë¡œ ë§¤í•‘í•©ë‹ˆë‹¤.sklearn.preprocessing.StandardScaler: ë°ì´í„°ì˜ í‰ê· ì„ 0, ë¶„ì‚°ì„ 1ë¡œ ìŠ¤ì¼€ì¼ë§fit_transform(X_train): í›ˆë ¨ ì„¸íŠ¸ë¥¼ ìŠ¤ì¼€ì¼ë§í•˜ê³  ë³€í™˜í•©ë‹ˆë‹¤._transform(X_train): í›ˆë ¨ ì„¸íŠ¸ë¥¼ ìŠ¤ì¼€ì¼ë§í•˜ê³  ë³€í™˜í•©ë‹ˆë‹¤.ï»¿â€‹ ëª¨ë¸ í•™ìŠµ ë° êµ°ì§‘í™”ëª¨ë¸ í•™ìŠµ ë° êµ°ì§‘í™” {5px}ëª¨ë¸ í•™ìŠµ ë° êµ°ì§‘í™” ï»¿â€‹Pythonë³µì‚¬# ìµœì ì˜ k ì°¾ê¸° (ì—˜ë³´ìš° ë°©ë²•)
inertia = []
K = range(1, 11)
for k in K:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(data_scaled)
    inertia.append(kmeans.inertia_)
# ì—˜ë³´ìš° ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
plt.figure(figsize=(10, 8))
plt.plot(K, inertia, 'bx-')
plt.xlabel('k')
plt.ylabel('Inertia')
plt.title('Elbow Method For Optimal k')
plt.show()
# k=5ë¡œ ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
kmeans = KMeans(n_clusters=5, random_state=42)
kmeans.fit(data_scaled)
# êµ°ì§‘ ê²°ê³¼ í• ë‹¹
data['Cluster'] = kmeans.labels_
â€‹sklearn.cluster.KMeans: k-means êµ°ì§‘í™” ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤n_clusters=k: êµ°ì§‘ì˜ ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤._clusters=k: êµ°ì§‘ì˜ ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.ï»¿â€‹random_state=42: ëœë¤ ì‹œë“œ ê°’ìœ¼ë¡œ, ê²°ê³¼ì˜ ì¬í˜„ì„±ì„ ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤._state=42: ëœë¤ ì‹œë“œ ê°’ìœ¼ë¡œ, ê²°ê³¼ì˜ ì¬í˜„ì„±ì„ ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.ï»¿â€‹fit(data_scaled): ë°ì´í„°ë¥¼ í•™ìŠµí•˜ì—¬ êµ°ì§‘ì„ í˜•ì„±í•©ë‹ˆë‹¤.(data_scaled): ë°ì´í„°ë¥¼ í•™ìŠµí•˜ì—¬ êµ°ì§‘ì„ í˜•ì„±í•©ë‹ˆë‹¤.ï»¿â€‹labels_: ê° ë°ì´í„° í¬ì¸íŠ¸ê°€ ì†í•œ êµ°ì§‘ ë ˆì´ë¸”ì„ ë°˜í™˜í•©ë‹ˆë‹¤._: ê° ë°ì´í„° í¬ì¸íŠ¸ê°€ ì†í•œ êµ°ì§‘ ë ˆì´ë¸”ì„ ë°˜í™˜í•©ë‹ˆë‹¤.ï»¿â€‹ êµ°ì§‘ ì‹œê°í™”êµ°ì§‘ ì‹œê°í™” {5px}êµ°ì§‘ ì‹œê°í™” ï»¿â€‹Pythonë³µì‚¬# 2ì°¨ì›ìœ¼ë¡œ êµ°ì§‘ ì‹œê°í™” (ì—°ë ¹ vs ì†Œë“)
plt.figure(figsize=(10, 8))
sns.scatterplot(x=data['Age'], y=data['Annual Income (k$)'], hue=data['Cluster'], palette='viridis')
plt.title('Clusters of customers (Age vs Annual Income)')
plt.show()
# 2ì°¨ì›ìœ¼ë¡œ êµ°ì§‘ ì‹œê°í™” (ì†Œë“ vs ì§€ì¶œ ì ìˆ˜)
plt.figure(figsize=(10, 8))
sns.scatterplot(x=data['Annual Income (k$)'], y=data['Spending Score (1-100)'], hue=data['Cluster'], palette='viridis')
plt.title('Clusters of customers (Annual Income vs Spending Score)')
plt.show()
â€‹matplotlib.pyplot.plot: ê·¸ë˜í”„ë¥¼ ê·¸ë¦½ë‹ˆë‹¤.K, inertia, â€™bx-â€™: xì¶•, yì¶• ë°ì´í„°ì™€ ê·¸ë˜í”„ ìŠ¤íƒ€ì¼ì„ ì„¤ì •í•©ë‹ˆë‹¤., inertia, â€™bx-â€™: xì¶•, yì¶• ë°ì´í„°ì™€ ê·¸ë˜í”„ ìŠ¤íƒ€ì¼ì„ ì„¤ì •í•©ë‹ˆë‹¤.ï»¿â€‹seaborn.scatterplot: ì‚°ì ë„ë¥¼ ê·¸ë¦½ë‹ˆë‹¤.x=data[â€™Ageâ€™]: xì¶• ë°ì´í„°=data[â€™Ageâ€™]: xì¶• ë°ì´í„°ï»¿â€‹y=data[â€™Annual Income (k$)â€™]: yì¶• ë°ì´í„°=data[â€™Annual Income (k$)â€™]: yì¶• ë°ì´í„°ï»¿â€‹hue=data[â€™Clusterâ€™]: ìƒ‰ìƒì— ë”°ë¼ êµ°ì§‘ì„ êµ¬ë¶„í•©ë‹ˆë‹¤.=data[â€™Clusterâ€™]: ìƒ‰ìƒì— ë”°ë¼ êµ°ì§‘ì„ êµ¬ë¶„í•©ë‹ˆë‹¤.ï»¿â€‹palette=â€™viridisâ€™: ìƒ‰ìƒ íŒ”ë ˆíŠ¸ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.=â€™viridisâ€™: ìƒ‰ìƒ íŒ”ë ˆíŠ¸ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.ï»¿â€‹.
[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 14. ê³¼ì í•© ë°©ì§€ ê¸°ë²•[SCC] ê¸°ì´ˆê°€ íƒ„íƒ„í•œ ë”¥ëŸ¬ë‹/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] ê¸°ì´ˆê°€ íƒ„íƒ„í•œ ë”¥ëŸ¬ë‹ - 6ì£¼ì°¨/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 14. ê³¼ì í•© ë°©ì§€ ê¸°ë²•ì œì‘:[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 14. ê³¼ì í•© ë°©ì§€ ê¸°ë²•[ìˆ˜ì—… ëª©í‘œ]ì—¬ëŸ¬ ê³¼ì í•© ë°©ì§€ ê¸°ë²•ì— ëŒ€í•´ì„œ ì•Œì•„ë´…ì‹œë‹¤.Pytorchë¡œ  ê³¼ì í•© ë°©ì§€ ê¸°ë²•ì— ëŒ€í•œ ì‹¤ìŠµ ì˜ˆì‹œ![ëª©ì°¨]01. ê³¼ì í™” ë°©ì§€ ê¸°ë²•02. ê³¼ì í•© ë°©ì§€ê¸°ë²• ì‹¤ìŠµ(Pytorch)ğŸ’¡
 
 01. ê³¼ì í™” ë°©ì§€ ê¸°ë²•âœ”ï¸ê³¼ì í•©ì„ ë°©ì§€í•˜ê¸° ìœ„í•œ ë°©ë²•ë“¤ì—ëŠ” ì–´ë– í•œ ê²ƒë“¤ì´ ìˆëŠ”ì§€ ë°°ì›Œë´…ì‹œë‹¤.1) ì •ê·œí™”ì™€ ë“œë¡­ì•„ì›ƒ ì •ê·œí™” (Normalization)ì •ê·œí™”ëŠ” ì…ë ¥ ë°ì´í„°ì˜ ë¶„í¬ë¥¼ ì¼ì •í•œ ë²”ìœ„ë¡œ ì¡°ì •í•˜ì—¬, ëª¨ë¸ì˜ í•™ìŠµì„ ì•ˆì •í™”í•˜ê³  ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤.ALTë°°ì¹˜ ì •ê·œí™” (Batch Normalization): ê° ë¯¸ë‹ˆë°°ì¹˜ì˜ í‰ê· ê³¼ ë¶„ì‚°ì„ ì‚¬ìš©í•˜ì—¬ ì •ê·œí™”í•©ë‹ˆë‹¤. ì´ëŠ” í•™ìŠµ ì†ë„ë¥¼ ë†’ì´ê³ , ê³¼ì í•©ì„ ë°©ì§€í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤.ë ˆì´ì–´ ì •ê·œí™” (Layer Normalization): ê° ë ˆì´ì–´ì˜ ë‰´ëŸ° ì¶œë ¥ì„ ì •ê·œí™”í•©ë‹ˆë‹¤. ë“œë¡­ì•„ì›ƒ (Dropout)ë“œë¡­ì•„ì›ƒì€ í•™ìŠµ ê³¼ì •ì—ì„œ ë¬´ì‘ìœ„ë¡œ ë‰´ëŸ°ì„ ë¹„í™œì„±í™”í•˜ì—¬, ëª¨ë¸ì˜ ê³¼ì í•©ì„ ë°©ì§€í•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤.ë“œë¡­ì•„ì›ƒì€ í•™ìŠµ ì‹œì—ë§Œ ì ìš©ë˜ë©°, í‰ê°€ ì‹œì—ëŠ” ëª¨ë“  ë‰´ëŸ°ì„ í™œì„±í™”í•©ë‹ˆë‹¤.2) ì¡°ê¸° ì¢…ë£Œì™€ ë°ì´í„° ì¦ê°• ì¡°ê¸° ì¢…ë£Œ(Early Stopping) ê¸°ë²•ì¡°ê¸° ì¢…ë£ŒëŠ” ê²€ì¦ ë°ì´í„°ì˜ ì„±ëŠ¥ì´ ë” ì´ìƒ í–¥ìƒë˜ì§€ ì•Šì„ ë•Œ í•™ìŠµì„ ì¤‘ë‹¨í•˜ì—¬, ê³¼ì í•©ì„ ë°©ì§€í•˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤.ì¡°ê¸° ì¢…ë£ŒëŠ” í•™ìŠµ ê³¼ì •ì—ì„œ ê²€ì¦ ì†ì‹¤ì´ ì¼ì • ì—í¬í¬ ë™ì•ˆ ê°ì†Œí•˜ì§€ ì•Šìœ¼ë©´ í•™ìŠµì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤. ë°ì´í„° ì¦ê°•(Data Augmentation) ê¸°ë²•ë°ì´í„° ì¦ê°•ì€ ì›ë³¸ ë°ì´í„°ë¥¼ ë³€í˜•í•˜ì—¬ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ìƒì„±í•¨ìœ¼ë¡œì¨, ë°ì´í„°ì…‹ì„ í™•ì¥í•˜ê³  ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤.ë°ì´í„° ì¦ê°• ê¸°ë²•ì—ëŠ” íšŒì „, ì´ë™, í¬ê¸° ì¡°ì ˆ, ìƒ‰ìƒ ë³€í™˜ ë“±ì´ ìˆìŠµë‹ˆë‹¤.02. ê³¼ì í•© ë°©ì§€ê¸°ë²• ì‹¤ìŠµ(Pytorch)âœ”ï¸ë“œë¡­ì•„ì›ƒê³¼ ë°°ì¹˜ ì •ê·œí™”ë¥¼ ì ìš©í•œ ëª¨ë¸ ì‹¤ìŠµê³¼ ë°ì´í„° ì¦ê°•ì„ ì ìš©í•œ ëª¨ë¸ì„ êµ¬í˜„í•˜ëŠ” ì‹¤ìŠµì„ ì§„í–‰í•´ ë´…ì‹œë‹¤.1)  ë“œë¡­ì•„ì›ƒê³¼ ì •ê·œí™”ë¥¼ ì ìš©í•œ ëª¨ë¸ ì‹¤ìŠµ PyTorch ë° í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸PyTorch ë° í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ {5px}PyTorch ë° í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ï»¿â€‹Pythonë³µì‚¬import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
â€‹ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬ {5px}ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬ ï»¿â€‹Pythonë³µì‚¬# ë°ì´í„°ì…‹ ì „ì²˜ë¦¬
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])
# CIFAR-10 ë°ì´í„°ì…‹ ë¡œë“œ
trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)

testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)
â€‹  ë“œë¡­ì•„ì›ƒê³¼ ë°°ì¹˜ ì •ê·œí™”ë¥¼ ì ìš©í•œ ëª¨ë¸ ì •ì˜ ë“œë¡­ì•„ì›ƒê³¼ ë°°ì¹˜ ì •ê·œí™”ë¥¼ ì ìš©í•œ ëª¨ë¸ ì •ì˜ {5px} ë“œë¡­ì•„ì›ƒê³¼ ë°°ì¹˜ ì •ê·œí™”ë¥¼ ì ìš©í•œ ëª¨ë¸ ì •ì˜ ï»¿â€‹Pythonë³µì‚¬class CNNWithDropoutAndBatchNorm(nn.Module):
def __init__(self):
super(CNNWithDropoutAndBatchNorm, self).__init__()
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(64)
        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(128)
        self.fc1 = nn.Linear(128 * 56 * 56, 256)
        self.dropout = nn.Dropout(0.5)
        self.fc2 = nn.Linear(256, 10)
def forward(self, x):
        x = torch.relu(self.bn1(self.conv1(x)))
        x = torch.max_pool2d(x, 2)
        x = torch.relu(self.bn2(self.conv2(x)))
        x = torch.max_pool2d(x, 2)
        x = x.view(x.size(0), -1)
        x = torch.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
return x

model = CNNWithDropoutAndBatchNorm()
â€‹nn.Conv2d: 2ì°¨ì› í•©ì„±ê³± ì¸µì„ ì •ì˜í•©ë‹ˆë‹¤. nn.Conv2d(in_channels, out_channels, kernel_size, padding)ì€ ì…ë ¥ ì±„ë„ ìˆ˜, ì¶œë ¥ ì±„ë„ ìˆ˜, ì»¤ë„ í¬ê¸°, íŒ¨ë”©ì„ ì§€ì •.Conv2d(in_channels, out_channels, kernel_size, padding)ì€ ì…ë ¥ ì±„ë„ ìˆ˜, ì¶œë ¥ ì±„ë„ ìˆ˜, ì»¤ë„ í¬ê¸°, íŒ¨ë”©ì„ ì§€ì •ï»¿â€‹nn.BatchNorm2d: 2ì°¨ì› ë°°ì¹˜ ì •ê·œí™” ì¸µì„ ì •ì˜í•©ë‹ˆë‹¤.nn.Dropout: ë“œë¡­ì•„ì›ƒ ì¸µì„ ì •ì˜í•©ë‹ˆë‹¤. nn.Dropout(p)ì€ ë“œë¡­ì•„ì›ƒ í™•ë¥ ì„ ì§€ì •í•©ë‹ˆë‹¤..Dropout(p)ì€ ë“œë¡­ì•„ì›ƒ í™•ë¥ ì„ ì§€ì •í•©ë‹ˆë‹¤.ï»¿â€‹torch.max_pool2d: 2ì°¨ì› ìµœëŒ€ í’€ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì†ì‹¤ í•¨ìˆ˜ì™€ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ ì •ì˜ì†ì‹¤ í•¨ìˆ˜ì™€ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ ì •ì˜ {5px}ì†ì‹¤ í•¨ìˆ˜ì™€ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ ì •ì˜ ï»¿â€‹Pythonë³µì‚¬criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
â€‹nn.CrossEntropyLoss: êµì°¨ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.optim.Adam: Adam ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì„ ì •ì˜í•©ë‹ˆë‹¤. lrì€ í•™ìŠµë¥ ì„ ì§€ì •í•©ë‹ˆë‹¤.ì€ í•™ìŠµë¥ ì„ ì§€ì •í•©ë‹ˆë‹¤.ï»¿â€‹ ëª¨ë¸ í•™ìŠµëª¨ë¸ í•™ìŠµ {5px}ëª¨ë¸ í•™ìŠµ ï»¿â€‹Pythonë³µì‚¬num_epochs = 10
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)
for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
for i, (inputs, labels) in enumerate(trainloader):
        inputs, labels = inputs.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
if i % 100 == 99: # ë§¤ 100 ë¯¸ë‹ˆë°°ì¹˜ë§ˆë‹¤ ì¶œë ¥
print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 100:.3f}')
            running_loss = 0.0
print('Finished Training')
â€‹model.train(): ëª¨ë¸ì„ í•™ìŠµ ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.optimizer.zero_grad(): ì´ì „ ë‹¨ê³„ì—ì„œ ê³„ì‚°ëœ ê¸°ìš¸ê¸°ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.loss.backward(): ì—­ì „íŒŒë¥¼ í†µí•´ ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.optimizer.step(): ê³„ì‚°ëœ ê¸°ìš¸ê¸°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. ëª¨ë¸ í‰ê°€ëª¨ë¸ í‰ê°€ {5px}ëª¨ë¸ í‰ê°€ ï»¿â€‹Pythonë³µì‚¬model.eval()
correct = 0
total = 0
with torch.no_grad():
for inputs, labels in testloader:
        inputs, labels = inputs.to(device), labels.to(device)
        outputs = model(inputs)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')
â€‹model.eval(): ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì „í™˜í•©ë‹ˆë‹¤.torch.no_grad(): í‰ê°€ ë‹¨ê³„ì—ì„œëŠ” ê¸°ìš¸ê¸°ë¥¼ ê³„ì‚°í•  í•„ìš”ê°€ ì—†ìœ¼ë¯€ë¡œ, ì´ë¥¼ ë¹„í™œì„±í™”í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ì„ ì¤„ì…ë‹ˆë‹¤.torch.max: í…ì„œì˜ ìµœëŒ€ ê°’ì„ ì°¾ìŠµë‹ˆë‹¤. torch.max(outputs.data, 1)ì€ ê° ìƒ˜í”Œì— ëŒ€í•´ ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ í´ë˜ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤..max(outputs.data, 1)ì€ ê° ìƒ˜í”Œì— ëŒ€í•´ ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ í´ë˜ìŠ¤ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.ï»¿â€‹labels.size(0): ë°°ì¹˜ í¬ê¸°ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.(predicted == labels).sum().item(): ì˜ˆì¸¡ ê°’ê³¼ ì‹¤ì œ ê°’ì´ ì¼ì¹˜í•˜ëŠ” ìƒ˜í”Œì˜ ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.2)  ë°ì´í„° ì¦ê°•ì„ í†µí•œ ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒ ì‹¤ìŠµë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬ {5px}ë°ì´í„°ì…‹ ë¡œë“œ ë° ì „ì²˜ë¦¬ ï»¿â€‹Pythonë³µì‚¬# ë°ì´í„° ì¦ê°• ì ìš©
transform_train = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomCrop(32, padding=4),
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

transform_test = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])
# CIFAR-10 ë°ì´í„°ì…‹ ë¡œë“œ
trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)

testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)
testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)
â€‹transforms.RandomHorizontalFlip(): ì´ë¯¸ì§€ë¥¼ ë¬´ì‘ìœ„ë¡œ ìˆ˜í‰ ë°˜ì „í•©ë‹ˆë‹¤.transforms.RandomCrop(size, padding): ì´ë¯¸ì§€ë¥¼ ë¬´ì‘ìœ„ë¡œ ìë¥´ê³ , íŒ¨ë”©ì„ ì¶”ê°€í•©ë‹ˆë‹¤.  ë“œë¡­ì•„ì›ƒê³¼ ë°°ì¹˜ ì •ê·œí™”ë¥¼ ì ìš©í•œ ëª¨ë¸ ì •ì˜ì²«ë²ˆì§¸ ì‹¤ìŠµì—ì„œ ì •ì˜í•œ ëª¨ë¸ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. ë“œë¡­ì•„ì›ƒê³¼ ë°°ì¹˜ ì •ê·œí™”ë¥¼ ì ìš©í•œ ëª¨ë¸ ì •ì˜ {5px} ë“œë¡­ì•„ì›ƒê³¼ ë°°ì¹˜ ì •ê·œí™”ë¥¼ ì ìš©í•œ ëª¨ë¸ ì •ì˜ ï»¿â€‹Pythonë³µì‚¬class CNNWithDropoutAndBatchNorm(nn.Module):
def __init__(self):
super(CNNWithDropoutAndBatchNorm, self).__init__()
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(64)
        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(128)
        self.fc1 = nn.Linear(128 * 56 * 56, 256)
        self.dropout = nn.Dropout(0.5)
        self.fc2 = nn.Linear(256, 10)
def forward(self, x):
        x = torch.relu(self.bn1(self.conv1(x)))
        x = torch.max_pool2d(x, 2)
        x = torch.relu(self.bn2(self.conv2(x)))
        x = torch.max_pool2d(x, 2)
        x = x.view(x.size(0), -1)
        x = torch.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
return x

model = CNNWithDropoutAndBatchNorm()
â€‹ ì†ì‹¤ í•¨ìˆ˜ì™€ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ ì •ì˜ì²«ë²ˆì§¸ ì‹¤ìŠµì—ì„œ ì •ì˜í•œ ì†ì‹¤í•¨ìˆ˜ì™€ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.ì†ì‹¤ í•¨ìˆ˜ì™€ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ ì •ì˜ {5px}ì†ì‹¤ í•¨ìˆ˜ì™€ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ ì •ì˜ ï»¿â€‹Pythonë³µì‚¬criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
â€‹ ëª¨ë¸ í•™ìŠµì²«ë²ˆì§¸ ì‹¤ìŠµì—ì„œ ì •ì˜í•œ ëª¨ë¸ í•™ìŠµ ì½”ë“œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.ëª¨ë¸ í•™ìŠµ {5px}ëª¨ë¸ í•™ìŠµ ï»¿â€‹Pythonë³µì‚¬num_epochs = 10
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)
for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
for i, (inputs, labels) in enumerate(trainloader):
        inputs, labels = inputs.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
if i % 100 == 99: # ë§¤ 100 ë¯¸ë‹ˆë°°ì¹˜ë§ˆë‹¤ ì¶œë ¥
print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 100:.3f}')
            running_loss = 0.0
print('Finished Training')
â€‹ ëª¨ë¸ í‰ê°€ì²«ë²ˆì§¸ ì‹¤ìŠµì—ì„œ ì •ì˜í•œ ëª¨ë¸ í‰ê°€ ì½”ë“œë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.ëª¨ë¸ í‰ê°€ {5px}ëª¨ë¸ í‰ê°€ ï»¿â€‹Pythonë³µì‚¬model.eval()
correct = 0
total = 0
with torch.no_grad():
for inputs, labels in testloader:
        inputs, labels = inputs.to(device), labels.to(device)
        outputs = model(inputs)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')
â€‹.
ë°ì´í„° ì „ì²˜ë¦¬: ì¸ì½”ë”© (Encoding)ì¸ê³µì§€ëŠ¥ì„ ìœ„í•œ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬/ì¸ê³µì§€ëŠ¥ì„ ìœ„í•œ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ - 5ì£¼ì°¨/ë°ì´í„° ì „ì²˜ë¦¬: ì¸ì½”ë”© (Encoding)ì œì‘:ë°ì´í„° ì „ì²˜ë¦¬: ì¸ì½”ë”© (Encoding)ìˆ˜ì—… ëª©í‘œë²”ì£¼í˜• ë°ì´í„°ë¥¼ ìˆ˜ì¹˜í˜• ë°ì´í„°ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì— ëŒ€í•´ì„œ í•™ìŠµí•©ë‹ˆë‹¤.ëª©ì°¨ì¸ì½”ë”©(Encoding)ì´ë€?ë ˆì´ë¸” ì¸ì½”ë”©(Label Encoding) ì›-í•« ì¸ì½”ë”©(One-Hot Encoding)ì°¨ì› ì¶•ì†Œ ì¸ì½”ë”© (Count or Frequency Encoding)ìˆœì„œí˜• ì¸ì½”ë”© (Ordinal Encoding)ì„ë² ë”©(Embedding)ìš”ì•½ì¸ì½”ë”©(Encoding)ì´ë€?ì¸ì½”ë”©(Encoding)ì€ ë²”ì£¼í˜• ë°ì´í„°(Categorical Data)ë¥¼ ìˆ˜ì¹˜í˜• ë°ì´í„°ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ë§ì€ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì€ ìˆ˜ì¹˜í˜• ë°ì´í„°ë§Œ ì²˜ë¦¬í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì—, ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ì¸ì½”ë”©í•˜ëŠ” ê²ƒì´ í•„ìˆ˜ì ì…ë‹ˆë‹¤.ë ˆì´ë¸” ì¸ì½”ë”©(Label Encoding) ë ˆì´ë¸” ì¸ì½”ë”©ì€ ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ìˆœì„œê°€ ìˆëŠ” ìˆ«ìë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ê° ë²”ì£¼ì— ê³ ìœ í•œ ìˆ«ìê°€ í• ë‹¹ë©ë‹ˆë‹¤.Pythonë³µì‚¬import pandas as pd
from sklearn.preprocessing import LabelEncoder

# ì˜ˆì‹œ ë°ì´í„°í”„ë ˆì„ ìƒì„±
data = {'ê³¼ì¼': ['ì‚¬ê³¼', 'ë°”ë‚˜ë‚˜', 'ì‚¬ê³¼', 'ì˜¤ë Œì§€', 'ë°”ë‚˜ë‚˜']}
df = pd.DataFrame(data)
# ë ˆì´ë¸” ì¸ì½”ë”©
label_encoder = LabelEncoder()
df['ê³¼ì¼_ì¸ì½”ë”©'] = label_encoder.fit_transform(df['ê³¼ì¼'])
print(df)

â€‹ê²°ê³¼:Plain Textë³µì‚¬      ê³¼ì¼  ê³¼ì¼_ì¸ì½”ë”©
0     ì‚¬ê³¼       0
1    ë°”ë‚˜ë‚˜       1
2     ì‚¬ê³¼       0
3    ì˜¤ë Œì§€       2
4    ë°”ë‚˜ë‚˜       1

â€‹ì‚¬ê³¼ëŠ” 0, ë°”ë‚˜ë‚˜ëŠ” 1, ì˜¤ë Œì§€ëŠ” 2ë¡œ ì¸ì½”ë”©ë˜ì—ˆìŠµë‹ˆë‹¤.ë ˆì´ë¸” ì¸ì½”ë”©ì˜ ì£¼ì˜ì ë ˆì´ë¸” ì¸ì½”ë”©ì€ ë²”ì£¼í˜• ë°ì´í„°ì— ìˆœì„œê°€ ìˆì„ ë•Œ ì í•©í•©ë‹ˆë‹¤. ìˆœì„œê°€ ì—†ëŠ” ë°ì´í„°ì— ì‚¬ìš©í•˜ë©´, ëª¨ë¸ì´ ì´ ê°’ì„ í¬ê¸°ë¡œ ì¸ì‹í•´ ì˜ëª»ëœ ê²°ê³¼ë¥¼ ì´ˆë˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ì›-í•« ì¸ì½”ë”©(One-Hot Encoding)ì›-í•« ì¸ì½”ë”©ì€ ê° ë²”ì£¼ë¥¼ ì´ì§„ ë²¡í„°ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ê° ë²”ì£¼ëŠ” ê³ ìœ í•œ ì—´ì„ ê°€ì§€ë©°, í•´ë‹¹í•˜ëŠ” ì—´ì—ëŠ” 1, ë‚˜ë¨¸ì§€ ì—´ì—ëŠ” 0ì´ í• ë‹¹ë©ë‹ˆë‹¤.Pythonë³µì‚¬# ì›-í•« ì¸ì½”ë”©
df_one_hot = pd.get_dummies(df['ê³¼ì¼'], prefix='ê³¼ì¼')
print(df_one_hot)

â€‹ê²°ê³¼:Plain Textë³µì‚¬   ê³¼ì¼_ë°”ë‚˜ë‚˜  ê³¼ì¼_ì‚¬ê³¼  ê³¼ì¼_ì˜¤ë Œì§€
0        0        1        0
1        1        0        0
2        0        1        0
3        0        0        1
4        1        0        0

â€‹ë°”ë‚˜ë‚˜, ì‚¬ê³¼, ì˜¤ë Œì§€ ê°ê°ì´ ë…ë¦½ëœ ì—´ë¡œ ë³€í™˜ë˜ì—ˆê³ , í•´ë‹¹í•˜ëŠ” ìœ„ì¹˜ì— 1ì´ í‘œì‹œë©ë‹ˆë‹¤.ì›-í•« ì¸ì½”ë”©ì˜ ì¥ì ë²”ì£¼í˜• ë°ì´í„°ì— ìˆœì„œê°€ ì—†ì„ ë•Œ ì‚¬ìš©í•˜ê¸° ì¢‹ìŠµë‹ˆë‹¤. ëª¨ë¸ì´ ë²”ì£¼ ê°„ì˜ ìˆœì„œë‚˜ ê´€ê³„ë¥¼ ì¸ì‹í•˜ì§€ ì•Šê³  ê° ë²”ì£¼ë¥¼ ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ì°¨ì› ì¶•ì†Œ ì¸ì½”ë”© (Count or Frequency Encoding)ì°¨ì› ì¶•ì†Œ ì¸ì½”ë”©ì€ ë²”ì£¼í˜• ë°ì´í„°ê°€ ë§ì„ ë•Œ ìœ ìš©í•©ë‹ˆë‹¤. ê° ë²”ì£¼ë¥¼ ë°ì´í„°ì…‹ ë‚´ì—ì„œì˜ ì¶œí˜„ ë¹ˆë„ë¡œ ì¸ì½”ë”©í•©ë‹ˆë‹¤.Pythonë³µì‚¬# ë¹ˆë„ ê¸°ë°˜ ì¸ì½”ë”©
df['ê³¼ì¼_ë¹ˆë„'] = df['ê³¼ì¼'].map(df['ê³¼ì¼'].value_counts())
print(df)

â€‹ê²°ê³¼:Plain Textë³µì‚¬      ê³¼ì¼  ê³¼ì¼_ì¸ì½”ë”©  ê³¼ì¼_ë¹ˆë„
0     ì‚¬ê³¼       0       2
1    ë°”ë‚˜ë‚˜       1       2
2     ì‚¬ê³¼       0       2
3    ì˜¤ë Œì§€       2       1
4    ë°”ë‚˜ë‚˜       1       2

â€‹ì‚¬ê³¼ì™€ ë°”ë‚˜ë‚˜ëŠ” ê°ê° 2íšŒ, ì˜¤ë Œì§€ëŠ” 1íšŒ ì¶œí˜„í•˜ì—¬ í•´ë‹¹ ë¹ˆë„ë¡œ ì¸ì½”ë”©ë˜ì—ˆìŠµë‹ˆë‹¤.ì£¼ì˜ì ì´ ë°©ë²•ì€ ë²”ì£¼í˜• ë°ì´í„°ì˜ ë¹ˆë„ê°€ ë§¤ìš° ì¤‘ìš”í•œ ê²½ìš°ì— ì í•©í•˜ë©°, ë²”ì£¼ì˜ ê³ ìœ ì„±ì„ ìƒì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì£¼ì˜í•´ì„œ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.ìˆœì„œí˜• ì¸ì½”ë”© (Ordinal Encoding)ìˆœì„œí˜• ì¸ì½”ë”©ì€ ìˆœì„œê°€ ìˆëŠ” ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ê·¸ ìˆœì„œì— ë”°ë¼ ìˆ«ìë¡œ ë³€í™˜í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.Pythonë³µì‚¬# ì˜ˆì‹œ ë°ì´í„°
data = {'ë“±ê¸‰': ['ë‚®ìŒ', 'ì¤‘ê°„', 'ë†’ìŒ', 'ì¤‘ê°„', 'ë†’ìŒ']}
df = pd.DataFrame(data)
# ìˆœì„œí˜• ì¸ì½”ë”©
order = {'ë‚®ìŒ': 1, 'ì¤‘ê°„': 2, 'ë†’ìŒ': 3}
df['ë“±ê¸‰_ì¸ì½”ë”©'] = df['ë“±ê¸‰'].map(order)
print(df)

â€‹ê²°ê³¼:Plain Textë³µì‚¬    ë“±ê¸‰  ë“±ê¸‰_ì¸ì½”ë”©
0  ë‚®ìŒ       1
1  ì¤‘ê°„       2
2  ë†’ìŒ       3
3  ì¤‘ê°„       2
4  ë†’ìŒ       3

â€‹ë‚®ìŒì€ 1, ì¤‘ê°„ì€ 2, ë†’ìŒì€ 3ìœ¼ë¡œ ì¸ì½”ë”©ë˜ì—ˆìŠµë‹ˆë‹¤.ì‚¬ìš© ì˜ˆì‹œìˆœì„œí˜• ì¸ì½”ë”©ì€ ë§Œì¡±ë„(ë‚®ìŒ, ì¤‘ê°„, ë†’ìŒ), ë“±ê¸‰(A, B, C) ë“± ìˆœì„œê°€ ì¤‘ìš”í•œ ê²½ìš°ì— ì í•©í•©ë‹ˆë‹¤.ì„ë² ë”©(Embedding)ì„ë² ë”©ì€ ë”¥ëŸ¬ë‹ì—ì„œ ì£¼ë¡œ ì‚¬ìš©ë˜ë©°, ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ë²¡í„° ê³µê°„ì— ë§¤í•‘í•˜ì—¬ ë³€í™˜í•©ë‹ˆë‹¤. ì´ëŠ” íŠ¹íˆ ê³ ì°¨ì› ë²”ì£¼í˜• ë°ì´í„°ì— ìœ ìš©í•©ë‹ˆë‹¤.ì„ë² ë”©ì˜ ì¥ì ì›-í•« ì¸ì½”ë”©ë³´ë‹¤ ì°¨ì› ì¶•ì†Œì™€ ë©”ëª¨ë¦¬ ì ˆì•½ íš¨ê³¼ê°€ ìˆìœ¼ë©°, ë²”ì£¼ ê°„ì˜ ë‚´ì¬ëœ ê´€ê³„ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ì„ë² ë”©ì€ ì£¼ë¡œ í…ìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ë‹¨ì–´ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•  ë•Œ ì‚¬ìš©ë˜ë©°, Keras ë“±ì˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì‰½ê²Œ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ìš”ì•½ìš”ì•½ì¸ì½”ë”©ì€ ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ìˆ˜ì¹˜í˜• ë°ì´í„°ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ìœ¼ë¡œ, ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì—ì„œ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ í•„ìˆ˜ì ì…ë‹ˆë‹¤.ë ˆì´ë¸” ì¸ì½”ë”©ì€ ìˆœì„œê°€ ìˆëŠ” ë²”ì£¼í˜• ë°ì´í„°ì— ì í•©í•˜ë©°, ì›-í•« ì¸ì½”ë”©ì€ ìˆœì„œê°€ ì—†ëŠ” ë²”ì£¼í˜• ë°ì´í„°ì— ì‚¬ìš©ë©ë‹ˆë‹¤.ë¹ˆë„ ì¸ì½”ë”©ì€ ë²”ì£¼ì˜ ì¶œí˜„ ë¹ˆë„ë¥¼ ì‚¬ìš©í•˜ê³ , ìˆœì„œí˜• ì¸ì½”ë”©ì€ ìˆœì„œê°€ ì¤‘ìš”í•œ ë²”ì£¼í˜• ë°ì´í„°ì— ìœ ìš©í•©ë‹ˆë‹¤.ì„ë² ë”©ì€ ë”¥ëŸ¬ë‹ì—ì„œ ê³ ì°¨ì› ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ì²˜ë¦¬í•  ë•Œ ì£¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.ì´ì œ ì ì ˆí•œ ì¸ì½”ë”© ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ì „ì²˜ë¦¬ë¥¼ ì™„ë£Œí•˜ê³ , ëª¨ë¸ í•™ìŠµì˜ ê¸°ì´ˆë¥¼ ë‹¤ì ¸ë³´ì„¸ìš”! â€‹
[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 15ê°•. ë¹„ì§€ë„í•™ìŠµ : êµ°ì§‘í™”ëª¨ë¸ - ê³„ì¸µì  êµ°ì§‘í™”[SCC] ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ - 4ì£¼ì°¨ /[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 15ê°•. ë¹„ì§€ë„í•™ìŠµ : êµ°ì§‘í™”ëª¨ë¸ - ê³„ì¸µì  êµ°ì§‘í™”ì œì‘:[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 15ê°•. ë¹„ì§€ë„í•™ìŠµ : êµ°ì§‘í™”ëª¨ë¸ - ê³„ì¸µì  êµ°ì§‘í™”[ìˆ˜ì—… ëª©í‘œ]ë¹„ì§€ë„í•™ìŠµ êµ°ì§‘í™”ëª¨ë¸ ì¤‘ ê³„ì¸µì  êµ°ì§‘í™” ì— ëŒ€í•œ ê°œë…ì„ ë°°ìš°ê³ , ë°ì´í„°ë¥¼ ì´ìš©í•´ ì‹¤ìŠµí•´ ë´…ë‹ˆë‹¤[ëª©ì°¨]01. ê³„ì¸µì  êµ°ì§‘í™” ê°œë…02. ê³„ì¸µì  êµ°ì§‘í™” ì‹¤ìŠµğŸ’¡
 
 01. ê³„ì¸µì  êµ°ì§‘í™” ê°œë…âœ”ï¸ê³„ì¸µì  êµ°ì§‘í™”ë€ ë¬´ì—‡ì¸ì§€ ì•Œì•„ë´…ì‹œë‹¤1) ê³„ì¸µì  êµ°ì§‘í™” ê³„ì¸µì  êµ°ì§‘í™”ë€?ê³„ì¸µì  êµ°ì§‘í™”(Hierarchical Clustering)ëŠ” ë°ì´í„°í¬ì¸íŠ¸ë¥¼ ê³„ì¸µ êµ¬ì¡°ë¡œ ê·¸ë£¹í™”í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ë°ì´í„° í¬ì¸íŠ¸ë¥¼ ì ì§„ì ìœ¼ë¡œ ë³‘í•©í•˜ê±°ë‚˜ ë¶„í• í•˜ì—¬ êµ°ì§‘ì„ í˜•ì„±í•©ë‹ˆë‹¤ê³„ì¸µì  êµ°ì§‘í™”ëŠ” ì£¼ë¡œ ë‘ê°€ì§€ ë°©ì‹ìœ¼ë¡œ ë‚˜ë‰©ë‹ˆë‹¤ê³„ì¸µì  êµ°ì§‘í™”ëŠ” ì£¼ë¡œ ë‘ê°€ì§€ ë°©ì‹ìœ¼ë¡œ ë‚˜ë‰©ë‹ˆë‹¤ï»¿
ë³‘í•© êµ°ì§‘í™”: ê° ë°ì´í„° í¬ì¸íŠ¸ë¥¼ ê°œë³„ êµ°ì§‘ìœ¼ë¡œ ì‹œì‘í•˜ì—¬, ê°€ì¥ ê°€ê¹Œìš´ êµ°ì§‘ì„ ë°˜ë³µì ìœ¼ë¡œ ë³‘í•©í•©ë‹ˆë‹¤.ë³‘í•© êµ°ì§‘í™”: ê° ë°ì´í„° í¬ì¸íŠ¸ë¥¼ ê°œë³„ êµ°ì§‘ìœ¼ë¡œ ì‹œì‘í•˜ì—¬, ê°€ì¥ ê°€ê¹Œìš´ êµ°ì§‘ì„ ë°˜ë³µì ìœ¼ë¡œ ë³‘í•©í•©ë‹ˆë‹¤.ï»¿â€‹ë¶„í•  êµ°ì§‘í™”: ëª¨ë“  ë°ì´í„° í¬ì¸íŠ¸ë¥¼ í•˜ë‚˜ì˜ êµ°ì§‘ìœ¼ë¡œ ì‹œì‘í•˜ì—¬, ë°˜ë³µì ìœ¼ë¡œ ê°€ì¥ ë©€ë¦¬ ë–¨ì–´ì§„ êµ°ì§‘ì„ ë¶„í• í•©ë‹ˆë‹¤.ë¶„í•  êµ°ì§‘í™”: ëª¨ë“  ë°ì´í„° í¬ì¸íŠ¸ë¥¼ í•˜ë‚˜ì˜ êµ°ì§‘ìœ¼ë¡œ ì‹œì‘í•˜ì—¬, ë°˜ë³µì ìœ¼ë¡œ ê°€ì¥ ë©€ë¦¬ ë–¨ì–´ì§„ êµ°ì§‘ì„ ë¶„í• í•©ë‹ˆë‹¤.ï»¿â€‹ ê³„ì¸µì  êµ°ì§‘í™”ì˜ ì‘ë™ ì›ë¦¬ê±°ë¦¬ í–‰ë ¬ ê³„ì‚°: ë°ì´í„° í¬ì¸íŠ¸ ê°„ì˜ ê±°ë¦¬ë¥¼ ê³„ì‚°í•˜ì—¬ ê±°ë¦¬ í–‰ë ¬ì„ ë§Œë“­ë‹ˆë‹¤.êµ°ì§‘ ë³‘í•©/ë¶„í• : ê±°ë¦¬ í–‰ë ¬ì„ ê¸°ë°˜ìœ¼ë¡œ ê°€ì¥ ê°€ê¹Œìš´ êµ°ì§‘ì„ ë³‘í•©í•˜ê±°ë‚˜, ê°€ì¥ ë©€ë¦¬ ë–¨ì–´ì§„ êµ°ì§‘ì„ ë¶„í• í•©ë‹ˆë‹¤.ë´ë“œë¡œê·¸ë¨ ìƒì„±: êµ°ì§‘í™” ê³¼ì •ì„ ì‹œê°í™”í•œ ë´ë“œë¡œê·¸ë¨ì„ ìƒì„±í•©ë‹ˆë‹¤.2) ë³‘í•© êµ°ì§‘í™” vs ë¶„í•  êµ°ì§‘í™” ë³‘í•© êµ°ì§‘í™”(Agglomerative Clustering)ë³‘í•© êµ°ì§‘í™”ëŠ” ê° ë°ì´í„° í¬ì¸íŠ¸ë¥¼ ê°œë³„ êµ°ì§‘ìœ¼ë¡œ ì‹œì‘í•˜ì—¬, ê°€ì¥ ê°€ê¹Œìš´ êµ°ì§‘ì„ ë°˜ë³µì ìœ¼ë¡œ ë³‘í•©í•©ë‹ˆë‹¤.ë³‘í•© êµ°ì§‘í™”ì˜ íŠ¹ì§•ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤ë‹¨ìˆœì„±: êµ¬í˜„ì´ ë¹„êµì  ê°„ë‹¨í•©ë‹ˆë‹¤.ê³„ì‚° ë¹„ìš©: ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜ê°€ ë§ì•„ì§ˆìˆ˜ë¡ ê³„ì‚° ë¹„ìš©ì´ ì¦ê°€í•©ë‹ˆë‹¤.ë´ë“œë¡œê·¸ë¨: êµ°ì§‘í™” ê³¼ì •ì„ ì‹œê°í™”í•œ ë´ë“œë¡œê·¸ë¨ì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¶„í•  êµ°ì§‘í™”(Divisive Clustering)ë¶„í•  êµ°ì§‘í™”ëŠ” ëª¨ë“  ë°ì´í„° í¬ì¸íŠ¸ë¥¼ í•˜ë‚˜ì˜ êµ°ì§‘ìœ¼ë¡œ ì‹œì‘í•˜ì—¬, ë°˜ë³µì ìœ¼ë¡œ ê°€ì¥ ë©€ë¦¬ ë–¨ì–´ì§„ êµ°ì§‘ì„ ë¶„í• í•©ë‹ˆë‹¤. ë¶„í•  êµ°ì§‘í™”ì˜ ì£¼ìš” íŠ¹ì§•ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:ìƒëŒ€ì ìœ¼ë¡œ ë³µì¡í•¨: ë³‘í•© êµ°ì§‘í™”ë³´ë‹¤ êµ¬í˜„ì´ ìƒëŒ€ì ìœ¼ë¡œ ë³µì¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.íš¨ìœ¨ì„±: í° ë°ì´í„°ì…‹ì—ì„œ ë³‘í•© êµ°ì§‘í™”ë³´ë‹¤ íš¨ìœ¨ì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.ë´ë“œë¡œê·¸ë¨: êµ°ì§‘í™” ê³¼ì •ì„ ì‹œê°í™”í•œ ë´ë“œë¡œê·¸ë¨ì„ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.02. ê³„ì¸µì  êµ°ì§‘í™” ì‹¤ìŠµâœ”ï¸Kaggle ì‡¼í•‘ëª° ë°ì´í„°ë¥¼ ì´ìš©í•˜ì—¬ ê³„ì¸µì  êµ°ì§‘í™” ì‹¤ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤1) ì‡¼í•‘ëª° ë°ì´í„° ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œKaggleì—ì„œ "Mall_Customers.csv" íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤.ë‹¤ìš´ë¡œë“œí•œ íŒŒì¼ì„ ì‘ì—… ë””ë ‰í† ë¦¬ì— ì €ì¥í•©ë‹ˆë‹¤. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ì‡¼í•‘ëª° ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ {5px}ì‡¼í•‘ëª° ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ ï»¿â€‹Pythonë³µì‚¬import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import AgglomerativeClustering
import scipy.cluster.hierarchy as sch

# ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°
df = pd.read_csv('Mall_Customers.csv')
# ë°ì´í„° í™•ì¸
print(df.head())
# í•„ìš”í•œ ì—´ë§Œ ì„ íƒ
X = df[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']]
# ë°ì´í„° ì •ê·œí™”
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
â€‹pandas.read_csv: CSV íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ ë°ì´í„°í”„ë ˆì„ ìƒì„±â€™Mall_Customers.csvâ€™: ë¡œë“œí•  íŒŒì¼ì˜ ê²½ë¡œì…ë‹ˆë‹¤.â€™Mall_Customers.csvâ€™: ë¡œë“œí•  íŒŒì¼ì˜ ê²½ë¡œì…ë‹ˆë‹¤.ï»¿â€‹pandas.DataFrame.dropna: ê²°ì¸¡ê°’ì´ ìˆëŠ” í–‰ì„ ì œê±°í•©ë‹ˆë‹¤.pandas.DataFrame.map: ë°ì´í„° ê°’ì„ ë‹¤ë¥¸ ê°’ìœ¼ë¡œ ë§¤í•‘í•©ë‹ˆë‹¤.sklearn.preprocessing.StandardScaler: ë°ì´í„°ì˜ í‰ê· ì„ 0, ë¶„ì‚°ì„ 1ë¡œ ìŠ¤ì¼€ì¼ë§fit_transform(X_train): í›ˆë ¨ ì„¸íŠ¸ë¥¼ ìŠ¤ì¼€ì¼ë§í•˜ê³  ë³€í™˜í•©ë‹ˆë‹¤._transform(X_train): í›ˆë ¨ ì„¸íŠ¸ë¥¼ ìŠ¤ì¼€ì¼ë§í•˜ê³  ë³€í™˜í•©ë‹ˆë‹¤.ï»¿â€‹ ë´ë“œë¡œê·¸ë¨ ìƒì„±ë´ë“œë¡œê·¸ë¨ ìƒì„± {5px}ë´ë“œë¡œê·¸ë¨ ìƒì„± ï»¿â€‹Pythonë³µì‚¬# ë´ë“œë¡œê·¸ë¨ ìƒì„±
plt.figure(figsize=(10, 7))
dendrogram = sch.dendrogram(sch.linkage(X_scaled, method='ward'))
plt.title('Dendrogram')
plt.xlabel('Customers')
plt.ylabel('Euclidean distances')
plt.show()
â€‹sklearn.cluster.KMeans: k-means êµ°ì§‘í™” ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤n_clusters=k: êµ°ì§‘ì˜ ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤._clusters=k: êµ°ì§‘ì˜ ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.ï»¿â€‹random_state=42: ëœë¤ ì‹œë“œ ê°’ìœ¼ë¡œ, ê²°ê³¼ì˜ ì¬í˜„ì„±ì„ ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤._state=42: ëœë¤ ì‹œë“œ ê°’ìœ¼ë¡œ, ê²°ê³¼ì˜ ì¬í˜„ì„±ì„ ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.ï»¿â€‹fit(data_scaled): ë°ì´í„°ë¥¼ í•™ìŠµí•˜ì—¬ êµ°ì§‘ì„ í˜•ì„±í•©ë‹ˆë‹¤.(data_scaled): ë°ì´í„°ë¥¼ í•™ìŠµí•˜ì—¬ êµ°ì§‘ì„ í˜•ì„±í•©ë‹ˆë‹¤.ï»¿â€‹labels_: ê° ë°ì´í„° í¬ì¸íŠ¸ê°€ ì†í•œ êµ°ì§‘ ë ˆì´ë¸”ì„ ë°˜í™˜í•©ë‹ˆë‹¤._: ê° ë°ì´í„° í¬ì¸íŠ¸ê°€ ì†í•œ êµ°ì§‘ ë ˆì´ë¸”ì„ ë°˜í™˜í•©ë‹ˆë‹¤.ï»¿â€‹ ê³„ì¸µì  êµ°ì§‘í™” ëª¨ë¸ êµ¬í˜„ë´ë“œë¡œê·¸ë¨ì„ í†µí•´ ìµœì ì˜ êµ°ì§‘ ìˆ˜ë¥¼ ê²°ì •í•œ í›„, ê³„ì¸µì  êµ°ì§‘í™” ëª¨ë¸ì„ êµ¬í˜„í•©ë‹ˆë‹¤.ê³„ì¸µì  êµ°ì§‘í™” ëª¨ë¸ êµ¬í˜„ {5px}ê³„ì¸µì  êµ°ì§‘í™” ëª¨ë¸ êµ¬í˜„ ï»¿â€‹Pythonë³µì‚¬# ê³„ì¸µì  êµ°ì§‘í™” ëª¨ë¸ ìƒì„±
hc = AgglomerativeClustering(n_clusters=5, metric='euclidean', linkage='ward')
# ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡
y_hc = hc.fit_predict(X_scaled)
# ê²°ê³¼ ì‹œê°í™”
plt.figure(figsize=(10, 7))
plt.scatter(X_scaled[y_hc == 0, 0], X_scaled[y_hc == 0, 1], s=100, c='red', label='Cluster 1')
plt.scatter(X_scaled[y_hc == 1, 0], X_scaled[y_hc == 1, 1], s=100, c='blue', label='Cluster 2')
plt.scatter(X_scaled[y_hc == 2, 0], X_scaled[y_hc == 2, 1], s=100, c='green', label='Cluster 3')
plt.scatter(X_scaled[y_hc == 3, 0], X_scaled[y_hc == 3, 1], s=100, c='cyan', label='Cluster 4')
plt.scatter(X_scaled[y_hc == 4, 0], X_scaled[y_hc == 4, 1], s=100, c='magenta', label='Cluster 5')
plt.title('Clusters of customers')
plt.xlabel('Age')
plt.ylabel('Annual Income (k$)')
plt.legend()
plt.show()
â€‹matplotlib.pyplot.plot: ê·¸ë˜í”„ë¥¼ ê·¸ë¦½ë‹ˆë‹¤.K, inertia, â€™bx-â€™: xì¶•, yì¶• ë°ì´í„°ì™€ ê·¸ë˜í”„ ìŠ¤íƒ€ì¼ì„ ì„¤ì •í•©ë‹ˆë‹¤., inertia, â€™bx-â€™: xì¶•, yì¶• ë°ì´í„°ì™€ ê·¸ë˜í”„ ìŠ¤íƒ€ì¼ì„ ì„¤ì •í•©ë‹ˆë‹¤.ï»¿â€‹seaborn.scatterplot: ì‚°ì ë„ë¥¼ ê·¸ë¦½ë‹ˆë‹¤.x=data[â€™Ageâ€™]: xì¶• ë°ì´í„°=data[â€™Ageâ€™]: xì¶• ë°ì´í„°ï»¿â€‹y=data[â€™Annual Income (k$)â€™]: yì¶• ë°ì´í„°=data[â€™Annual Income (k$)â€™]: yì¶• ë°ì´í„°ï»¿â€‹hue=data[â€™Clusterâ€™]: ìƒ‰ìƒì— ë”°ë¼ êµ°ì§‘ì„ êµ¬ë¶„í•©ë‹ˆë‹¤.=data[â€™Clusterâ€™]: ìƒ‰ìƒì— ë”°ë¼ êµ°ì§‘ì„ êµ¬ë¶„í•©ë‹ˆë‹¤.ï»¿â€‹palette=â€™viridisâ€™: ìƒ‰ìƒ íŒ”ë ˆíŠ¸ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.=â€™viridisâ€™: ìƒ‰ìƒ íŒ”ë ˆíŠ¸ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.ï»¿â€‹ ëª¨ë¸ í‰ê°€ëª¨ë¸ í‰ê°€ {5px}ëª¨ë¸ í‰ê°€ ï»¿â€‹Pythonë³µì‚¬from sklearn.metrics import silhouette_score

# ì‹¤ë£¨ì—£ ì ìˆ˜ ê³„ì‚°
silhouette_avg = silhouette_score(X_scaled, y_hc)
print(f'Silhouette Score: {silhouette_avg}')
â€‹.
[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 15. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹[SCC] ê¸°ì´ˆê°€ íƒ„íƒ„í•œ ë”¥ëŸ¬ë‹/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] ê¸°ì´ˆê°€ íƒ„íƒ„í•œ ë”¥ëŸ¬ë‹ - 6ì£¼ì°¨/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 15. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì œì‘:[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 15. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹[ìˆ˜ì—… ëª©í‘œ]ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¢…ë¥˜ì™€ ìë™í™” íŠœë‹ë°©ë²•ì— ëŒ€í•´ ë°°ì›Œë´…ì‹œë‹¤[ëª©ì°¨]01. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë°©ë²•ğŸ’¡
 
 01. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë°©ë²•âœ”ï¸í•˜ì´í¼íŒŒë¼ë¯¸í„°ë€ ë¬´ì—‡ì¸ì§€, ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°ì—” ì–´ë–¤ê²ƒë“¤ì´ ìˆëŠ”ì§€ ì•Œì•„ë³´ê³ , ìë™ íŠœë‹ê¸°ë²•ì— ëŒ€í•´ ë°°ì›Œë´…ì‹œë‹¤.1) ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°ì™€ íŠœë‹ ë°©ë²• í•˜ì´í¼íŒŒë¼ë¯¸í„°ë€?í•˜ì´í¼íŒŒë¼ë¯¸í„°ëŠ” ëª¨ë¸ í•™ìŠµ ê³¼ì •ì—ì„œ ì‚¬ìš©ìê°€ ì„¤ì •í•´ì•¼ í•˜ëŠ” ê°’ìœ¼ë¡œ, ëª¨ë¸ì˜ ì„±ëŠ¥ì— í° ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤.ALT í•™ìŠµë¥  (Learning Rate)í•™ìŠµë¥ ì€ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ì†ë„ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.ë„ˆë¬´ í¬ë©´ í•™ìŠµì´ ë¶ˆì•ˆì •í•´ì§€ê³ , ë„ˆë¬´ ì‘ìœ¼ë©´ í•™ìŠµì´ ëŠë ¤ì§‘ë‹ˆë‹¤.ì¼ë°˜ì ìœ¼ë¡œ 0.1, 0.01, 0.001 ë“±ì˜ ê°’ì„ ì‹œë„í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°°ì¹˜ í¬ê¸° (Batch Size)ë°°ì¹˜ í¬ê¸°ëŠ” í•œ ë²ˆì˜ ì—…ë°ì´íŠ¸ì— ì‚¬ìš©ë˜ëŠ” ë°ì´í„° ìƒ˜í”Œì˜ ìˆ˜ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.í° ë°°ì¹˜ í¬ê¸°ëŠ” í•™ìŠµ ì†ë„ë¥¼ ë†’ì´ì§€ë§Œ, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì¦ê°€í•©ë‹ˆë‹¤.ì¼ë°˜ì ìœ¼ë¡œ 32, 64, 128 ë“±ì˜ ê°’ì„ ì‹œë„í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—í¬í¬ ìˆ˜ (Number of Epochs)ì—í¬í¬ ìˆ˜ëŠ” ì „ì²´ ë°ì´í„°ì…‹ì„ ëª‡ ë²ˆ ë°˜ë³µí•˜ì—¬ í•™ìŠµí• ì§€ë¥¼ ê²°ì •í•©ë‹ˆë‹¤.ë„ˆë¬´ ì ìœ¼ë©´ ê³¼ì†Œì í•©ì´ ë°œìƒí•˜ê³ , ë„ˆë¬´ ë§ìœ¼ë©´ ê³¼ì í•©ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ì¡°ê¸° ì¢…ë£Œ(Early Stopping) ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ ì ì ˆí•œ ì—í¬í¬ ìˆ˜ë¥¼ ê²°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª¨ë©˜í…€ (Momentum)ëª¨ë©˜í…€ì€ ì´ì „ ê¸°ìš¸ê¸°ë¥¼ í˜„ì¬ ê¸°ìš¸ê¸°ì— ë°˜ì˜í•˜ì—¬, í•™ìŠµ ì†ë„ë¥¼ ë†’ì´ê³  ì§„ë™ì„ ì¤„ì…ë‹ˆë‹¤.ì¼ë°˜ì ìœ¼ë¡œ 0.9, 0.99 ë“±ì˜ ê°’ì„ ì‹œë„í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” (Weight Initialization)ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”ëŠ” ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì´ˆê¸°í™”í•˜ëŠ” ë°©ë²•ì„ ê²°ì •í•©ë‹ˆë‹¤.ì¼ë°˜ì ìœ¼ë¡œ Xavier ì´ˆê¸°í™”, He ì´ˆê¸°í™” ë“±ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.2) í•˜ì´í¼íŒŒë¼ë¯¸í„° ìë™ íŠœë‹ ê¸°ë²• Grid Searchí•˜ì´í¼íŒŒë¼ë¯¸í„°ì˜ ëª¨ë“  ì¡°í•©ì„ ì‹œë„í•˜ì—¬ ìµœì ì˜ ê°’ì„ ì°¾ìŠµë‹ˆë‹¤.ê³„ì‚° ë¹„ìš©ì´ ë§ì´ ë“¤ì§€ë§Œ, ëª¨ë“  ì¡°í•©ì„ íƒìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Random Searchí•˜ì´í¼íŒŒë¼ë¯¸í„° ê³µê°„ì—ì„œ ë¬´ì‘ìœ„ë¡œ ê°’ì„ ì„ íƒí•˜ì—¬ ìµœì ì˜ ê°’ì„ ì°¾ìŠµë‹ˆë‹¤.Grid Searchë³´ë‹¤ ê³„ì‚° ë¹„ìš©ì´ ì ê³ , ë” ë„“ì€ í•˜ì´í¼íŒŒë¼ë¯¸í„° ê³µê°„ì„ íƒìƒ‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Bayesian Optimizationë² ì´ì§€ì•ˆ ìµœì í™”ëŠ” ì´ì „ í‰ê°€ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ë‹¤ìŒ í‰ê°€í•  í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.ê³„ì‚° ë¹„ìš©ì´ ì ê³ , íš¨ìœ¨ì ìœ¼ë¡œ ìµœì ì˜ ê°’ì„ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤..
íŒë‹¤ìŠ¤ ì‹¬í™”: ë©€í‹° ì¸ë±ìŠ¤ì™€ ë³µí•© ì¸ë±ìŠ¤ì¸ê³µì§€ëŠ¥ì„ ìœ„í•œ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬/ì¸ê³µì§€ëŠ¥ì„ ìœ„í•œ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ - 6ì£¼ì°¨/íŒë‹¤ìŠ¤ ì‹¬í™”: ë©€í‹° ì¸ë±ìŠ¤ì™€ ë³µí•© ì¸ë±ìŠ¤ì œì‘:íŒë‹¤ìŠ¤ ì‹¬í™”: ë©€í‹° ì¸ë±ìŠ¤ì™€ ë³µí•© ì¸ë±ìŠ¤ìˆ˜ì—… ëª©í‘œíŒë‹¤ìŠ¤ì˜ ë©€í‹° ì¸ë±ìŠ¤ì™€ ë³µí•© ì¸ë±ìŠ¤ì— ëŒ€í•´ì„œ í•™ìŠµí•©ë‹ˆë‹¤.ë‹¤ì°¨ì› ë°ì´í„°ì˜ íš¨ê³¼ì ì¸ ê´€ë¦¬ ë°©ë²•ì— ëŒ€í•´ì„œ í•™ìŠµí•©ë‹ˆë‹¤.ëª©ì°¨ë©€í‹° ì¸ë±ìŠ¤(MultiIndex)ë€?ë©€í‹° ì¸ë±ìŠ¤ ìƒì„±í•˜ê¸°ë©€í‹° ì¸ë±ìŠ¤ ë°ì´í„° ì ‘ê·¼í•˜ê¸°ë³µí•© ì¸ë±ìŠ¤(MultiIndex) í™œìš©ë©€í‹° ì¸ë±ìŠ¤ì˜ ì‘ìš©ìš”ì•½ë©€í‹° ì¸ë±ìŠ¤(MultiIndex)ë€?ğŸ“šë©€í‹° ì¸ë±ìŠ¤(MultiIndex)ëŠ” í•˜ë‚˜ ì´ìƒì˜ ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°í”„ë ˆì„ì˜ í–‰ê³¼ ì—´ì„ êµ¬ì¡°í™”í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì´ëŠ” ë‹¤ì°¨ì› ë°ì´í„°ë¥¼ ë³´ë‹¤ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬í•˜ê³  ë¶„ì„í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤.ë©€í‹° ì¸ë±ìŠ¤ ìƒì„±í•˜ê¸°set_index()ë¡œ ë©€í‹° ì¸ë±ìŠ¤ ì„¤ì •ì—¬ëŸ¬ ì—´ì„ ì‚¬ìš©í•´ ë©€í‹° ì¸ë±ìŠ¤ë¥¼ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Pythonë³µì‚¬import pandas as pd

# ì˜ˆì‹œ ë°ì´í„°í”„ë ˆì„ ìƒì„±
data = {
'ë„ì‹œ': ['ì„œìš¸', 'ì„œìš¸', 'ë¶€ì‚°', 'ë¶€ì‚°'],
'ë…„ë„': [2021, 2022, 2021, 2022],
'ì¸êµ¬ìˆ˜': [9700000, 9720000, 3400000, 3450000]
}
df = pd.DataFrame(data)
# 'ë„ì‹œ'ì™€ 'ë…„ë„'ë¥¼ ë©€í‹° ì¸ë±ìŠ¤ë¡œ ì„¤ì •
df_multi_index = df.set_index(['ë„ì‹œ', 'ë…„ë„'])
print(df_multi_index)

â€‹ê²°ê³¼:Plain Textë³µì‚¬          ì¸êµ¬ìˆ˜
ë„ì‹œ   ë…„ë„
ì„œìš¸  2021  9700000
     2022  9720000
ë¶€ì‚°  2021  3400000
     2022  3450000

â€‹ë„ì‹œì™€ ë…„ë„ê°€ ì¸ë±ìŠ¤ë¡œ ì„¤ì •ë˜ì–´, ë°ì´í„°ê°€ ë” êµ¬ì¡°í™”ëœ í˜•íƒœë¡œ ë³´ì…ë‹ˆë‹¤.pd.MultiIndex.from_tuples()ë¡œ ë©€í‹° ì¸ë±ìŠ¤ ìƒì„±from_tuples()ë¥¼ ì‚¬ìš©í•´ íŠœí”Œë¡œ êµ¬ì„±ëœ ë©€í‹° ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.Pythonë³µì‚¬# ë©€í‹° ì¸ë±ìŠ¤ë¥¼ íŠœí”Œë¡œ ì§ì ‘ ìƒì„±
index = pd.MultiIndex.from_tuples([('ì„œìš¸', 2021), ('ì„œìš¸', 2022), ('ë¶€ì‚°', 2021), ('ë¶€ì‚°', 2022)], names=['ë„ì‹œ', 'ë…„ë„'])
# ë°ì´í„°í”„ë ˆì„ì— ì ìš©
df_multi_index = pd.DataFrame({'ì¸êµ¬ìˆ˜': [9700000, 9720000, 3400000, 3450000]}, index=index)
print(df_multi_index)

â€‹ê²°ê³¼:Plain Textë³µì‚¬          ì¸êµ¬ìˆ˜
ë„ì‹œ   ë…„ë„
ì„œìš¸  2021  9700000
     2022  9720000
ë¶€ì‚°  2021  3400000
     2022  3450000

â€‹ë©€í‹° ì¸ë±ìŠ¤ ë°ì´í„° ì ‘ê·¼í•˜ê¸°loc[]ì„ ì‚¬ìš©í•œ ë°ì´í„° ì ‘ê·¼loc[]ì„ ì‚¬ìš©í•´ ë©€í‹° ì¸ë±ìŠ¤ì—ì„œ íŠ¹ì • ë°ì´í„°ë¥¼ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Pythonë³µì‚¬# íŠ¹ì • ì¸ë±ìŠ¤ì˜ ë°ì´í„° ì„ íƒ
print(df_multi_index.loc['ì„œìš¸'])

â€‹ê²°ê³¼:Plain Textë³µì‚¬        ì¸êµ¬ìˆ˜
ë…„ë„
2021  9700000
2022  9720000

â€‹í•˜ìœ„ ë ˆë²¨ê¹Œì§€ ì§€ì •í•˜ì—¬ ë°ì´í„°ë¥¼ ë” êµ¬ì²´ì ìœ¼ë¡œ ì ‘ê·¼í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.Pythonë³µì‚¬# 'ì„œìš¸'ì˜ 2021ë…„ ë°ì´í„° ì„ íƒ
print(df_multi_index.loc[('ì„œìš¸', 2021)])

â€‹ê²°ê³¼:Plain Textë³µì‚¬ì¸êµ¬ìˆ˜    9700000
Name: (ì„œìš¸, 2021), dtype: int64

â€‹ìŠ¬ë¼ì´ì‹±(Slicing)ìœ¼ë¡œ ë°ì´í„° ì ‘ê·¼ë©€í‹° ì¸ë±ìŠ¤ëŠ” ìŠ¬ë¼ì´ì‹±ì„ í†µí•´ íŠ¹ì • êµ¬ê°„ì˜ ë°ì´í„°ë¥¼ ì‰½ê²Œ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Pythonë³µì‚¬# ë¶€ì‚°ì˜ ëª¨ë“  ë°ì´í„°ë¥¼ ì„ íƒ
df_multi_index = df_multi_index.sort_index()
print(df_multi_index.loc['ë¶€ì‚°':'ë¶€ì‚°'])

â€‹ê²°ê³¼:Plain Textë³µì‚¬          ì¸êµ¬ìˆ˜
ë„ì‹œ   ë…„ë„
ë¶€ì‚°  2021  3400000
     2022  3450000

â€‹xs()ë¥¼ ì‚¬ìš©í•œ ë©€í‹° ì¸ë±ìŠ¤ êµì°¨ ì„ íƒxs()ëŠ” íŠ¹ì • ë ˆë²¨ì—ì„œ ë°ì´í„°ë¥¼ ì„ íƒí•˜ê±°ë‚˜, ë ˆë²¨ì„ ë„˜ì–´ì„œ ë°ì´í„°ë¥¼ ì„ íƒí•  ë•Œ ìœ ìš©í•©ë‹ˆë‹¤.Pythonë³µì‚¬# 'ë„ì‹œ' ë ˆë²¨ì—ì„œ 'ì„œìš¸'ì˜ ë°ì´í„°ë¥¼ ì„ íƒ
print(df_multi_index.xs('ì„œìš¸', level='ë„ì‹œ'))

â€‹ê²°ê³¼:Plain Textë³µì‚¬        ì¸êµ¬ìˆ˜
ë…„ë„
2021  9700000
2022  9720000

â€‹ë³µí•© ì¸ë±ìŠ¤(MultiIndex) í™œìš©ì¸ë±ìŠ¤ ì •ë ¬ ë° ì •ë ¬ëœ ìƒíƒœ í™•ì¸ë©€í‹° ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•œ ë°ì´í„°í”„ë ˆì„ì€ ì •ë ¬ëœ ìƒíƒœë¡œ ê´€ë¦¬í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì…ë‹ˆë‹¤. sort_index()ë¡œ ì¸ë±ìŠ¤ë¥¼ ì •ë ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Pythonë³µì‚¬# ë©€í‹° ì¸ë±ìŠ¤ ì •ë ¬
df_sorted = df_multi_index.sort_index()
print(df_sorted)

â€‹unstack()ê³¼ stack()ìœ¼ë¡œ ì¸ë±ìŠ¤ ë³€í™˜unstack()ì€ ë©€í‹° ì¸ë±ìŠ¤ë¥¼ ì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ë°˜ëŒ€ë¡œ stack()ì€ ì—´ì„ ì¸ë±ìŠ¤ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.Pythonë³µì‚¬# ë©€í‹° ì¸ë±ìŠ¤ë¥¼ ì—´ë¡œ ë³€í™˜ (unstack)
df_unstacked = df_multi_index.unstack(level='ë…„ë„')
print(df_unstacked)

â€‹ê²°ê³¼:Plain Textë³µì‚¬         ì¸êµ¬ìˆ˜
ë…„ë„      2021      2022
ë„ì‹œ
ì„œìš¸   9700000  9720000
ë¶€ì‚°   3400000  3450000

â€‹*stack()**ì„ ì‚¬ìš©í•˜ë©´, ë‹¤ì‹œ ì¸ë±ìŠ¤ë¡œ ë³€í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Pythonë³µì‚¬# ë‹¤ì‹œ ì¸ë±ìŠ¤ë¡œ ë³€í™˜ (stack)
df_stacked = df_unstacked.stack()
print(df_stacked)

â€‹ë©€í‹° ì¸ë±ìŠ¤ì˜ ì‘ìš©ê·¸ë£¹í™”(Grouping)ì™€ í•¨ê»˜ ì‚¬ìš©ğŸ“šë©€í‹° ì¸ë±ìŠ¤ëŠ” ê·¸ë£¹í™”ì™€ í•¨ê»˜ ì‚¬ìš©í•  ë•Œ ë” ê°•ë ¥í•´ì§‘ë‹ˆë‹¤. ê·¸ë£¹í™”í•œ ë°ì´í„°ë¥¼ ë©€í‹° ì¸ë±ìŠ¤ë¡œ ë³€í™˜í•˜ì—¬ ë³µì¡í•œ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Pythonë³µì‚¬# ë°ì´í„°í”„ë ˆì„ ìƒì„±
data = {
'ë„ì‹œ': ['ì„œìš¸', 'ì„œìš¸', 'ë¶€ì‚°', 'ë¶€ì‚°', 'ì„œìš¸', 'ë¶€ì‚°'],
'ë…„ë„': [2021, 2022, 2021, 2022, 2021, 2022],
'ì¸êµ¬ìˆ˜': [9700000, 9720000, 3400000, 3450000, 9800000, 3500000],
'ì†Œë“': [60000, 62000, 45000, 46000, 63000, 47000]
}
df = pd.DataFrame(data)
# 'ë„ì‹œ'ì™€ 'ë…„ë„'ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í™”í•˜ì—¬ í‰ê·  ê³„ì‚°
grouped_df = df.groupby(['ë„ì‹œ', 'ë…„ë„']).mean()
print(grouped_df)

â€‹ê²°ê³¼:Plain Textë³µì‚¬             ì¸êµ¬ìˆ˜      ì†Œë“
ë„ì‹œ  ë…„ë„
ë¶€ì‚°  2021  3400000  45000
     2022  3450000  46500
ì„œìš¸  2021  9750000  61500
     2022  9720000  62000

â€‹ìš”ì•½ğŸ“šìš”ì•½ë©€í‹° ì¸ë±ìŠ¤(MultiIndex)ëŠ” í•˜ë‚˜ ì´ìƒì˜ ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‹¤ì°¨ì› ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.ë©€í‹° ì¸ë±ìŠ¤ëŠ” set_index()ë‚˜ from_tuples()ë¥¼ ì‚¬ìš©í•´ ìƒì„±í•  ìˆ˜ ìˆìœ¼ë©°, loc[], xs(), ìŠ¬ë¼ì´ì‹± ë“±ì„ í†µí•´ ë°ì´í„°ë¥¼ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ë³µí•© ì¸ë±ìŠ¤ë¥¼ ì‚¬ìš©í•˜ë©´ ë°ì´í„°ë¥¼ ì •ë ¬í•˜ê±°ë‚˜, unstack()ê³¼ stack()ì„ í†µí•´ ì¸ë±ìŠ¤ë¥¼ ë³€í™˜í•˜ëŠ” ì‘ì—…ì´ ìš©ì´í•©ë‹ˆë‹¤.ë©€í‹° ì¸ë±ìŠ¤ëŠ” ê·¸ë£¹í™”ì™€ ê°™ì€ ë³µì¡í•œ ë°ì´í„° ë¶„ì„ ì‘ì—…ì—ë„ ìœ ìš©í•˜ê²Œ ì‚¬ìš©ë©ë‹ˆë‹¤.ì´ì œ ë©€í‹° ì¸ë±ìŠ¤ë¥¼ í™œìš©í•˜ì—¬ ë” ë³µì¡í•˜ê³  ê¹Šì´ ìˆëŠ” ë°ì´í„° ë¶„ì„ì„ ì§„í–‰í•´ë³´ì„¸ìš”! â€‹
[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 16ê°•. ë¹„ì§€ë„í•™ìŠµ : êµ°ì§‘í™”ëª¨ë¸ - DBSCAN[SCC] ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ - 4ì£¼ì°¨ /[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 16ê°•. ë¹„ì§€ë„í•™ìŠµ : êµ°ì§‘í™”ëª¨ë¸ - DBSCANì œì‘:[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 16ê°•. ë¹„ì§€ë„í•™ìŠµ : êµ°ì§‘í™”ëª¨ë¸ - DBSCAN[ìˆ˜ì—… ëª©í‘œ]ë¹„ì§€ë„í•™ìŠµ êµ°ì§‘í™”ëª¨ë¸ ì¤‘ DBSCAN ì— ëŒ€í•œ ê°œë…ì„ ë°°ìš°ê³ , ë°ì´í„°ë¥¼ ì´ìš©í•´ ì‹¤ìŠµí•´ ë´…ë‹ˆë‹¤[ëª©ì°¨]01. DBSCAN ê°œë…02. DBSCAN ì‹¤ìŠµ
 
 01. DBSCAN ê°œë…DBSCANì´ ë¬´ì—‡ì¸ì§€ ì•Œì•„ë´…ì‹œë‹¤1) DBSCAN DBSCANì´ë€?DBSCAN(Density-Based Spatial Clustering of Applications with Noise)ì€ ë°€ë„ ê¸°ë°˜ êµ°ì§‘í™” ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤DBSCANì€ ë°ì´í„° ë°€ë„ê°€ ë†’ì€ ì˜ì—­ì„ êµ°ì§‘ìœ¼ë¡œ ê°„ì£¼í•˜ê³ , ë°€ë„ê°€ ë‚®ì€ ì˜ì—­ì€ ë…¸ì´ì¦ˆë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤ALTì£¼ìš” ë§¤ê°œë³€ìˆ˜ì£¼ìš” ë§¤ê°œë³€ìˆ˜ï»¿
eps: ë‘ ë°ì´í„° í¬ì¸íŠ¸ê°€ ê°™ì€ êµ°ì§‘ì— ì†í•˜ê¸° ìœ„í•´ ê°€ì ¸ì•¼ í•˜ëŠ” ìµœëŒ€ ê±°ë¦¬ì…ë‹ˆë‹¤.: ë‘ ë°ì´í„° í¬ì¸íŠ¸ê°€ ê°™ì€ êµ°ì§‘ì— ì†í•˜ê¸° ìœ„í•´ ê°€ì ¸ì•¼ í•˜ëŠ” ìµœëŒ€ ê±°ë¦¬ì…ë‹ˆë‹¤.ï»¿â€‹min_samples: í•œ êµ°ì§‘ì„ í˜•ì„±í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ìµœì†Œ ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜ì…ë‹ˆë‹¤_samples: í•œ êµ°ì§‘ì„ í˜•ì„±í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ìµœì†Œ ë°ì´í„° í¬ì¸íŠ¸ ìˆ˜ì…ë‹ˆë‹¤ï»¿â€‹ DBSCANì˜ ì‘ë™ ì›ë¦¬ì„ì˜ì˜ ë°ì´í„° í¬ì¸íŠ¸ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.ì„ íƒí•œ ë°ì´í„° í¬ì¸íŠ¸ì˜ eps ë°˜ê²½ ë‚´ì— ìˆëŠ” ëª¨ë“  ë°ì´í„° í¬ì¸íŠ¸ë¥¼ ì°¾ìŠµë‹ˆë‹¤.eps ë°˜ê²½ ë‚´ì˜ ë°ì´í„°ìˆ˜ â‰¥ min_samples : í•´ë‹¹ ë°ì´í„° í¬ì¸íŠ¸ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ìƒˆë¡œìš´ êµ°ì§‘ í˜•ì„±.eps ë°˜ê²½ ë‚´ì˜ ë°ì´í„°ìˆ˜ < min_samples : í•´ë‹¹ ë°ì´í„° í¬ì¸íŠ¸ë¥¼ ë…¸ì´ì¦ˆë¡œ ê°„ì£¼êµ°ì§‘ì— ì†í•œ ë°ì´í„° í¬ì¸íŠ¸ì— ëŒ€í•´ 2~4 ë‹¨ê³„ë¥¼ ë°˜ë³µí•©ë‹ˆë‹¤.ëª¨ë“  ë°ì´í„° í¬ì¸íŠ¸ê°€ ì²˜ë¦¬ë  ë•Œê¹Œì§€ ì´ ê³¼ì •ì„ ë°˜ë³µí•©ë‹ˆë‹¤. DBSCANì˜ ì¥ì ë¹„êµ¬í˜• êµ°ì§‘ íƒì§€: DBSCANì€ ë¹„êµ¬í˜• êµ°ì§‘ì„ íƒì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ë…¸ì´ì¦ˆ ì²˜ë¦¬: DBSCANì€ ë…¸ì´ì¦ˆë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.êµ°ì§‘ ìˆ˜ ìë™ ê²°ì •: DBSCANì€ êµ°ì§‘ ìˆ˜ë¥¼ ì‚¬ì „ì— ì§€ì •í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤.02. DBSCAN ì‹¤ìŠµKaggle ì‡¼í•‘ëª° ë°ì´í„°ë¥¼ ì´ìš©í•˜ì—¬ DBSCAN ì‹¤ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤1) ì‡¼í•‘ëª° ë°ì´í„° ë°ì´í„° ë¡œë“œ ì‡¼í•‘ëª° ë°ì´í„° ë¡œë“œ {5px}ì‡¼í•‘ëª° ë°ì´í„° ë¡œë“œ ï»¿â€‹Pythonë³µì‚¬import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import AgglomerativeClustering
import scipy.cluster.hierarchy as sch

# ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°
df = pd.read_csv('Mall_Customers.csv')
# ë°ì´í„° í™•ì¸
print(df.head())
# í•„ìš”í•œ ì—´ë§Œ ì„ íƒ
X = df[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']]
# ë°ì´í„° ì •ê·œí™”
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
â€‹ DBSCAN ìˆ˜í–‰Scikit-learnì˜ DBSCANì„ ì‚¬ìš©í•˜ì—¬ DBSCAN êµ°ì§‘í™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.DBSCANìˆ˜í–‰ {5px}DBSCANìˆ˜í–‰ ï»¿â€‹Pythonë³µì‚¬from sklearn.cluster import DBSCAN
import matplotlib.pyplot as plt
import seaborn as sns

# DBSCAN ëª¨ë¸ ìƒì„±
dbscan = DBSCAN(eps=5, min_samples=5)
# ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡
df['Cluster'] = dbscan.fit_predict(X)
# êµ°ì§‘í™” ê²°ê³¼ ì‹œê°í™”
plt.figure(figsize=(10, 7))
sns.scatterplot(x='Annual Income (k$)', y='Spending Score (1-100)', hue='Cluster', data=df, palette='viridis')
plt.title('DBSCAN Clustering of Mall Customers')
plt.show()
â€‹ íŒŒë¼ë¯¸í„° íŠœë‹DBSCANì˜ ì„±ëŠ¥ì€ epsì™€ min_samples íŒŒë¼ë¯¸í„°ì— í¬ê²Œ ì˜ì¡´í•©ë‹ˆë‹¤. ì ì ˆí•œ íŒŒë¼ë¯¸í„°ë¥¼ ì°¾ê¸° ìœ„í•´ ì—¬ëŸ¬ ê°’ì„ ì‹œë„í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.íŒŒë¼ë¯¸í„° íŠœë‹ {5px}íŒŒë¼ë¯¸í„° íŠœë‹ ï»¿â€‹Pythonë³µì‚¬# ë‹¤ì–‘í•œ epsì™€ min_samples ê°’ ì‹œë„
eps_values = [3, 5, 7, 10]
min_samples_values = [3, 5, 7, 10]
for eps in eps_values:
for min_samples in min_samples_values:
        dbscan = DBSCAN(eps=eps, min_samples=min_samples)
        df['Cluster'] = dbscan.fit_predict(X)
        
        plt.figure(figsize=(10, 7))
        sns.scatterplot(x='Annual Income (k$)', y='Spending Score (1-100)', hue='Cluster', data=df, palette='viridis')
        plt.title(f'DBSCAN Clustering (eps={eps}, min_samples={min_samples})')
        plt.show()
â€‹.
[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 16. ëª¨ë¸ í‰ê°€ì™€ ê²€ì¦ ë° Pytorch ë¬¸ë²• ì •ë¦¬[SCC] ê¸°ì´ˆê°€ íƒ„íƒ„í•œ ë”¥ëŸ¬ë‹/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] ê¸°ì´ˆê°€ íƒ„íƒ„í•œ ë”¥ëŸ¬ë‹ - 6ì£¼ì°¨/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 16. ëª¨ë¸ í‰ê°€ì™€ ê²€ì¦ ë° Pytorch ë¬¸ë²• ì •ë¦¬ì œì‘:[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 16. ëª¨ë¸ í‰ê°€ì™€ ê²€ì¦ ë° Pytorch ë¬¸ë²• ì •ë¦¬[ìˆ˜ì—… ëª©í‘œ]êµì°¨ê²€ì¦ì´ ë¬´ì—‡ì¸ì§€, ì™œ í•„ìš”í•œì§€ ì•Œì•„ë´…ì‹œë‹¤[ëª©ì°¨]01. êµì°¨ê²€ì¦ 02. Pytorch ë¬¸ë²• ì •ë¦¬ ğŸ’¡
 
 01. êµì°¨ê²€ì¦ âœ”ï¸êµì°¨ê²€ì¦ì´ ë¬´ì—‡ì¸ì§€, ì™œ í•„ìš”í•œì§€ ì•Œì•„ë³´ê³  êµì°¨ê²€ì¦ì˜ í•œ ì¢…ë¥˜ì¸ K-Fold êµì°¨ê²€ì¦ì— ëŒ€í•´ ë°°ì›Œë´…ì‹œë‹¤1) êµì°¨ê²€ì¦ ê°œë… ë° í•„ìš”ì„± êµì°¨ê²€ì¦ ì´ë€?êµì°¨ ê²€ì¦(Cross-Validation)ì€ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•´ ë°ì´í„°ë¥¼ ì—¬ëŸ¬ ë²ˆ ë‚˜ëˆ„ì–´ í•™ìŠµê³¼ ê²€ì¦ì„ ë°˜ë³µí•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤êµì°¨ ê²€ì¦ì€ ëª¨ë¸ì´ ê³¼ì í•©(overfitting)ë˜ì§€ ì•Šê³ , ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•´ ì˜ ì¼ë°˜í™”ë˜ëŠ”ì§€ í‰ê°€í•˜ëŠ” ë° ìœ ìš©í•©ë‹ˆë‹¤.ALT - ë°ì´í„°ë¥¼ ì—¬ëŸ¬ ê°œì˜ í´ë“œ(fold)ë¡œ ë‚˜ëˆ„ê³ , ê° í´ë“œì— ëŒ€í•´ í•™ìŠµê³¼ ê²€ì¦ì„ ë°˜ë³µí•©ë‹ˆë‹¤. - ë°ì´í„°ë¥¼ ì—¬ëŸ¬ ê°œì˜ í´ë“œ(fold)ë¡œ ë‚˜ëˆ„ê³ , ê° í´ë“œì— ëŒ€í•´ í•™ìŠµê³¼ ê²€ì¦ì„ ë°˜ë³µí•©ë‹ˆë‹¤.ï»¿
 - ê° í´ë“œê°€ í•œ ë²ˆì”© ê²€ì¦ ë°ì´í„°ë¡œ ì‚¬ìš©ë˜ë©°, ë‚˜ë¨¸ì§€ í´ë“œëŠ” í•™ìŠµ ë°ì´í„°ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.{
- ê° í´ë“œê°€ í•œ ë²ˆì”© ê²€ì¦ ë°ì´í„°ë¡œ ì‚¬ìš©ë˜ë©°, ë‚˜ë¨¸ì§€ í´ë“œëŠ” í•™ìŠµ ë°ì´í„°ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.} - ê° í´ë“œê°€ í•œ ë²ˆì”© ê²€ì¦ ë°ì´í„°ë¡œ ì‚¬ìš©ë˜ë©°, ë‚˜ë¨¸ì§€ í´ë“œëŠ” í•™ìŠµ ë°ì´í„°ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.ï»¿
 - ëª¨ë“  í´ë“œì— ëŒ€í•œ ê²€ì¦ ê²°ê³¼ë¥¼ í‰ê· í•˜ì—¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.{
- ëª¨ë“  í´ë“œì— ëŒ€í•œ ê²€ì¦ ê²°ê³¼ë¥¼ í‰ê· í•˜ì—¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.} - ëª¨ë“  í´ë“œì— ëŒ€í•œ ê²€ì¦ ê²°ê³¼ë¥¼ í‰ê· í•˜ì—¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.ï»¿â€‹ êµì°¨ê²€ì¦ì˜ í•„ìš”ì„±ê³¼ì í•© ë°©ì§€: êµì°¨ ê²€ì¦ì€ ëª¨ë¸ì´ íŠ¹ì • ë°ì´í„°ì…‹ì— ê³¼ì í•©ë˜ì§€ ì•Šë„ë¡ ë„ì™€ì¤ë‹ˆë‹¤.ì¼ë°˜í™” ì„±ëŠ¥ í‰ê°€: êµì°¨ ê²€ì¦ì€ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ì„ ë” ì •í™•í•˜ê²Œ í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ë°ì´í„° íš¨ìœ¨ì„±: êµì°¨ ê²€ì¦ì€ ë°ì´í„°ë¥¼ ìµœëŒ€í•œ í™œìš©í•˜ì—¬ ëª¨ë¸ì„ í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.2) K-Fold êµì°¨ ê²€ì¦ K-Fold êµì°¨ ê²€ì¦ì˜ ì›ë¦¬ë°ì´í„°ë¥¼ Kê°œì˜ í´ë“œë¡œ ë‚˜ëˆ•ë‹ˆë‹¤.ê° í´ë“œê°€ í•œ ë²ˆì”© ê²€ì¦ ë°ì´í„°ë¡œ ì‚¬ìš©ë˜ë©°, ë‚˜ë¨¸ì§€ K-1ê°œì˜ í´ë“œëŠ” í•™ìŠµ ë°ì´í„°ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.Kë²ˆì˜ í•™ìŠµê³¼ ê²€ì¦ì„ ë°˜ë³µí•˜ì—¬, ê° í´ë“œì— ëŒ€í•œ ê²€ì¦ ê²°ê³¼ë¥¼ í‰ê· í•˜ì—¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤. ì ìš© ë°©ë²•ë°ì´í„°ë¥¼ Kê°œì˜ í´ë“œë¡œ ë‚˜ëˆ„ê³ , ê° í´ë“œì— ëŒ€í•´ í•™ìŠµê³¼ ê²€ì¦ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.ê° í´ë“œì— ëŒ€í•œ ê²€ì¦ ê²°ê³¼ë¥¼ ì €ì¥í•˜ê³ , ìµœì¢…ì ìœ¼ë¡œ í‰ê· í•˜ì—¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.02. Pytorch ë¬¸ë²• ì •ë¦¬ âœ”ï¸í•œë²ˆ ë¬¸ë²•ì„ ì‚´í´ë³¼ê¹Œìš”?1) Pytorch PytorchPyTorchëŠ” ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬ë¡œ, ìœ ì—°ì„±ê³¼ ì‚¬ìš© í¸ì˜ì„±ì„ ì œê³µí•˜ì—¬ ì—°êµ¬ì™€ ê°œë°œì—ì„œ ë„ë¦¬ ì‚¬ìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤. PyTorchì˜ ì£¼ìš” APIë¥¼ ê¸°ë²•ë³„, ëª¨ë¸ë³„, ê¸°ëŠ¥ë³„ë¡œ ì •ë¦¬í•˜ê² ìŠµë‹ˆë‹¤.PyTorchì˜ ì£¼ìš” API ì •ë¦¬1. ëª¨ë¸ êµ¬ì¶• ë° í•™ìŠµê¸°ë³¸ ëª¨ë¸ êµ¬ì¶•torch.nn.Module: ëª¨ë“  ì‹ ê²½ë§ ëª¨ë¸ì˜ ê¸°ë³¸ í´ë˜ìŠ¤ì…ë‹ˆë‹¤.Pythonë³µì‚¬import torch.nn as nn

class MyModel(nn.Module):
def __init__(self):
super(MyModel, self).__init__()
        self.layer1 = nn.Linear(10, 20)
def forward(self, x):
        x = self.layer1(x)
return x

â€‹ì†ì‹¤ í•¨ìˆ˜torch.nn.CrossEntropyLoss: ë¶„ë¥˜ ë¬¸ì œì— ì£¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.Pythonë³µì‚¬loss_fn = nn.CrossEntropyLoss()

â€‹torch.nn.MSELoss: íšŒê·€ ë¬¸ì œì— ì£¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.Pythonë³µì‚¬loss_fn = nn.MSELoss()

â€‹ìµœì í™” ì•Œê³ ë¦¬ì¦˜torch.optim.SGD: í™•ë¥ ì  ê²½ì‚¬ í•˜ê°•ë²• ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.Pythonë³µì‚¬optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

â€‹torch.optim.Adam: Adam ìµœì í™” ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.Pythonë³µì‚¬optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

â€‹2. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ë°ì´í„°ì…‹ ë° ë°ì´í„°ë¡œë”torch.utils.data.Dataset: ì‚¬ìš©ì ì •ì˜ ë°ì´í„°ì…‹ì„ ë§Œë“¤ê¸° ìœ„í•œ ê¸°ë³¸ í´ë˜ìŠ¤ì…ë‹ˆë‹¤.Pythonë³µì‚¬from torch.utils.data import Dataset

class MyDataset(Dataset):
def __init__(self, data, targets):
        self.data = data
        self.targets = targets

    def __len__(self):
return len(self.data)
def __getitem__(self, idx):
return self.data[idx], self.targets[idx]

â€‹torch.utils.data.DataLoader: ë¯¸ë‹ˆ ë°°ì¹˜ í•™ìŠµì„ ìœ„í•œ ë°ì´í„° ë¡œë”ì…ë‹ˆë‹¤.Pythonë³µì‚¬from torch.utils.data import DataLoader

dataset = MyDataset(data, targets)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

â€‹ë°ì´í„° ë³€í™˜torchvision.transforms: ì´ë¯¸ì§€ ë°ì´í„° ë³€í™˜ì„ ìœ„í•œ ìœ í‹¸ë¦¬í‹°ì…ë‹ˆë‹¤.Pythonë³µì‚¬from torchvision import transforms

transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

â€‹3. GPU ì‚¬ìš©GPU ì„¤ì • ë° í…ì„œ ì´ë™ëª¨ë¸ì„ GPUë¡œ ì´ë™Pythonë³µì‚¬device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

â€‹í…ì„œë¥¼ GPUë¡œ ì´ë™Pythonë³µì‚¬inputs, targets = inputs.to(device), targets.to(device)

â€‹4. ëª¨ë¸ ê¸°ë²•ë³„ APIí•©ì„±ê³± ì‹ ê²½ë§ (CNN)torch.nn.Conv2d: 2D í•©ì„±ê³± ë ˆì´ì–´ì…ë‹ˆë‹¤.Pythonë³µì‚¬conv_layer = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)

â€‹ìˆœí™˜ ì‹ ê²½ë§ (RNN)torch.nn.RNN: ê¸°ë³¸ ìˆœí™˜ ì‹ ê²½ë§ ë ˆì´ì–´ì…ë‹ˆë‹¤.Pythonë³µì‚¬rnn_layer = nn.RNN(input_size=10, hidden_size=20, num_layers=2, batch_first=True)

â€‹torch.nn.LSTM: LSTM ë ˆì´ì–´ì…ë‹ˆë‹¤.Pythonë³µì‚¬lstm_layer = nn.LSTM(input_size=10, hidden_size=20, num_layers=2, batch_first=True)

â€‹torch.nn.GRU: GRU ë ˆì´ì–´ì…ë‹ˆë‹¤.Pythonë³µì‚¬gru_layer = nn.GRU(input_size=10, hidden_size=20, num_layers=2, batch_first=True)

â€‹íŠ¸ëœìŠ¤í¬ë¨¸ (Transformer)torch.nn.Transformer: íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì…ë‹ˆë‹¤.Pythonë³µì‚¬transformer_model = nn.Transformer(nhead=8, num_encoder_layers=6)

â€‹torch.nn.TransformerEncoderLayer: íŠ¸ëœìŠ¤í¬ë¨¸ ì¸ì½”ë” ë ˆì´ì–´ì…ë‹ˆë‹¤.Pythonë³µì‚¬encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)

â€‹5. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ì €ì¥ ë° ë¡œë“œëª¨ë¸ ì €ì¥Pythonë³µì‚¬torch.save(model.state_dict(), 'model.pth')

â€‹ëª¨ë¸ ë¡œë“œPythonë³µì‚¬model.load_state_dict(torch.load('model.pth'))
model.eval()

â€‹í•™ìŠµ ë° í‰ê°€ ëª¨ë“œ ì„¤ì •ëª¨ë¸ì„ í•™ìŠµ ëª¨ë“œë¡œ ì„¤ì •Pythonë³µì‚¬model.train()

â€‹ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •Pythonë³µì‚¬model.eval()

â€‹.
íŒë‹¤ìŠ¤ ì‹¬í™”: ë°ì´í„°í”„ë ˆì„ êµ¬ì¡°í™” ë‹¤ì‹œí•˜ê¸°ì™€ í¬ê¸° ì¡°ì •í•˜ê¸°ì¸ê³µì§€ëŠ¥ì„ ìœ„í•œ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬/ì¸ê³µì§€ëŠ¥ì„ ìœ„í•œ íŒŒì´ì¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ - 6ì£¼ì°¨/íŒë‹¤ìŠ¤ ì‹¬í™”: ë°ì´í„°í”„ë ˆì„ êµ¬ì¡°í™” ë‹¤ì‹œí•˜ê¸°ì™€ í¬ê¸° ì¡°ì •í•˜ê¸°ì œì‘:íŒë‹¤ìŠ¤ ì‹¬í™”: ë°ì´í„°í”„ë ˆì„ êµ¬ì¡°í™” ë‹¤ì‹œí•˜ê¸°ì™€ í¬ê¸° ì¡°ì •í•˜ê¸°ìˆ˜ì—… ëª©í‘œíŒë‹¤ìŠ¤ì—ì„œ ë°ì´í„°ë¥¼ ì›í•˜ëŠ” í˜•íƒœë¡œ ì¬êµ¬ì„±í•˜ê³  í¬ê¸°ë¥¼ ì¡°ì •í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ í•™ìŠµí•©ë‹ˆë‹¤.ëª©ì°¨ë°ì´í„°í”„ë ˆì„ êµ¬ì¡°í™” ë‹¤ì‹œí•˜ê¸°ë°ì´í„°í”„ë ˆì„ í¬ê¸° ì¡°ì •í•˜ê¸°ìš”ì•½ë°ì´í„°í”„ë ˆì„ êµ¬ì¡°í™” ë‹¤ì‹œí•˜ê¸°ë°ì´í„°í”„ë ˆì„ì˜ êµ¬ì¡°ë¥¼ ì¬ì¡°ì •í•˜ì—¬, ë°ì´í„°ë¥¼ ì›í•˜ëŠ” í˜•íƒœë¡œ ë³€í˜•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì£¼ìš” ì‘ì—…ìœ¼ë¡œëŠ” í”¼ë²—(pivot), ë³€ê²½(melt), ìŠ¤íƒ(stack)ê³¼ ì–¸ìŠ¤íƒ(unstack)ì´ ìˆìŠµë‹ˆë‹¤.pivot()ì„ ì‚¬ìš©í•œ í”¼ë²— í…Œì´ë¸” ìƒì„±pivot() í•¨ìˆ˜ëŠ” ì—´ ë°ì´í„°ë¥¼ í–‰ ë˜ëŠ” ì—´ë¡œ ì´ë™ì‹œì¼œ ìƒˆë¡œìš´ ë°ì´í„°í”„ë ˆì„ì„ ë§Œë“­ë‹ˆë‹¤. ì´ëŠ” ë°ì´í„°ë¥¼ ì¬êµ¬ì„±í•˜ê³  ë¶„ì„í•˜ëŠ” ë° ë§¤ìš° ìœ ìš©í•©ë‹ˆë‹¤.Pythonë³µì‚¬import pandas as pd

# ì˜ˆì‹œ ë°ì´í„°í”„ë ˆì„ ìƒì„±
data = {
'ë‚ ì§œ': ['2023-01-01', '2023-01-02', '2023-01-01', '2023-01-02'],
'ë„ì‹œ': ['ì„œìš¸', 'ì„œìš¸', 'ë¶€ì‚°', 'ë¶€ì‚°'],
'ì˜¨ë„': [2, 3, 6, 7],
'ìŠµë„': [55, 60, 80, 85]
}
df = pd.DataFrame(data)
# 'ë„ì‹œ'ë¥¼ ê¸°ì¤€ìœ¼ë¡œ 'ë‚ ì§œ'ë¥¼ ì¸ë±ìŠ¤ë¡œ, 'ì˜¨ë„'ë¥¼ ê°’ìœ¼ë¡œ í•˜ëŠ” í”¼ë²— í…Œì´ë¸” ìƒì„±
pivot_df = df.pivot(index='ë‚ ì§œ', columns='ë„ì‹œ', values='ì˜¨ë„')
print(pivot_df)

â€‹ê²°ê³¼:Plain Textë³µì‚¬ë„ì‹œ        ë¶€ì‚°  ì„œìš¸
ë‚ ì§œ
2023-01-01  6  2
2023-01-02  7  3

â€‹ë‚ ì§œë¥¼ ì¸ë±ìŠ¤ë¡œ, ë„ì‹œë¥¼ ì—´ë¡œ í•˜ì—¬ ê° ì˜¨ë„ ê°’ì„ í”¼ë²— í…Œì´ë¸”ë¡œ ë³€í™˜í–ˆìŠµë‹ˆë‹¤.melt()ë¥¼ ì‚¬ìš©í•œ ë°ì´í„° êµ¬ì¡° í•´ì²´melt() í•¨ìˆ˜ëŠ” í”¼ë²—ëœ ë°ì´í„°ë¥¼ ë‹¤ì‹œ ê¸´ í˜•ì‹(long format)ìœ¼ë¡œ ë³€í™˜í•  ë•Œ ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ëŠ” ì—¬ëŸ¬ ì—´ì„ í•˜ë‚˜ì˜ ì—´ë¡œ í†µí•©í•˜ëŠ” ë° ìœ ìš©í•©ë‹ˆë‹¤.Pythonë³µì‚¬# ë°ì´í„°í”„ë ˆì„ êµ¬ì¡° í•´ì²´ (melt)
melted_df = pd.melt(df, id_vars=['ë‚ ì§œ', 'ë„ì‹œ'], value_vars=['ì˜¨ë„', 'ìŠµë„'])
print(melted_df)

â€‹ê²°ê³¼:Plain Textë³µì‚¬          ë‚ ì§œ  ë„ì‹œ  variable  value
0  2023-01-01  ì„œìš¸     ì˜¨ë„      2
1  2023-01-02  ì„œìš¸     ì˜¨ë„      3
2  2023-01-01  ë¶€ì‚°     ì˜¨ë„      6
3  2023-01-02  ë¶€ì‚°     ì˜¨ë„      7
4  2023-01-01  ì„œìš¸     ìŠµë„     55
5  2023-01-02  ì„œìš¸     ìŠµë„     60
6  2023-01-01  ë¶€ì‚°     ìŠµë„     80
7  2023-01-02  ë¶€ì‚°     ìŠµë„     85

â€‹ì˜¨ë„ì™€ ìŠµë„ ì—´ì´ í•˜ë‚˜ì˜ ì—´(variable)ë¡œ í†µí•©ë˜ì–´ melt()ë˜ì—ˆìŠµë‹ˆë‹¤.stack()ê³¼ unstack()ì„ ì‚¬ìš©í•œ ë°ì´í„° ë³€í™˜stack()ì€ ì—´ ë°ì´í„°ë¥¼ ì¸ë±ìŠ¤ì˜ í•˜ìœ„ ë ˆë²¨ë¡œ ì´ë™ì‹œí‚¤ê³ , unstack()ì€ ê·¸ ë°˜ëŒ€ ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì´ëŠ” ë©€í‹° ì¸ë±ìŠ¤ ë°ì´í„°í”„ë ˆì„ì—ì„œ ìœ ìš©í•©ë‹ˆë‹¤.Pythonë³µì‚¬# 'ë„ì‹œ' ë ˆë²¨ì„ ì¸ë±ìŠ¤ë¡œ ìŠ¤íƒ(stack)
stacked_df = pivot_df.stack()
print(stacked_df)

â€‹ê²°ê³¼:Plain Textë³µì‚¬ë‚ ì§œ          ë„ì‹œ
2023-01-01  ë¶€ì‚°    6
            ì„œìš¸    2
2023-01-02  ë¶€ì‚°    7
            ì„œìš¸    3
dtype: int64

â€‹ë„ì‹œê°€ ì¸ë±ìŠ¤ë¡œ ë³€í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.Pythonë³µì‚¬# ë‹¤ì‹œ ì–¸ìŠ¤íƒ(unstack)í•˜ì—¬ ì›ë˜ êµ¬ì¡°ë¡œ ë³µì›
unstacked_df = stacked_df.unstack()
print(unstacked_df)

â€‹ê²°ê³¼ëŠ” ì›ë˜ì˜ í”¼ë²— í…Œì´ë¸” í˜•íƒœë¡œ ëŒì•„ì˜µë‹ˆë‹¤.ë°ì´í„°í”„ë ˆì„ í¬ê¸° ì¡°ì •í•˜ê¸°ë°ì´í„°í”„ë ˆì„ì˜ í¬ê¸°ë¥¼ ì¡°ì •í•˜ëŠ” ë°©ë²•ì—ëŠ” í–‰ê³¼ ì—´ì˜ ì¶”ê°€, ì‚­ì œ, ë°ì´í„° ë³‘í•© ë“±ì´ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê¸°ì— ì í•©í•œ í˜•íƒœë¡œ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.í–‰ê³¼ ì—´ ì¶”ê°€í•˜ê¸°ìƒˆë¡œìš´ ì—´ì„ ì¶”ê°€í•  ë•ŒëŠ” ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ í• ë‹¹í•˜ë©´ ë©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, df['ìƒˆë¡œìš´ ì—´'] = ê°’ì˜ í˜•íƒœë¡œ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.Pythonë³µì‚¬# ìƒˆë¡œìš´ ì—´ ì¶”ê°€
df['ë‚ ì”¨'] = ['ë§‘ìŒ', 'íë¦¼', 'ë§‘ìŒ', 'íë¦¼']
print(df)

â€‹ê²°ê³¼:Plain Textë³µì‚¬          ë‚ ì§œ  ë„ì‹œ  ì˜¨ë„  ìŠµë„  ë‚ ì”¨
0  2023-01-01  ì„œìš¸    2   55  ë§‘ìŒ
1  2023-01-02  ì„œìš¸    3   60  íë¦¼
2  2023-01-01  ë¶€ì‚°    6   80  ë§‘ìŒ
3  2023-01-02  ë¶€ì‚°    7   85  íë¦¼

â€‹ìƒˆë¡œìš´ ì—´(ë‚ ì”¨)ì´ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.í–‰ê³¼ ì—´ ì‚­ì œí•˜ê¸°í–‰ê³¼ ì—´ ì‚­ì œëŠ” drop() í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. axis=0ì€ í–‰ì„, axis=1ì€ ì—´ì„ ì‚­ì œí•©ë‹ˆë‹¤.Pythonë³µì‚¬# 'ìŠµë„' ì—´ ì‚­ì œ
df_dropped = df.drop(columns=['ìŠµë„'])
print(df_dropped)

â€‹ê²°ê³¼:Plain Textë³µì‚¬          ë‚ ì§œ  ë„ì‹œ  ì˜¨ë„  ë‚ ì”¨
0  2023-01-01  ì„œìš¸    2  ë§‘ìŒ
1  2023-01-02  ì„œìš¸    3  íë¦¼
2  2023-01-01  ë¶€ì‚°    6  ë§‘ìŒ
3  2023-01-02  ë¶€ì‚°    7  íë¦¼

â€‹ìŠµë„ ì—´ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.Pythonë³µì‚¬# íŠ¹ì • í–‰ ì‚­ì œ (ì˜ˆ: ì²« ë²ˆì§¸ í–‰)
df_dropped_row = df.drop(index=0)
print(df_dropped_row)

â€‹ê²°ê³¼:Plain Textë³µì‚¬          ë‚ ì§œ  ë„ì‹œ  ì˜¨ë„  ìŠµë„  ë‚ ì”¨
1  2023-01-02  ì„œìš¸    3   60  íë¦¼
2  2023-01-01  ë¶€ì‚°    6   80  ë§‘ìŒ
3  2023-01-02  ë¶€ì‚°    7   85  íë¦¼

â€‹ì²« ë²ˆì§¸ í–‰ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.ë°ì´í„° ë³‘í•©í•˜ê¸°ë°ì´í„°í”„ë ˆì„ì„ ë³‘í•©í•  ë•ŒëŠ” concat()ê³¼ merge()ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ì´ë“¤ì€ ë°ì´í„°ë¥¼ ì¶”ê°€í•˜ê±°ë‚˜ ë³‘í•©í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.Pythonë³µì‚¬# ìƒˆë¡œìš´ ë°ì´í„°í”„ë ˆì„ ìƒì„±
data2 = {
'ë‚ ì§œ': ['2023-01-03', '2023-01-04'],
'ë„ì‹œ': ['ì„œìš¸', 'ë¶€ì‚°'],
'ì˜¨ë„': [5, 8],
'ìŠµë„': [70, 75],
'ë‚ ì”¨': ['ë§‘ìŒ', 'íë¦¼']
}
df2 = pd.DataFrame(data2)
# í–‰ì„ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„°í”„ë ˆì„ ë³‘í•© (concat)
merged_df = pd.concat([df, df2], ignore_index=True)
print(merged_df)

â€‹ê²°ê³¼:Plain Textë³µì‚¬          ë‚ ì§œ  ë„ì‹œ  ì˜¨ë„  ìŠµë„  ë‚ ì”¨
0  2023-01-01  ì„œìš¸    2   55  ë§‘ìŒ
1  2023-01-02  ì„œìš¸    3   60  íë¦¼
2  2023-01-01  ë¶€ì‚°    6   80  ë§‘ìŒ
3  2023-01-02  ë¶€ì‚°    7   85  íë¦¼
4  2023-01-03  ì„œìš¸    5   70  ë§‘ìŒ
5  2023-01-04  ë¶€ì‚°    8   75  íë¦¼

â€‹ìƒˆë¡œìš´ ë°ì´í„°ê°€ ë³‘í•©ë˜ì–´ ë°ì´í„°í”„ë ˆì„ì˜ í¬ê¸°ê°€ í™•ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.ìš”ì•½ìš”ì•½ë°ì´í„°í”„ë ˆì„ êµ¬ì¡°í™” ë‹¤ì‹œí•˜ê¸°ì—ëŠ” pivot(), melt(), stack(), unstack() ë“±ì´ ì‚¬ìš©ë˜ë©°, ì´ë¥¼ í†µí•´ ë°ì´í„°ë¥¼ ë‹¤ì–‘í•œ ë°©ì‹ìœ¼ë¡œ ë³€í˜•í•˜ê³  ì¬êµ¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.í¬ê¸° ì¡°ì •ì—ëŠ” ìƒˆë¡œìš´ í–‰ê³¼ ì—´ì„ ì¶”ê°€í•˜ê±°ë‚˜, drop()ì„ ì‚¬ìš©í•´ ì‚­ì œí•˜ëŠ” ë°©ë²•ì´ ìˆìœ¼ë©°, concat()ê³¼ merge()ë¥¼ í†µí•´ ë°ì´í„°ë¥¼ ë³‘í•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ì´ëŸ¬í•œ ê¸°ëŠ¥ë“¤ì€ ë°ì´í„°ë¥¼ ë” ì˜ ë¶„ì„í•  ìˆ˜ ìˆë„ë¡ ì ì ˆí•œ í˜•íƒœë¡œ ë³€í™˜í•˜ê³  ì¡°ì •í•˜ëŠ” ë° í•„ìˆ˜ì ì…ë‹ˆë‹¤.ì´ì œ íŒë‹¤ìŠ¤ë¥¼ í™œìš©í•˜ì—¬ ë°ì´í„°í”„ë ˆì„ì„ ì›í•˜ëŠ” í˜•íƒœë¡œ êµ¬ì¡°í™”í•˜ê³  í¬ê¸°ë¥¼ ì¡°ì •í•˜ì—¬ ë¶„ì„ì„ ë”ìš± íš¨ê³¼ì ìœ¼ë¡œ ì§„í–‰í•´ë³´ì„¸ìš”! â€‹
[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 17ê°•. ë¹„ì§€ë„í•™ìŠµ : ì°¨ì›ì¶•ì†Œ - PCA [SCC] ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ - 4ì£¼ì°¨ /[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 17ê°•. ë¹„ì§€ë„í•™ìŠµ : ì°¨ì›ì¶•ì†Œ - PCA ì œì‘:[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 17ê°•. ë¹„ì§€ë„í•™ìŠµ : ì°¨ì›ì¶•ì†Œ - PCA [ìˆ˜ì—… ëª©í‘œ]ë¹„ì§€ë„í•™ìŠµ ì°¨ì›ì¶•ì†Œ ì¤‘ PCA ì— ëŒ€í•œ ê°œë…ì„ ë°°ìš°ê³ , ë°ì´í„°ë¥¼ ì´ìš©í•´ ì‹¤ìŠµí•´ ë´…ë‹ˆë‹¤[ëª©ì°¨]01. PCA ê°œë…02. PCA ì‹¤ìŠµ
 
 01. PCA ê°œë…PCAê°€ ë¬´ì—‡ì¸ì§€ ì•Œì•„ë´…ì‹œë‹¤1) PCA PCAë€?PCA(Principal Component Analysis, ì£¼ì„±ë¶„ ë¶„ì„)ëŠ” ê³ ì°¨ì› ë°ì´í„°ë¥¼ ì €ì°¨ì›ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì°¨ì› ì¶•ì†Œ ê¸°ë²•ì…ë‹ˆë‹¤PCAëŠ” ë°ì´í„°ì˜ ë¶„ì‚°ì„ ìµœëŒ€í•œ ë³´ì¡´í•˜ë©´ì„œ, ë°ì´í„°ì˜ ì£¼ìš” íŠ¹ì§•ì„ ì¶”ì¶œí•´ ì €ì°¨ì› ê³µê°„ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤ë°ì´í„°ì˜ ì‹œê°í™”, ë…¸ì´ì¦ˆ ì œê±°, ê³„ì‚° íš¨ìœ¨ì„± í–¥ìƒ ë“±ì˜ ì´ì ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.ALT PCAì˜ ì‘ë™ ì›ë¦¬ë°ì´í„° í‘œì¤€í™”: ê° íŠ¹ì„±ì˜ í‰ê· ì„ 0, ë¶„ì‚°ì„ 1ë¡œ ë§ì¶¥ë‹ˆë‹¤.ê³µë¶„ì‚° í–‰ë ¬ ê³„ì‚°: ë°ì´í„°ì˜ ê³µë¶„ì‚° í–‰ë ¬ì„ ê³„ì‚°í•©ë‹ˆë‹¤.ê³ ìœ ê°’ ë° ê³ ìœ ë²¡í„° ê³„ì‚°: ê³µë¶„ì‚° í–‰ë ¬ì˜ ê³ ìœ ê°’ê³¼ ê³ ìœ ë²¡í„°ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.ì£¼ì„±ë¶„ ì„ íƒ: ê³ ìœ ê°’ì´ í° ìˆœì„œëŒ€ë¡œ ê³ ìœ ë²¡í„°ë¥¼ ì •ë ¬í•˜ì—¬ ì£¼ì„±ë¶„ì„ ì„ íƒí•©ë‹ˆë‹¤.ë°ì´í„° ë³€í™˜: ì„ íƒëœ ì£¼ì„±ë¶„ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ ì €ì°¨ì› ê³µê°„ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.2) ê³µë¶„ì‚° í–‰ë ¬ ë° ì£¼ì„±ë¶„ ì„ íƒ ê³µë¶„ì‚° í–‰ë ¬ê³µë¶„ì‚° í–‰ë ¬ì€ ë°ì´í„°ì˜ ê° íŠ¹ì„± ê°„ì˜ ê³µë¶„ì‚°ì„ ë‚˜íƒ€ë‚´ëŠ” í–‰ë ¬ì…ë‹ˆë‹¤. ê³µë¶„ì‚° í–‰ë ¬ì„ í†µí•´ ë°ì´í„°ì˜ ë¶„ì‚°ê³¼ íŠ¹ì„± ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì£¼ì„±ë¶„ ì„ íƒê³ ìœ ê°’ì´ í° ìˆœì„œëŒ€ë¡œ ê³ ìœ ë²¡í„°ë¥¼ ì •ë ¬í•˜ì—¬ ì£¼ì„±ë¶„ì„ ì„ íƒí•©ë‹ˆë‹¤.ê³ ìœ ê°’ì´ í´ìˆ˜ë¡ í•´ë‹¹ ì£¼ì„±ë¶„ì´ ë°ì´í„°ì˜ ë¶„ì‚°ì„ ë” ë§ì´ ì„¤ëª…í•©ë‹ˆë‹¤.ì¼ë°˜ì ìœ¼ë¡œ, ì „ì²´ ë¶„ì‚°ì˜ 95% ì´ìƒì„ ì„¤ëª…í•˜ëŠ” ì£¼ì„±ë¶„ì„ ì„ íƒí•©ë‹ˆë‹¤.02. PCA ì‹¤ìŠµMNIST ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•œ PCA ì‹¤ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤1) MNIST ë°ì´í„°ì…‹ ë°ì´í„° ë¡œë“œ MNIST ë°ì´í„° ë¡œë“œ {5px}MNIST ë°ì´í„° ë¡œë“œ ï»¿â€‹Pythonë³µì‚¬from sklearn.datasets import fetch_openml
import pandas as pd

# MNIST ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°
mnist = fetch_openml('mnist_784', version=1)
# ë°ì´í„°ì™€ ë ˆì´ë¸” ë¶„ë¦¬
X = mnist.data
y = mnist.target

# ë°ì´í„° í”„ë ˆì„ì˜ ì²« 5í–‰ ì¶œë ¥
print(X.head())
print(y.head())
â€‹ ë°ì´í„° í‘œì¤€í™”PCAë¥¼ ìˆ˜í–‰í•˜ê¸° ì „ì— ë°ì´í„°ë¥¼ í‘œì¤€í™”í•©ë‹ˆë‹¤.ë°ì´í„° í‘œì¤€í™” {5px}ë°ì´í„° í‘œì¤€í™” ï»¿â€‹Pythonë³µì‚¬from sklearn.preprocessing import StandardScaler

# ë°ì´í„° í‘œì¤€í™”
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
â€‹ PCA ìˆ˜í–‰Scikit-learnì˜ PCAë¥¼ ì‚¬ìš©í•˜ì—¬ PCAë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.PCAìˆ˜í–‰ {5px}PCAìˆ˜í–‰ ï»¿â€‹Pythonë³µì‚¬from sklearn.decomposition import PCA

# PCA ëª¨ë¸ ìƒì„±
pca = PCA(n_components=0.95) # ì „ì²´ ë¶„ì‚°ì˜ 95%ë¥¼ ì„¤ëª…í•˜ëŠ” ì£¼ì„±ë¶„ ì„ íƒ
# PCA í•™ìŠµ ë° ë³€í™˜
X_pca = pca.fit_transform(X_scaled)
# ë³€í™˜ëœ ë°ì´í„°ì˜ í¬ê¸° í™•ì¸
print(X_pca.shape)
â€‹ ì£¼ì„±ë¶„ í™•ì¸ì„ íƒëœ ì£¼ì„±ë¶„ì˜ ìˆ˜ì™€ ê° ì£¼ì„±ë¶„ì´ ì„¤ëª…í•˜ëŠ” ë¶„ì‚° ë¹„ìœ¨ì„ í™•ì¸í•©ë‹ˆë‹¤.ì£¼ì„±ë¶„ í™•ì¸ {5px}ì£¼ì„±ë¶„ í™•ì¸ ï»¿â€‹Pythonë³µì‚¬# ì„ íƒëœ ì£¼ì„±ë¶„ì˜ ìˆ˜
print(f'ì„ íƒëœ ì£¼ì„±ë¶„ì˜ ìˆ˜: {pca.n_components_}')
# ê° ì£¼ì„±ë¶„ì´ ì„¤ëª…í•˜ëŠ” ë¶„ì‚° ë¹„ìœ¨
print(f'ê° ì£¼ì„±ë¶„ì´ ì„¤ëª…í•˜ëŠ” ë¶„ì‚° ë¹„ìœ¨: {pca.explained_variance_ratio_}')
# ëˆ„ì  ë¶„ì‚° ë¹„ìœ¨
print(f'ëˆ„ì  ë¶„ì‚° ë¹„ìœ¨: {pca.explained_variance_ratio_.cumsum()}')
â€‹ PCA ê²°ê³¼ ì‹œê°í™”PCA ê²°ê³¼ë¥¼ 2ì°¨ì› ë˜ëŠ” 3ì°¨ì›ìœ¼ë¡œ ì‹œê°í™”í•©ë‹ˆë‹¤.PCA ê²°ê³¼ ì‹œê°í™” {5px}PCA ê²°ê³¼ ì‹œê°í™” ï»¿â€‹Pythonë³µì‚¬import matplotlib.pyplot as plt
import seaborn as sns

# 2ì°¨ì› ì‹œê°í™”
plt.figure(figsize=(10, 7))
sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=y, palette='viridis', legend=None)
plt.title('PCA of MNIST Dataset (2D)')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.show()

â€‹.
[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 18ê°•. ë¹„ì§€ë„í•™ìŠµ : ì°¨ì›ì¶•ì†Œ - t-SNE[SCC] ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ - 4ì£¼ì°¨ /[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 18ê°•. ë¹„ì§€ë„í•™ìŠµ : ì°¨ì›ì¶•ì†Œ - t-SNEì œì‘:[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 18ê°•. ë¹„ì§€ë„í•™ìŠµ : ì°¨ì›ì¶•ì†Œ - t-SNE[ìˆ˜ì—… ëª©í‘œ]ë¹„ì§€ë„í•™ìŠµ ì°¨ì›ì¶•ì†Œ ì¤‘ t-SNE ì— ëŒ€í•œ ê°œë…ì„ ë°°ìš°ê³ , ë°ì´í„°ë¥¼ ì´ìš©í•´ ì‹¤ìŠµí•´ ë´…ë‹ˆë‹¤[ëª©ì°¨]01. t-SNE ê°œë…02. t-SNE ì‹¤ìŠµğŸ’¡
 
 01. t-SNE ê°œë…âœ”ï¸t-SNEê°€ ë¬´ì—‡ì¸ì§€ ì•Œì•„ë´…ì‹œë‹¤1) t-SNE t-SNEë€?t-SNE(t-Distributed Stochastic Neighbor Embedding)ëŠ” ê³ ì°¨ì› ë°ì´í„°ë¥¼ ì €ì°¨ì›ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì‹œê°í™”í•˜ëŠ” ì°¨ì› ì¶•ì†Œ ê¸°ë²•ì…ë‹ˆë‹¤.ë°ì´í„° í¬ì¸íŠ¸ ê°„ì˜ ìœ ì‚¬ì„±ì„ ë³´ì¡´í•˜ë©´ì„œ, ê³ ì°¨ì› ë°ì´í„°ë¥¼ 2ì°¨ì› ë˜ëŠ” 3ì°¨ì› ê³µê°„ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤ë°ì´í„°ì˜ êµ¬ì¡°ì™€ íŒ¨í„´ì„ ì‹œê°ì ìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. t-SNEì˜ ì‘ë™ ì›ë¦¬ê³ ì°¨ì› ê³µê°„ì—ì„œì˜ ìœ ì‚¬ì„± ê³„ì‚°: ê³ ì°¨ì› ë°ì´í„° í¬ì¸íŠ¸ ê°„ì˜ ìœ ì‚¬ì„±ì„ í™•ë¥  ë¶„í¬ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤.ì €ì°¨ì› ê³µê°„ì—ì„œì˜ ìœ ì‚¬ì„± ê³„ì‚°: ì €ì°¨ì› ë°ì´í„° í¬ì¸íŠ¸ ê°„ì˜ ìœ ì‚¬ì„±ì„ t-ë¶„í¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ê³„ì‚°í•©ë‹ˆë‹¤.KL ë°œì‚° ìµœì†Œí™”: ê³ ì°¨ì› ê³µê°„ê³¼ ì €ì°¨ì› ê³µê°„ ê°„ì˜ ìœ ì‚¬ì„± ë¶„í¬ ì°¨ì´ë¥¼ KL ë°œì‚°(Kullback-Leibler divergence)ì„ í†µí•´ ìµœì†Œí™”í•©ë‹ˆë‹¤.ë°˜ë³µì  ìµœì í™”: ì €ì°¨ì› ê³µê°„ì—ì„œì˜ ë°ì´í„° í¬ì¸íŠ¸ ìœ„ì¹˜ë¥¼ ë°˜ë³µì ìœ¼ë¡œ ì¡°ì •í•˜ì—¬ ìµœì ì˜ ì‹œê°í™”ë¥¼ ì–»ìŠµë‹ˆë‹¤. t-SNEì˜ ì¥ì ë¹„ì„ í˜• êµ¬ì¡° íƒì§€: t-SNEëŠ” ë¹„ì„ í˜• êµ¬ì¡°ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ íƒì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.í´ëŸ¬ìŠ¤í„° ì‹œê°í™”: t-SNEëŠ” ë°ì´í„°ì˜ í´ëŸ¬ìŠ¤í„°ë¥¼ ëª…í™•í•˜ê²Œ ì‹œê°í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ê³ ì°¨ì› ë°ì´í„° ì‹œê°í™”: t-SNEëŠ” ê³ ì°¨ì› ë°ì´í„°ë¥¼ 2ì°¨ì› ë˜ëŠ” 3ì°¨ì›ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì‹œê°í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.02. t-SNE ì‹¤ìŠµâœ”ï¸MNIST ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•œ t-SNE ì‹¤ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤1) MNIST ë°ì´í„°ì…‹ ë°ì´í„° ë¡œë“œ MNIST ë°ì´í„° ë¡œë“œ {5px}MNIST ë°ì´í„° ë¡œë“œ ï»¿â€‹Pythonë³µì‚¬from sklearn.datasets import fetch_openml
import pandas as pd

# MNIST ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°
mnist = fetch_openml('mnist_784', version=1)
# ë°ì´í„°ì™€ ë ˆì´ë¸” ë¶„ë¦¬
X = mnist.data
y = mnist.target

# ë°ì´í„° í”„ë ˆì„ì˜ ì²« 5í–‰ ì¶œë ¥
print(X.head())
print(y.head())
â€‹ ë°ì´í„° í‘œì¤€í™”ë°ì´í„° í‘œì¤€í™” {5px}ë°ì´í„° í‘œì¤€í™” ï»¿â€‹Pythonë³µì‚¬from sklearn.preprocessing import StandardScaler

# ë°ì´í„° í‘œì¤€í™”
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
â€‹ t-SNE ìˆ˜í–‰Scikit-learnì˜ TSNEë¥¼ ì‚¬ìš©í•˜ì—¬ t-SNEë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.t-SNEìˆ˜í–‰ {5px}t-SNEìˆ˜í–‰ ï»¿â€‹Pythonë³µì‚¬from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import seaborn as sns

# t-SNE ëª¨ë¸ ìƒì„±
tsne = TSNE(n_components=2, random_state=42)
# t-SNE í•™ìŠµ ë° ë³€í™˜
X_tsne = tsne.fit_transform(X_scaled)
# ë³€í™˜ëœ ë°ì´í„°ì˜ í¬ê¸° í™•ì¸
print(X_tsne.shape)
â€‹ t-SNE ê²°ê³¼ ì‹œê°í™”t-SNE ê²°ê³¼ë¥¼ 2ì°¨ì›ìœ¼ë¡œ ì‹œê°í™”í•©ë‹ˆë‹¤.t-SNE ê²°ê³¼ ì‹œê°í™” {5px}t-SNE ê²°ê³¼ ì‹œê°í™” ï»¿â€‹Pythonë³µì‚¬# 2ì°¨ì› ì‹œê°í™”
plt.figure(figsize=(10, 7))
sns.scatterplot(x=X_tsne[:, 0], y=X_tsne[:, 1], hue=y, palette='viridis', legend=None)
plt.title('t-SNE of MNIST Dataset (2D)')
plt.xlabel('t-SNE Component 1')
plt.ylabel('t-SNE Component 2')
plt.show()
â€‹.t-SNEëŠ” ë°ì´í„° í¬ì¸íŠ¸ ê°„ì˜ ìœ ì‚¬ì„±ì„ ë³´ì¡´í•˜ë©´ì„œ, ê³ ì°¨ì› ë°ì´í„°ë¥¼ 2ì°¨ì› ë˜ëŠ” 3ì°¨ì› ê³µê°„ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤
[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 19ê°•. ë¹„ì§€ë„í•™ìŠµ : ì°¨ì›ì¶•ì†Œ - LDA[SCC] ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ - 4ì£¼ì°¨ /[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 19ê°•. ë¹„ì§€ë„í•™ìŠµ : ì°¨ì›ì¶•ì†Œ - LDAì œì‘:[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 19ê°•. ë¹„ì§€ë„í•™ìŠµ : ì°¨ì›ì¶•ì†Œ - LDA[ìˆ˜ì—… ëª©í‘œ]ë¹„ì§€ë„í•™ìŠµ ì°¨ì›ì¶•ì†Œ ì¤‘ LDAì— ëŒ€í•œ ê°œë…ì„ ë°°ìš°ê³ , ë°ì´í„°ë¥¼ ì´ìš©í•´ ì‹¤ìŠµí•´ ë´…ë‹ˆë‹¤[ëª©ì°¨]01. LDA ê°œë…02. LDAì‹¤ìŠµğŸ’¡
 
 01. LDA ê°œë…âœ”ï¸LDAê°€ ë¬´ì—‡ì¸ì§€ ì•Œì•„ë´…ì‹œë‹¤1) LDA LDAë€?LDA(Linear Discriminant Analysis, ì„ í˜• íŒë³„ ë¶„ì„)ëŠ” ì°¨ì› ì¶•ì†Œì™€ ë¶„ë¥˜ë¥¼ ë™ì‹œì— ìˆ˜í–‰í•©ë‹ˆë‹¤.LDAëŠ” ë°ì´í„°ì˜ í´ë˜ìŠ¤ ê°„ ë¶„ì‚°ì„ ìµœëŒ€í™”í•˜ê³ , í´ë˜ìŠ¤ ë‚´ ë¶„ì‚°ì„ ìµœì†Œí™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ë°ì´í„°ë¥¼ ë³€í™˜í•©ë‹ˆë‹¤ë°ì´í„°ì˜ ë¶„ë¥˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê³ , ì €ì°¨ì› ê³µê°„ì—ì„œ ë°ì´í„°ì˜ êµ¬ì¡°ë¥¼ ì‹œê°í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ALT LDAì˜ ì‘ë™ ì›ë¦¬í´ë˜ìŠ¤ë³„ í‰ê·  ê³„ì‚°: ê° í´ë˜ìŠ¤ì˜ í‰ê·  ë²¡í„°ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.í´ë˜ìŠ¤ ë‚´ ë¶„ì‚° í–‰ë ¬ ê³„ì‚°: ê° í´ë˜ìŠ¤ ë‚´ ë°ì´í„° í¬ì¸íŠ¸ì˜ ë¶„ì‚°ì„ ê³„ì‚°í•´ í´ë˜ìŠ¤ ë‚´ ë¶„ì‚° í–‰ë ¬ì„ ë§Œë“­ë‹ˆë‹¤.í´ë˜ìŠ¤ ê°„ ë¶„ì‚° í–‰ë ¬ ê³„ì‚°: í´ë˜ìŠ¤ ê°„ í‰ê·  ë²¡í„°ì˜ ë¶„ì‚°ì„ ê³„ì‚°í•˜ì—¬ í´ë˜ìŠ¤ ê°„ ë¶„ì‚° í–‰ë ¬ì„ ë§Œë“­ë‹ˆë‹¤.ê³ ìœ ê°’ ë° ê³ ìœ ë²¡í„° ê³„ì‚°: í´ë˜ìŠ¤ ë‚´ ë¶„ì‚° í–‰ë ¬ì˜ ì—­í–‰ë ¬ê³¼ í´ë˜ìŠ¤ ê°„ ë¶„ì‚° í–‰ë ¬ì˜ ê³±ì˜ ê³ ìœ ê°’ê³¼ ê³ ìœ ë²¡í„°ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.ì„ í˜• íŒë³„ ì¶• ì„ íƒ: ê³ ìœ ê°’ì´ í° ìˆœì„œëŒ€ë¡œ ê³ ìœ ë²¡í„°ë¥¼ ì •ë ¬í•˜ì—¬ ì„ í˜• íŒë³„ ì¶•ì„ ì„ íƒí•©ë‹ˆë‹¤.ë°ì´í„° ë³€í™˜: ì„ íƒëœ ì„ í˜• íŒë³„ ì¶•ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ë¥¼ ì €ì°¨ì› ê³µê°„ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ì„ í˜• íŒë³„ ì¶• ì„ íƒê³ ìœ ê°’ì´ í° ìˆœì„œëŒ€ë¡œ ê³ ìœ ë²¡í„°ë¥¼ ì •ë ¬í•˜ì—¬ ì„ í˜• íŒë³„ ì¶•ì„ ì„ íƒí•©ë‹ˆë‹¤ê³ ìœ ê°’ì´ í´ìˆ˜ë¡ í•´ë‹¹ ì„ í˜• íŒë³„ ì¶•ì´ í´ë˜ìŠ¤ ê°„ ë¶„ì‚°ì„ ë” ë§ì´ ì„¤ëª…í•©ë‹ˆë‹¤ì¼ë°˜ì ìœ¼ë¡œ, í´ë˜ìŠ¤ì˜ ìˆ˜ - 1 ë§Œí¼ì˜ ì„ í˜• íŒë³„ ì¶•ì„ ì„ íƒí•©ë‹ˆë‹¤02. LDAì‹¤ìŠµâœ”ï¸MNIST ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•œ LDA ì‹¤ìŠµì„ ì§„í–‰í•©ë‹ˆë‹¤1) MNIST ë°ì´í„°ì…‹ ë°ì´í„° ë¡œë“œ MNIST ë°ì´í„° ë¡œë“œ {5px}MNIST ë°ì´í„° ë¡œë“œ ï»¿â€‹Pythonë³µì‚¬from sklearn.datasets import fetch_openml
import pandas as pd

# MNIST ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°
mnist = fetch_openml('mnist_784', version=1)
# ë°ì´í„°ì™€ ë ˆì´ë¸” ë¶„ë¦¬
X = mnist.data
y = mnist.target

# ë°ì´í„° í”„ë ˆì„ì˜ ì²« 5í–‰ ì¶œë ¥
print(X.head())
print(y.head())
â€‹ ë°ì´í„° í‘œì¤€í™”ë°ì´í„° í‘œì¤€í™” {5px}ë°ì´í„° í‘œì¤€í™” ï»¿â€‹Pythonë³µì‚¬from sklearn.preprocessing import StandardScaler

# ë°ì´í„° í‘œì¤€í™”
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
â€‹ LDA ìˆ˜í–‰Scikit-learnì˜ LinearDiscriminantAnalysisë¥¼ ì‚¬ìš©í•˜ì—¬ LDAë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.LDA ìˆ˜í–‰ {5px}LDA ìˆ˜í–‰ ï»¿â€‹Pythonë³µì‚¬from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

# LDA ëª¨ë¸ ìƒì„±
lda = LinearDiscriminantAnalysis(n_components=9) # í´ë˜ìŠ¤ì˜ ìˆ˜ - 1 ë§Œí¼ì˜ ì„ í˜• íŒë³„ ì¶• ì„ íƒ
# LDA í•™ìŠµ ë° ë³€í™˜
X_lda = lda.fit_transform(X_scaled, y)
# ë³€í™˜ëœ ë°ì´í„°ì˜ í¬ê¸° í™•ì¸
print(X_lda.shape)
â€‹ LDA ê²°ê³¼ ì‹œê°í™”LDA ê²°ê³¼ë¥¼ 2ì°¨ì› ë˜ëŠ” 3ì°¨ì›ìœ¼ë¡œ ì‹œê°í™”í•©ë‹ˆë‹¤.LDA ê²°ê³¼ ì‹œê°í™” {5px}LDA ê²°ê³¼ ì‹œê°í™” ï»¿â€‹Pythonë³µì‚¬import matplotlib.pyplot as plt
import seaborn as sns

# 2ì°¨ì› ì‹œê°í™”
plt.figure(figsize=(10, 7))
sns.scatterplot(x=X_lda[:, 0], y=X_lda[:, 1], hue=y, palette='viridis', legend=None)
plt.title('LDA of MNIST Dataset (2D)')
plt.xlabel('LDA Component 1')
plt.ylabel('LDA Component 2')
plt.show()

â€‹.
[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 20ê°•. ì•™ìƒë¸” í•™ìŠµ - ë°°ê¹…ê³¼ ë¶€ìŠ¤íŒ…[SCC] ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ - 5ì£¼ì°¨/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 20ê°•. ì•™ìƒë¸” í•™ìŠµ - ë°°ê¹…ê³¼ ë¶€ìŠ¤íŒ…ì œì‘:[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 20ê°•. ì•™ìƒë¸” í•™ìŠµ - ë°°ê¹…ê³¼ ë¶€ìŠ¤íŒ…[ìˆ˜ì—… ëª©í‘œ]ì•™ìƒë¸” í•™ìŠµì´ë€ ë¬´ì—‡ì¸ì§€ ì•Œì•„ë³´ê³ , ì•™ìƒë¸” í•™ìŠµì˜ ë°°ê¹…ê³¼ ë¶€ìŠ¤íŒ…ì— ëŒ€í•´ ë°°ì›Œë´…ì‹œë‹¤[ëª©ì°¨]01. ì•™ìƒë¸” í•™ìŠµ02.ë°°ê¹…ê³¼ ë¶€ìŠ¤íŒ… ëª¨ë¸ êµ¬í˜„ ë° í‰ê°€
 
 01. ì•™ìƒë¸” í•™ìŠµì•™ìƒë¸” í•™ìŠµì´ë€ ë¬´ì—‡ì¸ì§€ ë°°ìš°ê³ ,  ì•™ìƒë¸” í•™ìŠµì˜ ê¸°ë²•ì¸ ë°°ê¹…ê³¼ ë¶€ìŠ¤íŒ…ì— ëŒ€í•´ ì•Œì•„ ë´…ì‹œë‹¤1) ì•™ìƒë¸” í•™ìŠµ ì•™ìƒë¸” í•™ìŠµì´ë€?ì•™ìƒë¸” í•™ìŠµ(Ensemble Learning)ì€ ì—¬ëŸ¬ ê°œì˜ í•™ìŠµ ëª¨ë¸ì„ ê²°í•©í•˜ì—¬ í•˜ë‚˜ì˜ ê°•ë ¥í•œ ëª¨ë¸ì„ ë§Œë“œëŠ” ê¸°ë²•ì…ë‹ˆë‹¤ì•™ìƒë¸” í•™ìŠµì€ ê°œë³„ ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ê²°í•©í•¨ìœ¼ë¡œì¨, ë‹¨ì¼ ëª¨ë¸ë³´ë‹¤ ë” ë†’ì€ ì˜ˆì¸¡ ì„±ëŠ¥ê³¼ ì¼ë°˜í™” ëŠ¥ë ¥ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.ì•™ìƒë¸” í•™ìŠµì˜ ì£¼ìš” ê¸°ë²•ìœ¼ë¡œëŠ” ë°°ê¹…(Bagging)ê³¼ ë¶€ìŠ¤íŒ…(Boosting)ì´ ìˆìŠµë‹ˆë‹¤.ALT2) ë°°ê¹… : ë‹¤ìˆ˜ê²° ì›ë¦¬ ë°°ê¹…ì´ë€?ë°°ê¹…(Bootstrap Aggregating)ì€ ì—¬ëŸ¬ ê°œì˜ í•™ìŠµ ëª¨ë¸ì„ ë³‘ë ¬ë¡œ í•™ìŠµì‹œí‚¤ê³ , ê·¸ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í‰ê·  ë˜ëŠ” ë‹¤ìˆ˜ê²°ë¡œ ê²°í•©í•˜ëŠ” ì•™ìƒë¸” ê¸°ë²•ì…ë‹ˆë‹¤.ë°°ê¹…ì€ ë°ì´í„°ì˜ ìƒ˜í”Œë§ ê³¼ì •ì—ì„œ ë¶€íŠ¸ìŠ¤íŠ¸ë˜í•‘(bootstrap) ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬, ì›ë³¸ ë°ì´í„°ì…‹ì—ì„œ ì¤‘ë³µì„ í—ˆìš©í•œ ë¬´ì‘ìœ„ ìƒ˜í”Œì„ ìƒì„±í•©ë‹ˆë‹¤. ê° ëª¨ë¸ì€ ì„œë¡œ ë‹¤ë¥¸ ë°ì´í„° ìƒ˜í”Œì„ í•™ìŠµí•˜ê²Œ ë˜ì–´, ëª¨ë¸ ê°„ì˜ ìƒê´€ì„±ì„ ì¤„ì´ê³  ì˜ˆì¸¡ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ë°°ê¹…ì˜ ì¥ì ê³¼ì í•© ê°ì†Œ: ì—¬ëŸ¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ì„ ê²°í•©í•¨ìœ¼ë¡œì¨ ê³¼ì í•©ì„ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.ì•ˆì •ì„± í–¥ìƒ: ë°ì´í„°ì˜ ë³€ë™ì— ëœ ë¯¼ê°í•´ì§‘ë‹ˆë‹¤.ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥: ê° ëª¨ë¸ì„ ë…ë¦½ì ìœ¼ë¡œ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆì–´ ë³‘ë ¬ ì²˜ë¦¬ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.3) ë¶€ìŠ¤íŒ…: ì•½í•œ í•™ìŠµê¸°ë¥¼ ê²°í•©í•œ ê°•í•œ í•™ìŠµê¸° ë¶€ìŠ¤íŒ…ì´ë€?ë¶€ìŠ¤íŒ…(Boosting)ì€ ì—¬ëŸ¬ ê°œì˜ ì•½í•œ í•™ìŠµê¸°(weak learner)ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ í•™ìŠµì‹œí‚¤ê³ , ê·¸ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ê²°í•©í•˜ì—¬ ê°•í•œ í•™ìŠµê¸°(strong learner)ë¥¼ ë§Œë“œëŠ” ì•™ìƒë¸” ê¸°ë²•ì…ë‹ˆë‹¤ë¶€ìŠ¤íŒ…ì€ ì´ì „ ëª¨ë¸ì´ ì˜ëª» ì˜ˆì¸¡í•œ ë°ì´í„° í¬ì¸íŠ¸ì— ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ì—¬, ë‹¤ìŒ ëª¨ë¸ì´ ì´ë¥¼ ë” ì˜ í•™ìŠµí•˜ë„ë¡ í•©ë‹ˆë‹¤.  ë¶€ìŠ¤íŒ…ì˜ ì¥ì ë†’ì€ ì˜ˆì¸¡ ì„±ëŠ¥: ì•½í•œ í•™ìŠµê¸°ë¥¼ ê²°í•©í•˜ì—¬ ë†’ì€ ì˜ˆì¸¡ ì„±ëŠ¥ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.ê³¼ì í•© ë°©ì§€: ëª¨ë¸ì˜ ë³µì¡ë„ë¥¼ ì¡°ì ˆí•˜ì—¬ ê³¼ì í•©ì„ ë°©ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.ìˆœì°¨ì  í•™ìŠµ: ì´ì „ ëª¨ë¸ì˜ ì˜¤ë¥˜ë¥¼ ë³´ì™„í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµì´ ì§„í–‰ë©ë‹ˆë‹¤.02.ë°°ê¹…ê³¼ ë¶€ìŠ¤íŒ… ëª¨ë¸ êµ¬í˜„ ë° í‰ê°€ìœ ë°©ì•” ë°ì´í„°ì…‹ì„ ì´ìš©í•˜ì—¬ ë°°ê¹…ê³¼ ë¶€ìŠ¤íŒ…ëª¨ë¸ì„ êµ¬í˜„í•˜ê³  í‰ê°€í•˜ëŠ” ì‹¤ìŠµì„ í•©ë‹ˆë‹¤1) ìœ ë°©ì•” ë°ì´í„°ì…‹ ë°ì´í„° ë¡œë“œ ìœ ë°©ì•” ë°ì´í„°ì…‹ ë¡œë“œ {5px}ìœ ë°©ì•” ë°ì´í„°ì…‹ ë¡œë“œ ï»¿â€‹Pythonë³µì‚¬from sklearn.ensemble import BaggingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# ìœ ë°©ì•” ë°ì´í„° ë¡œë“œ
cancer_data = load_breast_cancer()
X, y = cancer_data.data, cancer_data.target

# ë°ì´í„° ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# ë°ì´í„° ìŠ¤ì¼€ì¼ë§
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
â€‹ ë°°ê¹… ëª¨ë¸ êµ¬í˜„ ë° í‰ê°€Scikit-learnì˜ BaggingRegressorë¥¼ ì‚¬ìš©í•˜ì—¬ ë°°ê¹… ëª¨ë¸ì„ êµ¬í˜„í•˜ê³  í‰ê°€í•©ë‹ˆë‹¤.ë°°ê¹… ëª¨ë¸ êµ¬í˜„ ë° í‰ê°€ {5px}ë°°ê¹… ëª¨ë¸ êµ¬í˜„ ë° í‰ê°€ ï»¿â€‹Pythonë³µì‚¬
# ë°°ê¹… ëª¨ë¸ ìƒì„±
bagging_model = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=100, random_state=42)
# ëª¨ë¸ í•™ìŠµ
bagging_model.fit(X_train_scaled, y_train)
# ì˜ˆì¸¡
y_pred_bagging = bagging_model.predict(X_test_scaled)
# í‰ê°€
mse_bagging = mean_squared_error(y_test, y_pred_bagging)
print(f'ë°°ê¹… ëª¨ë¸ì˜ MSE: {mse_bagging}')
â€‹ ë¶€ìŠ¤íŒ… ëª¨ë¸ êµ¬í˜„ ë° í‰ê°€Scikit-learnì˜ GradientBoostingRegressorë¥¼ ì‚¬ìš©í•˜ì—¬ ë¶€ìŠ¤íŒ… ëª¨ë¸ì„ êµ¬í˜„í•˜ê³  í‰ê°€í•©ë‹ˆë‹¤.ë¶€ìŠ¤íŒ… ëª¨ë¸ êµ¬í˜„ ë° í‰ê°€{5px}ë¶€ìŠ¤íŒ… ëª¨ë¸ êµ¬í˜„ ë° í‰ê°€ï»¿â€‹Pythonë³µì‚¬from sklearn.ensemble import GradientBoostingRegressor

# ë¶€ìŠ¤íŒ… ëª¨ë¸ ìƒì„±
boosting_model = GradientBoostingRegressor(n_estimators=100, random_state=42)
# ëª¨ë¸ í•™ìŠµ
boosting_model.fit(X_train_scaled, y_train)
# ì˜ˆì¸¡
y_pred_boosting = boosting_model.predict(X_test_scaled)
# í‰ê°€
mse_boosting = mean_squared_error(y_test, y_pred_boosting)
print(f'ë¶€ìŠ¤íŒ… ëª¨ë¸ì˜ MSE: {mse_boosting}')

â€‹.
[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 21ê°•. ì•™ìƒë¸” í•™ìŠµ - ëœë¤ í¬ë ˆìŠ¤íŠ¸[SCC] ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ - 5ì£¼ì°¨/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 21ê°•. ì•™ìƒë¸” í•™ìŠµ - ëœë¤ í¬ë ˆìŠ¤íŠ¸ì œì‘:[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 21ê°•. ì•™ìƒë¸” í•™ìŠµ - ëœë¤ í¬ë ˆìŠ¤íŠ¸[ìˆ˜ì—… ëª©í‘œ]ì•™ìƒë¸” í•™ìŠµì˜ ëœë¤í¬ë ˆìŠ¤íŠ¸ì— ëŒ€í•œ ê°œë…ì„ ë°°ìš°ê³ , ë°ì´í„°ë¥¼ ì´ìš©í•´ ì‹¤ìŠµí•´ ë´…ë‹ˆë‹¤[ëª©ì°¨]01. ëœë¤ í¬ë ˆìŠ¤íŠ¸02.ëœë¤ í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ êµ¬í˜„ ë° í‰ê°€ğŸ’¡
 
 01. ëœë¤ í¬ë ˆìŠ¤íŠ¸âœ”ï¸ì•™ìƒë¸” ê¸°ë²• ì¤‘ í•˜ë‚˜ì¸ ëœë¤ í¬ë ˆìŠ¤íŠ¸ì— ëŒ€í•´ ë°°ì›Œë´…ì‹œë‹¤1) ëœë¤ í¬ë ˆìŠ¤íŠ¸ ëœë¤ í¬ë ˆìŠ¤íŠ¸ë€?ëœë¤ í¬ë ˆìŠ¤íŠ¸(Random Forest)ëŠ” ë°°ê¹…(Bagging) ê¸°ë²•ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ì•™ìƒë¸” í•™ìŠµ ëª¨ë¸ì…ë‹ˆë‹¤.ì—¬ëŸ¬ ê°œì˜ ê²°ì • íŠ¸ë¦¬(Decision Tree)ë¥¼ í•™ìŠµì‹œí‚¤ê³ , ê·¸ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ê²°í•©í•˜ì—¬ ìµœì¢… ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ê° íŠ¸ë¦¬ê°€ ë…ë¦½ì ìœ¼ë¡œ í•™ìŠµë˜ê¸° ë•Œë¬¸ì—, ê³¼ì í•©ì„ ë°©ì§€í•˜ê³  ì˜ˆì¸¡ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.ALT ëœë¤ í¬ë ˆìŠ¤íŠ¸ì˜ êµ¬ì¡°ëœë¤ í¬ë ˆìŠ¤íŠ¸ëŠ” ì—¬ëŸ¬ ê°œì˜ ê²°ì • íŠ¸ë¦¬ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.ê° ê²°ì • íŠ¸ë¦¬ëŠ” ë°ì´í„°ì˜ ë¬´ì‘ìœ„ ìƒ˜í”Œì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµë˜ë©°, íŠ¸ë¦¬ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í‰ê·  ë˜ëŠ” ë‹¤ìˆ˜ê²°ë¡œ ê²°í•©í•˜ì—¬ ìµœì¢… ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ëœë¤ í¬ë ˆìŠ¤íŠ¸ì˜ ì›ë¦¬ë¶€íŠ¸ìŠ¤íŠ¸ë© ìƒ˜í”Œë§: ì›ë³¸ ë°ì´í„°ì…‹ì—ì„œ ì¤‘ë³µì„ í—ˆìš©í•œ ë¬´ì‘ìœ„ ìƒ˜í”Œì„ ìƒì„±í•©ë‹ˆë‹¤.ê²°ì • íŠ¸ë¦¬ í•™ìŠµ: ê° ë¶€íŠ¸ìŠ¤íŠ¸ë© ìƒ˜í”Œì„ ì‚¬ìš©í•˜ì—¬ ê²°ì • íŠ¸ë¦¬ë¥¼ í•™ìŠµì‹œí‚µë‹ˆë‹¤. ì´ë•Œ, ê° ë…¸ë“œì—ì„œ ë¬´ì‘ìœ„ë¡œ ì„ íƒëœ íŠ¹ì„±ì˜ ì¼ë¶€ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë¶„í• ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.ì˜ˆì¸¡ ê²°í•©: ëª¨ë“  ê²°ì • íŠ¸ë¦¬ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ê²°í•©í•˜ì—¬ ìµœì¢… ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. íšŒê·€ ë¬¸ì œì—ì„œëŠ” í‰ê· ì„ ì‚¬ìš©í•˜ê³ , ë¶„ë¥˜ ë¬¸ì œì—ì„œëŠ” ë‹¤ìˆ˜ê²°ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ë¬´ì‘ìœ„ì„±ëœë¤ í¬ë ˆìŠ¤íŠ¸ëŠ” ë‘ ê°€ì§€ ë¬´ì‘ìœ„ì„±ì„ ë„ì…í•˜ì—¬ ëª¨ë¸ì˜ ë‹¤ì–‘ì„±ì„ ì¦ê°€ì‹œí‚¤ê³ , ê³¼ì í•©ì„ ë°©ì§€í•©ë‹ˆë‹¤:ë°ì´í„° ìƒ˜í”Œë§ì˜ ë¬´ì‘ìœ„ì„±: ê° ê²°ì • íŠ¸ë¦¬ëŠ” ì›ë³¸ ë°ì´í„°ì…‹ì—ì„œ ë¬´ì‘ìœ„ë¡œ ìƒ˜í”Œë§ëœ ë°ì´í„°ë¡œ í•™ìŠµë©ë‹ˆë‹¤.íŠ¹ì„± ì„ íƒì˜ ë¬´ì‘ìœ„ì„±: ê° ë…¸ë“œì—ì„œ ë¶„í• ì„ ìˆ˜í–‰í•  ë•Œ, ë¬´ì‘ìœ„ë¡œ ì„ íƒëœ íŠ¹ì„±ì˜ ì¼ë¶€ë§Œì„ ì‚¬ìš©í•©ë‹ˆë‹¤.ì´ëŸ¬í•œ ë¬´ì‘ìœ„ì„±ì€ ëª¨ë¸ì˜ ìƒê´€ì„±ì„ ì¤„ì´ê³ , ì˜ˆì¸¡ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.02.ëœë¤ í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ êµ¬í˜„ ë° í‰ê°€âœ”ï¸ìœ ë°©ì•” ë°ì´í„°ì…‹ì„ ì´ìš©í•˜ì—¬ ëœë¤í¬ë ˆìŠ¤íŠ¸ë¥¼ êµ¬í˜„í•˜ê³  í‰ê°€í•˜ëŠ” ì‹¤ìŠµì„ í•©ë‹ˆë‹¤1)  ìœ ë°©ì•”  ë°ì´í„°ì…‹ ë°ì´í„° ë¡œë“œ ìë™ì°¨ ìˆ˜ìš”ì˜ˆì¸¡ ë°ì´í„°ì…‹ ë¡œë“œ {5px}ìë™ì°¨ ìˆ˜ìš”ì˜ˆì¸¡ ë°ì´í„°ì…‹ ë¡œë“œ ï»¿â€‹Pythonë³µì‚¬from sklearn.ensemble import BaggingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# ìœ ë°©ì•” ë°ì´í„° ë¡œë“œ
cancer_data = load_breast_cancer()
X, y = cancer_data.data, cancer_data.target

# ë°ì´í„° ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# ë°ì´í„° ìŠ¤ì¼€ì¼ë§
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
â€‹ ëœë¤ í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ êµ¬í˜„ ë° í‰ê°€Scikit-learnì˜ RandomForestRegressorë¥¼ ì‚¬ìš©í•˜ì—¬ ëœë¤ í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ì„ êµ¬í˜„í•˜ê³  í‰ê°€í•©ë‹ˆë‹¤.ëœë¤ í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ êµ¬í˜„ ë° í‰ê°€ {5px}ëœë¤ í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ êµ¬í˜„ ë° í‰ê°€ ï»¿â€‹Pythonë³µì‚¬from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

# ëœë¤ í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ ìƒì„±
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
# ëª¨ë¸ í•™ìŠµ
rf_model.fit(X_train_scaled, y_train)
# ì˜ˆì¸¡
y_pred_rf = rf_model.predict(X_test_scaled)
# í‰ê°€
mse_rf = mean_squared_error(y_test, y_pred_rf)
print(f'ëœë¤ í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ì˜ MSE: {mse_rf}')
â€‹ ì¤‘ìš” íŠ¹ì„± í™•ì¸ì¤‘ìš” íŠ¹ì„± í™•ì¸ {5px}ì¤‘ìš” íŠ¹ì„± í™•ì¸ ï»¿â€‹Pythonë³µì‚¬import matplotlib.pyplot as plt
import seaborn as sns

# íŠ¹ì„± ì¤‘ìš”ë„ ì¶”ì¶œ
feature_importances = rf_model.feature_importances_

# íŠ¹ì„± ì¤‘ìš”ë„ë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
feature_importances_df = pd.DataFrame({
'Feature': X.columns,
'Importance': feature_importances
})
# ì¤‘ìš”ë„ ìˆœìœ¼ë¡œ ì •ë ¬
feature_importances_df = feature_importances_df.sort_values(by='Importance', ascending=False)
# íŠ¹ì„± ì¤‘ìš”ë„ ì‹œê°í™”
plt.figure(figsize=(10, 7))
sns.barplot(x='Importance', y='Feature', data=feature_importances_df)
plt.title('Feature Importances in Random Forest')
plt.show()
â€‹.
[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 22ê°•. ì•™ìƒë¸” í•™ìŠµ - ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… ë¨¸ì‹  (GBM) [SCC] ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ - 5ì£¼ì°¨/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 22ê°•. ì•™ìƒë¸” í•™ìŠµ - ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… ë¨¸ì‹  (GBM) ì œì‘:[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 22ê°•. ì•™ìƒë¸” í•™ìŠµ - ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… ë¨¸ì‹  (GBM) [ìˆ˜ì—… ëª©í‘œ]ì•™ìƒë¸” í•™ìŠµì˜ ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…ë¨¸ì‹ ì— ëŒ€í•´ì„œ ë°°ìš°ê³  ì‹¤ìŠµí•´ ë´…ì‹œë‹¤[ëª©ì°¨]01. ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… ë¨¸ì‹ 02.GBM ì‹¤ìŠµğŸ’¡
 
 01. ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… ë¨¸ì‹ âœ”ï¸ì•™ìƒë¸” ê¸°ë²• ì¤‘ í•˜ë‚˜ì¸ ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… ë¨¸ì‹ ì— ëŒ€í•´ ë°°ì›Œë´…ì‹œë‹¤1) ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… ë¨¸ì‹  ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… ë¨¸ì‹ ì´ë€?ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… ë¨¸ì‹ (Gradient Boosting Machine, GBM)ì€ ì—¬ëŸ¬ ê°œì˜ ì•½í•œ í•™ìŠµê¸°(weak learner)ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ í•™ìŠµì‹œí‚¤ê³ , ê·¸ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ê²°í•©í•˜ì—¬ ê°•í•œ í•™ìŠµê¸°(strong learner)ë¥¼ ë§Œë“œëŠ” ì•™ìƒë¸” ê¸°ë²•ì…ë‹ˆë‹¤.GBMì€ ì´ì „ ëª¨ë¸ì´ ì˜ëª» ì˜ˆì¸¡í•œ ë°ì´í„° í¬ì¸íŠ¸ì— ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ì—¬, ë‹¤ìŒ ëª¨ë¸ì´ ì´ë¥¼ ë” ì˜ í•™ìŠµí•˜ë„ë¡ í•©ë‹ˆë‹¤.ê° íŠ¸ë¦¬ê°€ ë…ë¦½ì ìœ¼ë¡œ í•™ìŠµë˜ê¸° ë•Œë¬¸ì—, ê³¼ì í•©ì„ ë°©ì§€í•˜ê³  ì˜ˆì¸¡ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. GBMì˜ êµ¬ì¡°GBMì€ ì—¬ëŸ¬ ê°œì˜ ê²°ì • íŠ¸ë¦¬(Decision Tree)ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.ê° ê²°ì • íŠ¸ë¦¬ëŠ” ì´ì „ íŠ¸ë¦¬ì˜ ì˜ˆì¸¡ ì˜¤ë¥˜ë¥¼ ë³´ì™„í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµë©ë‹ˆë‹¤. GBMì€ ê° íŠ¸ë¦¬ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ê°€ì¤‘í•©í•˜ì—¬ ìµœì¢… ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. GBMì˜ ì›ë¦¬ì´ˆê¸° ëª¨ë¸ í•™ìŠµ: ì²« ë²ˆì§¸ ê²°ì • íŠ¸ë¦¬ë¥¼ í•™ìŠµì‹œì¼œ ì´ˆê¸° ëª¨ë¸ì„ ë§Œë“­ë‹ˆë‹¤.ì”ì—¬ ì˜¤ì°¨ ê³„ì‚°: ì´ˆê¸° ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ì™€ ì‹¤ì œ ê°’ ê°„ì˜ ì”ì—¬ ì˜¤ì°¨ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.ì”ì—¬ ì˜¤ì°¨ í•™ìŠµ: ì”ì—¬ ì˜¤ì°¨ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ìƒˆë¡œìš´ ê²°ì • íŠ¸ë¦¬ë¥¼ í•™ìŠµì‹œí‚µë‹ˆë‹¤.ëª¨ë¸ ì—…ë°ì´íŠ¸: ìƒˆë¡œìš´ ê²°ì • íŠ¸ë¦¬ë¥¼ ê¸°ì¡´ ëª¨ë¸ì— ì¶”ê°€í•˜ì—¬ ëª¨ë¸ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.ë°˜ë³µ: ì”ì—¬ ì˜¤ì°¨ê°€ ì¶©ë¶„íˆ ì‘ì•„ì§ˆ ë•Œê¹Œì§€ 2~4 ë‹¨ê³„ë¥¼ ë°˜ë³µí•©ë‹ˆë‹¤. ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…ì˜ ë‹¨ê³„ì  í•™ìŠµGBMì€ ë‹¨ê³„ì ìœ¼ë¡œ í•™ìŠµì„ ì§„í–‰í•˜ì—¬, ì´ì „ ëª¨ë¸ì˜ ì˜¤ë¥˜ë¥¼ ë³´ì™„í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì˜ˆì¸¡ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.ê° ë‹¨ê³„ì—ì„œ í•™ìŠµëœ ëª¨ë¸ì€ ì´ì „ ëª¨ë¸ì˜ ì”ì—¬ ì˜¤ì°¨ë¥¼ ì¤„ì´ëŠ” ë° ì§‘ì¤‘í•©ë‹ˆë‹¤02.GBM ì‹¤ìŠµâœ”ï¸ìœ ë°©ì•” ë°ì´í„°ì…‹ì„ ì´ìš©í•˜ì—¬ GBMì„ êµ¬í˜„í•˜ê³  í‰ê°€í•˜ëŠ” ì‹¤ìŠµì„ í•©ë‹ˆë‹¤1) ìœ ë°©ì•”  ë°ì´í„°ì…‹ ë°ì´í„° ë¡œë“œ ìë™ì°¨ ìˆ˜ìš”ì˜ˆì¸¡ ë°ì´í„°ì…‹ ë¡œë“œ {5px}ìë™ì°¨ ìˆ˜ìš”ì˜ˆì¸¡ ë°ì´í„°ì…‹ ë¡œë“œ ï»¿â€‹Pythonë³µì‚¬from sklearn.ensemble import BaggingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# ìœ ë°©ì•” ë°ì´í„° ë¡œë“œ
cancer_data = load_breast_cancer()
X, y = cancer_data.data, cancer_data.target

# ë°ì´í„° ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# ë°ì´í„° ìŠ¤ì¼€ì¼ë§
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
â€‹ GBM ëª¨ë¸ êµ¬í˜„ ë° í‰ê°€Scikit-learnì˜ GradientBoostingRegressorë¥¼ ì‚¬ìš©í•˜ì—¬ GBMì„ êµ¬í˜„í•˜ê³  í‰ê°€í•©ë‹ˆë‹¤.GBM ëª¨ë¸ êµ¬í˜„ ë° í‰ê°€ {5px}GBM ëª¨ë¸ êµ¬í˜„ ë° í‰ê°€ ï»¿â€‹Pythonë³µì‚¬from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error

# GBM ëª¨ë¸ ìƒì„±
gbm_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
# ëª¨ë¸ í•™ìŠµ
gbm_model.fit(X_train_scaled, y_train)
# ì˜ˆì¸¡
y_pred_gbm = gbm_model.predict(X_test_scaled)
# í‰ê°€
mse_gbm = mean_squared_error(y_test, y_pred_gbm)
print(f'GBM ëª¨ë¸ì˜ MSE: {mse_gbm}')
â€‹.
[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 23ê°•. ì•™ìƒë¸” í•™ìŠµ - XGBoost[SCC] ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ - 5ì£¼ì°¨/[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 23ê°•. ì•™ìƒë¸” í•™ìŠµ - XGBoostì œì‘:[ìŠ¤íŒŒë¥´íƒ€ì½”ë”©í´ëŸ½] 23ê°•. ì•™ìƒë¸” í•™ìŠµ - XGBoost[ìˆ˜ì—… ëª©í‘œ]ì•™ìƒë¸” í•™ìŠµì˜ XGBoostì— ëŒ€í•´ì„œ ë°°ìš°ê³  ì‹¤ìŠµí•´ ë´…ì‹œë‹¤[ëª©ì°¨]01. XGBoost02.XGBoost ì‹¤ìŠµğŸ’¡
 
 01. XGBoostâœ”ï¸ì•™ìƒë¸” ê¸°ë²• ì¤‘ í•˜ë‚˜ì¸ XGBoostì— ëŒ€í•´ ë°°ì›Œë´…ì‹œë‹¤1) XGBoost XGBoostë€?XGBoost(eXtreme Gradient Boosting)ëŠ” ê·¸ë˜ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… ì•Œê³ ë¦¬ì¦˜ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ê³ ì„±ëŠ¥ ì•™ìƒë¸” í•™ìŠµ ê¸°ë²•ì…ë‹ˆë‹¤.XGBoostëŠ” íš¨ìœ¨ì„±, ìœ ì—°ì„±, ì´ì‹ì„±ì„ ëª©í‘œë¡œ ì„¤ê³„ë˜ì—ˆìœ¼ë©°, ë‹¤ì–‘í•œ ë¨¸ì‹ ëŸ¬ë‹ ê²½ì§„ëŒ€íšŒì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ê³  ìˆìŠµë‹ˆë‹¤.ë³‘ë ¬ ì²˜ë¦¬, ì¡°ê¸° ì¢…ë£Œ, ì •ê·œí™” ë“±ì˜ ê¸°ëŠ¥ì„ í†µí•´ ì„±ëŠ¥ì„ ê·¹ëŒ€í™”í•©ë‹ˆë‹¤. XGBoostì˜ êµ¬ì¡°XGBoostëŠ” ì—¬ëŸ¬ ê°œì˜ ê²°ì • íŠ¸ë¦¬(Decision Tree)ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. ê° ê²°ì • íŠ¸ë¦¬ëŠ” ì´ì „ íŠ¸ë¦¬ì˜ ì˜ˆì¸¡ ì˜¤ë¥˜ë¥¼ ë³´ì™„í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ í•™ìŠµë©ë‹ˆë‹¤.XGBoostëŠ” ê° íŠ¸ë¦¬ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ê°€ì¤‘í•©í•˜ì—¬ ìµœì¢… ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. XGBoostì˜ ì›ë¦¬ì´ˆê¸° ëª¨ë¸ í•™ìŠµ: ì²« ë²ˆì§¸ ê²°ì • íŠ¸ë¦¬ë¥¼ í•™ìŠµì‹œì¼œ ì´ˆê¸° ëª¨ë¸ì„ ë§Œë“­ë‹ˆë‹¤.ì”ì—¬ ì˜¤ì°¨ ê³„ì‚°: ì´ˆê¸° ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ì™€ ì‹¤ì œ ê°’ ê°„ì˜ ì”ì—¬ ì˜¤ì°¨ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.ì”ì—¬ ì˜¤ì°¨ í•™ìŠµ: ì”ì—¬ ì˜¤ì°¨ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ìƒˆë¡œìš´ ê²°ì • íŠ¸ë¦¬ë¥¼ í•™ìŠµì‹œí‚µë‹ˆë‹¤.ëª¨ë¸ ì—…ë°ì´íŠ¸: ìƒˆë¡œìš´ ê²°ì • íŠ¸ë¦¬ë¥¼ ê¸°ì¡´ ëª¨ë¸ì— ì¶”ê°€í•˜ì—¬ ëª¨ë¸ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.ë°˜ë³µ: ì”ì—¬ ì˜¤ì°¨ê°€ ì¶©ë¶„íˆ ì‘ì•„ì§ˆ ë•Œê¹Œì§€ 2~4 ë‹¨ê³„ë¥¼ ë°˜ë³µí•©ë‹ˆë‹¤. XGBoostì˜ ì¥ì ë³‘ë ¬ ì²˜ë¦¬: íŠ¸ë¦¬ì˜ ë¶„í• ì„ ë³‘ë ¬ë¡œ ìˆ˜í–‰í•˜ì—¬ í•™ìŠµ ì†ë„ë¥¼ í–¥ìƒì‹œí‚µë‹ˆë‹¤.ì¡°ê¸° ì¢…ë£Œ: ê²€ì¦ ë°ì´í„°ì…‹ì˜ ì„±ëŠ¥ì´ í–¥ìƒë˜ì§€ ì•Šìœ¼ë©´ í•™ìŠµì„ ì¡°ê¸°ì— ì¢…ë£Œí•˜ì—¬ ê³¼ì í•©ì„ ë°©ì§€í•©ë‹ˆë‹¤.ì •ê·œí™”: L1 ë° L2 ì •ê·œí™”ë¥¼ í†µí•´ ëª¨ë¸ì˜ ë³µì¡ë„ë¥¼ ì¡°ì ˆí•˜ê³  ê³¼ì í•©ì„ ë°©ì§€í•©ë‹ˆë‹¤.ìœ ì—°ì„±: ë‹¤ì–‘í•œ ì†ì‹¤ í•¨ìˆ˜ì™€ í‰ê°€ ì§€í‘œë¥¼ ì§€ì›í•˜ì—¬ ë‹¤ì–‘í•œ ë¬¸ì œì— ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.02.XGBoost ì‹¤ìŠµâœ”ï¸ìë™ì°¨ ìˆ˜ìš”ì˜ˆì¸¡ ë°ì´í„°ì…‹ì„ ì´ìš©í•˜ì—¬ XGBoostì„ êµ¬í˜„í•˜ê³  í‰ê°€í•˜ëŠ” ì‹¤ìŠµì„ í•©ë‹ˆë‹¤1)  ìœ ë°©ì•”  ë°ì´í„°ì…‹ ë°ì´í„° ë¡œë“œ ìë™ì°¨ ìˆ˜ìš”ì˜ˆì¸¡ ë°ì´í„°ì…‹ ë¡œë“œ {5px}ìë™ì°¨ ìˆ˜ìš”ì˜ˆì¸¡ ë°ì´í„°ì…‹ ë¡œë“œ ï»¿â€‹Pythonë³µì‚¬from sklearn.ensemble import BaggingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# ìœ ë°©ì•” ë°ì´í„° ë¡œë“œ
cancer_data = load_breast_cancer()
X, y = cancer_data.data, cancer_data.target

# ë°ì´í„° ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# ë°ì´í„° ìŠ¤ì¼€ì¼ë§
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
â€‹ XGBoost ëª¨ë¸ êµ¬í˜„ ë° í‰ê°€XGBoost ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ XGBoost ëª¨ë¸ì„ êµ¬í˜„í•˜ê³  í‰ê°€í•©ë‹ˆë‹¤.XGBoost ëª¨ë¸ êµ¬í˜„ ë° í‰ê°€ {5px}XGBoost ëª¨ë¸ êµ¬í˜„ ë° í‰ê°€ ï»¿â€‹Pythonë³µì‚¬import xgboost as xgb
from sklearn.metrics import mean_squared_error

# XGBoost ëª¨ë¸ ìƒì„±
xgb_model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
# ëª¨ë¸ í•™ìŠµ
xgb_model.fit(X_train_scaled, y_train)
# ì˜ˆì¸¡
y_pred_xgb = xgb_model.predict(X_test_scaled)
# í‰ê°€
mse_xgb = mean_squared_error(y_test, y_pred_xgb)
print(f'XGBoost ëª¨ë¸ì˜ MSE: {mse_xgb}')
â€‹.
