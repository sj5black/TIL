Pandas 설치 및 Jupyter Notebook 설정하기 인공지능을 위한 파이썬 라이브러리/인공지능을 위한 파이썬 라이브러리 - 1주차/Pandas 설치 및 Jupyter Notebook 설정하기 제작:Pandas 설치 및 Jupyter Notebook 설정하기 수업 목표Pandas 설치를 진행합니다.Jupyter Notebook 설정 방법에 대해서 학습합니다.목차판다스(Pandas) 설치하기Jupyter Notebook 설정하기 Conda와 가상환경 설정하기가상환경을 Jupyter Notebook과 연결하기알면 좋은 Tip!판다스(Pandas) 설치하기판다스(Pandas)란?📚판다스는 데이터 조작 및 분석을 위한 파이썬 라이브러리입니다.
데이터프레임(DataFrame)을 사용해 엑셀과 유사한 방식으로 데이터를 다룰 수 있습니다.판다스 설치하기Shell복사pip install pandas

​위 명령어를 실행하면 pip을 통해 판다스가 설치됩니다!Jupyter Notebook 설정하기 ​Jupyter Notebook이란?📚Jupyter Notebook은 코드, 텍스트, 이미지 등을 한 곳에서 관리할 수 있는 대화형 컴퓨팅 환경입니다.
데이터 분석, 머신러닝, 교육 등 다양한 분야에서 사용됩니다.Jupyter Notebook 설치하기Shell복사pip install jupyter

​이 명령어로 Jupyter Notebook을 설치할 수 있습니다.Jupyter Notebook 실행하기Shell복사jupyter notebook

​명령어를 실행하면 웹 브라우저가 열리며 Jupyter Notebook 인터페이스로 이동합니다.Conda와 가상환경 설정하기Conda란?Conda는 파이썬 패키지 관리 및 가상환경 관리를 돕는 도구입니다.여러 프로젝트에 서로 다른 패키지 버전을 사용해야 할 때 유용합니다.

Conda 설치하기Anaconda나 Miniconda를 설치하여 Conda를 사용할 수 있습니다.설치 후, Conda 명령어를 사용할 수 있습니다.

가상환경 만들기Shell복사conda create --name myenv

​위 명령어로 myenv라는 이름의 가상환경을 만듭니다.Shell복사conda create --name myenv python=3.8 pandas
​원하는 파이썬 버전과 패키지를 지정할 수도 있습니다.가상환경 활성화 및 비활성화가상환경 활성화:Shell복사conda activate myenv

​가상환경 비활성화:Shell복사conda deactivate
​가상환경을 Jupyter Notebook과 연결하기ipykernel 설치가상환경을 Jupyter Notebook에서 사용하려면 ipykernel을 설치해야 합니다.Shell복사pip install ipykernel

​가상환경을 Jupyter Notebook에 추가하기Shell복사python -m ipykernel install --user --name myenv --display-name "My Env"

​이 명령어를 실행하면 Jupyter Notebook에서 My Env라는 이름으로 가상환경을 선택할 수 있습니다.Jupyter Notebook에서 가상환경 사용하기ALTJupyter Notebook에서 새로운 노트북을 열 때,  My Env를 선택합니다.알면 좋은 Tip!Conda 가상환경 관리 conda env list 명령어로 현재 사용 가능한 가상환경을 확인할 수 있습니다.Jupyter Notebook 확장 다양한 Jupyter Notebook 확장을 설치해 기능을 확장해 보세요! nbextensions 패키지를 사용하면 많은 유용한 확장을 사용할 수 있습니다.📚이렇게 하면 판다스와 Jupyter Notebook 설정이 모두 완료되었습니다! ​익명9월 25일아이콘 제거
[스파르타코딩클럽] 1강. 강의 소개[SCC] 바닥부터 시작하는 머신러닝/[스파르타코딩클럽] 바닥부터 시작하는 머신러닝 - 1주차/[스파르타코딩클럽] 1강. 강의 소개제작:[스파르타코딩클럽] 1강. 강의 소개[수업 목표]강의 소개 및 머신러닝 기본 개념을 알아봅시다[목차]01. 강좌 소개 딥러닝이 대세라던데, 왜 머신러닝부터 시작할까요?02. 머신러닝 소개
 
 01. 강좌 소개이번 머신러닝 강좌목표와 커리큘럼에 대해 안내합니다.1) 강좌 목표머신러닝의 정의, 주요 개념, 다양한 알고리즘 학습개념적 이해와 실습을 통한 심화 학습Scikit-learn, TensorFlow, Keras 등 파이썬 라이브러리 활용 인공지능, 머신러닝, 딥러닝? 뭐가 다른 걸까요?인공지능(AI): 사람처럼 생각하고 행동하는 기술 ​머신러닝(ML): 데이터를 통해 스스로 배우고 똑똑해지는 기술 ​딥러닝(DL): 이미지 인식, 자연어 처리 등에 뛰어난 머신러닝의 한 분야 ​ 수학? 걱정 마세요!수학 공식을 몰라도 괜찮아요! 이 강의에서는 머신러닝의 작동 원리와 코딩에 집중합니다. 필요한 수학 개념은 그때그때 함께 학습하면 됩니다! ​2) 강좌 커리큘럼머신러닝 정의 및 개념 다양한 머신러닝 알고리즘데이터셋 활용과 실습실무 데이터 프로젝트이번 강좌에서는 파이썬 기본 개념이 필요합니다.
강의 뒤쪽에서 나오는 어려운 문법들은 강좌 내에서 설명 후에 실습을 진행할 예정이니 걱정하지 않으셔도 됩니다!강의 뒤쪽에서 나오는 어려운 문법들은 강좌 내에서 설명 후에 실습을 진행할 예정이니 걱정하지 않으셔도 됩니다!﻿​ 딥러닝이 대세라던데, 왜 머신러닝부터 시작할까요?딥러닝의 기초는 탄탄한 머신러닝 지식입니다. 기초를 튼튼히 쌓아야 딥러닝도 쉽게 이해할 수 있죠! 차근차근 기초부터 다지면, 딥러닝 정복도 문제없어요! ​02. 머신러닝 소개머신러닝에 대한 기본적인 개념 및 역사등을 소개합니다1)머신러닝 소개 머신러닝의 정의컴퓨터가 명시적으로 프로그래밍 되지 않아도 데이터를 통해 학습하고, 예측할 수 있도록 하는 기능대량의 데이터를 알고리즘에 입력하여 학습과정을 통해 모델을 생성하고 예측을 수행기존 프로그램은 명시적으로 작성된 규칙과 논리에 따라 동작한다는것에서 머신러닝과의 차이가 있습니다!기존 프로그램은 명시적으로 작성된 규칙과 논리에 따라 동작한다는것에서 머신러닝과의 차이가 있습니다!﻿​전통적인 프로그래밍:규칙과 논리를 프로그래머가 직접 정의 ​명시적 명령과 조건문을 통해 문제 해결 ​머신러닝:데이터를 이용해 패턴과 규칙을 스스로 학습 ​예측 모델을 통해 새로운 데이터에 대한 결과 도출 ​프로그램이 아닌 모델이 중심 ​ 머신러닝의 구성요소데이터셋 : 모델을 학습시키기 위한 데이터 모음특징(Feature) : 데이터셋에서 모델이 학습할 수 있는 개별 속성레이블(label) : 예측하고자 하는 목표 변수훈련 : 모델이 데이터를 통해 학습하는 과정테스트 : 학습된 모델의 성능을 평가하는 과정2)머신러닝의 역사 및 발전 이유  머신러닝의 역사1950s : 앨런 튜링의 “튜링테스트”와 퍼셉트론의 개발1980s -90s : 백프로파게이션 알고리즘의 등장2000s :  대규모 데이터와 고성능 컴퓨팅 자원의 발전2010s - : 딥러닝의 부상과 다양한 산업에 적용 최근 머신러닝의 발전 이유데이터의 폭발적 증가컴퓨팅 파워의 향상알고리즘의 발전오픈소스 커뮤니티와 생태계의 발전이 강좌에서는 파이썬을 활용한 머신러닝의 기본 개념과 실습을 다룰 예정입니다!
머신러닝의 기본적인 원리부터 실습을 통해 직접 모델 구축/평가하는 방법까지 모두 함께 배워봅시다!.
[스파르타코딩클럽] 1. 딥러닝 개념을 잡아봅시다![SCC] 기초가 탄탄한 딥러닝/[스파르타코딩클럽] 기초가 탄탄한 딥러닝 - 1주차/[스파르타코딩클럽] 1. 딥러닝 개념을 잡아봅시다!제작:[스파르타코딩클럽] 1. 딥러닝 개념을 잡아봅시다![수업 목표]딥러닝이 무엇인지 개념에 대해 알아봅시다.딥러닝의 역사와 어디에 사용할 수 있을지 알아봅시다[목차]01. 딥러닝이란 무엇일까요?02. 딥러닝의 역사와 활용 방안03. 딥러닝을 배워야 하는 이유💡
 
 01. 딥러닝이란 무엇일까요?✔️딥러닝이란 무엇인지 개념에 대해서 알아봅시다!1) 딥러닝이란? 딥러닝 개념딥러닝은 인공신경망(Artificial Neural Networks)을 기반으로 한 기계 학습의 한 분야입니다.다층 신경망을 사용하여 데이터로부터 특징을 자동으로 학습하고, 이를 통해 복잡한 문제를 해결합니다.입력 데이터에서 중요한 패턴을 추출하고, 이를 바탕으로 예측, 분류, 생성 등의 다양한 작업을 수행할 수 있습니다.ALT 딥러닝의 특징비선형 추론: 딥러닝은 비선형 추론을 통해 복잡한 데이터의 패턴을 학습할 수 있습니다.다층 구조: 여러 층의 신경망을 사용하여 데이터의 고차원 특징을 학습합니다.자동 특징 추출: 데이터로부터 중요한 특징을 자동으로 추출하여 별도의 특징 공학(feature engineering) 과정이 필요 없습니다.02. 딥러닝의 역사와 활용 방안✔️딥러닝의 역사와 어디에 딥러닝을 쓸 수 있을지 배워봅시다!1) 딥러닝의 역사와 발전 발전 과정ALT 인공지능, 머신러닝, 딥러닝의 관계인공지능(AI) : 인공지능은 인간의 지능을 모방하여 문제를 해결하는 기술을 의미합니다. AI는 규칙 기반 시스템부터 자율 학습 시스템까지 다양한 접근 방식을 포함합니다.머신러닝(ML) : 머신러닝은 데이터를 이용해 모델을 학습하고, 이를 통해 예측이나 결정을 내리는 기술입니다. 머신러닝은 AI의 하위 분야로, 지도 학습, 비지도 학습, 강화 학습 등의 방법을 포함합니다.딥러닝(DL) : 딥러닝은 머신러닝의 하위 분야로, 다층 신경망을 사용하여 데이터를 학습합니다. 딥러닝은 특히 대규모 데이터와 복잡한 문제를 다루는 데 강력한 성능을 발휘합니다.ALT2) 최근의 활용 방안 이미지 인식딥러닝은 이미지 분류, 객체 검출, 이미지 생성 등 다양한 이미지 처리 작업에 활용됩니다. 예를 들어, 자율 주행 자동차는 딥러닝을 사용하여 도로 상황을 인식하고, 보행자와 차량을 감지합니다. 자연어 처리번역, 요약, 감정 분석 등 자연어 처리 작업에 사용됩니다. 예를 들어, 구글 번역은 딥러닝 모델을 사용하여 다양한 언어 간의 번역을 수행합니다. 음성 인식딥러닝은 음성 인식 시스템의 성능을 크게 향상시켰습니다.예를 들어, 애플의 Siri, 아마존의 Alexa와 같은 가상 비서는 딥러닝을 사용하여 사용자의 음성을 인식하고 명령을 수행합니다. 의료 분야의료 영상 분석, 질병 예측, 신약 개발 등 다양한 의료 분야에서도 활용됩니다.예를 들어, 딥러닝 모델은 MRI나 CT 스캔 이미지를 분석하여 암을 조기에 발견할 수 있습니다.03. 딥러닝을 배워야 하는 이유✔️그러면 왜? 딥러닝을 배워야 하는지 이유를 알아 봅시다1) 딥러닝을 배워야 하는 이유ALT.
LLM의 사용 준비하기 LLM & RAG를 활용한 AI 서비스 만들기/ LLM & RAG를 활용한 AI 서비스 만들기 - 5주차/LLM의 사용 준비하기LLM의 사용 준비하기수업 목표이번 주차에서 배울 내용들을 살펴봅니다.보안 문제, API 개념, 그리고 오픈 소스 LLM 구축에 대한 기본 내용을 학습합니다.목차이번에 배울 것보안 문제LLM을 사용할 때의 데이터 보안 문제보안을 강화하는 방법API 개념잡기API란 무엇인가?API의 기본 개념ChatGPT를 API로 사용하는 것의 장점
API 사용의 주요 장점벡터 DB와 LangChain 활용하기벡터 DB란?벡터 DB와 LangChain의 역할LLM + 벡터 DB + LangChain 구축 플로우결론이번에 배울 것이번 챕터에서는 LLM을 사용하기 전에 알아야 할 중요한 개념들을 다룰 거예요.LLM을 실무에서 사용하기 위해서는 보안 문제, API 개념, 그리고 오픈 소스 LLM 구축에 대한 이해가 필수입니다.각 개념을 차근차근 살펴보겠습니다! ​보안 문제LLM을 사용할 때는 데이터 보안에 대해 반드시 신경 써야 해요.특히 대화형 AI인 LLM은 사용자의 민감한 정보를 처리할 가능성이 있기 때문에 데이터 유출과 프라이버시 침해가 발생하지 않도록 주의해야 합니다.LLM을 사용할 때의 데이터 보안 문제개인 정보 보호
LLM은 대화를 통해 개인 정보(이름, 전화번호, 신용카드 정보 등)를 접할 수 있어요.
이런 정보가 저장되거나 제3자에게 공유될 경우 큰 문제가 될 수 있습니다.데이터 저장 및 전송
LLM이 처리한 데이터가 어디에 저장되는지, 어떻게 전송되는지를 확인해야 합니다. 
암호화된 전송 방식(예: HTTPS)을 사용해 데이터를 보호해야 해요.모델 학습 데이터
LLM은 학습에 사용된 데이터에 의존해 답변을 생성합니다.
만약 학습 데이터에 민감한 정보가 포함된다면, 해당 정보가 예기치 않게 모델의 출력으로 등장할 가능성도 있습니다.보안을 강화하는 방법민감 정보 필터링
입력된 데이터를 처리하기 전에 민감한 정보를 자동으로 걸러내는 필터링 시스템을 구축하세요.암호화
데이터는 저장 및 전송 중에 암호화되어야 합니다.
특히 SSL/TLS와 같은 안전한 전송 프로토콜을 사용해야 해요.데이터 저장 최소화
필요 이상으로 데이터를 저장하지 말고, 필요한 경우에도 데이터 보존 주기를 설정해 자동 삭제하도록 하세요.접근 통제
LLM을 사용할 수 있는 사람의 권한을 제한하고, 모델이 민감한 데이터에 접근하지 않도록 제한해야 해요.API 개념잡기API란 무엇인가?API(Application Programming Interface)는 서로 다른 소프트웨어 시스템 간에 데이터와 기능을 주고받을 수 있도록 해주는 인터페이스에요.쉽게 말해, 두 프로그램이 서로 소통할 수 있는 다리라고 보면 됩니다.API의 기본 개념클라이언트-서버 모델
클라이언트(요청하는 쪽)가 서버(응답하는 쪽)에게 데이터를 요청하면, 서버는 해당 데이터를 처리하여 클라이언트에게 반환합니다.HTTP/HTTPS 프로토콜
대부분의 API는 HTTP나 HTTPS를 통해 요청과 응답이 이루어져요.RESTful API
가장 흔히 사용되는 API 설계 방식으로, 각 엔드포인트(URI)와 HTTP 메소드(GET, POST, PUT, DELETE 등)를 사용해 데이터를 주고받아요.ChatGPT를 API로 사용하는 것의 장점ChatGPT 같은 LLM을 API로 사용하는 것은 매우 효율적이에요.이 API를 통해 다양한 애플리케이션에 LLM 기능을 쉽게 통합할 수 있습니다.
API 사용의 주요 장점유연성: 필요할 때마다 요청을 보내어 결과를 받을 수 있어, 실시간으로 다양한 애플리케이션에 적용할 수 있어요.확장성: 다양한 서비스나 플랫폼(예: 웹사이트, 앱)에 쉽게 통합할 수 있어, 여러 사용자가 동시에 사용할 수 있는 확장성을 가집니다.업데이트: LLM API 제공사가 모델을 업데이트하면 별다른 수정 없이 최신 기능을 바로 사용할 수 있어요.비용 효율성: API 호출에 따라 비용이 청구되므로, 대규모 서버를 유지할 필요 없이 필요한 만큼만 사용 가능해요.벡터 DB와 LangChain 활용하기오픈 소스 LLM을 구축할 때 벡터 DB와 LangChain을 함께 사용하면 더 강력한 시스템을 만들 수 있어요.벡터 DB란?벡터 DB는 문서나 데이터의 임베딩(embedding) 벡터를 저장하고, 이 벡터를 바탕으로 유사한 데이터를 빠르게 찾을 수 있는 데이터베이스입니다.LLM과 결합하면 유사 문서 검색이나 추천 시스템 등에 활용할 수 있습니다.벡터 DB와 LangChain의 역할벡터 DB
텍스트 데이터를 벡터로 변환한 후, 유사한 텍스트나 데이터를 빠르게 찾는 데 사용됩니다. 예를 들어, 유사한 질문을 찾거나 추천을 할 때 유용해요.LangChain
LLM과 벡터 DB를 연결해주는 프레임워크로, 데이터 흐름을 관리하고 API 호출을 더 간편하게 만들어줍니다.LLM + 벡터 DB + LangChain 구축 플로우텍스트 임베딩 생성
먼저 LLM을 통해 텍스트 데이터를 벡터(임베딩)로 변환합니다.벡터 DB 저장
변환된 임베딩을 벡터 DB에 저장합니다.질문 처리
사용자가 질문을 하면, 질문도 벡터로 변환하고 벡터 DB에서 유사한 임베딩을 찾습니다.답변 생성
찾은 유사한 데이터와 함께 LLM을 통해 최종 답변을 생성합니다.API로 제공
이 과정을 LangChain을 통해 API 형태로 제공하여, 실제 서비스에서 사용할 수 있게 합니다.결론이번 챕터에서는 LLM의 사용 준비를 위한 보안 문제, API 개념, 그리고 오픈 소스 LLM 구축 방법을 배웠습니다.데이터를 안전하게 관리하면서 LLM을 효과적으로 활용하고, 필요한 경우에는 오픈 소스 LLM을 구축하여 최적의 시스템을 만들 수 있죠!이제 LLM을 실무에서 사용할 준비가 다 되셨습니다! ​
NumPy 소개 및 설치인공지능을 위한 파이썬 라이브러리/인공지능을 위한 파이썬 라이브러리 - 2주차/NumPy 소개 및 설치제작:NumPy 소개 및 설치수업 목표Numpy가 무엇인지 알아봅니다.Numpy 세팅을 진행합니다.목차NumPy란? NumPy 설치하기NumPy의 기본 사용법 요약NumPy란? NumPy는 Numerical Python의 줄임말로,
과학 계산에 강력한 성능을 제공하는 파이썬 라이브러리입니다.NumPy?다차원 배열 객체인 ndarray와 배열을 효율적으로 처리할 수 있는 다양한 함수들을 제공합니다.데이터 분석, 머신러닝, 딥러닝에서 기초가 되는 라이브러리로, 판다스와 함께 자주 사용됩니다.NumPy의 주요 특징고속 배열 연산: C언어로 작성되어 있어 파이썬 기본 리스트보다 훨씬 빠른 연산 가능.다양한 수학 함수: 배열 간의 덧셈, 뺄셈, 곱셈 등의 연산을 효율적으로 수행.선형대수, 통계 함수: 복잡한 수학 계산도 간단히 처리 가능.NumPy 설치하기NumPy를 사용하려면 먼저 설치가 필요합니다.
가상환경을 사용하고 있다면 해당 가상환경에 설치하면 됩니다.Shell복사pip install numpy

​설치가 완료되면, 파이썬 코드에서 import numpy as np로 NumPy를 사용할 수 있습니다.NumPy의 기본 사용법 배열(ndarray) 생성하기리스트를 사용해 배열을 생성할 수 있어요.Python복사import numpy as np

# 1차원 배열 생성
arr = np.array([1, 2, 3, 4, 5])
print(arr)

​arr은 다음과 같이 출력됩니다:Plain Text복사[1 2 3 4 5]

​배열의 연산NumPy 배열은 벡터화 연산이 가능하여, 반복문 없이 배열 전체에 연산을 적용할 수 있습니다.Python복사# 배열 덧셈
arr2 = arr + 10
print(arr2)

​arr2는 다음과 같이 출력됩니다:Plain Text복사[11 12 13 14 15]

​배열의 다양한 기능배열의 모양 변경, 원소 접근 등 다양한 기능을 쉽게 사용할 수 있습니다.Python복사# 배열의 모양 변경
arr_reshaped = arr.reshape((1, 5))
print(arr_reshaped)

​arr_reshaped는 다음과 같이 출력됩니다:Plain Text복사[[1 2 3 4 5]]​요약요약NumPy는 빠르고 효율적인 수치 계산을 위한 필수 라이브러리입니다.pip 명령어로 설치 가능하며, 다양한 수학 연산과 배열 조작 기능을 제공합니다.NumPy를 통해 복잡한 계산을 단순하게 만들 수 있어, 데이터 분석에서 자주 사용됩니다.NumPy로 데이터 처리의 기초를 다지고, 더 나아가 판다스와 함께 활용해보세요! ​
[스파르타코딩클럽] 2강. 머신러닝 개요와 구성요소[SCC] 바닥부터 시작하는 머신러닝/[스파르타코딩클럽] 바닥부터 시작하는 머신러닝 - 1주차/[스파르타코딩클럽] 2강. 머신러닝 개요와 구성요소제작:[스파르타코딩클럽] 2강. 머신러닝 개요와 구성요소[수업 목표]머신러닝의 기본적인 구성요소, 학습과정을 알려 드립니다.다양한 머신러닝의 학습방법을 소개합니다.[목차]01. 머신러닝 구성 요소02. 머신러닝의 학습💡
 
 01. 머신러닝 구성 요소✔️머신러닝의 필수 구성요소를 학습합니다1) 머신러닝의 구성요소 데이터셋머신러닝은 데이터셋을 통해서 학습하며, 일반적으로 데이터셋은 입력/출력 데이터로 구성됩니다.입력 데이터 : 모델이 학습할 수 있는 정보출력 데이터(레이블) : 모델이 예측해야 하는 목표 값 Feature(특징)데이터에서 모델이 학습할 수 있는 개별 속성주택가격예측을 예시로 들 경우 주택의 크기, 위치, 방의 개수 등이 Feature에 해당합니다주택가격예측을 예시로 들 경우 주택의 크기, 위치, 방의 개수 등이 Feature에 해당합니다﻿​ 레이블예측하고자 하는 목표 변수지도학습 모델에서는 레이블이 있는 데이터셋을 이용하여 모델을 학습 시킵니다 모델데이터의 특징으로 부터 정답(레이블)을 예측할 수 있는 지식을 학습할 수 있는 프로그램/함수입력데이터와 출력 데이터간의 관계를 학습하여 새로운 데이터에 대한 예측 수행 학습모델이 데이터를 통해서 패턴을 인식하고, 이를 기반으로 예측을 수행 할 수 있도록 함수 내의 가중치를 조정하는 과정02. 머신러닝의 학습✔️머신러닝의 학습과정과 다양한 학습 종류에 대해서 학습합니다1) 머신러닝의 학습 과정머신러닝 학습 과정ALT데이터 수집 : 모델을 학습시키기 위한 필요 데이터 수집데이터 전처리 : 결측값 처리, 이상치 제거, 정규화 등등Feature 선택 : 중요 feature(특징)을 선택하고 불필요한 피쳐를 제거하여 학습효율 높임모델 선택 : 문제에 적합한 머신러닝 알고리즘을 선택모델 훈련 : 트레이닝 데이터셋을 사용해서 모델을 학습시킴모델 평가 : 테스트 데이터셋을 사용하여 모델 성능을 평가모델 배포 : 학습된 모델을 실제 환경에 배포하여 예측 수행2) 학습 방법 지도 학습 (Supervised Learning)레이블이 있는 데이터셋을 이용하여 모델을 학습시키는 방법회귀(Regression) : 연속적인 값을 예측하는 문제ex : 주택 가격 예측, 주식 가격예측 : 주택 가격 예측, 주식 가격예측﻿​분류(Classification) : 이산적인 값을 예측하는 문제ex : 이메일 스팸 필터링, 이미지 분류 : 이메일 스팸 필터링, 이미지 분류﻿​ 비지도 학습 (Unsupervised Learning)레이블이 없는 데이터셋을 이용하려 모델을 학습시키는 방법군집화(Clustering) : 데이터를 유사한 그룹으로 묶는 문제 ex : 고객 세분화, 이미지 세그멘테이션 : 고객 세분화, 이미지 세그멘테이션﻿​차원축소 (Dimensionality Reduction) : 고차원 데이터를 저차원으로 변환ex : PCA, t-SNE : PCA, t-SNE﻿​ 앙상블 학습 (Ensemble Learning)여러개의 머신러닝 모델을 결합하여 더 나은 성능을 얻는 방법배깅(Bagging) : 여러 모델을 독립적으로 학습시키고, 예측을 평균내거나 다수결 투표로 최종 예측ex : 랜덤포레스트 : 랜덤포레스트﻿​부스팅(Boosting) : 여러 모델을 순차적으로 학습시키고, 이전 모델의 오차를 보완하여 최종 예측을 수행ex : 그래디언트 부스팅, XGboost : 그래디언트 부스팅, XGboost﻿​스태킹(Stacking) : 여러 모델을 학습시키고 예측결과를 새로운 데이터로 사용하여 메타 모델을 학습 과적합이란?과적합(Overfitting):모델이 훈련 데이터에 지나치게 적응하여 새로운 데이터에 대한 일반화 성능이 떨어지는 현상입니다. 모델이 너무 복잡하여 훈련 데이터의 노이즈까지 학습해버리는 경우 발생합니다. ​방지 방법:더 많은 데이터 수집 ​교차 검증(Cross-validation) 사용 ​정규화(Regularization) 기법 적용 ​간단한 모델 사용 ​ 머신러닝에서는 "절대로 좋다"라는 개념이 없다!모델의 성능:모든 데이터셋에 대해 완벽한 성능을 보이는 모델은 없습니다. 각 모델은 특정 데이터와 상황에서만 최적의 성능을 발휘합니다. ​트레이드오프:모델의 복잡성과 일반화 성능 사이에는 항상 균형이 필요합니다. 너무 복잡한 모델은 과적합의 위험이 있고, 너무 단순한 모델은 충분히 학습하지 못할 수 있습니다. ​.
[스파르타코딩클럽] 2. 신경망의 기본 원리[SCC] 기초가 탄탄한 딥러닝/[스파르타코딩클럽] 기초가 탄탄한 딥러닝 - 1주차/[스파르타코딩클럽] 2. 신경망의 기본 원리제작:[스파르타코딩클럽] 2. 신경망의 기본 원리[수업 목표]퍼셉트론의 개념과 다층 퍼셉트론에 대해 배워봅시다.신경망을 강화하기 위한 활성화 함수/손실 함수/역전파/최적화 알고리즘에 대해 배워봅시다[목차]01. 퍼셉트론과 다층 퍼셉트론(XOR 문제 포함)02. 다층 퍼셉트론(MLP)03. 활성화 함수04. 손실 함수와 최적화 알고리즘05. 역전파에 대해 알아볼까요?
 
 01. 퍼셉트론과 다층 퍼셉트론(XOR 문제 포함)인공신공망의 가장 기본 단위인 퍼셉트론에 대해 배워봅시다1) 단일 퍼셉트론의 원리 단일 퍼셉트론의 개념퍼셉트론(Perceptron)은 인공 신경망의 가장 기본적인 단위로, 하나의 뉴런을 모델링한 것입니다.퍼셉트론은 입력 값을 받아 가중치(weight)를 곱하고, 이를 모두 더한 후 활성화 함수(activation function)를 통해 출력 값을 결정합니다.ALT 퍼셉트론의 수학적 표현y=f(∑i=1nwixi+b)y = f(_{i=1}^{n} w_i x_i + b) y=f(i=1∑n​wi​xi​+b)여기서 xi는 입력 값, wi는 가중치, b는 바이어스(bias), f는 활성화 함수입니다.여기서 xi​는 입력 값, wi​는 가중치, b는 바이어스(bias), f는 활성화 함수입니다.﻿​02. 다층 퍼셉트론(MLP)다층 퍼셉트론에 대해 배워봅시다1) 다층 퍼셉트론(MLP)과 XOR 문제 해결 다층 퍼셉트론(MLP)의 개념다층 퍼셉트론(Multi-Layer Perceptron, MLP)은 여러 층의 퍼셉트론을 쌓아 올린 신경망 구조입니다.MLP는 입력층(input layer), 은닉층(hidden layer), 출력층(output layer)으로 구성되며, 각 층의 뉴런들이 서로 연결되어 있습니다.ALT 입력, 은닉, 출력 레이어의 개념입력 레이어(Input Layer) : 외부 데이터가 신경망에 입력되는 부분입니다. 입력 레이어의 뉴런 수는 입력 데이터의 특징 수와 동일합니다.은닉 레이어(Hidden Layer) : 은닉 레이어는 입력 레이어와 출력 레이어 사이에 위치한 층으로, 입력 데이터를 처리하고 특징을 추출하는 역할을 합니다. 은닉 레이어의 뉴런 수와 층 수는 모델의 복잡성과 성능에 영향을 미칩니다.출력 레이어(Output Layer) : 출력 레이어는 신경망의 마지막 층으로, 최종 예측 값을 출력합니다. 출력 레이어의 뉴런 수는 예측하려는 클래스 수 또는 회귀 문제의 출력 차원과 동일합니다. XOR 문제와 MLP단일 퍼셉트론은 선형 분류기이기 때문에 XOR 문제와 같은 비선형 문제를 해결할 수 없습니다.XOR 문제는 두 입력 값이 다를 때만 1을 출력하는 문제로, 단일 퍼셉트론으로는 해결할 수 없습니다. 그러나 MLP는 은닉층을 통해 비선형성을 학습할 수 있어 XOR 문제를 해결할 수 있습니다.03. 활성화 함수활성화 함수라는게 무엇인지, 신경망에서 어떤 역할을 하는지 알아보고, 어떤 종류의 활성화 함수가 있는지 배워봅시다1) 활성화 함수의 필요성과 종류 활성화 함수의 필요성활성화 함수는 신경망의 각 뉴런에서 입력값을 출력값으로 변환하는 역할을 합니다.활성화 함수가 없다면 신경망은 단순 선형변환만 수행하게 되어 복잡한 패턴을 학습할 수 없습니다.활성화 함수는 비 선형성을 도입하여 신경망이 복잡한 패턴을 학습할 수 있게합니다. 활성화 함수의 종류ReLU (Rectified Linear Unit)f(x)=max⁡(0,x)f(x) = (0, x)f(x)=max(0,x)장점: 계산이 간단하고, 기울기 소실 문제(vanishing gradient problem)를 완화합니다.단점: 음수 입력에 대해 기울기가 0이 되는 '죽은 ReLU' 문제가 발생할 수 있습니다.Sigmoidf(x)=11+e−xf(x) = {1 + e^{-x}} f(x)=1+e−x1​장점: 출력 값이 0과 1 사이로 제한되어 확률을 표현하기에 적합합니다.단점: 기울기 소실 문제와 출력 값이 0 또는 1에 가까워질 때 학습이 느려지는 문제가 있습니다.Tanh (Hyperbolic Tangent)f(x)=tanh⁡(x)=ex−e−xex+e−xf(x) = (x) = }{e^x + e^{-x}}f(x)=tanh(x)=ex+e−xex−e−x​장점: 출력 값이 -1과 1 사이로 제한되어 중심이 0에 가까워집니다.단점: 기울기 소실 문제가 발생할 수 있습니다.04. 손실 함수와 최적화 알고리즘손실함수와 최적화 알고리즘이 무엇인지 배우고 주요 사용하는 함수의 종류를 배워봅시다.1) 손실 함수의 역할과 주요 종류 손실함수의 역할손실 함수(Loss Function)는 모델의 예측 값과 실제 값 사이의 차이를 측정하는 함수입니다.손실 함수는 모델의 성능을 평가하고, 최적화 알고리즘을 통해 모델을 학습시키는 데 사용됩니다. 주요 손실 함수의 종류MSE (Mean Squared Error)MSE=1n∑i=1n(yi−y^i)2 = {n} _{i=1}^{n} (y_i - _i)^2 MSE=n1​i=1∑n​(yi​−y^​i​)2사용 분야: 회귀 문제에서 주로 사용됩니다.특징: 예측 값과 실제 값의 차이를 제곱하여 평균을 구합니다.Cross-EntropyCross-Entropy=−∑i=1nyilog⁡(y^i) = -_{i=1}^{n} y_i (_i)Cross-Entropy=−i=1∑n​yi​log(y^​i​)사용 분야: 분류 문제에서 주로 사용됩니다.특징: 예측 확률과 실제 클래스 간의 차이를 측정합니다.2) 최적화 알고리즘의 개념과 종류 최적화 알고리즘의 개념최적화 알고리즘(Optimization Algorithm)은 손실 함수를 최소화하기 위해 모델의 가중치를 조정하는 방법입니다.최적화 알고리즘은 손실 함수의 기울기를 계산하고, 이를 바탕으로 가중치를 업데이트합니다. 주요 최적화 알고리즘의 종류SGD (Stochastic Gradient Descent)개념: 전체 데이터셋이 아닌 무작위로 선택된 일부 데이터(미니배치)를 사용하여 기울기를 계산하고 가중치를 업데이트합니다.장점: 계산이 빠르고, 큰 데이터셋에서도 효율적으로 동작합니다.단점: 최적점에 도달하기까지 진동이 발생할 수 있습니다.Adam (Adaptive Moment Estimation)개념: 모멘텀과 RMSProp을 결합한 알고리즘으로, 학습률을 적응적으로 조정합니다.장점: 빠른 수렴 속도와 안정적인 학습을 제공합니다.단점: 하이퍼파라미터 설정이 복잡할 수 있습니다.05. 역전파에 대해 알아볼까요?역전파에 대해 배워봅시다1) 역전파 알고리즘의 개념과 수학적 원리 역전파 알고리즘의 개념역전파(Backpropagation)는 신경망의 가중치를 학습시키기 위해 사용되는 알고리즘입니다. 출력에서 입력 방향으로 손실 함수의 기울기를 계산하고, 이를 바탕으로 가중치를 업데이트합니다. 역전파의 수학적 원리연쇄 법칙(Chain Rule)을 사용해 손실함수의 기울기를 계산합니다.각 층의 기울기는 이전 층의 기울기와 현재 층의 기울기를 곱하여 계산합니다.이를 통해 신경망의 모든 가중치가 업데이트 됩니다ALT.
Vector DB 개념 및 RAG (Retrieval-Augmented Generation) 개념 LLM & RAG를 활용한 AI 서비스 만들기/ LLM & RAG를 활용한 AI 서비스 만들기 - 5주차/Vector DB 개념 및 RAG (Retrieval-Augmented Generation) 개념제작:Vector DB 개념 및 RAG (Retrieval-Augmented Generation) 개념수업 목표vectorDB와 RAG의 개념에 대해서 학습하고, 한국어 임베딩 실습을 진행합니다.목차Vector DB 개념 Vector DB란?Faiss란?임베딩(Embedding) 개념 임베딩이란?RAG (Retrieval-Augmented Generation) 개념RAG의 동작 원리Retrieval (검색) 단계Generation (생성) 단계RAG의 장점Vector DB와 RAG의 결합동작 흐름한국어 임베딩 실습한국어 데이터 임베딩Vector DB와 RAG로 LLM 시스템 구축하기Vector DB 개념 Vector DB란?❗Vector DB는 데이터를 벡터 형식으로 저장하고, 그 벡터들을 효율적으로 검색할 수 있는 데이터베이스에요. 일반적인 데이터베이스는 정확한 일치를 바탕으로 데이터를 검색하지만, Vector DB는 유사한 벡터 간의 검색을 지원하죠.벡터(임베딩)의 역할텍스트나 이미지 등의 비정형 데이터를 벡터화(임베딩)해서 저장해요.
이 벡터는 데이터의 의미나 특징을 수치로 표현한 것이며, 이를 바탕으로 유사도를 계산해 관련성이 높은 항목을 찾습니다.예를 들어, "강아지"라는 텍스트는 벡터로 변환되며, 비슷한 의미를 가진 "반려견"도 벡터화되어 유사도가 높은 항목으로 검색될 수 있어요.Faiss란?💡Faiss는 Facebook AI Research에서 개발한 벡터 검색 엔진으로, Vector DB를 구현할 때 자주 사용돼요. 대규모 벡터를 효율적으로 검색하고, 유사도를 계산하는 데 탁월한 성능을 발휘합니다. 특히 빠른 속도와 확장성이 필요한 애플리케이션에서 많이 쓰이죠.임베딩(Embedding) 개념 임베딩이란?💡임베딩은 텍스트, 이미지 등의 데이터를 고차원 공간에서 벡터(숫자 배열)로 변환하는 작업이에요. LLM(대규모 언어 모델)이 문장을 이해하기 위해서는 단어와 문장을 벡터로 변환해야, 컴퓨터가 의미적 유사성을 기반으로 데이터를 처리할 수 있습니다.임베딩의 작동 방식임베딩은 단어 간의 의미적 관계를 벡터 공간에 투영해요.
예를 들어, "고양이"와 "개"는 비슷한 의미를 가지므로 벡터 공간에서도 서로 가까운 위치에 존재하게 됩니다. 
반대로 "사과"와 "자동차"처럼 전혀 다른 의미를 가진 단어들은 벡터 공간에서 멀리 떨어진 위치에 놓이게 되죠.RAG (Retrieval-Augmented Generation) 개념❗RAG는 Retrieval-Augmented Generation의 약자로, 
LLM(대규모 언어 모델)과 검색 시스템을 결합한 개념이에요.
RAG는 기존의 LLM만으로는 해결할 수 없는 문제를, 외부 정보 검색을 통해 보완할 수 있어요. 
최신 정보를 포함한 답변을 제공하는 데 매우 유리하죠.RAG의 동작 원리1️⃣Retrieval (검색) 단계사용자가 질문을 하면, 벡터 DB에서 질문과 유사한 문서나 데이터를 검색해요. 이때 임베딩 모델을 사용해 질문을 벡터로 변환하고, 벡터 간의 유사도를 계산해 관련 데이터를 찾아냅니다.2️⃣Generation (생성) 단계검색된 문서를 LLM에 전달하고, 이를 바탕으로 자연스러운 답변을 생성합니다. 검색된 문서를 참조해 최신 정보를 포함한 정확한 답변을 제공하죠.RAG의 장점최신 정보 제공: LLM이 학습한 데이터 외의 최신 문서를 검색해 정보의 정확도를 높일 수 있어요.유연성: LLM이 모르는 정보도 외부 검색을 통해 답변할 수 있어 지식의 확장성이 뛰어납니다.지식의 한계 극복: 학습 데이터에만 의존하지 않고, 외부 데이터베이스에서 실시간 정보를 제공받아 더욱 풍부한 답변을 할 수 있어요.Vector DB와 RAG의 결합💡Vector DB와 RAG의 결합은 매우 강력해요. Vector DB는 유사한 문서를 검색해주고, RAG는 검색된 문서를 바탕으로 정확한 답변을 생성하는 과정이죠.동작 흐름ALT한국어 임베딩 실습한국어 데이터 임베딩한국어 문장을 임베딩하려면 사전 학습된 한국어 임베딩 모델이 필요해요. [코드스니펫] 한국어 임베딩 실습 코드Python복사from sentence_transformers import SentenceTransformer
import numpy as np

# Multilingual-E5-large-instruct 모델 로드
model = SentenceTransformer('intfloat/multilingual-e5-large')
# 문장 리스트
sentences = [
"참새는 짹짹하고 웁니다.",
"LangChain과 Faiss를 활용한 예시입니다.",
"자연어 처리를 위한 임베딩 모델 사용법을 배워봅시다.",
"유사한 문장을 검색하는 방법을 살펴보겠습니다.",
"강좌를 수강하시는 수강생 여러분 감사합니다!"
]
# 문장들을 임베딩으로 변환
embeddings = model.encode(sentences)
# 임베딩 벡터 출력
print(embeddings.shape) # (4, 1024) - 4개의 문장이 1024 차원의 벡터로 변환됨
​Python복사from sentence_transformers import SentenceTransformer
import numpy as np

# Multilingual-E5-large-instruct 모델 로드
model = SentenceTransformer('intfloat/multilingual-e5-large')
# 문장 리스트
sentences = [
"참새는 짹짹하고 웁니다.",
"LangChain과 Faiss를 활용한 예시입니다.",
"자연어 처리를 위한 임베딩 모델 사용법을 배워봅시다.",
"유사한 문장을 검색하는 방법을 살펴보겠습니다.",
"강좌를 수강하시는 수강생 여러분 감사합니다!"
]
# 문장들을 임베딩으로 변환
embeddings = model.encode(sentences)
# 임베딩 벡터 출력
print(embeddings.shape) # (4, 1024) - 4개의 문장이 1024 차원의 벡터로 변환됨
​Vector DB와 RAG로 LLM 시스템 구축하기1️⃣질문을 임베딩
사용자의 질문을 벡터로 변환합니다.2️⃣벡터 DB에서 검색
벡터 DB에서 질문과 관련된 문서를 검색합니다.3️⃣검색된 문서를 기반으로 LLM이 답변 생성
검색된 문서를 LLM에 전달하고, 답변을 생성합니다.😀이러한 과정을 통해 RAG와 Vector DB는 최신 정보 기반의 대화형 AI 시스템을 구축하는 데 매우 유용해요!
Vector DB와 RAG 개념을 기반으로 LLM 시스템을 구축할 때, 더 나은 정보 검색과 답변 생성을 할 수 있습니다.
NumPy 배열(array) 생성 및 기초 연산인공지능을 위한 파이썬 라이브러리/인공지능을 위한 파이썬 라이브러리 - 2주차/NumPy 배열(array) 생성 및 기초 연산제작:NumPy 배열(array) 생성 및 기초 연산수업 목표NumPy 배열 생성 및 기초 연산에 대해 학습합니다.목차NumPy 배열(array) 생성하기NumPy 배열의 기초 연산 배열의 인덱싱과 슬라이싱요약NumPy 배열(array) 생성하기📚NumPy 배열은 파이썬의 리스트와 비슷하지만, 더 강력하고 효율적인 기능을 제공합니다. 배열을 생성하는 다양한 방법을 살펴볼까요?리스트로부터 배열 생성리스트를 배열로 변환할 수 있습니다.Python복사import numpy as np

# 리스트로부터 배열 생성
arr = np.array([1, 2, 3, 4, 5])
print(arr)

​출력 결과:Plain Text복사[1 2 3 4 5]

​다양한 형태의 배열 생성0으로 채워진 배열 생성Python복사zero_arr = np.zeros((2, 3)) # 2x3 크기의 배열
print(zero_arr)

​출력 결과Plain Text복사[[0. 0. 0.]
 [0. 0. 0.]]​1로 채워진 배열 생성Python복사ones_arr = np.ones((3, 2)) # 3x2 크기의 배열
print(ones_arr)

​출력 결과Plain Text복사[[1. 1.]
 [1. 1.]
 [1. 1.]]​특정 값으로 채워진 배열 생성Python복사full_arr = np.full((2, 2), 7) # 2x2 크기의 7로 채워진 배열
print(full_arr)

​출력 결과Plain Text복사[[7 7]
 [7 7]]

​연속적인 값으로 채워진 배열 생성:Python복사range_arr = np.arange(10) # 0부터 9까지의 연속된 값
print(range_arr)

​출력 결과Plain Text복사[0 1 2 3 4 5 6 7 8 9]

​NumPy 배열의 기초 연산 📚NumPy 배열에서는 반복문 없이 벡터화 연산을 통해 쉽게 연산을 수행할 수 있습니다.기본 연산덧셈, 뺄셈, 곱셈, 나눗셈 모두 가능합니다.Python복사arr = np.array([1, 2, 3, 4, 5])
# 각 원소에 2를 더하기
arr_add = arr + 2
print(arr_add)
# 각 원소에 2를 곱하기
arr_mul = arr * 2
print(arr_mul)

​출력 결과Plain Text복사[3 4 5 6 7]
[ 2  4  6  8 10]

​배열 간의 연산배열 간의 덧셈과 곱셈도 쉽습니다.Python복사arr1 = np.array([1, 2, 3])
arr2 = np.array([4, 5, 6])
# 배열 간 덧셈
arr_sum = arr1 + arr2
print(arr_sum)
# 배열 간 곱셈
arr_mul = arr1 * arr2
print(arr_mul)

​출력 결과Plain Text복사[5 7 9]
[ 4 10 18]

​배열의 인덱싱과 슬라이싱인덱싱(Indexing)배열의 특정 위치에 접근하는 방법입니다.Python복사arr = np.array([10, 20, 30, 40, 50])
# 첫 번째 원소
print(arr[0])
# 마지막 원소
print(arr[-1])

​출력 결과Plain Text복사10
50

​슬라이싱(Slicing)배열의 일부분을 잘라내는 방법입니다.Python복사arr = np.array([10, 20, 30, 40, 50])
# 두 번째부터 네 번째 원소까지
sliced_arr = arr[1:4]
print(sliced_arr)

​출력 결과Plain Text복사[20 30 40]

​다차원 배열의 인덱싱 및 슬라이싱다차원 배열의 경우, 콤마를 사용해 접근합니다.Python복사arr2d = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
# 특정 원소 접근 (2행 3열)
print(arr2d[1, 2])
# 슬라이싱 (2행까지, 2열까지)
sliced_arr2d = arr2d[:2, :2]
print(sliced_arr2d)

​출력 결과Plain Text복사6
[[1 2]
 [4 5]]

​요약📚요약NumPy 배열은 리스트보다 강력한 기능을 제공하며, 다양한 방법으로 생성할 수 있습니다.배열 간의 벡터화 연산을 통해 복잡한 수학 연산을 쉽게 수행할 수 있습니다.인덱싱과 슬라이싱을 사용해 배열의 특정 원소나 부분 배열에 쉽게 접근 가능합니다.이제 NumPy 배열을 자유자재로 다뤄보세요! ​Son minsuck9월 25일토글 리스트로 변환
[스파르타코딩클럽] 3강. Anaconda 설치 및 라이브러리 소개[SCC] 바닥부터 시작하는 머신러닝/[스파르타코딩클럽] 바닥부터 시작하는 머신러닝 - 1주차/[스파르타코딩클럽] 3강. Anaconda 설치 및 라이브러리 소개제작:[스파르타코딩클럽] 3강. Anaconda 설치 및 라이브러리 소개[수업 목표]Anaconda 를 설치합니다머신러닝에서 사용하는 주요 라이브러리 기능을 배우고 실습합니다[목차]01. Anaconda 소개 및 설치02. 주요 라이브러리 소개💡
 
 01. Anaconda 소개 및 설치✔️Anaconda가 무엇인지 알아보고, 같이 설치를 진행해 봅시다1) Anaconda 소개 Anaconda 란데이터 과학 및 머신러닝을 위한 통합 개발 환경(IDE)다양한 데이터 과학 도구와 라이브러리를 포함하고 있어, 데이터 분석 및 머신러닝 작업을 쉽게 시작가능Anaconda는 Python과 R 프로그래밍 언어를 지원2) Anaconda 설치 Anaconda 설치 방법Anaconda 다운로드: Anaconda 공식 웹사이트에서 운영 체제에 맞는 설치 파일을 다운로드 합니다설치 파일 실행: 다운로드한 설치 파일을 실행하여 설치를 진행 합니다환경 변수 설정: 설치 과정에서 "Add Anaconda to my PATH environment variable" 옵션을 선택하여 환경 변수를 설정 합니다설치 완료: 설치가 완료되면 Anaconda Navigator와 Anaconda Prompt를 사용할 수 있습니다. Anaconda 설치 확인Anaconda 설치 확인 {5px}Anaconda 설치 확인 ﻿​Python복사# Anaconda 버전 확인
conda --version
​ Anaconda 주요 기능패키지 관리: conda 명령어를 사용하여 패키지를 설치, 업데이트, 제거할 수 있습니다.환경 관리: 가상 환경을 생성하고 관리할 수 있습니다.Jupyter Notebook: 웹 기반의 대화형 개발 환경을 제공합니다.Spyder: 통합 개발 환경(IDE)으로, 코드 편집기와 디버거를 포함합니다.가상환경 생성 및 관리 {5px}가상환경 생성 및 관리 ﻿​Python복사# 새로운 가상 환경 생성
conda create --name myenv

# 가상 환경 활성화
conda activate myenv

# 가상 환경 비활성화
conda deactivate

# 가상 환경 제거
conda remove --name myenv --all
​02. 주요 라이브러리 소개✔️머신러닝 강의에서 사용하는 주요 라이브러리를 소개드리고 간단한 실습을 진행해 봅시다1) Numpy 라이브러리 소개  Numpy 라이브러리란?수치 계산을 위한 Python 라이브러리Numpy는 다차원 배열 객체인 ndarray를 제공벡터화 연산을 통해 빠르고 효율적인 수치 계산을 수행 가능 Numpy 설치Numpy 설치 {5px}Numpy 설치 ﻿​Python복사# conda를 사용하여 Numpy 설치
conda install numpy

# pip를 사용하여 Numpy 설치
pip install numpy
​라이브러리는 conda 또는pip를 사용하여 설치할 수 있습니다. { conda }라이브러리는 conda 또는pip를 사용하여 설치할 수 있습니다.﻿​ Numpy 주요 기능다차원 배열 객체(ndarray): 다차원 배열을 생성하고 조작할 수 있습니다.벡터화 연산: 배열 간의 연산을 빠르게 수행할 수 있습니다.수학 함수: 다양한 수학 함수를 제공합니다. Numpy 예제다차원 배열을 생성하고 기본 연산을 수행하는 예제를 실습해 봅시다Numpy 예제 {5px}Numpy 예제 ﻿​Python복사import numpy as np

# 1차원 배열 생성
arr1 = np.array([1, 2, 3, 4, 5])
# 2차원 배열 생성
arr2 = np.array([[1, 2, 3], [4, 5, 6]])
# 배열의 크기 확인
print(arr1.shape)
print(arr2.shape)
# 배열의 데이터 타입 확인
print(arr1.dtype)
print(arr2.dtype)
# 배열 간의 연산
arr_sum = arr1 + arr1
arr_product = arr1 * arr1

print(arr_sum)
print(arr_product)
​2) Pandas 라이브러리 소개 Pandas 라이브러리란?데이터 조작 및 분석을 위한 Python 라이브러리데이터 프레임(DataFrame)이라는 구조를 사용하여 데이터를 효율적으로 관리하고 조작 가능이 데이터 프레임이라는 구조 덕에 테이블 형식의 데이터를 다루는 데 매우 유용 합니다이 데이터 프레임이라는 구조 덕에 테이블 형식의 데이터를 다루는 데 매우 유용 합니다﻿​ Pandas 설치Pandas 설치 {5px}Pandas 설치 ﻿​Python복사# conda를 사용하여 Pandas 설치
conda install pandas

# pip를 사용하여 Pandas 설치
pip install pandas
​ Pandas 주요 기능데이터 프레임(DataFrame): 테이블 형식의 데이터를 생성하고 조작할 수 있습니다.데이터 불러오기 및 저장: CSV, 엑셀, SQL 등 다양한 형식의 데이터를 불러오고 저장할 수 있습니다.데이터 조작: 필터링, 그룹화, 병합 등 다양한 데이터 조작 기능을 제공합니다. Pandas 예제데이터프레임을 생성하고 기본 조작을 수행하는 예제를 실습해 봅시다Pandas 예제 {5px}Pandas 예제 ﻿​Python복사import pandas as pd

# 데이터 프레임 생성
data = {'Name': ['Alice', 'Bob', 'Charlie'],
'Age': [25, 30, 35],
'City': ['New York', 'Los Angeles', 'Chicago']}
df = pd.DataFrame(data)
# 데이터 프레임의 첫 5행 출력
print(df.head())
# 데이터 프레임의 크기 확인
print(df.shape)
# 데이터 프레임의 컬럼명 확인
print(df.columns)
# 데이터 프레임의 데이터 타입 확인
print(df.dtypes)
# 특정 컬럼 선택
print(df['Name'])
# 조건에 맞는 행 선택
print(df[df['Age'] > 30])
​3) Scikit-learn 라이브러리 소개 Scikit-learn 라이브러리란?머신러닝을 위한 Python 라이브러리다양한 머신러닝 알고리즘을 제공하며, 데이터 전처리, 모델 학습, 평가 및 예측을 위한 도구를 포함 Scikit-learn 설치Scikit-learn 설치{5px}Scikit-learn 설치﻿​Python복사# conda를 사용하여 Scikit-learn 설치
conda install scikit-learn

# pip를 사용하여 Scikit-learn 설치
pip install scikit-learn
​ Scikit-learn  주요 기능데이터 전처리: 스케일링, 인코딩, 결측값 처리 등 다양한 데이터 전처리 기능을 제공합니다.모델 학습: 회귀, 분류, 군집화 등 다양한 머신러닝 알고리즘을 제공합니다.모델 평가: 교차 검증, 성능 평가 지표 등 모델 평가를 위한 도구를 제공합니다.모델 예측: 학습된 모델을 사용하여 새로운 데이터에 대한 예측을 수행할 수 있습니다. Scikit-learn  예제Scikit-learn 예제{5px}Scikit-learn 예제﻿​Python복사from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

# 데이터 생성
X = [[1], [2], [3], [4], [5]]
y = [1, 4, 9, 16, 25]
# 데이터 분할 (훈련 데이터와 테스트 데이터)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# 회귀 모델 생성 및 학습
model = LinearRegression()
model.fit(X_train, y_train)
# 예측
y_pred = model.predict(X_test)
# 모델 평가
mse = mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')
​.
[스파르타코딩클럽] 3. 딥러닝 실습 환경 구축[SCC] 기초가 탄탄한 딥러닝/[스파르타코딩클럽] 기초가 탄탄한 딥러닝 - 1주차/[스파르타코딩클럽] 3. 딥러닝 실습 환경 구축제작:[스파르타코딩클럽] 3. 딥러닝 실습 환경 구축[수업 목표]딥러닝 실습을 위한 환경을 구축해 봅시다[목차]01. conda를 이용한 환경 설정02. jupyter notebook 03. 가상환경 설치 및 jupyter notebook 연결 04. pytorch 설치💡
 
 01. conda를 이용한 환경 설정✔️conda를 설치하고 환경설정을 진행해 봅시다1) conda를 이용한 환경 설정 conda란 무엇인가?Conda는 패키지 관리와 환경 관리를 위한 오픈 소스 플랫폼입니다. 다양한 라이브러리와 패키지를 쉽게 설치하고 관리할 수 있으며, 서로 다른 프로젝트 간의 종속성을 격리할 수 있습니다. conda 설치Anacondahttps://www.anaconda.com/downloadALTMinicondahttps://docs.anaconda.com/miniconda/ALT conda 환경 설정다음과 같이 새로운 환경을 생성하고 필요한 패키지를 설치 합니다.새로운 환경 생성 {5px}새로운 환경 생성 ﻿​Python복사conda create --name myenv python=3.8
​환경 활성화 {5px}환경 활성화 ﻿​Python복사conda activate myenv
​필요한 패키지 설치 {5px}필요한 패키지 설치 ﻿​Python복사conda install numpy pandas matplotlib
​02. jupyter notebook ✔️jupyter notebook이 무엇인지 알아보고 설치해 봅시다.1)  jupyter notebook 사용법 jupyter notebook이란?Jupyter Notebook은 데이터 과학자와 연구자들이 코드를 작성하고 실행하며, 결과를 시각화하고 문서화할 수 있는 대화형 환경입니다. jupyter notebook 설치Jupyter Notebook은 conda를 통해 쉽게 설치할 수 있습니다jupyter notebook 설치 {5px}jupyter notebook 설치 ﻿​Python복사conda install jupyter
​ jupyter notebook 시작하기Jupyter Notebook 실행jupyter notebook 실행 {5px}jupyter notebook 실행 ﻿​Python복사jupyter notebook
​이 명령어를 실행하면 웹 브라우저가 열리고, Jupyter Notebook 인터페이스가 나타납니다.새로운 노트북 생성:Jupyter Notebook 인터페이스에서 "New" 버튼을 클릭하고, "Python (myenv)"를 선택하여 새로운 노트북을 생성합니다.코드 작성 및 실행:셀(Cell)에 코드를 작성하고, 셀을 선택한 후 "Shift + Enter"를 눌러 코드를 실행합니다.03. 가상환경 설치 및 jupyter notebook 연결 ✔️가상환경이 무엇인지 알아보고 jupyter notebook과 가상환경을 연결해 봅시다1) 가상환경 설치  가상환경이란 무엇인가?가상환경(Virtual Environment)은 프로젝트마다 독립적인 파이썬 환경을 제공합니다.가상환경을 이용해 서로 다른 프로젝트 간의 패키지 충돌을 방지할 수 있습니다. Conda를 이용하면 쉽게 가상환경을 생성하고 관리할 수 있습니다. 가상환경 생성 및 관리가상 환경 생성 {5px}가상 환경 생성 ﻿​Python복사conda create --name myenv python=3.8
​여기서 myenv 는 가상환경의 이름이며 python=3.8은 설치할 파이썬 버전입니다여기서 myenv 는 가상환경의 이름이며 python=3.8은 설치할 파이썬 버전입니다﻿​가상 환경 비활성화 {5px}가상 환경 비활성화 ﻿​Python복사conda deactivate
​가상 환경 삭제 {5px}가상 환경 삭제 ﻿​Python복사conda remove --name myenv --all
​2) jupyter notebook 연결 jupyter notebook 연결가상환경을 Jupyter Notebook과 연결하여 사용할 수 있습니다이를 위해 ipykernel 패키지를 설치하고, 가상환경을 Jupyter Notebook에 커널로 추가합니다.ipykernel 설치 {5px}ipykernel 설치 ﻿​Python복사conda install ipykernel
​가상환경을 Jupyter Notebook에 커널로 추가 {5px}가상환경을 Jupyter Notebook에 커널로 추가 ﻿​Python복사python -m ipykernel install --user --name myenv --display-name "Python (myenv)"
​여기서 myenv는 가상환경의 이름이며, "Python (myenv)"는 Jupyter Notebook에서 표시될 커널 이름입니다.여기서 myenv는 가상환경의 이름이며, "Python (myenv)"는 Jupyter Notebook에서 표시될 커널 이름입니다.﻿​04. pytorch 설치✔️conda를 이용해 pytorch 설치해 봅시다1) pytorch 설치 pytorch란 무엇인가?PyTorch는 Facebook에서 개발한 오픈 소스 딥러닝 라이브러리로, 동적 계산 그래프(dynamic computation graph)를 지원하여 유연하고 직관적인 모델 설계가 가능합니다. pytorch 설치PyTorch는 conda를 통해 쉽게 설치할 수 있습니다.설치 명령어는 운영체제와 CUDA 버전에 따라 다르므로, PyTorch 공식 웹사이트에서 설치 명령어를 확인할 수 있습니다. 예를 들어, CUDA 10.2를 사용하는 Windows 환경에서는 다음과 같이 설치할 수 있습니다:pytorch 설치 {5px}pytorch 설치 ﻿​Python복사conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch
​.
텍스트 처리의 핵심 기법과 임베딩 활용하기 LLM & RAG를 활용한 AI 서비스 만들기/ LLM & RAG를 활용한 AI 서비스 만들기 - 5주차/텍스트 처리의 핵심 기법과 임베딩 활용하기제작:텍스트 처리의 핵심 기법과 임베딩 활용하기수업 목표주요 텍스트 처리 기법인 토큰화, 정규화 등을 학습합니다.임베딩의 개념과 중요성에 대해 학습합니다.목차텍스트 처리가 중요한 이유텍스트 처리의 목표주요 텍스트 처리 기법토큰화 (Tokenization)정규화 (Normalization)불용어 제거 (Stopword Removal)형태소 분석 (Morphological Analysis)어간 추출과 표제어 추출 (Stemming and Lemmatization)문장 분리 및 길이 조정임베딩의 개념과 중요성임베딩(Embedding)란?대표적인 임베딩 기법Bag of Words (BoW)TF-IDF (Term Frequency-Inverse Document Frequency)Word2Vec, GloVeTransformer 기반 임베딩 (BERT, GPT)배운 내용 정리하기!LLM(대규모 언어 모델)을 효율적으로 활용하려면 텍스트 처리가 기본이 되어야 합니다. 텍스트 처리는 모델이 자연어를 이해하고 분석할 수 있게 돕는 첫 단계이죠. 또한, 텍스트를 벡터로 변환하는 임베딩(embedding) 기법은 문장의 의미를 숫자로 표현해 모델이 문서의 유사성을 파악하거나 정확한 추론을 할 수 있게 해줘요.이번 강의에서는 텍스트 처리의 주요 기법과 임베딩의 개념을 살펴보며, 이 두 가지가 LLM에서 왜 중요한지, 그리고 실제로 어떻게 활용되는지 알아보겠습니다.텍스트 처리가 중요한 이유텍스트 처리(Text Preprocessing)는 데이터의 품질을 높이고 모델의 성능을 향상시키기 위한 필수 작업이에요. 자연어는 매우 복잡하고 다양하기 때문에, LLM이 텍스트를 정확하게 이해하고 처리하기 위해서는 데이터가 구조화되고 정제될 필요가 있어요. 잘못된 텍스트 처리 과정은 모델이 혼동하거나 잘못된 추론을 하게 할 수 있습니다.텍스트 처리의 목표노이즈 제거: 텍스트 내 불필요한 정보나 오류를 제거해 정확한 분석을 할 수 있도록 합니다.일관성 확보: 문장의 구조나 형태를 일관되게 유지하여 모델이 더 쉽게 패턴을 학습하게 돕습니다.효율적인 처리: 불필요한 단어를 제거하고 중요한 정보만 남겨, 모델이 더 빠르게 계산할 수 있도록 해줍니다.주요 텍스트 처리 기법토큰화 (Tokenization)토큰화는 텍스트를 단어 또는 서브워드 단위로 나누는 작업입니다. 이 과정은 텍스트를 숫자로 변환하기 전의 가장 중요한 단계에 해당해요.단어 단위 토큰화: 텍스트를 단어 단위로 나누는 기본 방법입니다.예: "나는 오늘 책을 읽었다." → ["나는", "오늘", "책을", "읽었다"]서브워드 토큰화: 단어를 더 작은 단위로 분리해 새로운 단어를 처리할 수 있도록 합니다. BPE나 WordPiece 같은 방법이 있어요.예: "읽었다" → ["읽", "었다"]이렇게 나눠진 토큰은 모델이 이해할 수 있는 형태로 변환됩니다.정규화 (Normalization)정규화는 텍스트를 표준화된 형식으로 변환하는 작업이에요. 텍스트에 포함된 대소문자, 특수문자 등을 일관되게 변환하여, 모델이 불필요한 변동에 혼란을 겪지 않도록 합니다.소문자 변환: 대문자와 소문자를 통일하여 같은 단어로 인식하게 합니다.예: "OpenAI" → "openai"불필요한 기호 제거: 분석에 필요 없는 특수문자나 기호를 제거합니다.예: "서울, 2023년!" → "서울 2023년"정규화를 통해 모델이 텍스트의 의미에 집중하게 할 수 있어요.불용어 제거 (Stopword Removal)불용어란 자주 등장하지만 정보가 없는 단어를 말해요. 예를 들어, "그리고", "이", "는" 같은 단어들은 문맥에 큰 영향을 미치지 않기 때문에 불용어로 처리해요. 이를 제거하면 모델이 중요한 단어에만 집중할 수 있습니다.예: "나는 책을 읽었다." → ["책", "읽었다"]형태소 분석 (Morphological Analysis)한국어와 같은 교착어에서는 형태소 분석이 필수적이에요. 형태소는 단어의 최소 의미 단위로, 한국어에서 특히 조사나 어미와 같은 부분을 정확하게 분리해내는 데 유용합니다.예: "책을 읽었다." → [책(Noun), 을(Postposition), 읽었다(Verb)]어간 추출과 표제어 추출 (Stemming and Lemmatization)텍스트에서 동사나 형용사의 변형을 기본 형태로 돌리는 작업입니다. 이를 통해 동일한 단어를 일관되게 처리할 수 있어요.어간 추출은 단어에서 어미를 제거하고, 기본 어간만 남깁니다.예: "studying", "studied", "study" → "study"표제어 추출은 단어를 사전적 기본형으로 변환합니다.예: "am", "is", "are" → "be"이 과정은 동사나 형용사처럼 변형이 많은 단어를 처리할 때 매우 유용해요.문장 분리 및 길이 조정텍스트가 너무 길거나 복잡할 경우, 이를 적절하게 나누거나 길이를 조정해야 해요. 긴 문장을 처리할 때, 모델의 메모리 제한이나 성능 저하를 방지할 수 있습니다.임베딩의 개념과 중요성임베딩(Embedding)란?임베딩은 텍스트 데이터를 **벡터(숫자 배열)**로 변환하는 과정이에요. 
LLM이 텍스트의 의미를 이해하려면 텍스트가 숫자로 변환되어야 하는데, 그 과정에서 각 문장이나 단어를 벡터 공간에 매핑하게 됩니다. 이를 통해 모델이 의미적 유사성을 파악하고, 효율적인 검색이나 문서 분류를 할 수 있어요.임베딩 벡터는 단어의 의미나 문장의 문맥을 반영하여, 
유사한 의미를 가진 텍스트는 유사한 벡터 값을 가집니다.대표적인 임베딩 기법Bag of Words (BoW)Bag of Words는 단어의 빈도만을 기반으로 텍스트를 벡터화하는 가장 단순한 방법입니다. 단어의 순서나 문맥을 고려하지 않기 때문에 의미 파악에 한계가 있지만, 간단한 문서 분류나 텍스트 분석에 유용해요.문장: "고양이가 야옹했다"벡터 표현: [1, 1, 1, 0, 0] (각 단어의 빈도수)TF-IDF (Term Frequency-Inverse Document Frequency)TF-IDF는 단순한 단어 빈도 외에도 단어의 중요도를 반영한 임베딩 기법이에요. 특정 단어가 문서 내에서 자주 등장하지만 전체 문서에서 드물게 등장한다면, 그 단어는 해당 문서에서 중요한 단어로 간주됩니다.TF: 단어의 빈도IDF: 단어의 전체 문서에서의 등장 빈도 반비례값이를 통해 문서 내에서 의미 있는 단어를 강조할 수 있어요.Word2Vec, GloVeWord2Vec과 GloVe는 단어 간의 의미적 유사성을 반영하는 임베딩 기법이에요. 단어를 고차원 벡터로 변환하여, 단어 간의 관계를 학습합니다.Word2Vec: 주위 단어들에 기반해 단어의 의미를 학습GloVe: 전체 문맥을 기반으로 단어 간의 공통 패턴을 학습이러한 임베딩 기법을 사용하면, 단어의 의미를 벡터로 비교해 문맥 유사성을 파악할 수 있어요.Transformer 기반 임베딩 (BERT, GPT)BERT나 GPT 같은 **Transformer* 모델들은 문장의 문맥을 고려하여 더 깊이 있는 의미를 반영한 임베딩을 생성해요. 특히, 이들은 문장 단위로 텍스트를 벡터화할 수 있어 문장 간의 유사도를 정확하게 파악합니다.BERT: 양방향으로 문맥을 고려한 임베딩 생성GPT: 자동 완성 및 생성에 강점을 둔 임베딩 생성임베딩의 활용임베딩을 활용하면 텍스트 검색, 문서 분류, 대화형 AI 등 다양한 응용 분야에서 의미 기반 검색과 유사성 분석을 수행할 수 있습니다.배운 내용 정리하기!텍스트 처리와 임베딩은 필수적인 기초!텍스트 처리와 임베딩은 LLM 시스템 구축의 핵심 기초입니다. 정확한 텍스트 처리와 적절한 임베딩 기법을 활용하면, LLM의 성능을 최대한 끌어올리고 더 나은 결과를 얻을 수 있어요.텍스트 처리는 모델의 입력을 정제하고, 임베딩은 모델이 추론할 수 있도록 텍스트를 벡터화합니다. 이 둘을 잘 이해하고 적용하는 것이 LLM 시스템 구축의 첫걸음이에요!
Pandas 설치 및 Jupyter Notebook 설정하기 인공지능을 위한 파이썬 라이브러리/인공지능을 위한 파이썬 라이브러리 - 1주차/Pandas 설치 및 Jupyter Notebook 설정하기 제작:Pandas 설치 및 Jupyter Notebook 설정하기 수업 목표Pandas 설치를 진행합니다.Jupyter Notebook 설정 방법에 대해서 학습합니다.목차판다스(Pandas) 설치하기Jupyter Notebook 설정하기 Conda와 가상환경 설정하기가상환경을 Jupyter Notebook과 연결하기알면 좋은 Tip!판다스(Pandas) 설치하기판다스(Pandas)란?📚판다스는 데이터 조작 및 분석을 위한 파이썬 라이브러리입니다.
데이터프레임(DataFrame)을 사용해 엑셀과 유사한 방식으로 데이터를 다룰 수 있습니다.판다스 설치하기Shell복사pip install pandas

​위 명령어를 실행하면 pip을 통해 판다스가 설치됩니다!Jupyter Notebook 설정하기 ​Jupyter Notebook이란?📚Jupyter Notebook은 코드, 텍스트, 이미지 등을 한 곳에서 관리할 수 있는 대화형 컴퓨팅 환경입니다.
데이터 분석, 머신러닝, 교육 등 다양한 분야에서 사용됩니다.Jupyter Notebook 설치하기Shell복사pip install jupyter

​이 명령어로 Jupyter Notebook을 설치할 수 있습니다.Jupyter Notebook 실행하기Shell복사jupyter notebook

​명령어를 실행하면 웹 브라우저가 열리며 Jupyter Notebook 인터페이스로 이동합니다.Conda와 가상환경 설정하기Conda란?Conda는 파이썬 패키지 관리 및 가상환경 관리를 돕는 도구입니다.여러 프로젝트에 서로 다른 패키지 버전을 사용해야 할 때 유용합니다.

Conda 설치하기Anaconda나 Miniconda를 설치하여 Conda를 사용할 수 있습니다.설치 후, Conda 명령어를 사용할 수 있습니다.

가상환경 만들기Shell복사conda create --name myenv

​위 명령어로 myenv라는 이름의 가상환경을 만듭니다.Shell복사conda create --name myenv python=3.8 pandas
​원하는 파이썬 버전과 패키지를 지정할 수도 있습니다.가상환경 활성화 및 비활성화가상환경 활성화:Shell복사conda activate myenv

​가상환경 비활성화:Shell복사conda deactivate
​가상환경을 Jupyter Notebook과 연결하기ipykernel 설치가상환경을 Jupyter Notebook에서 사용하려면 ipykernel을 설치해야 합니다.Shell복사pip install ipykernel

​가상환경을 Jupyter Notebook에 추가하기Shell복사python -m ipykernel install --user --name myenv --display-name "My Env"

​이 명령어를 실행하면 Jupyter Notebook에서 My Env라는 이름으로 가상환경을 선택할 수 있습니다.Jupyter Notebook에서 가상환경 사용하기ALTJupyter Notebook에서 새로운 노트북을 열 때,  My Env를 선택합니다.알면 좋은 Tip!Conda 가상환경 관리 conda env list 명령어로 현재 사용 가능한 가상환경을 확인할 수 있습니다.Jupyter Notebook 확장 다양한 Jupyter Notebook 확장을 설치해 기능을 확장해 보세요! nbextensions 패키지를 사용하면 많은 유용한 확장을 사용할 수 있습니다.📚이렇게 하면 판다스와 Jupyter Notebook 설정이 모두 완료되었습니다! ​
[스파르타코딩클럽] 4강. Jupyter Notebook 사용해보기[SCC] 바닥부터 시작하는 머신러닝/[스파르타코딩클럽] 바닥부터 시작하는 머신러닝 - 1주차/[스파르타코딩클럽] 4강. Jupyter Notebook 사용해보기제작:[스파르타코딩클럽] 4강. Jupyter Notebook 사용해보기[수업 목표]Jupyter Notebook 사용해보기![목차]Jupyter Notebook이란?
 
 Jupyter Notebook이란?Jupyter Notebook이 무엇인지 알아봅시다.1) Jupyter Notebook 소개 Jupyter Notebook이란Jupyter Notebook은 데이터 과학자, 연구자, 교육자들이 널리 사용하는 오픈 소스 웹 애플리케이션입니다. 이를 통해 사용자는 실시간으로 코드를 작성하고 실행하며, 그 결과를 시각적으로 확인하고, 문서화할 수 있습니다.Anaconda를 설치하면 자동으로 설치됩니다!2) Jupyter Notebook 사용하기 Jupyter Notebook 사용법환경 설정!가상환경 만들기conda create --name myenv가상환경 활성화conda activate myenv필요 패키지 설치conda install jupyter numpy pandas가상환경과 Jupyter 연결하기python -m ipykernel install --user --name=myenv --display-name "Python (myenv)"사용하기!ALTALTALT Jupyter 사용확인!세팅 확인{5px}세팅 확인﻿​ALT.
[스파르타코딩클럽] 4. 인공 신경망(ANN)[SCC] 기초가 탄탄한 딥러닝/[스파르타코딩클럽] 기초가 탄탄한 딥러닝 - 2주차/[스파르타코딩클럽] 4. 인공 신경망(ANN)제작:[스파르타코딩클럽] 4. 인공 신경망(ANN)[수업 목표]인공신경망의 개념에 대해서 배워보고 어떤 원리로 동작하는지 알아봅시다Pytorch로 간단한 인공신경망 모델 구현 실습을 진행해 봅시다[목차]01. 기본 구조와 동작원리02. 실습: 간단한 인공 신경망 모델 구현 (PyTorch)💡
 
 01. 기본 구조와 동작원리✔️ANN의 기본 구성요소와 어떤 방식으로 동작하는지 배워보고, ANN의 출력 레이어의 유형에 따라 어떻게 활용 할 수 있는지 확인해 봅시다1) ANN의 기본 구성 요소와 동작 방식 ANN의 기본 구성 요소인공 신경망(Artificial Neural Network, ANN)은 생물학적 신경망을 모방하여 설계된 컴퓨팅 시스템입니다ANN은 입력층(Input Layer), 은닉층(Hidden Layer), 출력층(Output Layer)으로 구성되며, 각 층은 뉴런(Neuron)으로 이루어져 있습니다.ALT입력층
 - 입력데이터를 받아들이는 층, 입력층의 뉴런수는 입력데이터 피쳐수와 동일 - 입력데이터를 받아들이는 층, 입력층의 뉴런수는 입력데이터 피쳐수와 동일﻿
은닉층
 - 입력데이터를 처리하고 특징을 추출하는 층, 은닉층의 뉴런수와 층수는 모델의 복잡성과 성능에 영향 - 입력데이터를 처리하고 특징을 추출하는 층, 은닉층의 뉴런수와 층수는 모델의 복잡성과 성능에 영향﻿
출력층
 - 최종 예측값을 출력하는 층, 출력층의 뉴런 수는 예측하려는 클래스 수 또는 회귀문제 출력차원과 동일 - 최종 예측값을 출력하는 층, 출력층의 뉴런 수는 예측하려는 클래스 수 또는 회귀문제 출력차원과 동일﻿​ 동작 방식순전파 (Forward Propagation)입력 데이터를 통해 각 층의 뉴런이 활성화되고, 최종 출력 값을 계산합니다.각 뉴런은 입력 값에 가중치(weight)를 곱하고, 바이어스(bias)를 더한 후 활성화 함수(activation function)를 통해 출력 값을 결정합니다.손실 계산 (Loss Calculation)예측 값과 실제 값의 차이를 손실 함수(Loss Function)로 계산합니다.역전파 (Backpropagation)손실 함수의 기울기를 출력층에서 입력층 방향으로 계산하고, 이를 바탕으로 가중치를 업데이트합니다.ALT2) 출력 레이어의 구성 출력레이어의 유형과 활용출력 레이어는 신경망의 최종 예측 값을 출력하는 층으로, 문제의 유형에 따라 다양한 형태로 구성될 수 있습니다.회귀 문제 (Regression):출력 레이어의 뉴런 수는 예측하려는 연속적인 값의 차원과 동일합니다.활성화 함수로는 주로 선형 함수(linear function)를 사용합니다.이진 분류 문제 (Binary Classification):출력 레이어의 뉴런 수는 1입니다.활성화 함수로는 시그모이드 함수(Sigmoid Function)를 사용하여 출력 값을 0과 1 사이의 확률로 변환합니다.다중 클래스 분류 문제 (Multi-Class Classification):출력 레이어의 뉴런 수는 예측하려는 클래스 수와 동일합니다.활성화 함수로는 소프트맥스 함수(Softmax Function)를 사용하여 각 클래스에 대한 확률을 출력합니다.02. 실습: 간단한 인공 신경망 모델 구현 (PyTorch)✔️ PyTorch를 사용하여 간단한 인공 신경망 모델을 구축하고 학습해보겠습니다. 예제로는 MNIST 데이터셋을 사용하여 숫자 이미지를 분류하는 모델을 구현하겠습니다.1)  간단한 ANN 모델 구축 및 학습 PyTorch 및 필요한 라이브러리 임포트PyTorch 및 필요한 라이브러리 임포트 {5px}PyTorch 및 필요한 라이브러리 임포트 ﻿​Python복사import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
​데이터셋 로드 및 전처리데이터셋 로드 및 전처리 {5px}데이터셋 로드 및 전처리 ﻿​Python복사# 데이터셋 전처리
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])
# MNIST 데이터셋 로드
trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)

testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)
​ 간단한 ANN 모델 정의간단한 ANN 모델 정의 {5px}간단한 ANN 모델 정의 ﻿​Python복사class SimpleANN(nn.Module):
def __init__(self):
super(SimpleANN, self).__init__()
        self.fc1 = nn.Linear(28 * 28, 128) # 입력층에서 은닉층으로
        self.fc2 = nn.Linear(128, 64) # 은닉층에서 은닉층으로
        self.fc3 = nn.Linear(64, 10) # 은닉층에서 출력층으로
def forward(self, x):
        x = x.view(-1, 28 * 28) # 입력 이미지를 1차원 벡터로 변환
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        x = self.fc3(x)
return x
​torch.nn.Module: 모든 신경망 모듈의 기본 클래스입니다.  사용자 정의 신경망은 이 클래스를 상속받아야 합니다.nn.Linear: 선형 변환을 적용하는 완전 연결(fully connected) 레이어를 정의합니다.nn.Linear(in_features, out_features)는 입력 특징의 수와 출력 특징의 수를 지정합니다..Linear(in_features, out_features)는 입력 특징의 수와 출력 특징의 수를 지정합니다.﻿​torch.relu: ReLU 활성화 함수를 적용합니다.view: 텐서의 크기를 변경합니다.x.view(-1, 28 * 28)은 입력 이미지를 1차원 벡터로 변환합니다..view(-1, 28 * 28)은 입력 이미지를 1차원 벡터로 변환합니다.﻿​ 모델 학습모델 학습 {5px}모델 학습 ﻿​Python복사# 모델 초기화
model = SimpleANN()
# 손실 함수와 최적화 알고리즘 정의
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
# 모델 학습
for epoch in range(10): # 10 에포크 동안 학습
    running_loss = 0.0
for i, data in enumerate(trainloader, 0):
        inputs, labels = data

        # 기울기 초기화
        optimizer.zero_grad()
# 순전파 + 역전파 + 최적화
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
# 손실 출력
        running_loss += loss.item()
if i % 100 == 99: # 매 100 미니배치마다 출력
print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 100:.3f}')
            running_loss = 0.0
print('Finished Training')
​nn.CrossEntropyLoss: 다중 클래스 분류 문제에서 주로 사용되는 손실 함수입니다. 예측 값과 실제 값 사이의 교차 엔트로피 손실을 계산합니다.optim.SGD: 확률적 경사 하강법(Stochastic Gradient Descent) 최적화 알고리즘을 정의합니다.  lr은 학습률, momentum은 모멘텀 값을 지정합니다.은 학습률, momentum은 모멘텀 값을 지정합니다.﻿​optimizer.zero_grad(): 이전 단계에서 계산된 기울기를 초기화합니다.loss.backward(): 역전파를 통해 기울기를 계산합니다.optimizer.step(): 계산된 기울기를 바탕으로 가중치를 업데이트합니다. 모델  평가모델 평가 {5px}모델 평가 ﻿​Python복사correct = 0
total = 0
with torch.no_grad():
for data in testloader:
        images, labels = data
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')

​torch.no_grad(): 평가 단계에서는 기울기를 계산할 필요가 없으므로, 이를 비활성화하여 메모리 사용을 줄입니다.torch.max: 텐서의 최대 값을 찾습니다. torch.max(outputs.data, 1)은 각 샘플에 대해 가장 높은 확률을 가진 클래스를 반환합니다..max(outputs.data, 1)은 각 샘플에 대해 가장 높은 확률을 가진 클래스를 반환합니다.﻿​labels.size(0): 배치 크기를 반환합니다.(predicted == labels).sum().item(): 예측 값과 실제 값이 일치하는 샘플의 수를 계산합니다..
LangChain: 개념과 활용  LLM & RAG를 활용한 AI 서비스 만들기/ LLM & RAG를 활용한 AI 서비스 만들기 - 5주차/LangChain: 개념과 활용 제작:LangChain: 개념과 활용 수업 목표Langchain에 대해서 학습하고, 사용 사례와 주요 장점을 살펴봅니다.목차 LangChain이란?LangChain이 왜 좋은가LangChain의 주요 개념LangChain 사용 사례배운 내용 정리하기!❗LangChain은 LLM(대형 언어 모델)과 다양한 컴포넌트를 연결하여 복잡한 작업을 수행하고 체인 형태로 구성된 애플리케이션을 만들 수 있는 프레임워크입니다. 이를 통해 LangChain은 간단한 텍스트 생성에서 복잡한 멀티스텝 워크플로우까지 다양한 AI 애플리케이션을 개발하는 데 도움을 줍니다. LangChain이란?💡LangChain은 언어 모델을 중심으로 다양한 데이터 소스와 툴을 연결하여 체인 기반 애플리케이션을 구축할 수 있는 Python 기반 프레임워크입니다. 

LangChain을 사용하면 하나의 언어 모델 응답만 받는 대신,
여러 단계로 구성된 체인 구조를 통해 다양한 연산과 데이터 처리, 멀티스텝 분석이 가능합니다.

예를 들어, 특정 질문에 대해 외부 데이터베이스에서 정보를 검색하고, 이를 종합하여 응답을 생성하는 과정 등을 자동화할 수 있습니다.LangChain이 왜 좋은가유연한 구성
LangChain은 언어 모델과 다양한 컴포넌트를 쉽게 연결할 수 있습니다. 모델의 응답을 다른 컴포넌트로 보내거나, 여러 단계에 걸친 데이터 처리가 가능합니다.모듈화된 컴포넌트
프롬프트 템플릿, 출력 파서, 벡터 데이터베이스, 에이전트 등을 통해 각 컴포넌트를 필요에 따라 조합할 수 있어 재사용성과 확장성이 높습니다.체인과 에이전트
단순한 질문-응답을 넘어서 여러 작업을 순차적으로 실행하는 체인과 상황에 따라 행동을 결정하는 에이전트를 통해 복잡한 작업을 자동화할 수 있습니다.강력한 통합 기능
LangChain은 OpenAI, HuggingFace, FAISS, ElasticSearch 등 다양한 언어 모델, 벡터 데이터베이스와의 통합이 가능해 데이터 소스 확장과 빠른 검색이 가능합니다.LangChain의 주요 개념언어 모델 (LLM)언어 모델은 주어진 입력을 바탕으로 텍스트를 생성합니다. LangChain은 OpenAI의 GPT 모델을 포함해 다양한 언어 모델과의 통합을 지원합니다.예시: "LangChain을 사용하여 OpenAI의 GPT-4 모델을 불러와 기본적인 질문에 답변하도록 설정"프롬프트 템플릿 (Prompt Templates)프롬프트 템플릿은 프롬프트를 동적으로 생성하는 데 사용됩니다. 특정 입력 값에 따라 템플릿이 채워져 모델에 전달되므로 반복적인 작업을 단순화합니다.예시: 사용자 질문에 따라 특정 정보 검색을 요구하는 시스템 프롬프트 생성체인 (Chains)여러 단계를 거치는 워크플로우를 하나로 묶어주는 기능입니다. 예를 들어, 사용자의 질문을 분석해 필요한 데이터를 검색하고, 검색 결과를 기반으로 응답을 생성하는 일련의 과정을 체인으로 구성할 수 있습니다.예시: 사용자 질문을 받은 후 검색 -> 분석 -> 응답 생성의 3단계 체인 생성에이전트 (Agents)에이전트는 동적으로 필요한 작업을 결정하고 수행하는 컴포넌트입니다. 질문에 따라 답변하기 위해 API 호출이 필요한지, 또는 단순히 텍스트 생성을 해야 하는지를 판단해 작업을 실행합니다.예시: 질문이 지리 정보에 관련된 경우, API를 호출해 최신 정보를 제공하는 에이전트벡터 데이터베이스 (Vector Databases)벡터 데이터베이스는 텍스트를 벡터로 변환해 저장하고, 이후 유사한 벡터를 빠르게 검색할 수 있도록 돕습니다. 이를 통해, 저장된 데이터와 유사한 질문에 빠르게 응답할 수 있습니다.예시: FAISS와 같은 벡터 데이터베이스에 문서를 저장해 유사성 검색을 수행LangChain 사용 사례1️⃣검색 기반 생성(RAG)
LangChain을 사용하여 질문에 답할 때 관련 문서를 검색하고, 해당 내용을 바탕으로 응답을 생성할 수 있습니다. 이를 통해 최신 정보에 기반한 답변을 생성할 수 있습니다.2️⃣FAQ 시스템
다양한 질문에 대한 답변을 벡터 데이터베이스에 저장하고, 유사성 검색을 통해 빠르게 적절한 답변을 제공합니다.3️⃣다단계 챗봇 워크플로우
복잡한 질문에 대해 여러 단계를 거쳐 답변을 구성하는 챗봇을 설계할 수 있습니다. 예를 들어, 질문을 받아 요약한 후, 관련된 추가 정보를 검색해 제공하는 방식입니다.4️⃣지능형 에이전트
주식 가격을 확인하거나 뉴스 데이터를 검색해 최신 정보를 제공하는 챗봇 에이전트를 구현할 수 있습니다.LangChain의 주요 장점 요약확장성: 언어 모델과 다양한 데이터 소스를 연결하여 유연하게 확장 가능자동화: 복잡한 멀티스텝 프로세스를 체인 형태로 구성하여 자동화 가능유사성 검색: 벡터 데이터베이스를 활용해 유사 문서를 빠르게 검색 및 응답 가능재사용성: 프롬프트 템플릿, 체인, 에이전트 등을 활용해 재사용 가능한 애플리케이션 구성 가능배운 내용 정리하기!😀LangChain은 LLM의 강력한 성능을 다양한 데이터 소스와 통합해 유연하고 확장성 있는 AI 애플리케이션을 개발하는 데 큰 도움을 줍니다. LangChain의 다양한 컴포넌트를 이해하고 사용하는 것은 AI 애플리케이션의 효율성과 생산성을 높이는 데 중요한 역할을 합니다.
배열 연산 및 브로드캐스팅인공지능을 위한 파이썬 라이브러리/인공지능을 위한 파이썬 라이브러리 - 2주차/배열 연산 및 브로드캐스팅제작:배열 연산 및 브로드캐스팅수업 목표Numpy의 배열 연산 및 브로드캐스팅에 대해서 학습합니다.목차배열 연산 및 브로드캐스팅기본적인 수학 함수와 통계 함수요약배열 연산 및 브로드캐스팅📚NumPy에서는 배열 간의 연산을 매우 효율적으로 수행할 수 있습니다. 특히 브로드캐스팅(Broadcasting) 기능은 크기가 다른 배열 간의 연산을 가능하게 해줍니다.배열 간의 기본 연산NumPy 배열 간의 덧셈, 뺄셈, 곱셈, 나눗셈 모두 요소별(element-wise)로 수행됩니다.Python복사import numpy as np

arr1 = np.array([1, 2, 3])
arr2 = np.array([4, 5, 6])
# 덧셈
add_result = arr1 + arr2
print(add_result)
# 곱셈
mul_result = arr1 * arr2
print(mul_result)

​출력 결과Plain Text복사[5 7 9]
[ 4 10 18]

​브로드캐스팅(Broadcasting)📚브로드캐스팅이란?작은 배열이 큰 배열의 형태에 맞게 자동으로 확장되어 연산이 이루어지는 것을 의미해요. 
이 기능 덕분에 코드를 더 간결하게 작성할 수 있습니다.서로 다른 크기의 배열 간에도 연산이 가능합니다. 작은 배열이 자동으로 확장되어 연산됩니다.Python복사arr1 = np.array([1, 2, 3])
arr2 = np.array([[10], [20], [30]])
# 브로드캐스팅을 이용한 덧셈
broadcast_result = arr1 + arr2
print(broadcast_result)

​출력 결과Plain Text복사[[11 12 13]
 [21 22 23]
 [31 32 33]]

​기본적인 수학 함수와 통계 함수📚NumPy는 배열을 다룰 때 자주 사용하는 다양한 수학 함수와 통계 함수를 제공합니다.기본적인 수학 함수배열의 합(sum)과 곱(prod)Python복사arr = np.array([1, 2, 3, 4])
# 합계
sum_result = np.sum(arr)
print(sum_result)
# 곱
prod_result = np.prod(arr)
print(prod_result)

​출력 결과Plain Text복사10
24

​제곱근과 로그Python복사# 제곱근
sqrt_result = np.sqrt(arr)
print(sqrt_result)
# 자연로그
log_result = np.log(arr)
print(log_result)

​출력 결과Plain Text복사[1.         1.41421356 1.73205081 2.        ]
[0.         0.69314718 1.09861229 1.38629436]

​기본적인 통계 함수평균(mean), 중간값(median), 표준편차(std)Python복사arr = np.array([1, 2, 3, 4, 5])
# 평균
mean_result = np.mean(arr)
print(mean_result)
# 중간값
median_result = np.median(arr)
print(median_result)
# 표준편차
std_result = np.std(arr)
print(std_result)

​출력 결과Plain Text복사3.0
3.0
1.4142135623730951

​최대값(max)과 최소값(min)Python복사arr = np.array([1, 3, 2, 7, 5])
# 최대값
max_result = np.max(arr)
print(max_result)
# 최소값
min_result = np.min(arr)
print(min_result)

​출력 결과Plain Text복사7
1

​요약📚요약배열 간 연산은 요소별로 수행되며, 크기가 다른 배열 간에도 브로드캐스팅을 통해 연산이 가능합니다.NumPy는 합계, 곱, 제곱근, 로그와 같은 다양한 수학 함수를 제공하며, 이를 통해 복잡한 계산을 간단하게 수행할 수 있어요.평균, 중간값, 표준편차, 최대값, 최소값 등의 통계 함수도 쉽게 사용할 수 있어 데이터 분석에 매우 유용합니다.이제 NumPy의 강력한 연산 능력을 활용해 더 복잡한 데이터 분석을 시도해보세요! ​
[스파르타코딩클럽] 5강. 데이터셋 불러오기[SCC] 바닥부터 시작하는 머신러닝/[스파르타코딩클럽] 바닥부터 시작하는 머신러닝 - 2주차/[스파르타코딩클럽] 5강. 데이터셋 불러오기제작:[스파르타코딩클럽] 5강. 데이터셋 불러오기[수업 목표]pandas 라이브러리를 이용하여 데이터를 불러오는 법을 배웁니다.캐글(Kaggle)에 대해 알아보고, 캐글의 데이터셋을 다운받아 불러오는 실습을 해봅니다.[목차]01. 데이터 불러오기 및 탐색02. 캐글(Kaggle) 소개 및 데이터셋 다운 받기01. 데이터 불러오기 및 탐색Pandas를 이용하여 데이터를 불러오고 기본 정보를 확인해 봅니다1) 데이터 불러오기 (CSV 및 엑셀파일) Pandas 라이브러리 소개데이터 조작 및 분석을 위한 Python 라이브러리pandas는 데이터 프레임(DataFrame) 구조를 사용해 데이터를 효율적으로 관리/조작 할 수있습니다 CSV 파일 불러오기CSV : 콤마로 구분된 값들을 저장하는 텍스트 파일Pandas의 read_csv 함수를 사용하여 CSV 파일을 불러올 수 있습니다.CSV 불러오기 {5px}CSV 불러오기 ﻿​Python복사import pandas as pd

# CSV 파일 불러오기
df = pd.read_csv('data.csv')
# 데이터 프레임의 첫 5행 출력
print(df.head())
​ 엑셀 파일 불러오기Pandas의 read_excel 함수를 사용하여 엑셀 파일을 불러올 수 있습니다.엑셀 불러오기 {5px}엑셀 불러오기 ﻿​Python복사import pandas as pd

# 엑셀 파일 불러오기
df = pd.read_excel('data.xlsx', sheet_name='Sheet1')
# 데이터 프레임의 첫 5행 출력
print(df.head())
​2) 데이터 구조 확인해 보기 데이터 프레임의 기본 정보 확인Pandas에서 제공하는 다양한 메서드들을 이용하여 데이터 프레임의 구조와 기본 정보 확인 가능데이터프레임 정보확인 {5px}데이터프레임 정보확인 ﻿​Python복사# 데이터 프레임의 크기 (행, 열) 확인
print(df.shape)
# 데이터 프레임의 컬럼명 확인
print(df.columns)
# 데이터 프레임의 데이터 타입 확인
print(df.dtypes)
# 데이터 프레임의 요약 통계량 확인
print(df.describe())
# 데이터 프레임의 정보 확인 (null 값, 데이터 타입 등)
print(df.info())
​02. 캐글(Kaggle) 소개 및 데이터셋 다운 받기사용 권한 없음동기화된 이 블록에 대한 사용 권한이 없습니다사용 권한 요청1) 캐글(Kaggle) 소개 캐글이란?데이터 과학 및 머신러닝 경진대회 플랫폼데이터 사이언티스트 / 머신러닝 엔지니어들이 다양한 문제를 해결하고 데이터를 분석모델을 개발하는데 필요한 데이터셋과 도구를 제공 캐글의 주요 기능경진대회: 다양한 데이터 과학 및 머신러닝 문제를 해결하는 경진대회가 열립니다.데이터셋: 다양한 주제의 데이터셋을 무료로 다운로드할 수 있습니다.커뮤니티: 데이터 과학자와 머신러닝 엔지니어들이 지식을 공유하고 협업할 수 있는 커뮤니티입니다.커널: 웹 기반의 코드 실행 환경으로, Jupyter 노트북과 유사한 기능을 제공합니다.2) 데이터셋 다운받기 캐글 데이터셋 다운로드 방법캐글에서 데이터셋을 다운로드 하기 위해서는 먼저 캐글 계정이 필요합니다캐글에서 데이터셋을 다운로드 하기 위해서는 먼저 캐글 계정이 필요합니다﻿​캐글 API 설치Python복사pip install kaggle
​캐글 API 키 설정캐글 계정에서 API키 생성하고 로컬 환경에 저장API 키는 ~/.kaggle/kaggle.json 파일에 저장됩니다.데이터셋 다운로드Python복사kaggle datasets download -d <dataset-identifier>
​ 예시 : 타이타닉 데이터셋 다운로드타이타닉 생존자 예측 경진대회의 데이터셋 다운로드 예시타이타닉 데이터셋 다운로드 {5px}타이타닉 데이터셋 다운로드 ﻿​Python복사# 타이타닉 데이터셋 다운로드
kaggle competitions download -c titanic

# 다운로드된 파일 압축 해제
unzip titanic.zip
​다운로드한 데이터 import {5px}다운로드한 데이터 import ﻿​Python복사import pandas as pd

# 타이타닉 데이터셋 불러오기
train_df = pd.read_csv('train.csv')
test_df = pd.read_csv('test.csv')
# 데이터 프레임의 첫 5행 출력
print(train_df.head())
print(test_df.head())
​.
[스파르타코딩클럽] 5. 합성곱 신경망(CNN)[SCC] 기초가 탄탄한 딥러닝/[스파르타코딩클럽] 기초가 탄탄한 딥러닝 - 2주차/[스파르타코딩클럽] 5. 합성곱 신경망(CNN)제작:[스파르타코딩클럽] 5. 합성곱 신경망(CNN)[수업 목표]합성곱 신경망의 개념에 대해서 배워보고 어떤 원리로 동작하는지 알아봅시다Pytorch로 간단한 CNN 모델 구현 실습을 진행해 봅시다[목차]01. CNN의 기본 구조와 동작 원리02. 실습: CNN을 이용한 이미지 분류 (PyTorch)
 
 01. CNN의 기본 구조와 동작 원리CNN의 기본 구성요소와 어떤 방식으로 동작하는지 배워봅시다1) CNN의 기본 구조 CNN의 기본 구조합성곱 신경망(Convolutional Neural Network, CNN)은 이미지와 같은 2차원 데이터의 특징을 효과적으로 추출하기 위해 설계된 신경망입니다.CNN은 주로 합성곱 층(Convolutional Layer), 풀링 층(Pooling Layer), 완전 연결 층(Fully Connected Layer)으로 구성됩니다.ALT합성곱 층 (Convolutional Layer):
 - 입력 이미지에 필터(커널)를 적용하여 특징 맵(feature map)을 생성합니다. - 입력 이미지에 필터(커널)를 적용하여 특징 맵(feature map)을 생성합니다.﻿
 - 필터는 이미지의 국소적인 패턴을 학습합니다. - 필터는 이미지의 국소적인 패턴을 학습합니다.﻿
풀링 층 (Pooling Layer)
 - 특징 맵의 크기를 줄이고, 중요한 특징을 추출합니다. - 특징 맵의 크기를 줄이고, 중요한 특징을 추출합니다.﻿
 - 주로 Max Pooling과 Average Pooling이 사용됩니다. - 주로 Max Pooling과 Average Pooling이 사용됩니다.﻿
완전 연결 층 (Fully Connected Layer)
 - 추출된 특징을 바탕으로 최종 예측을 수행합니다. - 추출된 특징을 바탕으로 최종 예측을 수행합니다.﻿
 - CNN이라는 분석레이어를 통해 추출한 특성을 바탕으로 결론을 내리는 부분 - CNN이라는 분석레이어를 통해 추출한 특성을 바탕으로 결론을 내리는 부분﻿​2) 합성곱 연산과 필터 합성곱 연산의 원리와 필터의 역할합성곱 연산은 입력 이미지에 필터(커널)를 적용하여 특징 맵을 생성하는 과정입니다. 필터는 작은 크기의 행렬로, 이미지의 국소적인 패턴을 학습합니다.합성곱 연산:필터를 이미지의 각 위치에 슬라이딩하며, 필터와 이미지의 해당 부분 간의 점곱(dot product)을 계산합니다.계산된 값은 특징 맵의 해당 위치에 저장됩니다.필터의 역할:필터는 이미지의 에지(edge), 코너(corner), 텍스처(texture) 등 다양한 국소적인 패턴을 학습합니다.여러 개의 필터를 사용하여 다양한 특징 맵을 생성할 수 있습니다.3) 풀링 레이어, 플래튼 풀링 레이어의 필요성과 종류풀링 층은 특징 맵의 크기를 줄이고, 중요한 특징을 추출하는 역할을 합니다. 풀링 층은 주로 Max Pooling과 Average Pooling이 사용됩니다.Max Pooling:필터 크기 내에서 최대 값을 선택합니다.중요한 특징을 강조하고, 불필요한 정보를 제거합니다.Average Pooling:필터 크기 내에서 평균 값을 계산합니다.특징 맵의 크기를 줄이면서, 정보의 손실을 최소화합니다. 플래튼 레이어의 역할플래튼 층(Flatten Layer)은 2차원 특징 맵을 1차원 벡터로 변환하는 역할을 합니다. 이는 완전 연결 층에 입력으로 사용하기 위해 필요합니다.4) CNN 구조와 응용 다양한 CNN 아키텍처LeNet:최초의 CNN 아키텍처 중 하나로, 손글씨 숫자 인식에 사용되었습니다.합성곱 층과 풀링 층을 반복한 후, 완전 연결 층을 사용합니다.AlexNet:2012년 이미지넷 대회에서 우승한 아키텍처로, 딥러닝의 가능성을 입증했습니다.ReLU 활성화 함수와 드롭아웃(dropout)을 도입하여 성능을 향상시켰습니다.VGG:깊고 규칙적인 구조를 가진 아키텍처로, 작은 3x3 필터를 사용하여 깊이를 증가시켰습니다.VGG16과 VGG19가 대표적인 모델입니다.02. 실습: CNN을 이용한 이미지 분류 (PyTorch) 이제 PyTorch를 사용하여 간단한 CNN 모델을 구축하고, CIFAR-10 데이터셋을 사용하여 이미지 분류를 수행해보겠습니다1)  간단한 CNN 모델을 이용한 이미지 분류 실습 PyTorch 및 필요한 라이브러리 임포트PyTorch 및 필요한 라이브러리 임포트 {5px}PyTorch 및 필요한 라이브러리 임포트 ﻿​Python복사import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
​데이터셋 로드 및 전처리데이터셋 로드 및 전처리 {5px}데이터셋 로드 및 전처리 ﻿​Python복사# 데이터셋 전처리
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])
# CIFAR-10 데이터셋 로드
trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)

testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)
​ 간단한 CNN 모델 정의간단한 CNN 모델 정의 {5px}간단한 CNN 모델 정의 ﻿​Python복사class SimpleCNN(nn.Module):
def __init__(self):
super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, 3, padding=1) # 입력 채널 3, 출력 채널 32, 커널 크기 3x3
        self.pool = nn.MaxPool2d(2, 2) # 풀링 크기 2x2
        self.conv2 = nn.Conv2d(32, 64, 3, padding=1) # 입력 채널 32, 출력 채널 64, 커널 크기 3x3
        self.fc1 = nn.Linear(64 * 8 * 8, 512) # 완전 연결 층
        self.fc2 = nn.Linear(512, 10) # 출력 층 (10개의 클래스)
def forward(self, x):
        x = self.pool(torch.relu(self.conv1(x)))
        x = self.pool(torch.relu(self.conv2(x)))
        x = x.view(-1, 64 * 8 * 8) # 플래튼
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
return x
​nn.Conv2d: 2차원 합성곱 층을 정의합니다. nn.Conv2d(in_channels, out_channels, kernel_size, padding)은 입력 채널 수, 출력 채널 수, 커널 크기, 패딩을 지정.Conv2d(in_channels, out_channels, kernel_size, padding)은 입력 채널 수, 출력 채널 수, 커널 크기, 패딩을 지정﻿​nn.MaxPool2d: 2차원 최대 풀링 층을 정의합니다.nn.MaxPool2d(kernel_size, stride)은 풀링 크기와 스트라이드를 지정합니다..MaxPool2d(kernel_size, stride)은 풀링 크기와 스트라이드를 지정합니다.﻿​view: 텐서의 크기를 변경합니다.x.view(-1, 64 * 8 * 8)은 특징 맵을 1차원 벡터로 변환합니다..view(-1, 64 * 8 * 8)은 특징 맵을 1차원 벡터로 변환합니다.﻿​ 모델 학습모델 학습 {5px}모델 학습 ﻿​Python복사# 모델 초기화
model = SimpleCNN()
# 손실 함수와 최적화 알고리즘 정의
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
# 모델 학습
for epoch in range(10): # 10 에포크 동안 학습
    running_loss = 0.0
for i, data in enumerate(trainloader, 0):
        inputs, labels = data

        # 기울기 초기화
        optimizer.zero_grad()
# 순전파 + 역전파 + 최적화
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
# 손실 출력
        running_loss += loss.item()
if i % 100 == 99: # 매 100 미니배치마다 출력
print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 100:.3f}')
            running_loss = 0.0
print('Finished Training')
​nn.CrossEntropyLoss: 다중 클래스 분류 문제에서 주로 사용되는 손실 함수입니다. 예측 값과 실제 값 사이의 교차 엔트로피 손실을 계산합니다.optim.SGD: 확률적 경사 하강법(Stochastic Gradient Descent) 최적화 알고리즘을 정의합니다.  lr은 학습률, momentum은 모멘텀 값을 지정합니다.은 학습률, momentum은 모멘텀 값을 지정합니다.﻿​optimizer.zero_grad(): 이전 단계에서 계산된 기울기를 초기화합니다.loss.backward(): 역전파를 통해 기울기를 계산합니다.optimizer.step(): 계산된 기울기를 바탕으로 가중치를 업데이트합니다. 모델  평가모델 평가 {5px}모델 평가 ﻿​Python복사correct = 0
total = 0
with torch.no_grad():
for data in testloader:
        images, labels = data
        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')

​torch.no_grad(): 평가 단계에서는 기울기를 계산할 필요가 없으므로, 이를 비활성화하여 메모리 사용을 줄입니다.torch.max: 텐서의 최대 값을 찾습니다. torch.max(outputs.data, 1)은 각 샘플에 대해 가장 높은 확률을 가진 클래스를 반환합니다..max(outputs.data, 1)은 각 샘플에 대해 가장 높은 확률을 가진 클래스를 반환합니다.﻿​labels.size(0): 배치 크기를 반환합니다.(predicted == labels).sum().item(): 예측 값과 실제 값이 일치하는 샘플의 수를 계산합니다..
Python LangChain과 FAISS LLM & RAG를 활용한 AI 서비스 만들기/ LLM & RAG를 활용한 AI 서비스 만들기 - 5주차/Python LangChain과 FAISS제작:Python LangChain과 FAISS수업 목표LangChain 사용을 위한 환경 설정을 진행합니다.FAISS를 활용한 벡터 데이터베이스를 구성하는 실습을 진행합니다.목차설치 및 기본 설정LangChain 기본 개념언어 모델 초기화프롬프트 템플릿 사용하기LangChain Expression Language (LCEL)로 체인 연결FAISS를 활용한 벡터 데이터베이스 구성 및 쿼리Step 1: OpenAI 임베딩 모델로 벡터 임베딩 생성Step 2: FAISS 인덱스 초기화Step 3: 벡터 데이터베이스에 문서 추가Step 4: 벡터 데이터베이스 쿼리RAG 체인에 FAISS 통합Step 1: Retriever로 변환Step 2: RAG 체인 생성FAISS 인덱스의 저장 및 로드FAISS 데이터베이스 병합이번에는 LangChain과 FAISS를 이용한 실습을 진행할 예정인데요,
실습은 아래와 같은 순서로 진행됩니다!

설치 및 기본 설정LangChain 기본 개념 - 언어 모델, 프롬프트 템플릿LangChain Expression Language (LCEL)로 체인 연결FAISS를 활용한 벡터 데이터베이스 구성 및 쿼리RAG 체인에 FAISS 통합FAISS 인덱스의 저장 및 로드, 병합설치 및 기본 설정LangChain, OpenAI, 그리고 FAISS 패키지를 설치합니다. GPU를 사용하는 경우 faiss-gpu를 사용할 수 있습니다.Shell복사pip install langchain langchain-openai faiss-cpu

​
설치 후, OpenAI API 키를 설정해 사용 환경을 준비합니다.Python복사import os
from getpass import getpass

os.environ["OPENAI_API_KEY"] = getpass("OpenAI API key 입력: ")

​LangChain 기본 개념언어 모델 초기화OpenAI의 GPT-4 모델을 LangChain을 통해 사용해 봅니다. ChatOpenAI를 이용해 초기화하고 invoke 메서드를 통해 메시지를 전달하여 응답을 받아옵니다.Python복사from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

# 모델 초기화
model = ChatOpenAI(model="gpt-4")
# 모델에 메시지 전달
response = model.invoke([HumanMessage(content="안녕하세요, 무엇을 도와드릴까요?")])
print(response.content)

​프롬프트 템플릿 사용하기프롬프트 템플릿은 다양한 입력을 받아 메시지를 생성하는데 도움을 줍니다. 예를 들어, 영어 문장을 다른 언어로 번역하는 프롬프트를 정의해 봅시다.Python복사from langchain_core.prompts import ChatPromptTemplate

# 시스템 메시지 설정
system_template = "Translate the following sentence from English to {language}:"
# 사용자 텍스트 입력
prompt_template = ChatPromptTemplate.from_messages([
("system", system_template),
("user", "{text}")
])
# 프롬프트 생성
result = prompt_template.invoke({"language": "French", "text": "How are you?"})
print(result.to_messages())

​LangChain Expression Language (LCEL)로 체인 연결여러 컴포넌트를 체인으로 연결하여 데이터 흐름을 통제하는 LCEL을 사용합니다.Python복사from langchain_core.output_parsers import StrOutputParser

# 응답을 파싱하는 파서 초기화
parser = StrOutputParser()
# 템플릿, 모델, 파서를 체인으로 연결
chain = prompt_template | model | parser

# 체인 실행
response = chain.invoke({"language": "Spanish", "text": "Where is the library?"})
print(response)

​FAISS를 활용한 벡터 데이터베이스 구성 및 쿼리FAISS는 벡터 유사성 검색을 위한 라이브러리입니다. OpenAIEmbeddings로 텍스트를 벡터로 변환해 FAISS 인덱스에 저장합니다.Step 1: OpenAI 임베딩 모델로 벡터 임베딩 생성Python복사from langchain_openai import OpenAIEmbeddings

# OpenAI 임베딩 모델 초기화
embeddings = OpenAIEmbeddings(model="text-embedding-ada-002")

​Step 2: FAISS 인덱스 초기화Python복사import faiss
from langchain_community.vectorstores import FAISS
from langchain_community.docstore.in_memory import InMemoryDocstore

# FAISS 인덱스 생성
index = faiss.IndexFlatL2(len(embeddings.embed_query("hello world")))
vector_store = FAISS(
    embedding_function=embeddings,
    index=index,
    docstore=InMemoryDocstore(),
    index_to_docstore_id={}
)

​Step 3: 벡터 데이터베이스에 문서 추가Python복사from langchain_core.documents import Document
from uuid import uuid4

# 문서 생성
documents = [
    Document(page_content="LangChain을 사용해 프로젝트를 구축하고 있습니다!", metadata={"source": "tweet"}),
    Document(page_content="내일 날씨는 맑고 따뜻할 예정입니다.", metadata={"source": "news"}),
    Document(page_content="오늘 아침에는 팬케이크와 계란을 먹었어요.", metadata={"source": "personal"}),
    Document(page_content="주식 시장이 경기 침체 우려로 하락 중입니다.", metadata={"source": "news"}),
]
# 고유 ID 생성 및 문서 추가
uuids = [str(uuid4()) for _ in range(len(documents))]
vector_store.add_documents(documents=documents, ids=uuids)

​Step 4: 벡터 데이터베이스 쿼리유사성 검색을 통해 특정 쿼리와 유사한 문서를 검색해보겠습니다.Python복사# 기본 유사성 검색
results = vector_store.similarity_search("내일 날씨는 어떨까요?", k=2, filter={"source": "news"})
for res in results:
print(f"* {res.page_content} [{res.metadata}]")
# 점수와 함께 유사성 검색
results_with_scores = vector_store.similarity_search_with_score("LangChain에 대해 이야기해주세요.", k=2, filter={"source": "tweet"})
for res, score in results_with_scores:
print(f"* [SIM={score:.3f}] {res.page_content} [{res.metadata}]")

​RAG 체인에 FAISS 통합RAG (Retrieval-Augmented Generation) 체인을 구성하여 검색된 문서를 바탕으로 질문에 응답할 수 있도록 구성합니다.Step 1: Retriever로 변환FAISS를 retriever로 변환해 RAG 체인에서 사용합니다.Python복사retriever = vector_store.as_retriever(search_type="similarity", search_kwargs={"k": 1})

​Step 2: RAG 체인 생성LangChain의 모델과 프롬프트를 연결하여 RAG 체인을 구성합니다.Python복사
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough

# 프롬프트 템플릿 정의
contextual_prompt = ChatPromptTemplate.from_messages([
("system", "Answer the question using only the following context."),
("user", "Context: {context}\\: {question}")
])
class DebugPassThrough(RunnablePassthrough):
def invoke(self, *args, **kwargs):
        output = super().invoke(*args, **kwargs)
print("Debug Output:", output)
return output
# 문서 리스트를 텍스트로 변환하는 단계 추가
class ContextToText(RunnablePassthrough):
def invoke(self, inputs, config=None, **kwargs): # config 인수 추가
# context의 각 문서를 문자열로 결합
        context_text = "".join([doc.page_content for doc in inputs["context"]])
return {"context": context_text, "question": inputs["question"]}
# RAG 체인에서 각 단계마다 DebugPassThrough 추가
rag_chain_debug = {
"context": retriever, # 컨텍스트를 가져오는 retriever
"question": DebugPassThrough() # 사용자 질문이 그대로 전달되는지 확인하는 passthrough
} | DebugPassThrough() | ContextToText()|   contextual_prompt | model

# 질문 실행 및 각 단계 출력 확인
response = rag_chain_debug.invoke("강사이름은?")
print("Final Response:")
print(response.content)
​FAISS 인덱스의 저장 및 로드FAISS 인덱스를 저장해 다시 사용할 수 있습니다.Python복사# 인덱스 저장
vector_store.save_local("faiss_index")
# 저장된 인덱스 로드
new_vector_store = FAISS.load_local("faiss_index", embeddings)

​FAISS 데이터베이스 병합두 개의 FAISS 데이터베이스를 병합할 수 있습니다.Python복사db1 = FAISS.from_texts(["문서 1 내용"], embeddings)
db2 = FAISS.from_texts(["문서 2 내용"], embeddings)
# 병합
db1.merge_from(db2)

​
판다스 사용을 위해 데이터를 불러오기와 저장하기인공지능을 위한 파이썬 라이브러리/인공지능을 위한 파이썬 라이브러리 - 3주차/판다스 사용을 위해 데이터를 불러오기와 저장하기제작:판다스 사용을 위해 데이터를 불러오기와 저장하기수업 목표판다스를 이용한 데이터 불러오기 및 저장 방법에 대해서 학습합니다.목차CSV, Excel, JSON 등 다양한 형식에서 데이터 불러오기CSV 파일 불러오기Excel 파일 불러오기JSON 파일 불러오기CSV, Excel, JSON, SQL 등 다양한 형식으로 데이터 저장하기CSV 파일로 저장하기Excel 파일로 저장하기JSON 파일로 저장하기 SQL 데이터베이스에 저장하기요약CSV, Excel, JSON 등 다양한 형식에서 데이터 불러오기CSV 파일 불러오기CSV 파일은 가장 많이 사용되는 데이터 형식 중 하나입니다. 판다스에서는 read_csv 함수를 사용해 쉽게 불러올 수 있어요.Python복사import pandas as pd

# CSV 파일 불러오기
df_csv = pd.read_csv('data.csv')
print(df_csv.head())

​ 주요 옵션sep : 구분자를 지정합니다. 기본값은 ,입니다.header : 헤더 행을 지정합니다. 기본값은 첫 번째 행(0)입니다.예시: 만약 데이터가 탭으로 구분되어 있다면?Python복사df_csv = pd.read_csv('data.csv', sep='\')

​URL에서 CSV 파일 불러오기: 인터넷에 있는 CSV 파일을 바로 불러올 수 있습니다.Python복사url = '<https://example.com/data.csv>'
df_csv_url = pd.read_csv(url)
print(df_csv_url.head())
​Excel 파일 불러오기Excel 파일도 판다스로 쉽게 불러올 수 있습니다. read_excel 함수를 사용하세요.Python복사# Excel 파일 불러오기
df_excel = pd.read_excel('data.xlsx')
print(df_excel.head())

​ 주요 옵션sheet_name : 읽고자 하는 시트 이름 또는 번호를 지정합니다. 기본값은 첫 번째 시트(0)입니다.header : 헤더 행을 지정합니다. 기본값은 첫 번째 행(0)입니다.예시: 특정 시트의 데이터를 불러오고 싶다면?Python복사df_excel = pd.read_excel('data.xlsx', sheet_name='Sheet2')

​URL에서 Excel 파일 불러오기: URL을 통해 Excel 파일도 불러올 수 있습니다.Python복사url = '<https://example.com/data.xlsx>'
df_excel_url = pd.read_excel(url)
print(df_excel_url.head())

​JSON 파일 불러오기JSON 파일은 웹에서 자주 사용하는 데이터 형식입니다. 판다스에서는 read_json 함수를 사용해 불러올 수 있습니다.Python복사# JSON 파일 불러오기
df_json = pd.read_json('data.json')
print(df_json.head())

​ 주요 옵션orient : JSON 데이터의 형식을 지정합니다. 기본값은 columns입니다.URL에서 JSON 파일 불러오기: 웹에서 JSON 데이터를 직접 불러올 수도 있습니다.Python복사url = '<https://example.com/data.json>'
df_json_url = pd.read_json(url)
print(df_json_url.head())

​요약판다스는 CSV, Excel, JSON 등 다양한 형식의 데이터를 쉽게 불러올 수 있는 기능을 제공합니다.URL을 통해 웹에서 직접 데이터를 불러오는 것도 가능하며, 이 기능을 통해 데이터 수집이 매우 간편해집니다.다양한 파일 형식의 데이터 로드를 익히면, 다양한 소스의 데이터를 활용하여 데이터 분석을 할 수 있습니다.이제 다양한 데이터 소스를 자유롭게 불러와서 분석을 시작해보세요! ​CSV, Excel, JSON, SQL 등 다양한 형식으로 데이터 저장하기CSV 파일로 저장하기CSV 파일은 데이터를 저장할 때 가장 널리 사용되는 형식 중 하나입니다. 판다스에서는 to_csv 함수를 사용해 데이터를 쉽게 CSV 파일로 저장할 수 있습니다.Python복사import pandas as pd

# 예시 데이터프레임 생성
df = pd.DataFrame({
'이름': ['철수', '영희', '민수'],
'나이': [25, 30, 35],
'직업': ['학생', '회사원', '프리랜서']
})
# CSV 파일로 저장하기
df.to_csv('data.csv', index=False)

​ 주요 옵션index=False : 인덱스를 제외하고 저장합니다.sep : 구분자를 지정합니다. 기본값은 ,입니다.URL로 저장하기: URL 경로가 아닌, 로컬 파일 시스템에 저장하는 방법이므로, URL을 지정할 수 없습니다.Excel 파일로 저장하기Excel 파일 형식으로 저장하려면 to_excel 함수를 사용합니다.Python복사# Excel 파일로 저장하기
df.to_excel('data.xlsx', index=False)

​ 주요 옵션sheet_name : 저장할 시트 이름을 지정합니다. 기본값은 Sheet1입니다.index=False : 인덱스를 제외하고 저장합니다.JSON 파일로 저장하기 JSON 파일 형식으로 데이터를 저장하려면 to_json 함수를 사용합니다.Python복사# JSON 파일로 저장하기
df.to_json('data.json')

​ 주요 옵션orient : JSON 형식을 지정합니다. 기본값은 columns입니다. records, index 등 다양한 형식으로 저장할 수 있습니다.Python복사# 예시: 레코드 형식으로 저장하기
df.to_json('data_records.json', orient='records')

​SQL 데이터베이스에 저장하기데이터를 SQL 데이터베이스에 저장하려면 to_sql 함수를 사용하고, 데이터베이스 연결이 필요합니다.Python복사import sqlite3

# SQLite 데이터베이스 연결
conn = sqlite3.connect('database.db')
# 데이터프레임을 SQL 테이블로 저장하기
df.to_sql('table_name', conn, if_exists='replace', index=False)
# 연결 종료
conn.close()

​ 주요 옵션name : 저장할 테이블 이름을 지정합니다.con : 데이터베이스 연결 객체를 지정합니다.if_exists : 테이블이 이미 존재할 경우 동작을 지정합니다. replace, append, fail 중 선택합니다.index=False : 인덱스를 제외하고 저장합니다.요약요약CSV, Excel, JSON, SQL 등 다양한 형식으로 데이터를 쉽게 저장할 수 있습니다.각 저장 형식에 맞는 옵션을 활용하면, 원하는 방식으로 데이터를 저장할 수 있습니다.데이터를 저장하는 과정은 분석 결과를 공유하거나, 나중에 다시 사용할 수 있도록 하는 데 매우 중요합니다.이제 필요한 형식으로 데이터를 저장하고, 언제든지 다시 불러와서 활용해보세요! ​Son minsuck9월 25일 (편집됨)[피드백]
주석으로 타이타닉 데이터셋 URL 링크 작성해주면 좋을 것 같습니다. 3-2 강의에서 밑에 URL가지고 실습하고 있습니다.
(링크: https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv)… 더 보기
[스파르타코딩클럽] 6강. 데이터 전처리[SCC] 바닥부터 시작하는 머신러닝/[스파르타코딩클럽] 바닥부터 시작하는 머신러닝 - 2주차/[스파르타코딩클럽] 6강. 데이터 전처리제작:[스파르타코딩클럽] 6강. 데이터 전처리[수업 목표]데이터 전처리 개념을 알아봅시다데이터 전처리 방법을 알아 봅시다[목차]01. 데이터 전처리 개념 및 API 소개
 
 01. 데이터 전처리 개념 및 API 소개데이터 전처리가 무엇인지 알아보고, 사용되는 API 를 배워봅시다1) 데이터 전처리 개념  데이터 전처리(Data Cleaning)란?데이터 분석 및 머신러닝 모델링을 위해 데이터를 준비하는 과정원시 데이터(raw data)는 종종 불완전하거나, 노이즈가 많거나, 형식이 일관되지 않아 직접 모델링에 사용하기 어려움데이터 전처리는 데이터의 품질을 높이고, 분석 결과의 신뢰성을 확보하기 위한 필수적인 과정데이터 전처리ALT데이터 품질 향상: 결측치, 이상치, 중복 데이터를 처리하여 데이터의 품질을 높입니다.모델 성능 향상: 적절한 스케일링, 정규화를 통해 모델의 학습 속도와 성능을 개선할 수 있습니다.데이터 일관성 확보: 서로 다른 출처에서 수집된 데이터를 일관된 형식으로 변환합니다.특성 공학(Feature Engineering): 유용한 특성(Feature)을 생성하거나 변환하여 모델의 예측 능력을 향상시킵니다.2) 데이터 전처리의 주요 기법 결측치 처리 (Handling Missing Data)결측치는 데이터셋에 누락된 값을 의미합니다. 결측치를 처리하지 않으면 모델의 성능이 저하될 수 있습니다.삭제: 결측치가 있는 행이나 열을 삭제합니다. 결측치가 적을 때 유용하지만, 데이터 손실이 발생할 수 있습니다.대체: 평균, 중앙값, 최빈값 등으로 결측치를 대체합니다.예측: 다른 특성을 사용하여 결측치를 예측하고 채웁니다. 이상치 처리 (Handling Outliers)이상치는 데이터에서 비정상적으로 크거나 작은 값을 의미합니다. 이상치는 모델의 성능을 저하시킬 수 있습니다.제거: 이상치를 데이터셋에서 제거합니다.변환: 이상치를 다른 값으로 변환합니다 (예: 상한선이나 하한선으로 대체).IQR 방법: IQR(Interquartile Range)을 사용하여 이상치를 탐지하고 처리합니다.  데이터 정규화 (Normalization)정규화는 데이터를 일정한 범위로 스케일링하는 과정입니다. 일반적으로 [0, 1] 범위로 변환합니다.Min-Max 정규화: 최소값을 0, 최대값을 1로 변환합니다.
Xnorm=Xmax−XminX−XminXnorm=Xmax−XminX−XminXnorm=Xmax−XminX−Xmin﻿​Xnorm=X−XminXmax−XminXnorm=X−XminXmax−XminXnorm=X−XminXmax−XminX_{norm} = }{X_{max} - X_{min}}Xnorm=X−XminXmax−XminXnorm​=Xmax​−Xmin​X−Xmin​​﻿​ 데이터 표준화 (Standardization)표준화는 데이터를 평균 0, 분산 1로 변환하는 과정입니다.Z-점수 표준화: Xstd=σX−μXstd=σX−μXstd=σX−μ﻿
여기서 μμ﻿는 평균, σσ﻿는 표준편차입니다.Xstd=X−μσXstd=X−μσXstd=X−μσX_{std} = {}Xstd=X−μσXstd​=σX−μ​﻿​  특성 공학 (Feature Engineering)특성 공학은 데이터로부터 새로운 유용한 특성을 생성하는 과정입니다.특성 생성: 기존 데이터를 기반으로 새로운 특성을 생성합니다 (예: 날짜 데이터를 사용하여 요일 특성 생성).특성 선택: 모델 성능에 중요한 특성을 선택하고, 중요하지 않은 특성을 제거합니다.  데이터 인코딩 (Data Encoding)비정형 데이터를 모델이 이해할 수 있는 형태로 변환합니다.레이블 인코딩 (Label Encoding): 범주형 데이터를 숫자로 변환합니다.원-핫 인코딩 (One-Hot Encoding): 범주형 데이터를 이진 벡터로 변환합니다.  데이터 분할 (Data Splitting)데이터를 학습용(train), 검증용(validation), 테스트용(test)으로 분할합니다. 이를 통해 모델의 일반화 성능을 평가할 수 있습니다.학습 데이터 (Training Data): 모델 학습에 사용되는 데이터.검증 데이터 (Validation Data): 모델 튜닝 및 성능 검증에 사용되는 데이터.테스트 데이터 (Test Data): 최종 모델 평가에 사용되는 데이터..
[스파르타코딩클럽] 6. 순환 신경망(RNN)[SCC] 기초가 탄탄한 딥러닝/[스파르타코딩클럽] 기초가 탄탄한 딥러닝 - 2주차/[스파르타코딩클럽] 6. 순환 신경망(RNN)제작:[스파르타코딩클럽] 6. 순환 신경망(RNN)[수업 목표]순환 신경망(RNN) 개념에 대해서 배워보고 어떤 원리로 동작하는지 알아봅시다Pytorch로 간단한 RNN 모델 구현 실습을 진행해 봅시다[목차]01. RNN의 기본 구조와 동작 원리02. RNN과 LSTM을 이용한 시계열 데이터 예측 (PyTorch)
 
 01. RNN의 기본 구조와 동작 원리RNN의 기본 구성요소와 어떤 방식으로 동작하는지 배워보고, LSTM 과 GRU에 대해 알아보고 비교해 봅시다.1) RNN의 기본 구조와 작동 방식 RNN의 기본 구조순환 신경망(Recurrent Neural Network, RNN)은 시계열 데이터나 순차적인 데이터를 처리하기 위해 설계된 신경망입니다RNN은 이전 시간 단계의 정보를 현재 시간 단계로 전달해, 시퀀스 데이터의 패턴을 학습할 수 있습니다.ALT RNN의 동작 원리순환 구조RNN은 입력 데이터와 이전 시간 단계의 은닉 상태(hidden state)를 입력으로 받아, 현재 시간 단계의 은닉 상태를 출력합니다.은닉 상태는 시퀀스의 정보를 저장하고, 다음 시간 단계로 전달됩니다.동작 원리RNN은 시퀀스의 각 시간 단계에서 동일한 가중치를 공유하여, 시퀀스의 패턴을 학습합니다.순전파(Forward Propagation)와 역전파(Backpropagation Through Time, BPTT)를 통해 가중치를 학습합니다.2) LSTM 및 GRU LSTM & GRURNN은 장기 의존성 문제(long-term dependency problem)를 겪을 수 있습니다. 이를 해결하기 위해 LSTM과 GRU가 개발되었습니다. LSTM(Long Short-Term Memory)LSTM은 셀 상태(cell state)와 게이트(gate) 구조를 도입, 장기 의존성을 효과적으로 학습가능 합니다.LSTM은 입력 게이트(input gate), 출력 게이트(output gate), 망각 게이트(forget gate)를 사용하여 정보를 조절합니다. GRU (Gated Recurrent Unit)GRU는 LSTM의 변형으로, 셀 상태 대신 은닉 상태(hidden state)만을 사용하여 구조를 단순화합니다.GRU는 업데이트 게이트(update gate)와 리셋 게이트(reset gate)를 사용하여 정보를 조절합니다. 차이점LSTM은 셀 상태와 은닉 상태를 모두 사용하며, 더 복잡한 게이트 구조를 가집니다.GRU는 은닉 상태만을 사용하며, 더 간단한 게이트 구조를 가집니다. 따라서 계산 비용이 적고, 학습이 빠를 수 있습니다.ALT3) 시계열 데이터 처리 RNN을 이용한 시계열 데이터 처리 방법RNN은 시계열 데이터나 순차적인 데이터를 처리하는 데 적합합니다. 예를 들어, 주식 가격 예측, 날씨 예측, 텍스트 생성 등이 있습니다.데이터 전처리:시계열 데이터를 적절한 형태로 변환하고, 정규화(normalization)합니다.입력 시퀀스와 출력 시퀀스를 정의합니다.모델 구축:RNN, LSTM, GRU 등의 모델을 정의합니다.입력 크기, 은닉 상태 크기, 출력 크기 등을 설정합니다.모델 학습:손실 함수와 최적화 알고리즘을 정의합니다.순전파와 역전파를 통해 모델을 학습시킵니다.모델 평가:테스트 데이터를 사용하여 모델의 성능을 평가합니다.02. RNN과 LSTM을 이용한 시계열 데이터 예측 (PyTorch) 이제 PyTorch를 사용하여 간단한 RNN과 LSTM 모델을 구축하고, 시계열 데이터를 예측해보겠습니다. 예제로는 Sine 파형 데이터를 사용하겠습니다.1)  간단한 RNN/LSTM 모델을 이용한 시계열 데이터 예측 실습 PyTorch 및 필요한 라이브러리 임포트PyTorch 및 필요한 라이브러리 임포트 {5px}PyTorch 및 필요한 라이브러리 임포트 ﻿​Python복사import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
​데이터셋 생성 및 전처리데이터셋 생성 및 전처리 {5px}데이터셋 생성 및 전처리 ﻿​Python복사# Sine 파형 데이터 생성
def create_sine_wave_data(seq_length, num_samples):
    X = []
    y = []
for _ in range(num_samples):
        start = np.random.rand()
        x = np.linspace(start, start + 2 * np.pi, seq_length)
        X.append(np.sin(x))
        y.append(np.sin(x + 0.1))
return np.array(X), np.array(y)

seq_length = 50
num_samples = 1000
X, y = create_sine_wave_data(seq_length, num_samples)
# 데이터셋을 PyTorch 텐서로 변환
X = torch.tensor(X, dtype=torch.float32).unsqueeze(-1)
y = torch.tensor(y, dtype=torch.float32).unsqueeze(-1)
​ 간단한 RNN 모델 정의간단한 RNN 모델 정의 {5px}간단한 RNN 모델 정의 ﻿​Python복사class SimpleRNN(nn.Module):
def __init__(self, input_size, hidden_size, output_size):
super(SimpleRNN, self).__init__()
        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)
def forward(self, x):
        h0 = torch.zeros(1, x.size(0), hidden_size) # 초기 은닉 상태
        out, _ = self.rnn(x, h0)
        out = self.fc(out[:, -1, :]) # 마지막 시간 단계의 출력
return out

input_size = 1
hidden_size = 32
output_size = 1
model = SimpleRNN(input_size, hidden_size, output_size)
​nn.RNN: 순환 신경망(RNN) 층을 정의합니다.nn.RNN(input_size, hidden_size, batch_first)는 입력 크기, 은닉 상태 크기, 배치 차원을 첫 번째로 설정합니다..RNN(input_size, hidden_size, batch_first)는 입력 크기, 은닉 상태 크기, 배치 차원을 첫 번째로 설정합니다.﻿​nn.Linear: 선형 변환을 적용하는 완전 연결(fully connected) 레이어를 정의합니다.nn.Linear(in_features, out_features)는 입력 특징의 수와 출력 특징의 수를 지정합니다..Linear(in_features, out_features)는 입력 특징의 수와 출력 특징의 수를 지정합니다.﻿​ 간단한 LSTM 모델 정의간단한 LSTM 모델 정의 {5px}간단한 LSTM 모델 정의 ﻿​Python복사class SimpleLSTM(nn.Module):
def __init__(self, input_size, hidden_size, output_size):
super(SimpleLSTM, self).__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)
        self.fc = nn.Linear(hidden_size, output_size)
def forward(self, x):
        h0 = torch.zeros(1, x.size(0), hidden_size) # 초기 은닉 상태
        c0 = torch.zeros(1, x.size(0), hidden_size) # 초기 셀 상태
        out, _ = self.lstm(x, (h0, c0))
        out = self.fc(out[:, -1, :]) # 마지막 시간 단계의 출력
return out

model = SimpleLSTM(input_size, hidden_size, output_size)
​nn.LSTM: 장단기 메모리(LSTM) 층을 정의합니다.nn.LSTM(input_size, hidden_size, batch_first)는 입력 크기, 은닉 상태 크기, 배치 차원을 첫 번째로 설정합니다..LSTM(input_size, hidden_size, batch_first)는 입력 크기, 은닉 상태 크기, 배치 차원을 첫 번째로 설정합니다.﻿​ 모델 학습모델 학습 {5px}모델 학습 ﻿​Python복사# 손실 함수와 최적화 알고리즘 정의
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)
# 모델 학습
num_epochs = 100
for epoch in range(num_epochs):
    outputs = model(X)
    optimizer.zero_grad()
    loss = criterion(outputs, y)
    loss.backward()
    optimizer.step()
if (epoch + 1) % 10 == 0:
print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')
print('Finished Training')
​nn.MSELoss: 평균 제곱 오차(MSE) 손실 함수를 정의합니다.optim.Adam: Adam 최적화 알고리즘을 정의합니다. lr은 학습률을 지정합니다.optimizer.zero_grad(): 이전 단계에서 계산된 기울기를 초기화합니다.loss.backward(): 역전파를 통해 기울기를 계산합니다.optimizer.step(): 계산된 기울기를 바탕으로 가중치를 업데이트합니다. 모델 평가 및 시각화모델 평가 및 시각화 {5px}모델 평가 및 시각화 ﻿​Python복사# 모델 평가
model.eval()
with torch.no_grad():
    predicted = model(X).detach().numpy()
# 시각화
plt.figure(figsize=(10, 5))
plt.plot(y.numpy().flatten(), label='True')
plt.plot(predicted.flatten(), label='Predicted')
plt.legend()
plt.show()
​model.eval(): 모델을 평가 모드로 전환합니다.torch.no_grad(): 평가 단계에서는 기울기를 계산할 필요가 없으므로, 이를 비활성화하여 메모리 사용을 줄입니다.detach(): 텐서를 계산 그래프에서 분리합니다..
Sentence-Transformer, Word2Vec, 그리고 Transformer 기반 임베딩 LLM & RAG를 활용한 AI 서비스 만들기/ LLM & RAG를 활용한 AI 서비스 만들기 - 5주차/Sentence-Transformer, Word2Vec, 그리고 Transformer 기반 임베딩제작:Sentence-Transformer, Word2Vec, 그리고 Transformer 기반 임베딩수업 목표한국어 임베딩의 특징과 임베딩 실습을 진행합니다.목차Word2Vec을 사용한 한국어 임베딩Word2Vec 한국어 임베딩 실습Sentence-Transformer로 한국어 문장 임베딩Sentence-Transformer 한국어 임베딩 실습Transformer 기반 최신 한국어 임베딩Transformer 한국어 임베딩 실습: KoBERT 사용한국어 임베딩의 특징과 도전 과제왜 한국어 임베딩이 어려운가?최신 모델과 접근 방식배운 내용 정리하기!❗이번 실습에서는 한국어 임베딩을 위한 세 가지 대표적인 기법인 Sentence-Transformer, Word2Vec, 그리고 Transformer 기반 임베딩을 살펴보고 실습해볼 거예요. 특히 최신 Transformer 모델을 사용하여 한국어를 잘 처리하는 방법에 대해 알아볼게요.Word2Vec을 사용한 한국어 임베딩❗Word2Vec은 단어를 고차원 벡터로 변환하여 의미적 유사성을 측정하는 임베딩 기법이에요. 단어 간의 문맥적 관계를 반영하여, 비슷한 의미를 가진 단어들이 유사한 벡터 값을 갖도록 학습합니다.Word2Vec 한국어 임베딩 실습Python복사from gensim.models import Word2Vec

# 샘플 한국어 문장 데이터
sentences = [
"나는 오늘 책을 읽었다",
"고양이가 야옹하고 울었다",
"인공지능은 정말 흥미로운 주제다",
"한국어 임베딩을 학습하는 중이다"
]
# Python 기본 split() 사용해 간단하게 토큰화
tokenized_sentences = [sentence.split() for sentence in sentences]
# Word2Vec 모델 학습
word2vec_model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)
# 단어 '고양이'와 유사한 단어 찾기
similar_words = word2vec_model.wv.most_similar("고양이")
print(similar_words)

​Word2Vec 특징장점: 단어 간의 의미적 관계를 파악하는 데 적합단점: 단어 자체만 학습하므로, 문장 단위에서는 유연성이 부족Sentence-Transformer로 한국어 문장 임베딩💡Sentence-Transformer는 문장 단위로 임베딩을 생성할 수 있는 BERT 기반의 모델이에요. 특히 한국어에 특화된 Ko-Sentence-BERT 같은 모델을 사용하면, 문장 간의 의미적 유사성을 정확하게 계산할 수 있습니다.Sentence-Transformer 한국어 임베딩 실습Python복사from sentence_transformers import SentenceTransformer

# 최신 Ko-Sentence-BERT 모델 로드
model = SentenceTransformer('sentence-transformers/kr-sentence_bert-base')
# 샘플 문장들
sentences = [
"나는 오늘 책을 읽었다.",
"고양이가 야옹하고 울었다.",
"인공지능은 흥미로운 주제다.",
"한국어 임베딩을 학습하는 중이다."
]
# 문장 임베딩 생성
embeddings = model.encode(sentences)
# 첫 번째 문장 임베딩 확인
print(embeddings[0])

​Sentence-Transformer 특징장점: 문장 간의 유사성을 정확하게 측정, 문맥을 고려한 임베딩 생성단점: 학습 속도가 상대적으로 느림Transformer 기반 최신 한국어 임베딩💡Transformer 기반 모델들은 BERT, GPT 같은 모델들로 발전해왔고, 최근에는 KoBERT, KoGPT와 같은 한국어 특화 모델이 등장했어요. 이 모델들은 문맥을 고려하여 정교한 임베딩을 생성해줍니다.Transformer 한국어 임베딩 실습: KoBERT 사용Python복사from transformers import BertTokenizer, BertModel
import torch

# KLUE-BERT 모델과 토크나이저 로드
tokenizer = BertTokenizer.from_pretrained('klue/bert-base')
model = BertModel.from_pretrained('klue/bert-base')
# 입력 문장
sentence = "한국어 임베딩을 학습하고 있습니다."
# 토큰화 및 텐서 변환
inputs = tokenizer(sentence, return_tensors='pt')
# 임베딩 생성
with torch.no_grad():
    outputs = model(**inputs)
# 임베딩 벡터 추출 (평균값으로 계산)
embedding = outputs.last_hidden_state.mean(dim=1)
print(embedding)
​Transformer 특징장점: 문맥을 양방향으로 이해, 문장 전체의 의미를 깊이 반영단점: 계산 비용이 크고, 모델 크기가 큼한국어 임베딩의 특징과 도전 과제왜 한국어 임베딩이 어려운가?1️⃣교착어 특성: 한국어는 조사와 어미를 많이 사용하는 교착어로, 단어 변형이 많아 정확한 형태소 분석이 중요해요.2️⃣어순의 유연성: 한국어는 어순이 자유롭기 때문에, 동일한 의미라도 다양한 형태로 표현될 수 있어요.3️⃣데이터 부족: 영어에 비해 한국어로 학습된 데이터가 상대적으로 적어, 임베딩 모델이 충분히 학습되지 않은 경우가 많습니다.최신 모델과 접근 방식KoBERT, KoGPT 등 한국어 전용 Transformer 모델들은 이러한 문제를 해결하기 위해 한국어에 특화된 데이터셋을 사용하여 학습되었습니다.특히 Sentence-Transformer는 문장 간 의미적 유사성을 정확하게 파악하는 데 강점을 가지며, 한국어 문장 처리에 탁월한 성능을 보여줍니다.배운 내용 정리하기!요약Word2Vec: 단어 단위의 의미적 유사성을 벡터로 표현.Sentence-Transformer: 문장 간 유사성을 벡터로 표현, Ko-Sentence-BERT 모델로 한국어 문장 처리.Transformer 모델 (특히? KoBERT): 문맥을 고려한 고차원 임베딩, 한국어 전용 모델로 강력한 성능.😀이러한 다양한 임베딩 기법을 통해 한국어 텍스트 분석의 정확성과 효율성을 높일 수 있습니다. 한국어 임베딩의 특성과 최신 기법을 잘 활용하여 LLM 시스템에 적용해보세요!
불러온 데이터 미리보기 및 기본 정보 확인인공지능을 위한 파이썬 라이브러리/인공지능을 위한 파이썬 라이브러리 - 3주차/불러온 데이터 미리보기 및 기본 정보 확인제작:불러온 데이터 미리보기 및 기본 정보 확인수업 목표판다스를 통해 불러온 데이터를 확인하는 방법에 대해서 학습합니다.목차기초 정보를 확인하는 다양한 기능데이터 미리보기데이터프레임의 기본 정보 확인하기
데이터프레임의 개별 데이터 확인하기요약기초 정보를 확인하는 다양한 기능데이터 미리보기📚데이터를 불러온 후, 전체 데이터를 다 확인하기에는 부담이 될 수 있습니다. 그래서 판다스는 데이터를 미리보기 할 수 있는 다양한 방법을 제공합니다. head()로 상위 데이터 미리보기head() 함수는 데이터프레임의 상위 몇 개의 행을 미리보기 합니다. 기본적으로 5개 행을 보여줍니다.Python복사import pandas as pd

# CSV 파일 불러오기
df = pd.read_csv('data.csv')
# 상위 5개 행 미리보기
print(df.head())

​특정 개수의 행을 보고 싶다면?Python복사print(df.head(10)) # 상위 10개 행 미리보기
​tail()로 하위 데이터 미리보기tail() 함수는 하위 몇 개의 행을 미리보기 합니다. 역시 기본값은 5개 행입니다.Python복사print(df.tail())

​특정 개수의 하위 행을 보고 싶다면?Python복사print(df.tail(3)) # 하위 3개 행 미리보기
​데이터프레임의 기본 정보 확인하기📚데이터프레임의 기본 정보를 확인하면 데이터의 구조와 내용을 쉽게 파악할 수 있습니다.info()로 데이터프레임의 요약 정보 확인info() 함수는 데이터프레임의 전체 구조를 한눈에 보여줍니다. 각 열의 데이터 타입과 널 값 여부를 확인할 수 있습니다.Python복사print(df.info())

​출력 예시:Plain Text복사<class 'pandas.core.frame.DataFrame'>
RangeIndex: 100 entries, 0 to 99
Data columns (total 4 columns):
 #   Column   Non-Null Count  Dtype
---  ------   --------------  -----
 0   이름      100 non-null    object
 1   나이      100 non-null    int64
 2   직업      100 non-null    object
 3   지역      95 non-null     object
dtypes: int64(1), object(3)
memory usage: 3.2+ KB

​describe()로 주요 통계 정보 확인describe() 함수는 숫자형 데이터에 대한 주요 통계 정보를 제공합니다. 평균, 표준편차, 최소값, 최대값 등을 확인할 수 있습니다.Python복사print(df.describe())

​특정 열에 대한 통계를 보고 싶다면?Python복사print(df['나이'].describe())

​출력 예시:Plain Text복사              나이
count  100.000000
mean    30.500000
std      4.5
min     25.000000
25%     27.000000
50%     30.000000
75%     34.000000
max     35.000000

​columns와 index로 열과 행 정보 확인데이터프레임의 열 이름과 행 인덱스를 확인할 수 있습니다.Python복사print(df.columns) # 열 이름 확인
print(df.index) # 행 인덱스 확인

​
데이터프레임의 개별 데이터 확인하기 loc[]로 행과 열의 위치를 지정하여 접근하기loc[]은 라벨(인덱스, 컬럼명)을 사용해 특정 데이터를 조회합니다.Python복사# 특정 행 조회
print(df.loc[0]) # 첫 번째 행
# 특정 행과 열 조회
print(df.loc[0, '이름']) # 첫 번째 행의 '이름' 열 데이터

​iloc[]로 행과 열의 위치 인덱스를 사용해 접근하기iloc[]은 정수 인덱스를 사용해 데이터를 조회합니다.Python복사# 첫 번째 행 조회
print(df.iloc[0])
# 첫 번째 행의 두 번째 열 데이터 조회
print(df.iloc[0, 1])

​특정 열이나 행 전체를 선택하기특정 열 전체를 선택할 수 있습니다.Python복사print(df['이름']) # '이름' 열 전체

​여러 열을 선택하고 싶다면?Python복사print(df[['이름', '나이']]) # '이름'과 '나이' 열 전체

​특정 행 전체를 선택하려면 loc[] 또는 iloc[]을 사용하세요.요약📚요약데이터를 불러온 후, head()와 tail()로 데이터의 상위 및 하위 행을 미리 볼 수 있습니다.info()와 describe()를 사용해 데이터프레임의 구조와 주요 통계 정보를 확인할 수 있습니다.loc[], iloc[]을 사용해 개별 데이터나 특정 행과 열의 값을 쉽게 조회할 수 있습니다.이제 데이터를 미리 파악하고 분석을 시작해보세요! ​
[스파르타코딩클럽] 7강. 데이터 전처리 실습 [SCC] 바닥부터 시작하는 머신러닝/[스파르타코딩클럽] 바닥부터 시작하는 머신러닝 - 2주차/[스파르타코딩클럽] 7강. 데이터 전처리 실습 제작:[스파르타코딩클럽] 7강. 데이터 전처리 실습 [수업 목표]데이터 전처리 개념 및 Pandas에서 사용하는 API를 알아봅시다데이터 전처리 방법을 알아 봅시다[목차]01. 데이터 전처리Data💡
 
 01. 데이터 전처리✔️데이터 전처리 방법을 알아 봅시다DataPython복사data = {
'A': [1, 2, np.nan, 4, 5, 100, 1, 2, 2, 4, '1', '2', '3', '4', '5', 10, 20, 30, 40, 50],
'B': [5, np.nan, np.nan, 8, 10, 60, 10, 20, 20, 40, '10', '20', '30', '40', '50', 5, 4, 3, 2, 1],
'C': [1, 2, 3, 4, 5, 5, 100, 200, 200, 400, 100, 200, 300, 400, 500, 1, 2, 3, 4, 5],
'D': [np.nan, np.nan, 3, 3, 3, 5, 5, 5, 5, 5, np.nan, np.nan, np.nan, np.nan, np.nan, 2, 3, 4, 5, 6],
'category_column': [np.nan]*10 + ['A', 'B', 'A', 'C', 'B'] + [np.nan]*5,
'value_column': [np.nan]*10 + [1, 2, 3, 4, 5] + [np.nan]*5,
'target': [np.nan]*15 + [1, 0, 1, 0, 1]
}
​1)결측값 처리 결측값 처리 방법제거: 결측값이 포함된 행 또는 열을 제거합니다.대체: 결측값을 특정 값으로 대체합니다.예측: 머신러닝 모델을 사용하여 결측값을 예측합니다.결측값 제거 {5px}결측값 제거 ﻿​Python복사# 결측값이 포함된 행 제거
df_dropped_rows = df.dropna()
# 결측값이 포함된 열 제거
df_dropped_cols = df.dropna(axis=1)
​dropna()를 사용하여 결측값 제거()를 사용하여 결측값 제거﻿​결측값 대체 {5px}결측값 대체 ﻿​Python복사# 결측값을 0으로 대체
df_filled = df.fillna(0)
# 결측값을 각 열의 평균값으로 대체
df_filled_mean = df.fillna(df.mean())
# 결측값을 각 열의 중간값으로 대체
df_filled_median = df.fillna(df.median())
# 결측값을 각 열의 최빈값으로 대체
df_filled_mode = df.fillna(df.mode().iloc[0])
​fillna()를 사용하여 결측값 대체()를 사용하여 결측값 대체﻿​결측값 예측 {5px}결측값 예측 ﻿​Python복사from sklearn.linear_model import LinearRegression

# 결측값이 있는 열과 없는 열 분리
df_with_na = df[df['column_with_na'].isnull()]
df_without_na = df[df['column_with_na'].notnull()]
# 회귀 모델 학습
model = LinearRegression()
model.fit(df_without_na[['feature1', 'feature2']], df_without_na['column_with_na'])
# 결측값 예측
predicted_values = model.predict(df_with_na[['feature1', 'feature2']])
# 예측된 값으로 결측값 대체
df.loc[df['column_with_na'].isnull(), 'column_with_na'] = predicted_values
​2) 이상값 처리 이상치란?데이터셋에서 비정상적으로 큰 값이나 작은 값이상치는 분석 결과에 큰 영향을 미칠 수 있으므로, 이를 적절히 처리하는 것이 중요이상치 확인 방법 {5px}이상치 확인 방법 ﻿​Python복사# 특정 열의 이상치 확인 (IQR 방법)
Q1 = df['column_name'].quantile(0.25)
Q3 = df['column_name'].quantile(0.75)
IQR = Q3 - Q1

# 이상치 범위 설정
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# 이상치 확인
outliers = df[(df['column_name'] < lower_bound) | (df['column_name'] > upper_bound)]
print(outliers)
​ 이상치 처리 방법제거: 이상치를 데이터셋에서 제거합니다.대체: 이상치를 특정 값으로 대체합니다.변환: 이상치를 변환하여 데이터의 분포를 조정합니다.이상치 처리 방법 {5px}이상치 처리 방법 ﻿​Python복사# 이상치 제거
df_no_outliers = df[(df['column_name'] >= lower_bound) & (df['column_name'] <= upper_bound)]
# 이상치를 평균값으로 대체
mean_value = df['column_name'].mean()
df['column_name'] = df['column_name'].apply(lambda x: mean_value if x < lower_bound or x > upper_bound else x)
​3) 중복값 제거 중복 데이터 제거중복 데이터 제거 {5px}중복 데이터 제거 ﻿​Python복사# 중복된 행 확인
print(df.duplicated().sum())
# 중복된 행 제거
df_no_duplicates = df.drop_duplicates()
​4) 데이터 타입 변환 데이터 타입 변환의 필요성잘못된 데이터 타입은 분석 결과에 영향을 미칠 수 있으며, 모델 학습에 오류를 발생시킬 수있어 적절한 데이터 타입 변환이 필요합니다데이터 타입 변환 방법 {5px}데이터 타입 변환 방법 ﻿​Python복사# 특정 열의 데이터 타입을 정수형으로 변환
df['column_name'] = df['column_name'].astype(int)
# 특정 열의 데이터 타입을 문자열로 변환
df['column_name'] = df['column_name'].astype(str)
# 특정 열의 데이터 타입을 부동 소수점으로 변환
df['column_name'] = df['column_name'].astype(float)
​Pandas의 astype() 메서드를 사용하여 데이터 타입을 변환의 astype() 메서드를 사용하여 데이터 타입을 변환﻿​5) 인코딩 인코딩이란?범주형 데이터를 수치형 데이터로 변환하는 과정머신러닝 모델은 수치형 데이터를 입력으로 받기때문에, 범주형 데이터를 수치형으로 변환하는 것이 필요인코딩 방법 {5px}인코딩 방법 ﻿​Python복사# 범주형 데이터를 더미 변수로 변환
df_encoded = pd.get_dummies(df, columns=['category_column'])
# 결과 출력
print(df_encoded.head())
​Pandas의 get_dummies() 메서드를 사용하여 범주형 데이터를 더미 변수로 변환의 get_dummies() 메서드를 사용하여 범주형 데이터를 더미 변수로 변환﻿​6) 샘플링 샘플링이란?데이터셋의 크기를 줄이거나 늘리는 과정데이터셋의 대표성을 유지하면서 데이터의 크기를 조절하는 데 사용샘플링 방법 {5px}샘플링 방법 ﻿​Python복사# 데이터셋에서 50% 샘플 추출
df_sampled = df.sample(frac=0.5)
# 데이터셋에서 100개의 샘플 추출
df_sampled_n = df.sample(n=100)
​Pandas의 sample() 메서드를 사용하여 데이터셋에서 샘플을 추출의 sample() 메서드를 사용하여 데이터셋에서 샘플을 추출﻿​7) 특징 선택 및 추출 특징 선택 및 추출이란?특징 선택(Feature Selection) 및 추출(Feature Extraction)은 모델 성능을 높이기 위해 중요한 특징을 선택하거나 새로운 특징을 추출하는 과정특징 선택 방법 {5px}특징 선택 방법 ﻿​Python복사from sklearn.feature_selection import SelectKBest, f_classif

# 특징 선택 (상위 5개의 특징 선택)
selector = SelectKBest(score_func=f_classif, k=5)
X_new = selector.fit_transform(X, y)
# 선택된 특징의 인덱스
selected_features = selector.get_support(indices=True)
print(selected_features)
​Pandas와 Scikit-learn을 사용하여 특징 선택을 수행와 Scikit-learn을 사용하여 특징 선택을 수행﻿​특징 추출 방법 {5px}특징 추출 방법 ﻿​Python복사# 두 열의 곱을 새로운 특징으로 추가
df['new_feature'] = df['feature1'] * df['feature2']
# 두 열의 합을 새로운 특징으로 추가
df['new_feature_sum'] = df['feature1'] + df['feature2']
​.
[스파르타코딩클럽] 7. 어텐션 (Attention) 메커니즘[SCC] 기초가 탄탄한 딥러닝/[스파르타코딩클럽] 기초가 탄탄한 딥러닝 - 3주차/[스파르타코딩클럽] 7. 어텐션 (Attention) 메커니즘제작:[스파르타코딩클럽] 7. 어텐션 (Attention) 메커니즘[수업 목표]최근 가장 성능 좋은 매커니즘! 어텐션 메커니즘에 대해 알아봅시다Pytorch의 구현 예시를 살펴봅시다[목차]01. 개념02. 실습:  Attention 메커니즘의 구현💡
 
 01. 개념✔️어텐션 메커니즘의 기본 개념과 동작 방식에 대해 알아봅시다1) 어텐션의 기본 구성 요소와 동작 방식 Attention 메커니즘이란?Attention 메커니즘은 시퀀스 데이터에서 중요한 부분에 더 많은 가중치를 할당하여 정보를 효율적으로 처리하는 기법주로 자연어 처리(NLP)와 시계열 데이터에서 사용되며, 기계 번역, 요약, 질의응답 시스템 등 다양한 분야에서 뛰어난 성능을 발휘 동작 방식개요Attention 메커니즘은 입력 시퀀스의 각 요소에 대해 중요도를 계산하여 가중치를 부여합니다. 이를 통해 중요한 정보에 집중하고, 불필요한 정보를 무시할 수 있습니다. Attention 메커니즘은 주로 세 가지 주요 구성 요소로 이루어집니다: Query, Key, Value.Attention 스코어 계산Attention 스코어는 Query와 Key 간의 유사도를 측정하여 중요도를 계산합니다. 이 유사도는 내적(dot product) 등을 사용하여 계산할 수 있습니다.score(Q,K)=Q⋅KT(Q, K) = Q  K^Tscore(Q,K)=Q⋅KT﻿​Softmax를 통한 가중치 계산계산된 Attention 스코어는 Softmax 함수를 통해 확률 분포로 변환됩니다. 이를 통해 가중치의 합이 1이 되도록 합니다.αi=exp⁡(score(Q,Ki))∑jexp⁡(score(Q,Kj))_i = (Q, K_i))}{_{j} ((Q, K_j))}αi​=∑j​exp(score(Q,Kj​))exp(score(Q,Ki​))​﻿​Softmax를 통한 가중치 계산Softmax를 통해 얻어진 가중치를 Value에 곱하여 최종 Attention 출력을 계산합니다.Attention(Q,K,V)=∑iαiVi(Q, K, V) = _{i} _i V_iAttention(Q,K,V)=∑i​αi​Vi​﻿​2)  Self-Attention과 Multi-Head Attention Self-AttentionSelf-Attention은 시퀀스 내의 각 요소가 서로를 참조하는 메커니즘입니다. 입력 시퀀스의 모든 요소가 Query, Key, Value로 사용됩니다. 이를 통해 각 요소가 시퀀스 내 다른 요소들과의 관계를 학습할 수 있습니다.예를 들어, 문장 내에서 단어 간의 관계를 학습하여 번역이나 요약에 활용할 수 있습니다. Multi-Head AttentionMulti-Head Attention은 여러 개의 Self-Attention을 병렬로 수행하는 메커니즘입니다. 각 헤드는 서로 다른 부분의 정보를 학습하며, 이를 통해 모델이 다양한 관점에서 데이터를 처리할 수 있습니다.02. 실습:  Attention 메커니즘의 구현✔️ 어텐션의 구현에 대한 코드입니다! 사실 어텐션을 직접 구현하는 일은 굉장히 적어요.
한번 훑는 정도로 넘어갑시다!1)  Attention Scaled Dot-Product AttentionScaled Dot-Product attention 메커니즘 구현{5px}Scaled Dot-Product attention 메커니즘 구현﻿​Python복사import torch
import torch.nn.functional as F

def scaled_dot_product_attention(Q, K, V):
    d_k = Q.size(-1) # Key의 차원 수
    scores = torch.matmul(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_k, dtype=torch.float32)) # 유사도 계산 및 스케일링
    attn_weights = F.softmax(scores, dim=-1) # Softmax를 통한 가중치 계산
    output = torch.matmul(attn_weights, V) # 가중합을 통한 최종 출력 계산
return output, attn_weights

​ Multi-Head Attention Multi-Head Attention 메커니즘 구현{5px} Multi-Head Attention 메커니즘 구현﻿​Python복사class MultiHeadAttention(nn.Module):
def __init__(self, embed_size, heads):
super(MultiHeadAttention, self).__init__()
        self.embed_size = embed_size
        self.heads = heads
        self.head_dim = embed_size // heads

        assert (
            self.head_dim * heads == embed_size
        ), "Embedding size needs to be divisible by heads"

        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)
        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)
        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)
        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)
def forward(self, values, keys, query, mask=None):
        N = query.shape[0]
        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]
# Linear transformations
        values = self.values(values).view(N, value_len, self.heads, self.head_dim)
        keys = self.keys(keys).view(N, key_len, self.heads, self.head_dim)
        queries = self.queries(query).view(N, query_len, self.heads, self.head_dim)
# Scaled dot-product attention
        out, _ = scaled_dot_product_attention(queries, keys, values)

        out = out.view(N, query_len, self.heads * self.head_dim)
        out = self.fc_out(out)
return out

​.
문서 임베딩 실습하기 LLM & RAG를 활용한 AI 서비스 만들기/ LLM & RAG를 활용한 AI 서비스 만들기 - 5주차/문서 임베딩 실습하기제작:문서 임베딩 실습하기수업 목표문서 임베딩의 기본 개념과 Faiss로 유사도 검색 실습을 진행합니다.목차실습 준비물 문서 임베딩의 기본 개념문서 단위 임베딩 전체 문서 임베딩하기문장 단위 임베딩하기문단 단위 임베딩하기Faiss로 유사도 검색하기Faiss 초기화 및 인덱싱유사 문장 검색하기유사도에 대한 실험길이 단위 선택 가이드추가 실험: 임베딩 벡터 시각화하기추가 실험: t-SNE를 이용한 고차원 임베딩 시각화💡여러분 안녕하세요! 오늘은 input.txt라는 txt 파일을 가지고 문서 임베딩을 실습해보려고 합니다.  특히 Faiss를 활용해 유사 단어 검색까지 해볼 거예요! 문서를 임베딩하는 여러 방법에 대해서도 실험해볼 테니 기대해 주세요. ​실습 준비물 Python 코드 실행 환경 (Jupyter Notebook 또는 IDE 추천)input.txt 파일 (이 파일에는 우리가 분석할 텍스트 데이터가 들어있어요!)sentence-transformers 라이브러리 설치faiss 라이브러리 설치Plain Text복사pip install sentence-transformers faiss-cpu​문서 임베딩의 기본 개념❗문서를 임베딩할 때, 텍스트를 벡터 형태로 변환하여 컴퓨터가 이해하기 쉽게 만들어요. 변환된 벡터들은 문서 간의 유사도를 계산하거나, 검색을 위해 활용됩니다. 이번 실습에서는 최신 한국어 모델을 사용해 문서를 임베딩할 거예요! 최신 모델은 sentence-transformers를 통해 불러올 예정입니다. ​참고: 임베딩이란 텍스트, 이미지 등 다양한 데이터를 고차원의 벡터 공간에 표현하는 것을 말합니다. 
이렇게 표현된 벡터들은 기계 학습 모델들이 데이터를 더 쉽게 이해하고 분석할 수 있게 도와줘요!문서 단위 임베딩 💡문서를 임베딩할 때 중요한 결정 중 하나는 임베딩할 텍스트의 길이를 어떻게 설정할지입니다. 한 번 전체 문서를 통째로 임베딩할 수도 있고, 문장을 단위로 하거나 문단 단위로 쪼개서 임베딩할 수도 있어요. 각 방법의 장단점을 비교해 보죠!전체 문서 임베딩하기먼저 전체 문서를 하나의 벡터로 임베딩하는 방법입니다.
이 방법은 문서 전체의 맥락을 이해하는 데 유리하지만, 길이가 너무 길 경우 중요한 정보가 손실될 수 있습니다.Python복사from sentence_transformers import SentenceTransformer
import numpy as np

# 모델 불러오기
model = SentenceTransformer('sentence-transformers/xlm-r-100langs-bert-base-nli-stsb-mean-tokens')
# input.txt 파일 불러오기
with open('input.txt', 'r', encoding='utf-8') as file:
    document = file.read()
# 전체 문서 임베딩
doc_embedding = model.encode(document)
​실습 실험: 전체 문서 임베딩의 성능을 확인하기 위해 doc_embedding을 사용해 문서 전체의 요약본을 다른 문서와 비교해 보세요. 전체 문서를 하나의 벡터로 표현하는 것이 실제 문서의 의미를 얼마나 잘 반영하는지 확인할 수 있어요.문장 단위 임베딩하기문서를 문장 단위로 나누어 임베딩해 볼까요? 이렇게 하면 문장별 유사도를 계산하기에 좋습니다. 특히 검색 시 특정 문장과 유사한 문장을 찾는 데 유용해요.Python복사from nltk.tokenize import sent_tokenize

# 문장으로 나누기
sentences = sent_tokenize(document)
# 각 문장을 임베딩
sentence_embeddings = model.encode(sentences)
​실습 실험: 문장 단위로 임베딩한 후, 특정 문장을 입력해 유사 문장을 찾아보세요. 예를 들어, "이 문서의 핵심은 무엇인가요?" 같은 질문을 던지고 그와 유사한 문장을 검색해 봅시다.문단 단위 임베딩하기문서를 문단 단위로 쪼개 임베딩하는 방법도 있어요. 이 방법은 문서 내 주제별 정보를 분석할 때 유리합니다.Python복사paragraphs = document.split('') # 두 줄 개행 기준으로 문단 나누기
# 각 문단을 임베딩
paragraph_embeddings = model.encode(paragraphs)
​실습 실험: 문단 단위로 나눈 후 각 문단의 벡터를 시각화해서 문단별 유사도를 비교해 보세요. 이 과정에서 PCA 또는 t-SNE 같은 차원 축소 방법을 사용하면 벡터 간의 관계를 쉽게 확인할 수 있어요!Faiss로 유사도 검색하기💡임베딩이 끝났다면, 이제 Faiss를 사용해 유사도를 계산해 봅시다. Faiss는 벡터 간 유사도를 빠르게 계산할 수 있도록 도와주는 라이브러리입니다.Faiss 초기화 및 인덱싱먼저, 임베딩된 벡터들을 인덱싱해서 검색할 준비를 해볼게요!Python복사import faiss

# 문장 임베딩을 사용해 인덱스 생성
dimension = sentence_embeddings.shape[1]
index = faiss.IndexFlatL2(dimension) # L2 거리 기반 인덱스
# 인덱스에 벡터 추가
index.add(np.array(sentence_embeddings))
​유사 문장 검색하기특정 문장과 유사한 문장을 찾아봅시다! 검색하고 싶은 문장을 입력하고, 인덱스를 활용해 유사도를 계산합니다.Python복사query = "검색할 문장을 입력하세요."
query_vec = model.encode(query)
# 유사한 문장 3개 찾기
D, I = index.search(np.array([query_vec]), k=3)
# 결과 출력
for idx in I[0]:
print(f"유사 문장: {sentences[idx]}")
​실습 실험: 다른 문장들을 입력해서 어떤 문장이 가장 유사하게 검색되는지 실험해 보세요. 예를 들어, "오늘 날씨는 어때?"와 같은 문장을 넣어보면 유사한 주제를 가진 문장들이 잘 검색되는지 확인할 수 있습니다.유사도에 대한 실험유사도 검색 결과의 정확도를 평가해봅시다. 다음과 같은 실험을 통해 유사도 결과를 분석해 볼 수 있어요

1️⃣다양한 쿼리 테스트
다양한 쿼리를 사용하여 유사 문장 검색 결과를 비교해 보세요. 예를 들어, 간단한 문장과 복잡한 문장을 각각 입력해 보고 결과가 어떻게 달라지는지 관찰합니다.2️⃣유사도 점수 분석
검색 결과로 반환된 유사도 점수(D)를 분석하여, 높은 유사도와 낮은 유사도 간의 차이를 확인해 보세요. 예를 들어, D 값을 출력하여 얼마나 유사한지 정량적으로 평가해 볼 수 있습니다.Python복사# 유사도 점수와 함께 결과 출력
for idx, score in zip(I[0], D[0]):
print(f"유사 문장: {sentences[idx]}, 유사도 점수: {score}")
​실습 실험: 유사도 점수가 높은 문장과 낮은 문장을 비교해 보고, 그 차이가 문장의 의미나 표현 방식에서 어떻게 드러나는지 분석해 보세요. 이렇게 하면 모델이 유사도를 어떻게 판단하는지 더 깊이 이해할 수 있습니다.길이 단위 선택 가이드전체 문서 임베딩: 문서 전체 맥락을 분석하고 싶을 때.문장 단위 임베딩: 특정 문장과의 유사도 계산이나 세밀한 검색이 필요할 때.문단 단위 임베딩: 주제별 유사도를 분석하고 싶을 때.Tip: 각 단위로 임베딩했을 때의 차이를 비교해 보는 것도 좋은 실습이 될 수 있어요. 예를 들어, 문장 단위로 했을 때와 문단 단위로 했을 때의 검색 결과가 어떻게 달라지는지 확인해 보세요. ​추가 실험: 임베딩 벡터 시각화하기❗임베딩된 벡터들은 고차원 공간에 위치하게 됩니다. 이를 시각화하면 벡터 간의 관계를 더 직관적으로 이해할 수 있어요. PCA나 t-SNE 같은 차원 축소 기법을 사용해 2D나 3D로 시각화해 봅시다!Python복사from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

# 문장 임베딩 벡터의 차원을 축소하여 시각화
pca = PCA(n_components=2)
reduced_embeddings = pca.fit_transform(sentence_embeddings)
# 시각화
plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1])
plt.title('Sentence Embeddings Visualization')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.show()
​실습 실험: 각 색깔로 다른 문단이나 문서의 벡터를 시각화해서 벡터들이 어떻게 군집되는지 확인해 보세요. 이는 유사한 문장들이 벡터 공간에서 얼마나 가깝게 위치하는지 직관적으로 이해하는 데 도움이 됩니다. ​추가 실험: t-SNE를 이용한 고차원 임베딩 시각화❗t-SNE는 고차원 데이터를 저차원으로 시각화하는 데 유용한 기법이에요. 임베딩된 벡터들을 t-SNE를 사용해 시각화해 보면, 벡터 간의 관계를 더 명확하게 이해할 수 있습니다.Python복사from sklearn.manifold import TSNE

# t-SNE로 차원 축소
tsne = TSNE(n_components=2, perplexity=30, n_iter=300)
tsne_results = tsne.fit_transform(sentence_embeddings)
# 시각화
plt.scatter(tsne_results[:, 0], tsne_results[:, 1])
plt.title('t-SNE Sentence Embeddings Visualization')
plt.xlabel('t-SNE Component 1')
plt.ylabel('t-SNE Component 2')
plt.show()
​Tip: t-SNE는 계산 비용이 많이 들기 때문에, 데이터의 크기가 클 경우 일부만 샘플링해서 시각화해 보세요. t-SNE는 특히 데이터 간의 지역적 구조를 잘 드러내는 데 유리해요.
데이터를 선택하는 기본 방법인공지능을 위한 파이썬 라이브러리/인공지능을 위한 파이썬 라이브러리 - 3주차/데이터를 선택하는 기본 방법제작:데이터를 선택하는 기본 방법수업 목표판다스에서 인덱스, 열, 행을 지정하는 방법에 대해서 학습합니다.목차인덱스를 사용한 데이터 선택열(Column) 지정하여 데이터 선택하기행(Row) 지정하여 데이터 선택하기요약인덱스를 사용한 데이터 선택데이터프레임에서 인덱스는 각 행을 고유하게 식별할 수 있는 라벨입니다. 인덱스를 사용해 특정 행이나 그룹을 쉽게 선택할 수 있습니다.set_index()로 인덱스 설정하기특정 열을 인덱스로 설정하면, 데이터 선택이 더 직관적이 됩니다.Python복사import pandas as pd

# 예시 데이터프레임 생성
data = {
'이름': ['철수', '영희', '민수'],
'나이': [25, 30, 35],
'직업': ['학생', '회사원', '프리랜서']
}
df = pd.DataFrame(data)
# '이름' 열을 인덱스로 설정
df = df.set_index('이름')
print(df)

​결과:Plain Text복사      나이       직업
이름
철수   25      학생
영희   30    회사원
민수   35  프리랜서

​loc[]을 사용해 인덱스로 데이터 선택인덱스를 설정한 후, loc[]을 사용해 쉽게 데이터에 접근할 수 있습니다.Python복사# '영희'의 데이터 선택
print(df.loc['영희'])

​결과:Plain Text복사나이       30
직업    회사원
Name: 영희, dtype: object​열(Column) 지정하여 데이터 선택하기데이터프레임의 열을 지정하여 특정 데이터에 접근하는 방법입니다.단일 열 선택열 이름을 사용해 단일 열을 선택할 수 있습니다.Python복사# '나이' 열 선택
print(df['나이'])

​결과:Plain Text복사이름
철수    25
영희    30
민수    35
Name: 나이, dtype: int64​여러 열 선택여러 열을 선택하고 싶다면, 열 이름의 리스트를 사용하세요.Python복사# '나이'와 '직업' 열 선택
print(df[['나이', '직업']])

​결과:Plain Text복사      나이       직업
이름
철수   25      학생
영희   30    회사원
민수   35  프리랜서

​행(Row) 지정하여 데이터 선택하기행을 지정하여 데이터프레임의 특정 부분을 선택하는 방법입니다.iloc[]을 사용한 행 선택iloc[]을 사용하면 정수 인덱스로 행을 선택할 수 있습니다.Python복사# 첫 번째 행 선택
print(df.iloc[0])

​결과:Plain Text복사나이     25
직업     학생
Name: 철수, dtype: object

​loc[]을 사용한 특정 행 선택loc[]을 사용해 인덱스 라벨로 특정 행을 선택할 수 있습니다.Python복사# '민수'의 데이터 선택
print(df.loc['민수'])

​결과:Plain Text복사나이       35
직업    프리랜서
Name: 민수, dtype: object

​여러 행 선택iloc[]이나 loc[]을 사용해 여러 행을 선택할 수 있습니다.Python복사# 첫 번째와 두 번째 행 선택
print(df.iloc[0:2])

​결과:Plain Text복사      나이    직업
이름
철수   25   학생
영희   30  회사원

​요약요약인덱스는 데이터프레임의 특정 행을 고유하게 식별하는 데 사용되며, set_index()로 설정할 수 있습니다.loc[]은 인덱스와 열 라벨을 사용해 데이터에 접근하며, iloc[]은 정수 인덱스를 사용합니다.단일 열 또는 여러 열을 선택하여 특정 데이터를 추출할 수 있습니다.행과 열을 함께 지정해 정교한 데이터 선택이 가능합니다.이제 데이터프레임에서 원하는 데이터를 자유롭게 선택해보세요! ​
[스파르타코딩클럽] 8강. 지도학습 : 회귀모델 [SCC] 바닥부터 시작하는 머신러닝/[스파르타코딩클럽] 바닥부터 시작하는 머신러닝 - 3주차/[스파르타코딩클럽] 8강. 지도학습 : 회귀모델 제작:[스파르타코딩클럽] 8강. 지도학습 : 회귀모델 [수업 목표]다양한 회귀 모델에 대해서 배워봅시다회귀(Regression)모델은 연속적인 값을 예측하는 문제입니다.회귀(Regression)모델은 연속적인 값을 예측하는 문제입니다.﻿
오늘은 선형회귀/다항회귀/리지회귀/라쏘회귀 를 다뤄볼 예정입니다오늘은 선형회귀/다항회귀/리지회귀/라쏘회귀 를 다뤄볼 예정입니다﻿​[목차]01. 회귀모델
 
 01. 회귀모델다양한 회귀 모델(선형회귀/다항회귀/리지회귀/라쏘회귀)에 대해서 알아보고 실습해 봅시다1) 선형 회귀 선형 회귀선형회귀는 종속 변수와 하나 이상의 독립 변수 간의 선형 관계를 모델링 하는 방법입니다독립변수의 수에 따라 단순 선형회귀와 다중 선형회귀로 나뉩니다 단순 선형 회귀 VS 다중 선형 회귀ALT단순 선형 회귀 : 하나의 독립 변수와 하나의 종속 변수 간의 관계를 모델링다중 선형 회귀 : 여러 독립 변수와 하나의 종속 변수 간의 관계를 모델링선형 회귀의 기본 수식은 다음과 같습니다.선형 회귀의 기본 수식은 다음과 같습니다.﻿
y=β0​+β1​x1​+β2​x2​+⋯+βn​xn​+ϵy=β0​+β1​x1​+β2​x2​+⋯+βn​xn​+ϵy=β0​+β1​x1​+β2​x2​+⋯+βn​xn​+ϵ﻿
여기서 y는 종속변수, x1,x2,…,xn 은 독립변수, b0 는 절편, b1,b2,…,bn은 회귀계수, e는 오차입니다여기서 y는 종속변수, x1,x2,…,xn 은 독립변수, b0 는 절편, b1,b2,…,bn은 회귀계수, e는 오차입니다﻿
⋄ 단순 선형 회귀일경우 ⋄ 단순 선형 회귀일경우 ﻿
y=β0​+β1​x+ϵy=β0​+β1​x+ϵ﻿
 Scikit-learn 을 사용한 선형 회귀 모델 구현 및 평가선형 회귀 모델 구현 및 평가 {5px}선형 회귀 모델 구현 및 평가 ﻿​Python복사import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# 데이터 생성
X = np.array([[1, 1], [2, 2], [3, 3], [4, 4], [5, 5],[6,6]])
y = np.array([1, 2, 3, 4, 5, 6])
# 데이터 분할 (훈련 데이터와 테스트 데이터)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# 선형 회귀 모델 생성 및 학습
model = LinearRegression()
model.fit(X_train, y_train)
# 예측
y_pred = model.predict(X_test)
# 모델 평가
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f'Mean Squared Error: {mse}')
print(f'R^2 Score: {r2}')
​2) 다항 회귀 다항 회귀다항 회귀(Polynomial Regression)는 종속 변수와 독립 변수 간의 비선형 관계를 모델링하는 방법독립변수의 다항식을 사용하여 관계를 모델링 합니다.다항 회귀의 기본 수식은 다음과 같습니다다항 회귀의 기본 수식은 다음과 같습니다﻿
y=β0​+β1​x+β2​x2+⋯+βn​xn+ϵy=β0​+β1​x+β2​x^2+⋯+βn​x^n+ϵy=β0​+β1​x+β2​x2+⋯+βn​xn+ϵ﻿
여기서 y는 종속변수, x1,x2,…,xn 은 독립변수, b0 는 절편, b1,b2,…,bn은 회귀계수, e는 오차입니다여기서 y는 종속변수, x1,x2,…,xn 은 독립변수, b0 는 절편, b1,b2,…,bn은 회귀계수, e는 오차입니다﻿​ 다항 회귀 차수 선택다항회귀 차수(degree) : 독립 변수의 최대 차수차수가 높을수록 모델이 더 복잡해지며 과적합(overfitting)의 위험 존재 → 적절한 차수 선택 필요과적합이란 학습데이터에 모델이 과도하게 적합(fitting)되는 현상입니다과적합이란 학습데이터에 모델이 과도하게 적합(fitting)되는 현상입니다﻿​ Scikit-learn을 사용한 다항 회귀 모델 구현 및 평가다항 회귀 모델 구현 및 평가 {5px}다항 회귀 모델 구현 및 평가 ﻿​Python복사import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# 데이터 생성
X = np.array([[1], [2], [3], [4], [5], [6]])
y = np.array([1, 4, 9, 16, 25, 36])
# 다항 특징 생성 (차수 2)
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)
# 데이터 분할 (훈련 데이터와 테스트 데이터)
X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=42)
# 다항 회귀 모델 생성 및 학습
model = LinearRegression()
model.fit(X_train, y_train)
# 예측
y_pred = model.predict(X_test)
# 모델 평가
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f'Mean Squared Error: {mse}')
print(f'R^2 Score: {r2}')
​3) 리지 회귀 리지 회귀리지 회귀(Ridge Regression)는 선형 회귀의 일종회귀 계수의 크기를 제어하여 과적합을 방지하는 정규화 기법L2 정규화(regularization)를 사용하여 회귀 계수의 제곱합을 최소화 합니다리지 회귀의 기본 수식은 다음과 같습니다리지 회귀의 기본 수식은 다음과 같습니다﻿
J(β)=∑i=1n(yi−y^i)2+λ∑j=1pβj2J() = _{i=1}^{n} (y_i - _i)^2 +  _{j=1}^{p} _j^2J(β)=∑i=1n​(yi​−y^​i​)2+λ∑j=1p​βj2​﻿
여기서λ는 정규화 강도를 조절하는 하이퍼파라미터 입니다. 여기서λ는 정규화 강도를 조절하는 하이퍼파라미터 입니다.﻿​ L2 정규화 L2 정규화는 모든 가중치를 작게 만들어 모델의 복잡도를 줄입니다.손실 함수에 제곱항을 추가하여 매끄러운 최적화가 가능합니다.정규화는 모델의 복잡도를 제어하여 과적합을 방지하는 데 필요합니다. Scikit-learn을 사용한 리지 회귀 모델 구현 및 평가리지 회귀 모델 구현 및 평가 {5px}리지 회귀 모델 구현 및 평가 ﻿​Python복사import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error, r2_score

# 데이터 생성
X = np.array([[1, 1], [2, 2], [3, 3], [4, 4], [5, 5], [6, 6]])
y = np.array([1, 2, 3, 4, 5, 6])
# 데이터 분할 (훈련 데이터와 테스트 데이터)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# 리지 회귀 모델 생성 및 학습
model = Ridge(alpha=1.0)
model.fit(X_train, y_train)
# 예측
y_pred = model.predict(X_test)
# 모델 평가
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f'Mean Squared Error: {mse}')
print(f'R^2 Score: {r2}')
​4) 라쏘 회귀 라쏘 회귀라쏘 회귀(Lasso Regression)는 선형 회귀의 일종회귀 계수의 크기를 제어하여 과적합을 방지하는 정규화 기법L1 정규화(regularization)를 사용하여 회귀 계수의 절대값 합을 최소화 합니다라쏘 회귀의 기본 수식은 다음과 같습니다라쏘 회귀의 기본 수식은 다음과 같습니다﻿
J(β)=∑i=1n(yi−y^i)2+λ∑j=1p∣βj∣J() = _{i=1}^{n} (y_i - _i)^2 +  _{j=1}^{p} |_j|J(β)=∑i=1n​(yi​−y^​i​)2+λ∑j=1p​∣βj​∣﻿
여기서λ는 정규화 강도를 조절하는 하이퍼파라미터 입니다. 여기서λ는 정규화 강도를 조절하는 하이퍼파라미터 입니다.﻿​ L1 정규화와 특징 선택L1 정규화는 일부 회귀 계수를 0으로 만들어 특징 선택(feature selection)을 수행모델의 해석 가능성을 높이고, 불필요한 특징을 제거하는 데 유용합니다 Scikit-learn을 사용한 라쏘 회귀 모델 구현 및 평가라쏘 회귀 모델 구현 및 평가 {5px}라쏘 회귀 모델 구현 및 평가 ﻿​Python복사import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Lasso
from sklearn.metrics import mean_squared_error, r2_score

# 데이터 생성
X = np.array([[1, 1], [2, 2], [3, 3], [4, 4], [5, 5], [6,6]])
y = np.array([1, 2, 3, 4, 5, 6])
# 데이터 분할 (훈련 데이터와 테스트 데이터)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# 라쏘 회귀 모델 생성 및 학습
model = Lasso(alpha=1.0)
model.fit(X_train, y_train)
# 예측
y_pred = model.predict(X_test)
# 모델 평가
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f'Mean Squared Error: {mse}')
print(f'R^2 Score: {r2}')
​.
[스파르타코딩클럽] 8. 자연어 처리(NLP) 모델[SCC] 기초가 탄탄한 딥러닝/[스파르타코딩클럽] 기초가 탄탄한 딥러닝 - 3주차/[스파르타코딩클럽] 8. 자연어 처리(NLP) 모델제작:[스파르타코딩클럽] 8. 자연어 처리(NLP) 모델[수업 목표]자연어 처리 모델에 대해서 알아보고 동작 원리에 대해서 학습해 봅시다Pytorch로 간단한 텍스트 분류 및 생성 모델 구현 실습을 진행해 봅시다[목차]01. 워드 임베딩과 시퀀스 모델링02. Transformer와 BERT💡
 
 01. 워드 임베딩과 시퀀스 모델링✔️워드임베딩 기법이 무엇인지 알아보고 시퀀스 모델링이 무엇인지 학습해 봅시다1) 워드 임베딩 기법 워드 임베딩 기법워드 임베딩(Word Embedding)은 단어를 고정된 크기의 벡터로 변환하는 기법으로, 단어 간의 의미적 유사성을 반영합니다.대표적인 워드 임베딩 기법으로는 Word2Vec과 GloVe가 있습니다.ALT Word2VecWord2Vec은 단어를 벡터로 변환하는 두 가지 모델(CBOW와 Skip-gram)을 제공합니다.CBOW (Continuous Bag of Words): 주변 단어(context)로 중심 단어(target)를 예측합니다.Skip-gram: 중심 단어(target)로 주변 단어(context)를 예측합니다. GloVe (Global Vectors for Word Representation)GloVe는 단어-단어 공기행렬(word-word co-occurrence matrix)을 사용, 단어 벡터를 학습합니다.전역적인 통계 정보를 활용하여 단어 간의 의미적 유사성을 반영합니다.2) 시퀀스 모델링 시퀀스 모델링의 기본 개념시퀀스 모델링(Sequence Modeling)은 순차적인 데이터를 처리하고 예측하는 모델링 기법입니다. 시퀀스 모델링은 주로 RNN, LSTM, GRU와 같은 순환 신경망을 사용합니다.ALT 입력 시퀀스시퀀스 모델링에서는 입력 데이터가 순차적인 형태로 제공됩니다.예를 들어, 텍스트 데이터는 단어의 시퀀스로 표현됩니다. 은닉 상태순환 신경망은 이전 시간 단계의 은닉 상태를 현재 시간 단계로 전달하여, 시퀀스의 패턴을 학습합니다. 출력 시퀀스시퀀스 모델링의 출력은 입력 시퀀스와 동일한 길이의 시퀀스일 수도 있고, 단일 값일 수도 있습니다.02. Transformer와 BERT✔️Transformer의 구조에 대해 알아보고 이를 이용한 BERT 모델에 대해서 배워봅시다1) Transformer의 구조와 원리 Transformer의 구조와 원리Transformer는 순차적인 데이터를 병렬로 처리할 수 있는 모델로, 자연어 처리에서 뛰어난 성능을 보입니다.Transformer는 인코더-디코더(Encoder-Decoder) 구조로 구성됩니다.ALT 인코더 (Encoder)입력 시퀀스를 처리하여 인코딩된 표현을 생성합니다.각 인코더 층은 셀프 어텐션(Self-Attention)과 피드포워드 신경망(Feed-Forward Neural Network)으로 구성됩니다. 디코더 (Decoder)인코딩된 표현을 바탕으로 출력 시퀀스를 생성합니다.각 디코더 층은 셀프 어텐션, 인코더-디코더 어텐션, 피드포워드 신경망으로 구성됩니다. 어텐션 메커니즘 (Attention Mechanism)어텐션 메커니즘은 입력 시퀀스의 각 위치에 가중치를 부여하여, 중요한 정보를 강조합니다.셀프 어텐션은 입력 시퀀스 내의 단어 간의 관계를 학습합니다.2) BERT의 개념과 응용 BERT란?BERT(Bidirectional Encoder Representations from Transformers)는 Transformer 인코더를 기반으로 한 사전 학습된 언어 모델입니다.BERT는 양방향으로 문맥을 이해할 수 있어, 다양한 자연어 처리 작업에서 뛰어난 성능을 보입니다. 사전 학습(Pre-training)BERT는 대규모 텍스트 코퍼스를 사용하여 사전 학습됩니다.마스킹 언어 모델(Masked Language Model)과 다음 문장 예측(Next Sentence Prediction) 작업을 통해 학습됩니다. 파인튜닝 (Fine-tuning)사전 학습된 BERT 모델을 특정 작업에 맞게 파인튜닝합니다.텍스트 분류, 질의 응답, 텍스트 생성 등 다양한 자연어 처리 작업에 적용할 수 있습니다..
조건부 필터링과 데이터 타입 변환인공지능을 위한 파이썬 라이브러리/인공지능을 위한 파이썬 라이브러리 - 3주차/조건부 필터링과 데이터 타입 변환제작:조건부 필터링과 데이터 타입 변환수업 목표판다스에서 필터링과 데이터 타입을 변환하는 방법에 대해서 학습합니다.목차조건부 필터링데이터 타입 변환요약조건부 필터링조건부 필터링을 통해 데이터프레임에서 특정 조건을 만족하는 데이터만 선택할 수 있습니다. 이 기능은 데이터 분석에서 매우 유용합니다.기본 조건부 필터링조건을 사용해 필터링된 데이터프레임을 반환할 수 있습니다.Python복사import pandas as pd

# 예시 데이터프레임 생성
data = {
'이름': ['철수', '영희', '민수', '지수'],
'나이': [25, 30, 22, 35],
'직업': ['학생', '회사원', '학생', '프리랜서']
}
df = pd.DataFrame(data)
# 나이가 25 이상인 행만 선택
filtered_df = df[df['나이'] >= 25]
print(filtered_df)

​결과:Plain Text복사   이름  나이       직업
0  철수   25      학생
1  영희   30    회사원
3  지수   35  프리랜서

​여러 조건을 사용한 필터링여러 조건을 결합하여 필터링할 수 있습니다. AND(&), OR(|) 연산자를 사용하세요.Python복사# 나이가 25 이상이고, 직업이 '학생'인 행만 선택
filtered_df = df[(df['나이'] >= 25) & (df['직업'] == '학생')]
print(filtered_df)

​결과:Plain Text복사   이름  나이  직업
0  철수   25  학생

​isin()을 사용한 필터링특정 값들이 포함된 행을 필터링할 때 유용합니다.Python복사# 직업이 '학생' 또는 '프리랜서'인 행 선택
filtered_df = df[df['직업'].isin(['학생', '프리랜서'])]
print(filtered_df)

​결과:Plain Text복사   이름  나이       직업
0  철수   25      학생
2  민수   22      학생
3  지수   35  프리랜서

​~ 연산자를 사용한 반대 조건 필터링특정 조건을 부정하는 데이터를 선택할 때 사용합니다.Python복사# 직업이 '학생'이 아닌 행 선택
filtered_df = df[~(df['직업'] == '학생')]
print(filtered_df)

​결과:Plain Text복사   이름  나이       직업
1  영희   30    회사원
3  지수   35  프리랜서

​데이터 타입 변환데이터프레임의 데이터 타입을 확인하고, 필요에 따라 변환하는 방법입니다. 데이터 분석 및 처리 시, 올바른 데이터 타입을 사용하는 것이 중요합니다.dtype으로 데이터 타입 확인각 열의 데이터 타입을 확인할 수 있습니다.Python복사print(df.dtypes)

​결과:Plain Text복사이름    object
나이     int64
직업    object
dtype: object

​astype()을 사용한 데이터 타입 변환astype() 함수로 특정 열의 데이터 타입을 변경할 수 있습니다.Python복사# '나이' 열을 정수형(int)에서 실수형(float)으로 변환
df['나이'] = df['나이'].astype(float)
print(df.dtypes)

​결과:Plain Text복사이름    object
나이    float64
직업    object
dtype: object

​날짜 타입으로 변환날짜 데이터는 pd.to_datetime() 함수를 사용해 날짜 타입으로 변환할 수 있습니다.Python복사# 예시 데이터프레임 생성
data = {
'이름': ['철수', '영희', '민수'],
'가입일': ['2023-01-01', '2022-12-15', '2023-05-22']
}
df = pd.DataFrame(data)
# '가입일'을 날짜 타입으로 변환
df['가입일'] = pd.to_datetime(df['가입일'])
print(df.dtypes)

​결과:Plain Text복사이름            object
가입일    datetime64[ns]
dtype: object

​카테고리 데이터로 변환카테고리는 메모리를 절약하고, 성능을 높이기 위해 사용할 수 있습니다.Python복사# '직업' 열을 카테고리형으로 변환
df['직업'] = df['직업'].astype('category')
print(df.dtypes)

​결과:Plain Text복사이름     object
나이    float64
직업   category
dtype: object

​요약요약조건부 필터링은 특정 조건을 만족하는 데이터만 선택하는 강력한 도구입니다. &, |, ~ 등의 연산자를 결합하여 복잡한 조건을 구성할 수 있습니다.데이터 타입 변환은 데이터를 올바르게 처리하기 위해 필수적이며, astype(), pd.to_datetime() 등을 사용해 변환할 수 있습니다.데이터의 정확한 타입을 지정함으로써 메모리 절약과 성능 향상을 기대할 수 있습니다.이제 다양한 조건으로 데이터를 필터링하고, 적절한 데이터 타입으로 변환하여 데이터 분석의 효율을 높여보세요! ​
[스파르타코딩클럽] 9강. 지도학습 : 분류모델 - 로지스틱 회귀[SCC] 바닥부터 시작하는 머신러닝/[스파르타코딩클럽] 바닥부터 시작하는 머신러닝 - 3주차/[스파르타코딩클럽] 9강. 지도학습 : 분류모델 - 로지스틱 회귀제작:[스파르타코딩클럽] 9강. 지도학습 : 분류모델 - 로지스틱 회귀[수업 목표]지도학습 : 분류모델의 로지스틱 회귀에 대해 알아보고 실습을 통해 배워봅시다[목차]01. 로지스틱 회귀 개념02. 로지스틱 회귀분석 실습💡
 
 01. 로지스틱 회귀 개념✔️로지스틱 회귀란 무엇인지, 왜 사용하는지 배워봅시다1) 로지스틱 회귀 로지스틱 회귀란?종속 변수가 이진형일 때(즉, 결과가 두 가지 중 하나일 때) 사용되는 통계 기법로지스틱 회귀는 선형 회귀와 달리 결과값이 0과 1 사이에 위치하게 하기 위해 시그모이드 함수(Sigmoid Function)를 사용합니다.로지스틱 회귀는 회귀 분석이지만, 실제로는 분류 작업에 사용됩니다!!로지스틱 회귀는 회귀 분석이지만, 실제로는 분류 작업에 사용됩니다!!﻿​ 시그모이드 함수시그모이드 함수는 로지스틱 회귀의 핵심으로 입력값을 0과 1사이의 값으로 변환합니다σ(z)=11+e−z(z) = {1 + e^{-z}} σ(z)=1+e−z1​여기서 z 는 선형 회귀 방정식z=β0+β1x1+β2x2+…+βnxn입니다.{z = _0 + _1x_1 + _2x_2 +  + _nx_n }여기서 z 는 선형 회귀 방정식z=β0​+β1​x1​+β2​x2​+…+βn​xn​입니다.﻿​ 로지스틱 회귀의 목적로지스틱 회귀는 데이터를 학습하여 각 데이터 포인트가 특정 클래스에 속할 확률을 예측합니다.예를 들어, 유방암 데이터에서는 환자가 암에 걸렸을 확률, 타이타닉 데이터 에서는 승객이 생존할 확률을 예측합니다 비용 함수로지스틱 회귀의 비용 함수는 모델의 예측 확률과 실제 레이블 사이의 차이를 측정합니다.로그 손실 함수(Log Loss) 또는 크로스 엔트로피 손실 함수(Cross-Entropy loss)라고 불립니다.J(θ)=−1m∑i=1m[y(i)log⁡(hθ(x(i)))+(1−y(i))log⁡(1−hθ(x(i)))]J() = -{m} _{i=1}^{m} [ y^{(i)} (h_(x^{(i)})) + (1 - y^{(i)}) (1 - h_(x^{(i)})) ]J(θ)=−m1​i=1∑m​[y(i)log(hθ​(x(i)))+(1−y(i))log(1−hθ​(x(i)))]02. 로지스틱 회귀분석 실습✔️Scikit-learn의 유방암데이터와 Seaborn의 타이타닉 데이터로 로지스틱 회귀분석 실습을 진행합니다1) 유방암 데이터 데이터 로드 및 전처리유방암 데이터 로드 및 전처리 {5px}유방암 데이터 로드 및 전처리 ﻿​Python복사import numpy as np
import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# 데이터 로드
data = load_breast_cancer()
X = data.data
y = data.target

# 데이터 분할
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# 데이터 스케일링
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
​sklearn.datasets.load_breast_cancer: 유방암 데이터셋 로드return_X_y=False: 데이터와 타겟을 함께 반환할지 여부. 기본값은 False입니다._X_y=False: 데이터와 타겟을 함께 반환할지 여부. 기본값은 False입니다.﻿​sklearn.model_selection.train_test_split: 데이터를 훈련 세트/ 테스트 세트로 분할test_size=0.2: 테스트 세트의 비율을 0.2로 설정합니다._size=0.2: 테스트 세트의 비율을 0.2로 설정합니다.﻿​random_state=42: 랜덤 시드 값으로, 데이터 분할의 재현성을 위해 사용됩니다._state=42: 랜덤 시드 값으로, 데이터 분할의 재현성을 위해 사용됩니다.﻿​sklearn.preprocessing.StandardScaler: 데이터의 평균을 0, 분산을 1로 스케일링fit_transform(X_train): 훈련 세트를 스케일링하고 변환합니다._transform(X_train): 훈련 세트를 스케일링하고 변환합니다.﻿​transform(X_test): 테스트 세트를 변환합니다.(X_test): 테스트 세트를 변환합니다.﻿​ 모델 학습모델 학습 {5px}모델 학습 ﻿​Python복사from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# 모델 생성 및 학습
model = LogisticRegression()
model.fit(X_train, y_train)
# 예측
y_pred = model.predict(X_test)
# 평가
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
print(f"Classification Report:")
print(f"Confusion Matrix:")
​sklearn.linear_model.LogisticRegression: 로지스틱 회귀 모델 생성fit(X_train, y_train): 모델을 훈련 세트에 맞추어 학습시킵니다.(X_train, y_train): 모델을 훈련 세트에 맞추어 학습시킵니다.﻿​predict(X_test): 테스트 세트에 대해 예측을 수행합니다.(X_test): 테스트 세트에 대해 예측을 수행합니다.﻿​sklearn.metrics.accuracy_score: 정확도 계산accuracy_score(y_test, y_pred): 실제 값과 예측 값을 비교하여 정확도를 반환합니다._score(y_test, y_pred): 실제 값과 예측 값을 비교하여 정확도를 반환합니다.﻿​sklearn.metrics.classification_report: 분류 보고서 생성classification_report(y_test, y_pred): 정확도, 정밀도, 재현율 등의 메트릭을 포함한 보고서를 출력합니다._report(y_test, y_pred): 정확도, 정밀도, 재현율 등의 메트릭을 포함한 보고서를 출력합니다.﻿​sklearn.metrics.confusion_matrix: 혼동 행렬 생성confusion_matrix(y_test, y_pred): 실제 값과 예측 값의 혼동 행렬을 반환합니다._matrix(y_test, y_pred): 실제 값과 예측 값의 혼동 행렬을 반환합니다.﻿​2) 타이타닉 데이터 데이터 로드 및 전처리타이타닉 데이터 로드 및 전처리 {5px}타이타닉 데이터 로드 및 전처리 ﻿​Python복사import seaborn as sns

# 데이터 로드
titanic = sns.load_dataset('titanic')
# 필요한 열 선택 및 결측값 처리
titanic = titanic[['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']].dropna()
# 성별과 탑승한 곳 인코딩
titanic['sex'] = titanic['sex'].map({'male': 0, 'female': 1})
titanic['embarked'] = titanic['embarked'].map({'C': 0, 'Q': 1, 'S': 2})
# 특성과 타겟 분리
X = titanic.drop('survived', axis=1)
y = titanic['survived']
# 데이터 분할
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# 데이터 스케일링
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
​seaborn.load_dataset: seaborn의 내장 데이터셋 로드’titanic’: 타이타닉 데이터셋을 로드합니다.’titanic’: 타이타닉 데이터셋을 로드합니다.﻿​ pandas.DataFrame.dropna: 결측값이 있는 행 제거pandas.DataFrame.map: 데이터 값을 다른 값으로 매핑’male’: 0, ’female’: 1: 성별을 숫자로 매핑합니다.: 성별을 숫자로 매핑합니다.}’male’: 0, ’female’: 1: 성별을 숫자로 매핑합니다.﻿​’C’: 0, ’Q’: 1, ’S’: 2: 탑승한 곳을 숫자로 매핑합니다.: 탑승한 곳을 숫자로 매핑합니다.}’C’: 0, ’Q’: 1, ’S’: 2: 탑승한 곳을 숫자로 매핑합니다.﻿​ 모델 학습모델 학습 {5px}모델 학습 ﻿​Python복사# 모델 생성 및 학습
model = LogisticRegression()
model.fit(X_train, y_train)
# 예측
y_pred = model.predict(X_test)
# 평가
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
print(f"Classification Report:")
print(f"Confusion Matrix:")
​.
[스파르타코딩클럽] 9. ResNet[SCC] 기초가 탄탄한 딥러닝/[스파르타코딩클럽] 기초가 탄탄한 딥러닝 - 4주차/[스파르타코딩클럽] 9. ResNet제작:[스파르타코딩클럽] 9. ResNet[수업 목표]비전 모델의 길을 열어준 ResNet!왜 좋은 지 한번 알아봅시다[목차]01. 개념💡
 
 01. 개념✔️ResNet 기본 개념과 동작 방식에 대해 알아봅시다1) ResNet 기본 개념과 동작 방식 ResNet이란?ResNet(Residual Network)은 깊은 신경망을 학습하기 위해 개발된 모델로, 잔차 학습(Residual Learning) 개념을 도입하여 매우 깊은 네트워크에서도 효율적인 학습이 가능하도록 합니다. ResNet은 2015년 Microsoft Research에서 개발되었으며, 딥러닝 모델이 너무 깊어질 때 발생하는 기울기 소실 문제를 해결합니다.  ResNet의 기본 개념깊은 신경망의 문제깊은 신경망은 더 많은 계층을 쌓아 복잡한 패턴을 학습할 수 있지만, 너무 깊어지면 학습이 어려워지는 문제가 있습니다. 주로 기울기 소실(Vanishing Gradient)이나 기울기 폭발(Exploding Gradient) 같은 현상 때문에 발생합니다. 이는 모델이 더 이상 깊어지지 못하고 성능이 저하되는 결과를 초래합니다.잔차 학습(Residual Learning)ResNet은 이러한 문제를 해결하기 위해 잔차 학습(Residual Learning)을 도입합니다. 잔차 학습은 각 층의 출력이 바로 다음 층의 입력으로 전달되지 않고, 이전 층의 입력을 더해줌으로써 학습을 돕습니다. 이를 통해 기울기 소실 문제를 완화할 수 있습니다.2)  ResNet의 주요 특징 기울기 소실 문제 해결ResNet은 잔차 학습을 통해 깊은 네트워크에서도 기울기 소실 문제를 해결합니다.입력을 출력에 더해줌으로써 신호가 더욱 쉽게 전달되어 학습이 원활하게 이루어집니다. 간단한 블록 구조ResNet은 간단한 블록 구조를 사용하여 네트워크를 쉽게 확장할 수 있습니다.  높은 성능ResNet은 이미지 분류, 객체 검출 등 다양한 컴퓨터 비전 작업에서 높은 성능을 발휘합니다. 깊은 네트워크에서도 안정적으로 학습할 수 있어, 복잡한 패턴을 잘 학습합니다.3) ResNet 실습 코드Python복사import torch
import torch.nn as nn
import torch.nn.functional as F

class Block(nn.Module):
def __init__(self, in_ch, out_ch, stride=1):
super(Block, self).__init__()
# 첫 번째 컨볼루션 레이어
        self.conv1 = nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(out_ch) # 배치 정규화
# 두 번째 컨볼루션 레이어
        self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(out_ch) # 배치 정규화
# 입력과 출력의 차원이 다를 경우 shortcut 경로 정의
        self.skip_connection = nn.Sequential()
if stride != 1 or in_ch != out_ch:
            self.skip_connection = nn.Sequential(
                nn.Conv2d(in_ch, out_ch, kernel_size=1, stride=stride, bias=False), # 차원 맞추기 위한 1x1 컨볼루션
                nn.BatchNorm2d(out_ch) # 배치 정규화
)
def forward(self, x):
# 첫 번째 컨볼루션 + ReLU 활성화 함수
        output = F.relu(self.bn1(self.conv1(x)))
# 두 번째 컨볼루션 후 배치 정규화
        output = self.bn2(self.conv2(output))
# shortcut 경로 출력과 현재 블록의 출력 더하기
        output += self.skip_connection(x)
# 최종 ReLU 활성화 함수 적용
        output = F.relu(output)
return output

# ResNet 모델 정의
class CustomResNet(nn.Module):
def __init__(self, block, layers, num_classes=10):
super(CustomResNet, self).__init__()
        self.initial_channels = 64 # 첫 번째 레이어의 입력 채널 수 정의
# 첫 번째 컨볼루션 레이어
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(64) # 배치 정규화
# ResNet의 각 레이어 생성
        self.layer1 = self._create_layer(block, 64, layers[0], stride=1)
        self.layer2 = self._create_layer(block, 128, layers[1], stride=2)
        self.layer3 = self._create_layer(block, 256, layers[2], stride=2)
        self.layer4 = self._create_layer(block, 512, layers[3], stride=2)
# 평균 풀링 레이어
        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
# 최종 완전 연결 레이어
        self.fc = nn.Linear(512, num_classes)
# ResNet의 각 레이어를 생성하는 함수
def _create_layer(self, block, out_ch, num_layers, stride):
        layer_list = []
# 첫 번째 블록은 stride를 받을 수 있음
        layer_list.append(block(self.initial_channels, out_ch, stride))
        self.initial_channels = out_ch  # 다음 블록을 위해 채널 수 업데이트
# 나머지 블록들은 기본 stride를 사용
for _ in range(1, num_layers):
            layer_list.append(block(out_ch, out_ch))
return nn.Sequential(*layer_list)
def forward(self, x):
# 첫 번째 컨볼루션 + ReLU 활성화 함수
        x = F.relu(self.bn1(self.conv1(x)))
# 각 레이어를 순차적으로 통과
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
# 평균 풀링 및 텐서의 차원 축소
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
# 최종 완전 연결 레이어를 통해 클래스별 예측값 출력
        x = self.fc(x)
return x

# Custom ResNet-18 모델 생성 (각 레이어의 블록 수는 2개씩)
model = CustomResNet(Block, [2, 2, 2, 2], num_classes=10)

​.
데이터 변형해보기: 데이터 정렬과 병합인공지능을 위한 파이썬 라이브러리/인공지능을 위한 파이썬 라이브러리 - 4주차/데이터 변형해보기: 데이터 정렬과 병합제작:데이터 변형해보기: 데이터 정렬과 병합수업 목표판다스를 통해 데이터를 정렬하고 병합하는 방법에 대해 학습합니다.목차데이터 정렬데이터 병합요약데이터 정렬데이터를 정렬하는 것은 데이터 분석의 기본입니다. 판다스에서는 sort_values()와 sort_index()를 사용해 쉽게 데이터를 정렬할 수 있습니다.sort_values()를 사용한 값(Value) 기준 정렬특정 열의 값을 기준으로 데이터를 오름차순 또는 내림차순으로 정렬할 수 있습니다.Python복사import pandas as pd

# 예시 데이터프레임 생성
data = {
'이름': ['철수', '영희', '민수', '지수'],
'나이': [25, 30, 22, 35],
'직업': ['학생', '회사원', '학생', '프리랜서']
}
df = pd.DataFrame(data)
# '나이' 기준으로 오름차순 정렬
sorted_df = df.sort_values(by='나이')
print(sorted_df)

​결과:Plain Text복사   이름  나이       직업
2  민수   22      학생
0  철수   25      학생
1  영희   30    회사원
3  지수   35  프리랜서

​내림차순으로 정렬하려면 ascending=False를 지정하세요.Python복사# '나이' 기준으로 내림차순 정렬
sorted_df_desc = df.sort_values(by='나이', ascending=False)
print(sorted_df_desc)

​sort_values()를 사용한 여러 열 기준 정렬여러 열을 기준으로 정렬할 수 있습니다. 우선순위에 따라 첫 번째 열부터 정렬됩니다.Python복사# '직업'을 기준으로, 같은 직업 내에서 '나이' 오름차순 정렬
sorted_df_multi = df.sort_values(by=['직업', '나이'])
print(sorted_df_multi)

​sort_index()를 사용한 인덱스 기준 정렬인덱스를 기준으로 데이터를 정렬할 수 있습니다.Python복사# 인덱스 기준으로 정렬
sorted_index_df = df.sort_index()
print(sorted_index_df)

​내림차순으로 인덱스를 정렬하려면 ascending=False를 지정합니다.Python복사# 인덱스를 내림차순으로 정렬
sorted_index_df_desc = df.sort_index(ascending=False)
print(sorted_index_df_desc)

​데이터 병합데이터 분석 시 여러 개의 데이터를 병합하는 작업이 필요할 때가 많습니다. 판다스에서는 merge(), concat(), join()을 사용해 데이터를 병합할 수 있습니다.merge()를 사용한 데이터프레임 병합SQL의 JOIN과 유사하게 두 데이터프레임을 공통 열을 기준으로 병합합니다.Python복사# 예시 데이터프레임 생성
df1 = pd.DataFrame({
'이름': ['철수', '영희', '민수'],
'나이': [25, 30, 22]
})

df2 = pd.DataFrame({
'이름': ['철수', '영희', '지수'],
'직업': ['학생', '회사원', '프리랜서']
})
# '이름'을 기준으로 병합
merged_df = pd.merge(df1, df2, on='이름')
print(merged_df)

​결과:Plain Text복사   이름  나이    직업
0  철수   25   학생
1  영희   30  회사원

​다양한 merge() 방식inner (기본값): 공통된 데이터만 병합.outer: 공통되지 않은 데이터도 포함하여 병합, 없는 값은 NaN으로 채움.left: 왼쪽 데이터프레임 기준으로 병합.right: 오른쪽 데이터프레임 기준으로 병합.Python복사# outer join을 사용한 병합
merged_df_outer = pd.merge(df1, df2, on='이름', how='outer')
print(merged_df_outer)

​결과:Plain Text복사   이름   나이       직업
0  철수  25.0      학생
1  영희  30.0    회사원
2  민수  22.0      NaN
3  지수   NaN  프리랜서

​concat()을 사용한 데이터프레임 연결행(row) 또는 열(column) 단위로 데이터프레임을 연결할 수 있습니다.Python복사# 행 단위로 데이터프레임 연결
concat_df = pd.concat([df1, df2], axis=0)
print(concat_df)

​결과:Plain Text복사    이름    나이       직업
0   철수  25.0       NaN
1   영희  30.0       NaN
2   민수  22.0       NaN
0   철수   NaN      학생
1   영희   NaN    회사원
2   지수   NaN  프리랜서

​열 단위로 연결하고 싶다면 axis=1을 지정하세요.Python복사# 열 단위로 데이터프레임 연결
concat_df_axis1 = pd.concat([df1, df2], axis=1)
print(concat_df_axis1)

​join()을 사용한 데이터프레임 병합인덱스를 기준으로 데이터프레임을 병합할 때 사용합니다.Python복사# 예시 데이터프레임 생성
df3 = pd.DataFrame({
'직업': ['학생', '회사원', '프리랜서'],
'연봉': [2000, 3000, 4000]
}, index=['철수', '영희', '지수'])
# 인덱스를 기준으로 병합
joined_df = df1.set_index('이름').join(df3)
print(joined_df)

​결과:Plain Text복사      나이       직업     연봉
이름
철수     25      학생  2000.0
영희     30    회사원  3000.0
민수     22      NaN     NaN

​요약요약데이터 정렬은 sort_values()로 특정 열 기준으로, sort_index()로 인덱스 기준으로 정렬할 수 있습니다.데이터 병합은 merge()를 사용해 공통 열을 기준으로 병합하거나, concat()을 사용해 행 또는 열 단위로 연결할 수 있습니다.인덱스를 기준으로 병합할 때는 join()을 사용할 수 있으며, 다양한 병합 방식(inner, outer, left, right)이 지원됩니다.이제 데이터를 효율적으로 정렬하고, 필요에 따라 병합하여 더 깊이 있는 분석을 진행해보세요! ​
[스파르타코딩클럽] 10강. 지도학습 : 분류모델 - SVM[SCC] 바닥부터 시작하는 머신러닝/[스파르타코딩클럽] 바닥부터 시작하는 머신러닝 - 3주차/[스파르타코딩클럽] 10강. 지도학습 : 분류모델 - SVM제작:[스파르타코딩클럽] 10강. 지도학습 : 분류모델 - SVM[수업 목표]SVM(Support Vector Machine)에 대한 개념을 배우고, 데이터를 이용해 실습해 봅니다[목차]01. SVM 개념02. SVM 실습💡
 
 01. SVM 개념✔️SVM이 무엇인지 알아봅시다1) SVM SVM이란?서포트 벡터 머신(SVM)은 분류와 회귀 분석에 사용되는 강력한 지도학습 모델데이터를 분류하기 위해 결정 경계(결정 초평면, hyperplane)를 찾아 분류합니다.초평면은 두 클래스 사이의 최대 마진을 보장하는 방식으로 선택합니다.ALT마진 : 두 클래스 간의 가장 가까운 데이터 포인트 사이의 거리마진 : 두 클래스 간의 가장 가까운 데이터 포인트 사이의 거리﻿
서포트 벡터 : 결정 초평면에 가장 가까이 위치한 데이터 포인트 - 결정 초평면을 정의합니다서포트 벡터 : 결정 초평면에 가장 가까이 위치한 데이터 포인트 - 결정 초평면을 정의합니다﻿
커널 함수 : 데이터를 더 높은 차원으로 매핑하여 선형적으로 분리 할 수 없는 데이터를 분리하게 합니다. 커널 함수 : 데이터를 더 높은 차원으로 매핑하여 선형적으로 분리 할 수 없는 데이터를 분리하게 합니다. ﻿​ SVM의 목적SVM의 목표는 마진을 최대화하면서 결정 초평면을 찾아 데이터 포인트를 정확하게 분류하는 것입니다. 이는 일반화 성능을 높이는 데 도움을 줍니다.w⋅x−b=0   - b = 0 w⋅x−b=0여기서 w는 가중치 벡터, x는 입력 벡터, b는 절편입니다.\)는 가중치 벡터, \(\)는 입력 벡터, \(b\)는 절편입니다.}여기서 w는 가중치 벡터, x는 입력 벡터, b는 절편입니다.﻿​02. SVM 실습✔️Scikit-learn의 유방암데이터와 Seaborn의 타이타닉 데이터로 SVM 실습을 진행합니다1) 유방암 데이터 데이터 로드 및 전처리유방암 데이터 로드 및 전처리 {5px}유방암 데이터 로드 및 전처리 ﻿​Python복사import numpy as np
import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# 데이터 로드
data = load_breast_cancer()
X = data.data
y = data.target

# 데이터 분할
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# 데이터 스케일링
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
​sklearn.datasets.load_breast_cancer: 유방암 데이터셋 로드return_X_y=False: 데이터와 타겟을 함께 반환할지 여부. 기본값은 False입니다._X_y=False: 데이터와 타겟을 함께 반환할지 여부. 기본값은 False입니다.﻿​sklearn.model_selection.train_test_split: 데이터를 훈련 세트/ 테스트 세트로 분할test_size=0.2: 테스트 세트의 비율을 0.2로 설정합니다._size=0.2: 테스트 세트의 비율을 0.2로 설정합니다.﻿​random_state=42: 랜덤 시드 값으로, 데이터 분할의 재현성을 위해 사용됩니다._state=42: 랜덤 시드 값으로, 데이터 분할의 재현성을 위해 사용됩니다.﻿​sklearn.preprocessing.StandardScaler: 데이터의 평균을 0, 분산을 1로 스케일링fit_transform(X_train): 훈련 세트를 스케일링하고 변환합니다._transform(X_train): 훈련 세트를 스케일링하고 변환합니다.﻿​transform(X_test): 테스트 세트를 변환합니다.(X_test): 테스트 세트를 변환합니다.﻿​ 모델 학습모델 학습 {5px}모델 학습 ﻿​Python복사from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# 모델 생성 및 학습
model = SVC(kernel='linear')
model.fit(X_train, y_train)
# 예측
y_pred = model.predict(X_test)
# 평가
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
print(f"Classification Report:")
print(f"Confusion Matrix:")
​sklearn.svm.SVC: 서포트 벡터 머신 분류 모델 생성kernel=’linear’: 선형 커널을 사용하여 SVM을 학습합니다.=’linear’: 선형 커널을 사용하여 SVM을 학습합니다.﻿​fit(X_train, y_train): 모델을 훈련 세트에 맞추어 학습시킵니다(X_train, y_train): 모델을 훈련 세트에 맞추어 학습시킵니다﻿​predict(X_test): 테스트 세트에 대해 예측을 수행합니다.(X_test): 테스트 세트에 대해 예측을 수행합니다.﻿​sklearn.metrics.accuracy_score: 정확도 계산accuracy_score(y_test, y_pred): 실제 값과 예측 값을 비교하여 정확도를 반환합니다._score(y_test, y_pred): 실제 값과 예측 값을 비교하여 정확도를 반환합니다.﻿​sklearn.metrics.classification_report: 분류 보고서 생성classification_report(y_test, y_pred): 정확도, 정밀도, 재현율 등의 메트릭을 포함한 보고서를 출력합니다._report(y_test, y_pred): 정확도, 정밀도, 재현율 등의 메트릭을 포함한 보고서를 출력합니다.﻿​sklearn.metrics.confusion_matrix: 혼동 행렬 생성confusion_matrix(y_test, y_pred): 실제 값과 예측 값의 혼동 행렬을 반환합니다._matrix(y_test, y_pred): 실제 값과 예측 값의 혼동 행렬을 반환합니다.﻿​2) 타이타닉 데이터 데이터 로드 및 전처리타이타닉 데이터 로드 및 전처리 {5px}타이타닉 데이터 로드 및 전처리 ﻿​Python복사import seaborn as sns

# 데이터 로드
titanic = sns.load_dataset('titanic')
# 필요한 열 선택 및 결측값 처리
titanic = titanic[['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']].dropna()
# 성별과 탑승한 곳 인코딩
titanic['sex'] = titanic['sex'].map({'male': 0, 'female': 1})
titanic['embarked'] = titanic['embarked'].map({'C': 0, 'Q': 1, 'S': 2})
# 특성과 타겟 분리
X = titanic.drop('survived', axis=1)
y = titanic['survived']
# 데이터 분할
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# 데이터 스케일링
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
​seaborn.load_dataset: seaborn의 내장 데이터셋 로드’titanic’: 타이타닉 데이터셋을 로드합니다.’titanic’: 타이타닉 데이터셋을 로드합니다.﻿​ pandas.DataFrame.dropna: 결측값이 있는 행 제거pandas.DataFrame.map: 데이터 값을 다른 값으로 매핑’male’: 0, ’female’: 1: 성별을 숫자로 매핑합니다.: 성별을 숫자로 매핑합니다.}’male’: 0, ’female’: 1: 성별을 숫자로 매핑합니다.﻿​’C’: 0, ’Q’: 1, ’S’: 2: 탑승한 곳을 숫자로 매핑합니다.: 탑승한 곳을 숫자로 매핑합니다.}’C’: 0, ’Q’: 1, ’S’: 2: 탑승한 곳을 숫자로 매핑합니다.﻿​ 모델 학습모델 학습 {5px}모델 학습 ﻿​Python복사# 모델 생성 및 학습
model = SVC(kernel='linear')
model.fit(X_train, y_train)
# 예측
y_pred = model.predict(X_test)
# 평가
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
print(f"Classification Report:")
print(f"Confusion Matrix:")
​.
[스파르타코딩클럽] 10. 이미지 처리 모델[SCC] 기초가 탄탄한 딥러닝/[스파르타코딩클럽] 기초가 탄탄한 딥러닝 - 4주차/[스파르타코딩클럽] 10. 이미지 처리 모델제작:[스파르타코딩클럽] 10. 이미지 처리 모델[수업 목표]이미지 처리 모델에 대해 배워봅시다Pytorch로 간단한 YOLO 모델 구현 실습을 진행해 봅시다[목차]01. CNN기반 이미지 분류💡
 
 01. CNN기반 이미지 분류✔️CNN기반의 이미지분류 아키텍쳐 소개와 YOLO, 이미지 세그멘테이션에 대해 배워봅시다1) ResNet 등 주요 CNN 아키텍쳐 소개 ResNet (Residual Network)ResNet은 매우 깊은 신경망을 학습할 수 있도록 설계된 아키텍처입니다.잔차 연결(Residual Connection)을 도입하여, 기울기 소실 문제를 해결합니다.ResNet-50, ResNet-101, ResNet-152 등의 변형이 있습니다. VGGVGG는 작은 3x3 필터를 사용하여 깊이를 증가시킨 아키텍처입니다.VGG16과 VGG19가 대표적인 모델입니다.단순하고 규칙적인 구조로 인해, 다양한 변형이 가능합니다. InceptionInception은 다양한 크기의 필터를 병렬로 적용하여, 여러 수준의 특징을 추출합니다.Inception 모듈을 사용하여, 네트워크의 깊이와 너비를 동시에 확장합니다.GoogLeNet(Inception v1), Inception v2, Inception v3 등이 있습니다.2) 객체 탐지(YOLO) YOLO(You Only Look Once) YOLO(You Only Look Once)는 객체 탐지(Object Detection) 모델로, 이미지에서 객체의 위치와 클래스를 동시에 예측합니다.YOLO는 이미지 전체를 한 번에 처리하여, 빠르고 정확한 객체 탐지를 수행합니다.ALT YOLO의 개념YOLO는 이미지를 SxS 그리드로 나누고, 각 그리드 셀에서 객체의 존재 여부를 예측합니다.각 그리드 셀은 B개의 바운딩 박스와 C개의 클래스 확률을 출력합니다. YOLO의 동작 원리입력 이미지를 CNN을 통해 특징 맵으로 변환합니다.특징 맵을 SxS 그리드로 나누고, 각 그리드 셀에서 바운딩 박스와 클래스 확률을 예측합니다.예측된 바운딩 박스와 클래스 확률을 바탕으로, 객체의 위치와 클래스를 결정합니다.3) 이미지 세그멘테이션 이미지 세그멘테이션 기법과 응용이미지 세그멘테이션(Image Segmentation)은 이미지의 각 픽셀을 클래스 레이블로 분류하는 작업입니다. 이미지 세그멘테이션은 주로 시맨틱 세그멘테이션과 인스턴스 세그멘테이션 두가지로 나뉩니다ALT시맨틱 세그멘테이션 (Semantic Segmentation)
이미지의 각 픽셀을 클래스 레이블로 분류합니다이미지의 각 픽셀을 클래스 레이블로 분류합니다﻿
인스턴스 세그멘테이션 (Instance Segmentation)
시맨틱 세그멘테이션과 달리, 같은 클래스 내에서도 개별 객체를 구분합니다.시맨틱 세그멘테이션과 달리, 같은 클래스 내에서도 개별 객체를 구분합니다.﻿​ 주요 세그멘테이션 모델FCN (Fully Convolutional Network): 모든 레이어를 합성곱 레이어로 구성하여, 픽셀 단위의 예측을 수행합니다.U-Net: U자형 구조를 가지며, 인코더-디코더 아키텍처를 사용하여 세그멘테이션을 수행합니다.Mask R-CNN: 객체 탐지와 인스턴스 세그멘테이션을 동시에 수행하는 모델입니다..
데이터 변형해보기: 그룹화 및 집계, 피벗테이블인공지능을 위한 파이썬 라이브러리/인공지능을 위한 파이썬 라이브러리 - 4주차/데이터 변형해보기: 그룹화 및 집계, 피벗테이블제작:데이터 변형해보기: 그룹화 및 집계, 피벗테이블수업 목표판다스에서 그룹화 및 피벗테이블을 생성하는 방법에 대해서 알아봅니다.목차데이터 그룹화 및 집계 피벗테이블 사용하기요약데이터 그룹화 및 집계 📚데이터 그룹화는 데이터를 특정 기준에 따라 그룹으로 나누고, 집계는 각 그룹에 대해 요약 통계를 계산하는 과정입니다. 판다스에서는 groupby()와 집계 함수들을 사용해 이를 손쉽게 수행할 수 있습니다.groupby()를 사용한 그룹화groupby() 함수는 데이터를 특정 열을 기준으로 그룹화합니다.Python복사import pandas as pd

# 예시 데이터프레임 생성
data = {
'이름': ['철수', '영희', '민수', '지수', '철수', '영희'],
'과목': ['수학', '수학', '과학', '과학', '영어', '영어'],
'점수': [90, 85, 95, 80, 75, 88]
}
df = pd.DataFrame(data)
# '이름'을 기준으로 그룹화
grouped = df.groupby('이름')

​집계 함수 사용하기그룹화한 데이터에 대해 다양한 집계 함수(mean, sum, count 등)를 사용할 수 있습니다.Python복사# 각 학생의 평균 점수 계산
mean_scores = grouped['점수'].mean()
print(mean_scores)

​결과:Plain Text복사이름
민수    95.0
영희    86.5
철수    82.5
지수    80.0
Name: 점수, dtype: float64

​여러 집계 함수를 동시에 사용할 수도 있습니다.Python복사# 각 학생의 점수 합계와 평균 계산
agg_scores = grouped['점수'].agg(['sum', 'mean'])
print(agg_scores)

​결과:Plain Text복사      sum  mean
이름
민수    95  95.0
영희   173  86.5
철수   165  82.5
지수    80  80.0

​여러 열을 기준으로 그룹화하기여러 열을 기준으로 그룹화할 수도 있습니다.Python복사# '이름'과 '과목'을 기준으로 그룹화하여 점수 합계 계산
grouped_multi = df.groupby(['이름', '과목'])['점수'].sum()
print(grouped_multi)

​결과:Plain Text복사이름  과목
민수  과학    95
영희  수학    85
      영어    88
철수  수학    90
      영어    75
지수  과학    80
Name: 점수, dtype: int64

​피벗테이블 사용하기📚피벗테이블은 데이터를 요약하는 강력한 도구입니다. 엑셀의 피벗테이블과 유사하며, 특정 기준에 따라 데이터를 재구조화할 수 있습니다.pivot_table() 기본 사용법pivot_table() 함수는 데이터를 요약하고, 특정 기준에 따라 재구조화합니다.Python복사# 피벗테이블 생성: '이름'을 인덱스로, '과목'을 컬럼으로 하여 점수의 평균 계산
pivot = pd.pivot_table(df, index='이름', columns='과목', values='점수', aggfunc='mean')
print(pivot)

​결과:Plain Text복사과목   과학    수학    영어
이름
민수  95.0   NaN   NaN
영희   NaN  85.0  88.0
철수   NaN  90.0  75.0
지수  80.0   NaN   NaN

​여러 집계 함수 사용하기aggfunc에 여러 집계 함수를 지정하면, 다양한 요약 통계를 얻을 수 있습니다.Python복사# 점수의 합계와 평균을 계산하는 피벗테이블 생성
pivot_multi = pd.pivot_table(df, index='이름', columns='과목', values='점수', aggfunc=['sum', 'mean'])
print(pivot_multi)

​결과:Plain Text복사        sum          mean
과목     과학   수학   영어   과학   수학   영어
이름
민수   95.0   NaN   NaN  95.0   NaN   NaN
영희    NaN  85.0  88.0   NaN  85.0  88.0
철수    NaN  90.0  75.0   NaN  90.0  75.0
지수   80.0   NaN   NaN  80.0   NaN   NaN

​margins 옵션을 사용해 전체 합계 추가하기margins=True 옵션을 사용하면, 각 행과 열의 합계가 추가된 피벗테이블을 생성할 수 있습니다.Python복사# 각 이름별, 과목별 총합을 포함한 피벗테이블 생성
pivot_with_totals = pd.pivot_table(df, index='이름', columns='과목', values='점수', aggfunc='sum', margins=True)
print(pivot_with_totals)

​결과:Plain Text복사과목      과학    수학    영어     All
이름
민수    95.0   NaN   NaN    95.0
영희     NaN  85.0  88.0   173.0
철수     NaN  90.0  75.0   165.0
지수    80.0   NaN   NaN    80.0
All   175.0  175.0  163.0  513.0

​요약📚요약그룹화는 groupby()를 사용해 데이터를 특정 기준에 따라 그룹으로 나누고, 집계 함수를 사용해 각 그룹에 대한 통계를 계산할 수 있습니다.피벗테이블은 pivot_table()을 사용해 데이터를 요약하고 재구조화할 수 있으며, 여러 집계 함수와 margins 옵션을 통해 전체 합계를 포함할 수 있습니다.그룹화와 피벗테이블은 데이터 분석에서 데이터를 효율적으로 요약하고 패턴을 발견하는 데 매우 유용합니다.이제 데이터를 그룹화하고 피벗테이블을 사용하여 분석을 한 단계 더 깊이 있게 진행해보세요! ​익명9월 25일텍스트 누락 된 부분 추가했습니다
[스파르타코딩클럽] 11강. 지도학습 : 분류모델 - KNN[SCC] 바닥부터 시작하는 머신러닝/[스파르타코딩클럽] 바닥부터 시작하는 머신러닝 - 3주차/[스파르타코딩클럽] 11강. 지도학습 : 분류모델 - KNN제작:[스파르타코딩클럽] 11강. 지도학습 : 분류모델 - KNN[수업 목표]KNN(K-Nearest Neighbors)에 대한 개념을 배우고, 데이터를 이용해 실습해 봅니다[목차]01. KNN 개념02. KNN 실습💡
 
 01. KNN 개념✔️KNN(K 최근접 이웃 , K-Nearest Neighbors)이 무엇인지 알아봅시다1) KNN KNN이란?KNN 알고리즘은 분류와 회귀 분석에 사용되는 비모수적 방법 입니다.새로운 데이터 포인트를 기존 데이터 포인트 중 가장 가까운 K개의 이웃과 비교하여 분류합니다데이터 포인트의 특성을 기준으로 거리 계산을 통해 가장 가까운 이웃을 찾습니다.ALT거리 측정 : KNN 알고리즘에서 가장 중요한 요소입니다. 일반적으로 유클리드 거리가 사용됩니다.거리 측정 : KNN 알고리즘에서 가장 중요한 요소입니다. 일반적으로 유클리드 거리가 사용됩니다.﻿
K값 : K는 결정 경계의 매끄러움에 영향을 미치는 하이퍼파라미터입니다. 값 : K는 결정 경계의 매끄러움에 영향을 미치는 하이퍼파라미터입니다. ﻿
 작은 K값은 더 노이즈에 민감하고,큰 K값은 더 매끄러운 경계를 만듭니다. 작은 K값은 더 노이즈에 민감하고,큰 K값은 더 매끄러운 경계를 만듭니다.﻿
다수결 투표 : K개의 가장 가까운 이웃의 클래스중 가장 빈번한 클래스로 새로운 데이터 포인트를 분류합니다다수결 투표 : K개의 가장 가까운 이웃의 클래스중 가장 빈번한 클래스로 새로운 데이터 포인트를 분류합니다﻿​ KNN의 목적KNN의 목표는 학습 데이터를 기반으로 새로운 데이터 포인트의 클래스를 예측하는 것입니다이는 분류 문제에서 주로 사용되며 다양한 응용 분야에 활용될 수 있습니다02. KNN 실습✔️Scikit-learn의 유방암데이터와 Seaborn의 타이타닉 데이터로 KNN 실습을 진행합니다1) 유방암 데이터 데이터 로드 및 전처리유방암 데이터 로드 및 전처리 {5px}유방암 데이터 로드 및 전처리 ﻿​Python복사import numpy as np
import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# 데이터 로드
data = load_breast_cancer()
X = data.data
y = data.target

# 데이터 분할
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# 데이터 스케일링
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
​sklearn.datasets.load_breast_cancer: 유방암 데이터셋 로드return_X_y=False: 데이터와 타겟을 함께 반환할지 여부. 기본값은 False입니다._X_y=False: 데이터와 타겟을 함께 반환할지 여부. 기본값은 False입니다.﻿​sklearn.model_selection.train_test_split: 데이터를 훈련 세트/ 테스트 세트로 분할test_size=0.2: 테스트 세트의 비율을 0.2로 설정합니다._size=0.2: 테스트 세트의 비율을 0.2로 설정합니다.﻿​random_state=42: 랜덤 시드 값으로, 데이터 분할의 재현성을 위해 사용됩니다._state=42: 랜덤 시드 값으로, 데이터 분할의 재현성을 위해 사용됩니다.﻿​sklearn.preprocessing.StandardScaler: 데이터의 평균을 0, 분산을 1로 스케일링fit_transform(X_train): 훈련 세트를 스케일링하고 변환합니다._transform(X_train): 훈련 세트를 스케일링하고 변환합니다.﻿​transform(X_test): 테스트 세트를 변환합니다.(X_test): 테스트 세트를 변환합니다.﻿​ 모델 학습모델 학습 {5px}모델 학습 ﻿​Python복사from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# 모델 생성 및 학습
model = KNeighborsClassifier(n_neighbors=5)
model.fit(X_train, y_train)
# 예측
y_pred = model.predict(X_test)
# 평가
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
print(f"Classification Report:")
print(f"Confusion Matrix:")
​sklearn.neighbors.KNeighborsClassifier: KNN 모델 생성n_neighbors=5: 사용할 이웃의 수(K)를 설정합니다._neighbors=5: 사용할 이웃의 수(K)를 설정합니다.﻿​fit(X_train, y_train): 모델을 훈련 세트에 맞추어 학습시킵니다(X_train, y_train): 모델을 훈련 세트에 맞추어 학습시킵니다﻿​predict(X_test): 테스트 세트에 대해 예측을 수행합니다.(X_test): 테스트 세트에 대해 예측을 수행합니다.﻿​sklearn.metrics.accuracy_score: 정확도 계산accuracy_score(y_test, y_pred): 실제 값과 예측 값을 비교하여 정확도를 반환합니다._score(y_test, y_pred): 실제 값과 예측 값을 비교하여 정확도를 반환합니다.﻿​sklearn.metrics.classification_report: 분류 보고서 생성classification_report(y_test, y_pred): 정확도, 정밀도, 재현율 등의 메트릭을 포함한 보고서를 출력합니다._report(y_test, y_pred): 정확도, 정밀도, 재현율 등의 메트릭을 포함한 보고서를 출력합니다.﻿​sklearn.metrics.confusion_matrix: 혼동 행렬 생성confusion_matrix(y_test, y_pred): 실제 값과 예측 값의 혼동 행렬을 반환합니다._matrix(y_test, y_pred): 실제 값과 예측 값의 혼동 행렬을 반환합니다.﻿​2) 타이타닉 데이터 데이터 로드 및 전처리타이타닉 데이터 로드 및 전처리 {5px}타이타닉 데이터 로드 및 전처리 ﻿​Python복사import seaborn as sns

# 데이터 로드
titanic = sns.load_dataset('titanic')
# 필요한 열 선택 및 결측값 처리
titanic = titanic[['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']].dropna()
# 성별과 탑승한 곳 인코딩
titanic['sex'] = titanic['sex'].map({'male': 0, 'female': 1})
titanic['embarked'] = titanic['embarked'].map({'C': 0, 'Q': 1, 'S': 2})
# 특성과 타겟 분리
X = titanic.drop('survived', axis=1)
y = titanic['survived']
# 데이터 분할
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# 데이터 스케일링
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
​seaborn.load_dataset: seaborn의 내장 데이터셋 로드’titanic’: 타이타닉 데이터셋을 로드합니다.’titanic’: 타이타닉 데이터셋을 로드합니다.﻿​pandas.DataFrame.dropna: 결측값이 있는 행 제거pandas.DataFrame.map: 데이터 값을 다른 값으로 매핑’male’: 0, ’female’: 1: 성별을 숫자로 매핑합니다.: 성별을 숫자로 매핑합니다.}’male’: 0, ’female’: 1: 성별을 숫자로 매핑합니다.﻿​’C’: 0, ’Q’: 1, ’S’: 2: 탑승한 곳을 숫자로 매핑합니다.: 탑승한 곳을 숫자로 매핑합니다.}’C’: 0, ’Q’: 1, ’S’: 2: 탑승한 곳을 숫자로 매핑합니다.﻿​ 모델 학습모델 학습 {5px}모델 학습 ﻿​Python복사# 모델 생성 및 학습
model = KNeighborsClassifier(n_neighbors=5)
model.fit(X_train, y_train)
# 예측
y_pred = model.predict(X_test)
# 평가
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
print(f"Classification Report:")
print(f"Confusion Matrix:")
​.
[스파르타코딩클럽] 11. 오토인코더[SCC] 기초가 탄탄한 딥러닝/[스파르타코딩클럽] 기초가 탄탄한 딥러닝 - 5주차/[스파르타코딩클럽] 11. 오토인코더제작:[스파르타코딩클럽] 11. 오토인코더[수업 목표]오토인코더의 개념에 대해 알아봅시다[목차]01. 오토인코더02. 오토인코더의 구조💡
 
 01. 오토인코더1) 오토인코더의 기본 개념 오토인코더란?오토인코더(Autoencoder)는 입력 데이터를 압축하고, 이를 다시 복원하는 과정을 통해 데이터를 효율적으로 표현하는 비지도 학습 모델입니다. 주로 차원 축소, 잡음 제거, 생성 모델 등 다양한 분야에서 활용됩니다.2) 동작 원리 인코더(Encoder)인코더는 입력 데이터를 저차원(latent space) 표현으로 변환하는 역할을 합니다. 인코더의 목적은 중요한 특징을 추출하고, 입력 데이터를 압축하는 것입니다. 디코더(Decoder)디코더는 인코더에 의해 생성된 저차원 표현을 다시 원래의 고차원 데이터로 복원하는 역할을 합니다. 디코더의 목적은 입력 데이터를 최대한 원본과 가깝게 복원하는 것입니다. 잠재 공간(Latent Space)잠재 공간은 인코더에 의해 생성된 저차원 표현 공간입니다. 이 공간에서는 입력 데이터의 중요한 특징만을 포함하고 있으며, 디코더는 이를 이용해 원래 데이터를 복원합니다.02. 오토인코더의 구조✔️오토인코더의 다양한 구조 및 종류에 대해 알아봅시다1) 오토인코더의 종류 기본 오토인코더ALT 변형된 오토인코더오토인코더는 다양한 변형 모델들이 존재합니다. 대표적인 예로는 다음과 같습니다:딥 오토인코더(Deep Autoencoder): 더 깊은 인코더와 디코더 구조를 가지며, 복잡한 데이터 표현을 학습합니다.변분 오토인코더(Variational Autoencoder, VAE): 확률적 잠재 공간을 사용하여 데이터의 분포를 학습합니다.희소 오토인코더(Sparse Autoencoder): 잠재 공간의 표현을 희소하게 유지하여 중요한 특징만을 학습합니다.잡음 제거 오토인코더(Denoising Autoencoder): 입력 데이터에 잡음을 추가하고, 이를 제거하는 학습을 통해 데이터 복원 능력을 향상시킵니다..
데이터 전처리: 결측치 탐지와 다양한 처리 방법인공지능을 위한 파이썬 라이브러리/인공지능을 위한 파이썬 라이브러리 - 5주차/데이터 전처리: 결측치 탐지와 다양한 처리 방법제작:데이터 전처리: 결측치 탐지와 다양한 처리 방법수업 목표불러온 데이터에서 결측치를 찾고 대체하는 방법에 대해서 학습합니다.목차결측치 탐지결측치 처리 방법고급 결측치 처리 방법요약결측치 탐지📚데이터 분석에서 결측치(Missing Values)는 흔히 발생하며, 이를 올바르게 처리하는 것이 중요합니다. 판다스는 결측치를 쉽게 탐지하고 처리할 수 있는 다양한 기능을 제공합니다.isna()와 isnull()로 결측치 탐지isna()와 isnull() 함수는 데이터프레임의 각 요소가 결측치인지 여부를 확인합니다. 두 함수는 동일한 기능을 합니다.Python복사import pandas as pd

# 예시 데이터프레임 생성
data = {
'이름': ['철수', '영희', '민수', '지수'],
'나이': [25, 30, None, 35],
'직업': ['학생', '회사원', '학생', None]
}
df = pd.DataFrame(data)
# 결측치 여부 확인
print(df.isna())

​결과:Plain Text복사     이름     나이    직업
0  False  False  False
1  False  False  False
2  False   True  False
3  False  False   True

​sum()을 사용한 결측치 개수 확인결측치의 총 개수를 확인하려면 isna() 또는 isnull()과 sum()을 조합해 사용합니다.Python복사# 각 열별 결측치 개수 확인
print(df.isna().sum())

​결과:Plain Text복사이름    0
나이    1
직업    1
dtype: int64

​결측치 처리 방법📚결측치를 적절하게 처리하는 방법은 데이터의 특성과 분석 목적에 따라 달라집니다. 여기서는 대표적인 처리 방법들을 소개합니다.결측치 제거결측치가 포함된 행이나 열을 삭제할 수 있습니다. dropna() 함수를 사용합니다.Python복사# 결측치가 있는 행 제거
df_dropped_rows = df.dropna()
print(df_dropped_rows)

​결과:Plain Text복사   이름   나이    직업
0  철수  25.0   학생
1  영희  30.0  회사원

​결측치가 있는 열을 제거하려면 axis=1을 지정합니다.Python복사# 결측치가 있는 열 제거
df_dropped_columns = df.dropna(axis=1)
print(df_dropped_columns)

​결과:Plain Text복사   이름
0  철수
1  영희
2  민수
3  지수

​결측치 대체(채우기)결측치를 특정 값으로 대체(채우기)할 수 있습니다. fillna() 함수를 사용합니다.Python복사# 결측치를 '없음'으로 대체
df_filled = df.fillna('없음')
print(df_filled)

​결과:Plain Text복사   이름    나이    직업
0  철수   25.0    학생
1  영희   30.0  회사원
2  민수  없음    학생
3  지수   35.0   없음

​평균, 중앙값, 최빈값 등으로 결측치를 채울 수 있습니다.Python복사# '나이' 열의 결측치를 평균값으로 대체
df['나이'] = df['나이'].fillna(df['나이'].mean())
print(df)

​결과:Plain Text복사   이름    나이    직업
0  철수  25.0    학생
1  영희  30.0  회사원
2  민수  30.0    학생
3  지수  35.0   없음

​결측치 보간(Interpolation)결측치를 주변 값들을 기반으로 보간할 수 있습니다. interpolate() 함수를 사용합니다. 특히 시간 데이터를 다룰 때 유용합니다.Python복사# 예시 데이터프레임 생성
data = {
'날짜': pd.date_range('2023-01-01', periods=5),
'온도': [20, 22, None, 24, 25]
}
df2 = pd.DataFrame(data)
# 선형 보간법으로 결측치 채우기
df2['온도'] = df2['온도'].interpolate()
print(df2)

​결과:Plain Text복사         날짜    온도
0 2023-01-01  20.0
1 2023-01-02  22.0
2 2023-01-03  23.0
3 2023-01-04  24.0
4 2023-01-05  25.0

​고급 결측치 처리 방법특정 조건을 기반으로 결측치 처리특정 조건을 기반으로 결측치를 처리할 수도 있습니다. 예를 들어, 다른 열의 값을 기준으로 결측치를 채우는 방법입니다.Python복사# '직업'이 '학생'인 경우 '나이'를 20으로 채우기
df.loc[(df['직업'] == '학생') & (df['나이'].isna()), '나이'] = 20
print(df)

​결과:Plain Text복사   이름    나이    직업
0  철수  25.0    학생
1  영희  30.0  회사원
2  민수  20.0    학생
3  지수  35.0   없음

​apply()를 사용한 사용자 정의 함수 적용apply() 함수를 사용해 결측치를 처리하는 사용자 정의 함수를 적용할 수 있습니다.Python복사# 예시: '나이'가 결측치일 경우, 기본값으로 18을 채우는 함수
def fill_missing_age(x):
if pd.isna(x):
return 18
return x

df['나이'] = df['나이'].apply(fill_missing_age)
print(df)

​결과:Plain Text복사   이름    나이    직업
0  철수  25.0    학생
1  영희  30.0  회사원
2  민수  20.0    학생
3  지수  35.0   없음

​요약📚요약결측치 탐지는 isna()와 isnull()을 사용해 수행하며, sum()을 통해 열별 결측치 개수를 확인할 수 있습니다.결측치 처리 방법으로는 행 또는 열 삭제(dropna()), 특정 값으로 채우기(fillna()), 보간법(interpolate()), 조건 기반 처리 및 사용자 정의 함수 적용 등이 있습니다.데이터의 특성과 분석 목적에 따라 적절한 결측치 처리 방법을 선택하는 것이 중요합니다.이제 결측치를 효과적으로 처리하여 더 정확한 분석 결과를 얻어보세요! ​
[스파르타코딩클럽] 12강. 지도학습 : 분류모델 - 나이브베이즈[SCC] 바닥부터 시작하는 머신러닝/[스파르타코딩클럽] 바닥부터 시작하는 머신러닝 - 3주차/[스파르타코딩클럽] 12강. 지도학습 : 분류모델 - 나이브베이즈제작:[스파르타코딩클럽] 12강. 지도학습 : 분류모델 - 나이브베이즈[수업 목표]분류모델중 나이브베이즈에 대한 개념을 배우고, 데이터를 이용해 실습해 봅니다[목차]01. 나이브베이즈 개념02. 나이브베이즈 실습💡
 
 01. 나이브베이즈 개념✔️1) 나이브베이즈 나이브베이즈란?나이브베이즈(Naive Bayes) 분류기는 베이즈 정리를 기반으로 하는 통계적 분류 기법입니다. 나이브라는 이름이 붙은 이유는 각 특징(feature)이 독립적이라고 가정하기 때문입니다.주로 텍스트 분류 문제에서 널리 사용됩니다. 베이즈정리P(A∣B)=P(B∣A)⋅P(A)P(B)P(A|B) = {P(B)}P(A∣B)=P(B)P(B∣A)⋅P(A)​P(A∣B): B가 주어졌을 때 A의 확률 (사후 확률)P(B∣A): A가 주어졌을 때 B의 확률 (우도)P(A): A의 사전 확률P(B): B의 사전 확률 나이브베이즈의 종류가우시안 나이브베이즈: 특징들이 연속적이고 정규 분포를 따른다고 가정합니다.베르누이 나이브베이즈: 특징들이 이진수(0 또는 1)로 표현되는 경우 사용합니다.멀티노미얼 나이브베이즈: 특징들이 다항 분포를 따르는 경우 사용합니다. 나이브베이즈의 목적나이브베이즈의 목표는 주어진 데이터 포인트가 특정 클래스에 속할 확률을 계산하여 분류하는 것입니다. 이 모델은 단순하고 계산이 효율적이며, 텍스트 분류와 같은 문제에서 좋은 성능을 발휘합니다.02. 나이브베이즈 실습✔️Scikit-learn의 유방암데이터와 Seaborn의 타이타닉 데이터로 나이브베이즈 실습을 진행합니다1) 유방암 데이터 데이터 로드 및 전처리유방암 데이터 로드 및 전처리 {5px}유방암 데이터 로드 및 전처리 ﻿​Python복사import numpy as np
import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# 데이터 로드
data = load_breast_cancer()
X = data.data
y = data.target

# 데이터 분할
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# 데이터 스케일링
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
​sklearn.datasets.load_breast_cancer: 유방암 데이터셋 로드return_X_y=False: 데이터와 타겟을 함께 반환할지 여부. 기본값은 False입니다._X_y=False: 데이터와 타겟을 함께 반환할지 여부. 기본값은 False입니다.﻿​sklearn.model_selection.train_test_split: 데이터를 훈련 세트/ 테스트 세트로 분할test_size=0.2: 테스트 세트의 비율을 0.2로 설정합니다._size=0.2: 테스트 세트의 비율을 0.2로 설정합니다.﻿​random_state=42: 랜덤 시드 값으로, 데이터 분할의 재현성을 위해 사용됩니다._state=42: 랜덤 시드 값으로, 데이터 분할의 재현성을 위해 사용됩니다.﻿​sklearn.preprocessing.StandardScaler: 데이터의 평균을 0, 분산을 1로 스케일링fit_transform(X_train): 훈련 세트를 스케일링하고 변환합니다._transform(X_train): 훈련 세트를 스케일링하고 변환합니다.﻿​transform(X_test): 테스트 세트를 변환합니다.(X_test): 테스트 세트를 변환합니다.﻿​ 모델 학습모델 학습 {5px}모델 학습 ﻿​Python복사from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# 모델 생성 및 학습
model = GaussianNB()
model.fit(X_train, y_train)
# 예측
y_pred = model.predict(X_test)
# 평가
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
print(f"Classification Report:")
print(f"Confusion Matrix:")
​sklearn.naive_bayes.GaussianNB: 가우시안 나이브베이즈 분류 모델을 생성합니다fit(X_train, y_train): 모델을 훈련 세트에 맞추어 학습시킵니다.(X_train, y_train): 모델을 훈련 세트에 맞추어 학습시킵니다.﻿​predict(X_test): 테스트 세트에 대해 예측을 수행합니다.(X_test): 테스트 세트에 대해 예측을 수행합니다.﻿​sklearn.metrics.accuracy_score: 정확도 계산accuracy_score(y_test, y_pred): 실제 값과 예측 값을 비교하여 정확도를 반환합니다._score(y_test, y_pred): 실제 값과 예측 값을 비교하여 정확도를 반환합니다.﻿​sklearn.metrics.classification_report: 분류 보고서 생성classification_report(y_test, y_pred): 정확도, 정밀도, 재현율 등의 메트릭을 포함한 보고서를 출력합니다._report(y_test, y_pred): 정확도, 정밀도, 재현율 등의 메트릭을 포함한 보고서를 출력합니다.﻿​sklearn.metrics.confusion_matrix: 혼동 행렬 생성confusion_matrix(y_test, y_pred): 실제 값과 예측 값의 혼동 행렬을 반환합니다._matrix(y_test, y_pred): 실제 값과 예측 값의 혼동 행렬을 반환합니다.﻿​2) 타이타닉 데이터 데이터 로드 및 전처리타이타닉 데이터 로드 및 전처리 {5px}타이타닉 데이터 로드 및 전처리 ﻿​Python복사import seaborn as sns

# 데이터 로드
titanic = sns.load_dataset('titanic')
# 필요한 열 선택 및 결측값 처리
titanic = titanic[['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']].dropna()
# 성별과 탑승한 곳 인코딩
titanic['sex'] = titanic['sex'].map({'male': 0, 'female': 1})
titanic['embarked'] = titanic['embarked'].map({'C': 0, 'Q': 1, 'S': 2})
# 특성과 타겟 분리
X = titanic.drop('survived', axis=1)
y = titanic['survived']
# 데이터 분할
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# 데이터 스케일링
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
​seaborn.load_dataset: seaborn의 내장 데이터셋 로드’titanic’: 타이타닉 데이터셋을 로드합니다.’titanic’: 타이타닉 데이터셋을 로드합니다.﻿​pandas.DataFrame.dropna: 결측값이 있는 행 제거pandas.DataFrame.map: 데이터 값을 다른 값으로 매핑’male’: 0, ’female’: 1: 성별을 숫자로 매핑합니다.: 성별을 숫자로 매핑합니다.}’male’: 0, ’female’: 1: 성별을 숫자로 매핑합니다.﻿​’C’: 0, ’Q’: 1, ’S’: 2: 탑승한 곳을 숫자로 매핑합니다.: 탑승한 곳을 숫자로 매핑합니다.}’C’: 0, ’Q’: 1, ’S’: 2: 탑승한 곳을 숫자로 매핑합니다.﻿​ 모델 학습모델 학습 {5px}모델 학습 ﻿​Python복사# 모델 생성 및 학습
model = GaussianNB()
model.fit(X_train, y_train)
# 예측
y_pred = model.predict(X_test)
# 평가
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
print(f"Classification Report:")
print(f"Confusion Matrix:")
​.✔️나이브베이즈가 무엇인지 알아봅시다
[스파르타코딩클럽] 12. 생성형 모델 [SCC] 기초가 탄탄한 딥러닝/[스파르타코딩클럽] 기초가 탄탄한 딥러닝 - 5주차/[스파르타코딩클럽] 12. 생성형 모델 제작:[스파르타코딩클럽] 12. 생성형 모델 [수업 목표]생성형 모델인 GAN 과 VAE 모델에 대해 배워봅시다[목차]01. GAN(Generative Adversarial Network)02. VAE (Variational Autoencoder)💡
 
 01. GAN(Generative Adversarial Network)1) GAN의 개념 GAN이란GAN은 2014년 Ian Goodfellow와 그의 동료들에 의해 제안된 생성형 모델입니다GAN은 두 개의 신경망, 즉 생성자(Generator)와 판별자(Discriminator)로 구성되어 있습니다.생성자는 가짜 데이터를 생성하고, 판별자는 이 데이터가 진짜인지 가짜인지 판별하며, 서로 경쟁하여 동시에 학습합니다ALT2) 동작 원리 생성자(Generator)랜덤 노이즈 벡터를 입력으로 받아서 이를 통해 가짜 데이터를 생성합니다생성된 데이터는 판별자에게 전달되어 진짜 데이터처럼 보이도록 학습됩니다. 판별자(Discriminator)진짜 데이터와 생성된 가짜 데이터를 입력으로 받아서 이를 구분하는 역할을 합니다판별자는 진짜 데이터를 1로, 가짜 데이터를 0으로 분류하도록 학습됩니다. 경쟁 과정생성자는 판별자를 속이기 위해 점점 더 진짜 같은 데이터를 생성하려고 노력하게 됩니다.판별자는 생성자가 만든 가짜 데이터를 더 잘 구분하려고 노력하게 됩니다이 과정에서 두 네트워크는 서로 경쟁하며 동시에 발전하게 됩니다02. VAE (Variational Autoencoder)✔️생성형 모델중 하나인 VAE모델이 무엇인지, 어디에 사용할 수 있을지 알아 봅시다.1) VAE의 개념 VAE이란?VAE는 2013년 Kingma와 Welling에 의해 제안된 생성형 모델입니다.VAE는 인코더(Encoder)와 디코더(Decoder)로 구성된 오토인코더의 변형입니다.인코더는 입력 데이터를 잠재 공간(latent space)으로 매핑하고, 디코더는 이 잠재 공간에서 데이터를 다시 원래 공간으로 복원합니다.VAE는 잠재 공간을 확률 분포로 모델링하여, 새로운 데이터를 생성할 수 있는 능력을 갖추게 됩니다.ALT2) 응용 이미지 생성VAE는 새로운 이미지를 생성하는 데 사용될 수 있다.예를 들어, 얼굴 이미지 데이터셋을 학습한 VAE는 새로운 얼굴 이미지를 생성할 수 있다. 데이터 압축VAE는 데이터를 잠재 공간으로 압축하고, 이를 통해 데이터 압축 및 복원에 사용할 수 있다. 노이즈 제거VAE는 노이즈가 있는 데이터를 입력으로 받아서 노이즈를 제거한 깨끗한 데이터를 출력할 수 있다..
데이터 전처리: 이상치 탐지 및 처리인공지능을 위한 파이썬 라이브러리/인공지능을 위한 파이썬 라이브러리 - 5주차/데이터 전처리: 이상치 탐지 및 처리데이터 전처리: 이상치 탐지 및 처리수업 목표데이터의 이상치가 무엇인지 학습합니다.전처리 과정에서 이상치를 처리하는 방법에 대해서 학습합니다.목차이상치(Outlier)란이상치 탐지 방법이상치 처리 방법요약이상치(Outlier)란❗이상치는 데이터의 일반적인 패턴에서 벗어난 값을 의미합니다.

이러한 값들은 데이터 분석에 부정적인 영향을 미칠 수 있기 때문에,
탐지하고 적절히 처리하는 것이 중요합니다!이상치 탐지 방법📚이상치를 탐지하는 방법에는 여러 가지가 있습니다. 여기서는 기술 통계, 시각화, IQR(사분위수 범위)를 사용한 방법을 살펴보겠습니다.기술 통계 기반 이상치 탐지describe() 함수를 사용하여 데이터의 기본 통계량을 확인하고, 이상치를 의심할 수 있습니다.Python복사import pandas as pd

# 예시 데이터프레임 생성
data = {
'이름': ['철수', '영희', '민수', '지수', '상수'],
'나이': [25, 30, 22, 35, 120], # 120은 이상치로 의심됨
'점수': [90, 85, 95, 80, 88]
}
df = pd.DataFrame(data)
# 기술 통계량 확인
print(df['나이'].describe())

​결과:Plain Text복사count      5.000000
mean      46.400000
std       41.892848
min       22.000000
25%       25.000000
50%       30.000000
75%       35.000000
max      120.000000
Name: 나이, dtype: float64

​평균(mean)과 표준편차(std)가 큰 차이를 보이는 경우, 또는 최대값(max)이 비정상적으로 높은 경우 이상치를 의심할 수 있습니다.시각화를 사용한 이상치 탐지박스플롯(Box Plot)과 히스토그램을 사용하면 데이터의 분포를 시각적으로 확인할 수 있어, 이상치를 탐지하기 용이합니다.Python복사import matplotlib.pyplot as plt

# 박스플롯으로 이상치 시각화
plt.boxplot(df['나이'])
plt.title('나이의 박스플롯')
plt.show()

​박스플롯의 이상치는 통상적으로 박스(사분위수 범위)의 위아래에 위치한 점으로 표시됩니다.IQR(Interquartile Range)을 사용한 이상치 탐지IQR은 1사분위수(Q1)와 3사분위수(Q3)의 차이로, 이 범위를 벗어나는 데이터를 이상치로 간주할 수 있습니다.Python복사# IQR 계산
Q1 = df['나이'].quantile(0.25)
Q3 = df['나이'].quantile(0.75)
IQR = Q3 - Q1

# IQR을 이용한 이상치 탐지
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

outliers = df[(df['나이'] < lower_bound) | (df['나이'] > upper_bound)]
print(outliers)

​결과:Plain Text복사    이름   나이  점수
4  상수  120  88

​120이라는 값이 IQR 범위를 벗어나므로 이상치로 탐지됩니다.이상치 처리 방법📚이상치를 탐지한 후, 데이터의 특성과 분석 목적에 따라 적절히 처리해야 합니다. 대표적인 처리 방법들을 소개합니다.이상치 제거이상치를 데이터프레임에서 제거할 수 있습니다.Python복사# 이상치를 제거한 데이터프레임
df_without_outliers = df[(df['나이'] >= lower_bound) & (df['나이'] <= upper_bound)]
print(df_without_outliers)

​결과:Plain Text복사   이름   나이  점수
0  철수  25  90
1  영희  30  85
2  민수  22  95
3  지수  35  80

​이상치를 특정 값으로 대체이상치를 특정 값으로 대체할 수 있습니다. 예를 들어, 해당 열의 중앙값으로 대체할 수 있습니다.Python복사# '나이'의 중앙값으로 이상치 대체
median_age = df['나이'].median()
df['나이'] = df['나이'].apply(lambda x: median_age if x > upper_bound or x < lower_bound else x)
print(df)

​결과:Plain Text복사   이름   나이  점수
0  철수  25.0  90
1  영희  30.0  85
2  민수  22.0  95
3  지수  35.0  80
4  상수  30.0  88

​120이었던 이상치가 30으로 대체되었습니다.이상치를 그대로 유지이상치가 중요한 분석 포인트가 될 수 있다고 판단되면, 별도의 처리를 하지 않고 그대로 유지할 수도 있습니다.Python복사# 이상치를 그대로 유지하는 경우 (처리하지 않음)
print(df)

​이 방법은 분석 목적에 따라 선택적으로 사용됩니다.요약📚요약이상치는 데이터의 일반적인 패턴에서 벗어난 값이며, 분석 결과에 큰 영향을 줄 수 있습니다.탐지 방법으로는 기술 통계량 확인, 시각화(박스플롯, 히스토그램), IQR을 활용한 방법 등이 있습니다.처리 방법으로는 이상치 제거, 특정 값으로 대체, 또는 이상치를 그대로 유지하는 방법이 있습니다.이제 데이터의 이상치를 효과적으로 탐지하고, 적절히 처리하여 분석의 정확도를 높여보세요! ​익명9월 25일matplotlib 패키지 설치 명령어 추가하면 좋을 것 같습니다.

!pip install matplotlib
[스파르타코딩클럽] 13강. 지도학습 : 분류모델 - 의사결정나무[SCC] 바닥부터 시작하는 머신러닝/[스파르타코딩클럽] 바닥부터 시작하는 머신러닝 - 3주차/[스파르타코딩클럽] 13강. 지도학습 : 분류모델 - 의사결정나무제작:[스파르타코딩클럽] 13강. 지도학습 : 분류모델 - 의사결정나무[수업 목표]분류모델중 의사결정나무에 대한 개념을 배우고, 데이터를 이용해 실습해 봅니다[목차]01. 의사결정나무 개념02. 의사결정나무 실습💡
 
 01. 의사결정나무 개념✔️지도학습:분류모델의 의사결정나무가 무엇인지 알아봅시다1) 의사결정나무 의사결정나무란?의사결정나무(Decision Tree)는 예측 모델 중 하나로, 데이터의 특징(feature)을 기준으로 의사결정 규칙을 만들고 이를 바탕으로 데이터를 분류하거나 회귀하는 데 사용됩니다의사결정나무는 트리 구조를 가지며, 각 내부 노드는 데이터의 특정 특징에 대한 테스트를 나타내고, 각 가지(branch)는 테스트 결과를 나타내며, 각 리프 노드(leaf)는 클래스 레이블을 나타냅니다.ALT노드: 트리의 각 분기점으로, 하나의 특징(feature)에 대한 테스트를 나타냅니다.노드: 트리의 각 분기점으로, 하나의 특징(feature)에 대한 테스트를 나타냅니다.﻿​루트 노드: 트리의 최상위 노드로, 전체 데이터셋을 나타냅니다.루트 노드: 트리의 최상위 노드로, 전체 데이터셋을 나타냅니다.﻿​리프 노드: 트리의 끝 노드로, 최종 클래스 레이블을 나타냅니다.리프 노드: 트리의 끝 노드로, 최종 클래스 레이블을 나타냅니다.﻿​깊이: 트리의 루트 노드부터 리프 노드까지의 최대 거리입니다.깊이: 트리의 루트 노드부터 리프 노드까지의 최대 거리입니다.﻿​분할 기준: 노드를 나눌 때 사용하는 기준으로, 정보 이득(Information Gain), 지니 계수(Gini Index) 등이 있습니다.분할 기준: 노드를 나눌 때 사용하는 기준으로, 정보 이득(Information Gain), 지니 계수(Gini Index) 등이 있습니다.﻿​ 분할기준정보 이득(Information Gain) : 엔트로피(Entropy)값으로 데이터를 나누는 기준입니다. 엔트로피는 불확실성을 나타내며, 엔트로피가 낮을수록 불확실성이 적습니다.Information Gain(D,A)=Entropy(D)−∑v∈values(A)∣Dv∣∣D∣Entropy(Dv)(D, A) = (D) - _{v  (A)} {|D|} (D_v)Information Gain(D,A)=Entropy(D)−v∈values(A)∑​∣D∣∣Dv​∣​Entropy(Dv​)지니 계수(Gini Index): 불순도를 측정하는 방법으로, 지니 계수가 낮을수록 불순도가 적습니다Gini(D)=1−∑i=1Cpi2(D) = 1 - _{i=1}^{C} p_i^2Gini(D)=1−i=1∑C​pi2​여기서 pi는 클래스 i의 비율입니다.여기서 pi​는 클래스 i의 비율입니다.﻿​02. 의사결정나무 실습✔️Scikit-learn의 유방암데이터와 Seaborn의 타이타닉 데이터로 의사결정나무 실습을 진행합니다1) 유방암 데이터 데이터 로드 및 전처리유방암 데이터 로드 및 전처리 {5px}유방암 데이터 로드 및 전처리 ﻿​Python복사import numpy as np
import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# 데이터 로드
data = load_breast_cancer()
X = data.data
y = data.target

# 데이터 분할
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# 데이터 스케일링
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
​sklearn.datasets.load_breast_cancer: 유방암 데이터셋 로드return_X_y=False: 데이터와 타겟을 함께 반환할지 여부. 기본값은 False입니다._X_y=False: 데이터와 타겟을 함께 반환할지 여부. 기본값은 False입니다.﻿​sklearn.model_selection.train_test_split: 데이터를 훈련 세트/ 테스트 세트로 분할test_size=0.2: 테스트 세트의 비율을 0.2로 설정합니다._size=0.2: 테스트 세트의 비율을 0.2로 설정합니다.﻿​random_state=42: 랜덤 시드 값으로, 데이터 분할의 재현성을 위해 사용됩니다._state=42: 랜덤 시드 값으로, 데이터 분할의 재현성을 위해 사용됩니다.﻿​sklearn.preprocessing.StandardScaler: 데이터의 평균을 0, 분산을 1로 스케일링fit_transform(X_train): 훈련 세트를 스케일링하고 변환합니다._transform(X_train): 훈련 세트를 스케일링하고 변환합니다.﻿​transform(X_test): 테스트 세트를 변환합니다.(X_test): 테스트 세트를 변환합니다.﻿​ 모델 학습모델 학습 {5px}모델 학습 ﻿​Python복사from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# 모델 생성 및 학습
model = DecisionTreeClassifier(random_state=42)
model.fit(X_train, y_train)
# 예측
y_pred = model.predict(X_test)
# 평가
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
print(f"Classification Report:")
print(f"Confusion Matrix:")
​sklearn.tree.DecisionTreeClassifier: 의사결정나무 분류 모델 생성random_state=42: 랜덤 시드 값으로, 트리의 재현성을 위해 사용됩니다._state=42: 랜덤 시드 값으로, 트리의 재현성을 위해 사용됩니다.﻿​fit(X_train, y_train): 모델을 훈련 세트에 맞추어 학습시킵니다.(X_train, y_train): 모델을 훈련 세트에 맞추어 학습시킵니다.﻿​predict(X_test): 테스트 세트에 대해 예측을 수행합니다.(X_test): 테스트 세트에 대해 예측을 수행합니다.﻿​sklearn.metrics.accuracy_score: 정확도 계산accuracy_score(y_test, y_pred): 실제 값과 예측 값을 비교하여 정확도를 반환합니다._score(y_test, y_pred): 실제 값과 예측 값을 비교하여 정확도를 반환합니다.﻿​sklearn.metrics.classification_report: 분류 보고서 생성classification_report(y_test, y_pred): 정확도, 정밀도, 재현율 등의 메트릭을 포함한 보고서를 출력합니다._report(y_test, y_pred): 정확도, 정밀도, 재현율 등의 메트릭을 포함한 보고서를 출력합니다.﻿​sklearn.metrics.confusion_matrix: 혼동 행렬 생성confusion_matrix(y_test, y_pred): 실제 값과 예측 값의 혼동 행렬을 반환합니다._matrix(y_test, y_pred): 실제 값과 예측 값의 혼동 행렬을 반환합니다.﻿​2) 타이타닉 데이터 데이터 로드 및 전처리타이타닉 데이터 로드 및 전처리 {5px}타이타닉 데이터 로드 및 전처리 ﻿​Python복사import seaborn as sns

# 데이터 로드
titanic = sns.load_dataset('titanic')
# 필요한 열 선택 및 결측값 처리
titanic = titanic[['survived', 'pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']].dropna()
# 성별과 탑승한 곳 인코딩
titanic['sex'] = titanic['sex'].map({'male': 0, 'female': 1})
titanic['embarked'] = titanic['embarked'].map({'C': 0, 'Q': 1, 'S': 2})
# 특성과 타겟 분리
X = titanic.drop('survived', axis=1)
y = titanic['survived']
# 데이터 분할
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# 데이터 스케일링
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)
​seaborn.load_dataset: seaborn의 내장 데이터셋 로드’titanic’: 타이타닉 데이터셋을 로드합니다.’titanic’: 타이타닉 데이터셋을 로드합니다.﻿​pandas.DataFrame.dropna: 결측값이 있는 행 제거pandas.DataFrame.map: 데이터 값을 다른 값으로 매핑’male’: 0, ’female’: 1: 성별을 숫자로 매핑합니다.: 성별을 숫자로 매핑합니다.}’male’: 0, ’female’: 1: 성별을 숫자로 매핑합니다.﻿​’C’: 0, ’Q’: 1, ’S’: 2: 탑승한 곳을 숫자로 매핑합니다.: 탑승한 곳을 숫자로 매핑합니다.}’C’: 0, ’Q’: 1, ’S’: 2: 탑승한 곳을 숫자로 매핑합니다.﻿​ 모델 학습모델 학습 {5px}모델 학습 ﻿​Python복사# 모델 생성 및 학습
model = DecisionTreeClassifier(random_state=42)
model.fit(X_train, y_train)
# 예측
y_pred = model.predict(X_test)
# 평가
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")
print(f"Classification Report:")
print(f"Confusion Matrix:")
​.
[스파르타코딩클럽] 13. 전이학습[SCC] 기초가 탄탄한 딥러닝/[스파르타코딩클럽] 기초가 탄탄한 딥러닝 - 5주차/[스파르타코딩클럽] 13. 전이학습제작:[스파르타코딩클럽] 13. 전이학습[수업 목표]전이학습에 대해서 배워보고, 언제 전이학습을 사용하는지 알아 봅시다Pytorch로 사전 학습된 모델을 이용해 전이학습 구현 실습을 진행해 봅시다[목차]01. 전이학습
 
 01. 전이학습전이학습이란 무엇인지, 어떨때 적용하는지 배워봅시다1) 전이학습의 필요성과 원리 전이 학습이란?전이학습(Transfer Learning)은 이미 학습된 모델의 지식을 새로운 문제에 적용하는 방법입니다. 전이학습은 특히 데이터가 부족한 상황에서 유용하며, 모델 학습 시간을 단축하고 성능을 향상시킬 수 있습니다.ALT 전이학습의 필요성데이터 부족: 새로운 문제에 대한 데이터가 충분하지 않을 때, 전이학습을 통해 기존 모델의 지식을 활용할 수 있습니다.학습 시간 단축: 사전 학습된 모델 사용시, 처음부터 모델을 학습하는것 보다 빠르게 학습할 수 있습니다.성능 향상: 사전 학습된 모델은 대규모 데이터셋에서 학습되었기 때문에, 통상 더 나은 성능을 보입니다. 전이학습의 원리특징 추출기 (Feature Extractor): 사전 학습된 모델의 초기 층을 고정하고, 새로운 데이터에 맞게 마지막 층만 재학습합니다.미세 조정 (Fine-Tuning): 사전 학습된 모델 전체를 새로운 데이터에 맞게 재학습합니다.2) 전이학습을 통해 모델 만들어 보기 전이학습을 적용한 모델 구축 과정 사전 학습된 모델 로드:PyTorch에서 제공하는 사전 학습된 모델을 로드합니다.예를 들어, ResNet, VGG, Inception 등의 모델을 사용할 수 있습니다.모델 수정:사전 학습된 모델의 마지막 층을 새로운 문제에 맞게 수정합니다.예를 들어, 이미지 분류 문제에서 클래스 수를 변경합니다.모델 학습:수정된 모델을 새로운 데이터에 맞게 학습시킵니다.특징 추출기 방식이나 미세 조정 방식을 사용할 수 있습니다..
데이터 전처리: 데이터 정규화와 표준화 (비선형 변환 포함)인공지능을 위한 파이썬 라이브러리/인공지능을 위한 파이썬 라이브러리 - 5주차/데이터 전처리: 데이터 정규화와 표준화 (비선형 변환 포함)제작:데이터 전처리: 데이터 정규화와 표준화 (비선형 변환 포함)수업 목표데이터 전처리 과정에서의 정규화와 표준화 방법에 대해서 알아봅니다.목차1. 데이터 정규화(Normalization) 데이터 표준화(Standardization) 3. 비선형 변환(Non-linear Transformation) 요약1. 데이터 정규화(Normalization) ​정규화(Normalization)는 데이터의 범위를 0과 1 사이로 변환하는 과정입니다. 이는 서로 다른 범위를 가진 데이터를 동일한 스케일로 맞추어 비교하기 쉽게 만듭니다.Min-Max 정규화Min-Max 정규화는 가장 일반적인 정규화 방법으로, 각 데이터를 최소값을 0, 최대값을 1로 변환합니다.Python복사import pandas as pd
from sklearn.preprocessing import MinMaxScaler

# 예시 데이터프레임 생성
data = {
'특성1': [10, 20, 30, 40, 50],
'특성2': [1, 2, 3, 4, 5]
}
df = pd.DataFrame(data)
# Min-Max 정규화
scaler = MinMaxScaler()
normalized_df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)
print(normalized_df)

​결과:Plain Text복사   특성1  특성2
0   0.0   0.0
1   0.25  0.25
2   0.5   0.5
3   0.75  0.75
4   1.0   1.0

​이 방식으로 각 열의 모든 데이터가 0에서 1 사이의 값으로 변환되었습니다.데이터 표준화(Standardization) 표준화(Standardization)는 데이터를 평균이 0, 표준편차가 1이 되도록 변환하는 과정입니다. 이는 정규 분포를 가정한 많은 분석 기법에 유리합니다.Z-점수 표준화Z-점수 표준화는 데이터에서 평균을 빼고 표준편차로 나누어, 모든 데이터가 표준 정규분포(평균 0, 표준편차 1)를 따르도록 만듭니다.Python복사from sklearn.preprocessing import StandardScaler

# Z-점수 표준화
scaler = StandardScaler()
standardized_df = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)
print(standardized_df)

​결과:Plain Text복사      특성1      특성2
0 -1.414214 -1.414214
1 -0.707107 -0.707107
2  0.000000  0.000000
3  0.707107  0.707107
4  1.414214  1.414214

​각 열의 데이터가 평균이 0, 표준편차가 1이 되도록 변환되었습니다.3. 비선형 변환(Non-linear Transformation) ​비선형 변환은 데이터의 비정상적인 분포를 정규 분포에 가깝게 만들기 위해 사용됩니다. 대표적인 방법으로 로그 변환, 제곱근 변환, 박스-콕스 변환 등이 있습니다.로그(Log) 변환로그 변환은 양의 데이터에서 주로 사용되며, 데이터의 분포를 좁히는 데 유용합니다. 특히, 지수 분포를 가진 데이터를 다룰 때 효과적입니다.Python복사import numpy as np

# 로그 변환
df['특성1_log'] = np.log(df['특성1'])
print(df)

​결과:Plain Text복사   특성1  특성2  특성1_log
0    10     1   2.302585
1    20     2   2.995732
2    30     3   3.401197
3    40     4   3.688879
4    50     5   3.912023

​로그 변환 후 데이터의 분포가 평탄해지는 효과를 얻을 수 있습니다.제곱근(Square Root) 변환제곱근 변환은 데이터의 분포를 평탄하게 만들기 위한 또 다른 방법입니다. 특히, 포아송 분포를 가진 데이터에서 사용됩니다.Python복사# 제곱근 변환
df['특성1_sqrt'] = np.sqrt(df['특성1'])
print(df)

​결과:Plain Text복사   특성1  특성2  특성1_log  특성1_sqrt
0    10     1   2.302585    3.162278
1    20     2   2.995732    4.472136
2    30     3   3.401197    5.477226
3    40     4   3.688879    6.324555
4    50     5   3.912023    7.071068

​제곱근 변환을 통해 분포가 줄어드는 효과를 얻습니다.박스-콕스(Box-Cox) 변환박스-콕스 변환은 다양한 형태의 데이터 분포를 정규분포에 가깝게 변환하기 위해 사용됩니다. 이 방법은 양수 데이터에서만 사용 가능합니다.Python복사from scipy.stats import boxcox

# 박스-콕스 변환
df['특성1_boxcox'], _ = boxcox(df['특성1'])
print(df)

​결과:Plain Text복사   특성1  특성2  특성1_log  특성1_sqrt  특성1_boxcox
0    10     1   2.302585    3.162278      2.850350
1    20     2   2.995732    4.472136      3.992679
2    30     3   3.401197    5.477226      4.872105
3    40     4   3.688879    6.324555      5.609646
4    50     5   3.912023    7.071068      6.245548

​박스-콕스 변환을 통해 데이터의 분포가 정규분포에 가까워지는 효과를 얻습니다.요약요약정규화(Normalization)는 데이터의 범위를 0과 1 사이로 변환하는 과정으로, 주로 Min-Max 스케일링을 사용합니다.표준화(Standardization)는 데이터를 평균이 0, 표준편차가 1이 되도록 변환하는 과정으로, Z-점수 표준화가 대표적입니다.비선형 변환은 로그, 제곱근, 박스-콕스 변환 등을 통해 비정상적인 데이터 분포를 정규 분포에 가깝게 변환합니다.이제 데이터를 정규화하고 표준화하여 분석에 적합한 상태로 만들어 보세요! ​익명9월 25일pip install scikit-learn
[스파르타코딩클럽] 14강. 비지도학습 : 군집화모델 - k-means clustering[SCC] 바닥부터 시작하는 머신러닝/[스파르타코딩클럽] 바닥부터 시작하는 머신러닝 - 4주차 /[스파르타코딩클럽] 14강. 비지도학습 : 군집화모델 - k-means clustering제작:[스파르타코딩클럽] 14강. 비지도학습 : 군집화모델 - k-means clustering[수업 목표]비지도학습 군집화모델 중 k-means clustering 에 대한 개념을 배우고, 데이터를 이용해 실습해 봅니다[목차]01. k-means clustering 개념02. k-means clustering 실습💡
 
 01. k-means clustering 개념✔️k-means clustering이 무엇인지 알아봅시다1) k-means clustering k-means clustering 이란?ALT 알고리즘의 단계초기화: k개의 군집 중심을 랜덤하게 설정합니다.할당 단계: 각 데이터 포인트를 가장 가까운 군집 중심에 할당합니다.업데이트 단계: 각 군집의 중심을 해당 군집에 속한 데이터 포인트들의 평균으로 업데이트합니다.반복: 할당 단계와 업데이트 단계를 군집 중심이 더 이상 변화하지 않을 때까지 반복합니다 거리 측정 방법k-means 알고리즘은 주로 유클리드 거리(Euclidean Distance)를 사용하여 데이터 포인트와 군집 중심 간의 거리를 계산합니다.d(p,q)=∑i=1n(pi−qi)2d(p, q) = ^{n} (p_i - q_i)^2}d(p,q)=i=1∑n​(pi​−qi​)2​엘보우 방법최적의 k를 선택하는 데 사용됩니다.k를 증가시키면서 각 k에 대한 군집의 응집도(또는 관성, Inertia)를 계산하고, 이를 그래프로 나타내어 그래프에서 응집도가 급격히 감소하는 지점을 찾습니다ALT02. k-means clustering 실습✔️Kaggle 쇼핑몰 데이터를 이용하여 K-means clustering 실습을 진행합니다1) 쇼핑몰 데이터 데이터셋 다운로드Kaggle에서 "Mall_Customers.csv" 파일을 다운로드합니다.다운로드한 파일을 작업 디렉토리에 저장합니다. 데이터 로드 및 전처리유방암 데이터 로드 및 전처리 {5px}유방암 데이터 로드 및 전처리 ﻿​Python복사import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import seaborn as sns

# 데이터 로드
data = pd.read_csv('Mall_Customers.csv')
# 필요한 열 선택 및 결측값 처리
data = data[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']]
# 데이터 스케일링
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data)
​pandas.read_csv: CSV 파일을 로드하여 데이터프레임 생성’Mall_Customers.csv’: 로드할 파일의 경로입니다.’Mall_Customers.csv’: 로드할 파일의 경로입니다.﻿​pandas.DataFrame.dropna: 결측값이 있는 행을 제거합니다.pandas.DataFrame.map: 데이터 값을 다른 값으로 매핑합니다.sklearn.preprocessing.StandardScaler: 데이터의 평균을 0, 분산을 1로 스케일링fit_transform(X_train): 훈련 세트를 스케일링하고 변환합니다._transform(X_train): 훈련 세트를 스케일링하고 변환합니다.﻿​ 모델 학습 및 군집화모델 학습 및 군집화 {5px}모델 학습 및 군집화 ﻿​Python복사# 최적의 k 찾기 (엘보우 방법)
inertia = []
K = range(1, 11)
for k in K:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(data_scaled)
    inertia.append(kmeans.inertia_)
# 엘보우 그래프 그리기
plt.figure(figsize=(10, 8))
plt.plot(K, inertia, 'bx-')
plt.xlabel('k')
plt.ylabel('Inertia')
plt.title('Elbow Method For Optimal k')
plt.show()
# k=5로 모델 생성 및 학습
kmeans = KMeans(n_clusters=5, random_state=42)
kmeans.fit(data_scaled)
# 군집 결과 할당
data['Cluster'] = kmeans.labels_
​sklearn.cluster.KMeans: k-means 군집화 모델을 생성합니다n_clusters=k: 군집의 수를 설정합니다._clusters=k: 군집의 수를 설정합니다.﻿​random_state=42: 랜덤 시드 값으로, 결과의 재현성을 위해 사용됩니다._state=42: 랜덤 시드 값으로, 결과의 재현성을 위해 사용됩니다.﻿​fit(data_scaled): 데이터를 학습하여 군집을 형성합니다.(data_scaled): 데이터를 학습하여 군집을 형성합니다.﻿​labels_: 각 데이터 포인트가 속한 군집 레이블을 반환합니다._: 각 데이터 포인트가 속한 군집 레이블을 반환합니다.﻿​ 군집 시각화군집 시각화 {5px}군집 시각화 ﻿​Python복사# 2차원으로 군집 시각화 (연령 vs 소득)
plt.figure(figsize=(10, 8))
sns.scatterplot(x=data['Age'], y=data['Annual Income (k$)'], hue=data['Cluster'], palette='viridis')
plt.title('Clusters of customers (Age vs Annual Income)')
plt.show()
# 2차원으로 군집 시각화 (소득 vs 지출 점수)
plt.figure(figsize=(10, 8))
sns.scatterplot(x=data['Annual Income (k$)'], y=data['Spending Score (1-100)'], hue=data['Cluster'], palette='viridis')
plt.title('Clusters of customers (Annual Income vs Spending Score)')
plt.show()
​matplotlib.pyplot.plot: 그래프를 그립니다.K, inertia, ’bx-’: x축, y축 데이터와 그래프 스타일을 설정합니다., inertia, ’bx-’: x축, y축 데이터와 그래프 스타일을 설정합니다.﻿​seaborn.scatterplot: 산점도를 그립니다.x=data[’Age’]: x축 데이터=data[’Age’]: x축 데이터﻿​y=data[’Annual Income (k$)’]: y축 데이터=data[’Annual Income (k$)’]: y축 데이터﻿​hue=data[’Cluster’]: 색상에 따라 군집을 구분합니다.=data[’Cluster’]: 색상에 따라 군집을 구분합니다.﻿​palette=’viridis’: 색상 팔레트를 설정합니다.=’viridis’: 색상 팔레트를 설정합니다.﻿​.
[스파르타코딩클럽] 14. 과적합 방지 기법[SCC] 기초가 탄탄한 딥러닝/[스파르타코딩클럽] 기초가 탄탄한 딥러닝 - 6주차/[스파르타코딩클럽] 14. 과적합 방지 기법제작:[스파르타코딩클럽] 14. 과적합 방지 기법[수업 목표]여러 과적합 방지 기법에 대해서 알아봅시다.Pytorch로  과적합 방지 기법에 대한 실습 예시![목차]01. 과적화 방지 기법02. 과적합 방지기법 실습(Pytorch)💡
 
 01. 과적화 방지 기법✔️과적합을 방지하기 위한 방법들에는 어떠한 것들이 있는지 배워봅시다.1) 정규화와 드롭아웃 정규화 (Normalization)정규화는 입력 데이터의 분포를 일정한 범위로 조정하여, 모델의 학습을 안정화하고 성능을 향상시키는 기법입니다.ALT배치 정규화 (Batch Normalization): 각 미니배치의 평균과 분산을 사용하여 정규화합니다. 이는 학습 속도를 높이고, 과적합을 방지하는 데 도움이 됩니다.레이어 정규화 (Layer Normalization): 각 레이어의 뉴런 출력을 정규화합니다. 드롭아웃 (Dropout)드롭아웃은 학습 과정에서 무작위로 뉴런을 비활성화하여, 모델의 과적합을 방지하는 기법입니다.드롭아웃은 학습 시에만 적용되며, 평가 시에는 모든 뉴런을 활성화합니다.2) 조기 종료와 데이터 증강 조기 종료(Early Stopping) 기법조기 종료는 검증 데이터의 성능이 더 이상 향상되지 않을 때 학습을 중단하여, 과적합을 방지하는 기법입니다.조기 종료는 학습 과정에서 검증 손실이 일정 에포크 동안 감소하지 않으면 학습을 중단합니다. 데이터 증강(Data Augmentation) 기법데이터 증강은 원본 데이터를 변형하여 새로운 데이터를 생성함으로써, 데이터셋을 확장하고 모델의 일반화 성능을 향상시키는 기법입니다.데이터 증강 기법에는 회전, 이동, 크기 조절, 색상 변환 등이 있습니다.02. 과적합 방지기법 실습(Pytorch)✔️드롭아웃과 배치 정규화를 적용한 모델 실습과 데이터 증강을 적용한 모델을 구현하는 실습을 진행해 봅시다.1)  드롭아웃과 정규화를 적용한 모델 실습 PyTorch 및 필요한 라이브러리 임포트PyTorch 및 필요한 라이브러리 임포트 {5px}PyTorch 및 필요한 라이브러리 임포트 ﻿​Python복사import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
​데이터셋 로드 및 전처리데이터셋 로드 및 전처리 {5px}데이터셋 로드 및 전처리 ﻿​Python복사# 데이터셋 전처리
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])
# CIFAR-10 데이터셋 로드
trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)

testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)
​  드롭아웃과 배치 정규화를 적용한 모델 정의 드롭아웃과 배치 정규화를 적용한 모델 정의 {5px} 드롭아웃과 배치 정규화를 적용한 모델 정의 ﻿​Python복사class CNNWithDropoutAndBatchNorm(nn.Module):
def __init__(self):
super(CNNWithDropoutAndBatchNorm, self).__init__()
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(64)
        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(128)
        self.fc1 = nn.Linear(128 * 56 * 56, 256)
        self.dropout = nn.Dropout(0.5)
        self.fc2 = nn.Linear(256, 10)
def forward(self, x):
        x = torch.relu(self.bn1(self.conv1(x)))
        x = torch.max_pool2d(x, 2)
        x = torch.relu(self.bn2(self.conv2(x)))
        x = torch.max_pool2d(x, 2)
        x = x.view(x.size(0), -1)
        x = torch.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
return x

model = CNNWithDropoutAndBatchNorm()
​nn.Conv2d: 2차원 합성곱 층을 정의합니다. nn.Conv2d(in_channels, out_channels, kernel_size, padding)은 입력 채널 수, 출력 채널 수, 커널 크기, 패딩을 지정.Conv2d(in_channels, out_channels, kernel_size, padding)은 입력 채널 수, 출력 채널 수, 커널 크기, 패딩을 지정﻿​nn.BatchNorm2d: 2차원 배치 정규화 층을 정의합니다.nn.Dropout: 드롭아웃 층을 정의합니다. nn.Dropout(p)은 드롭아웃 확률을 지정합니다..Dropout(p)은 드롭아웃 확률을 지정합니다.﻿​torch.max_pool2d: 2차원 최대 풀링을 수행합니다. 손실 함수와 최적화 알고리즘 정의손실 함수와 최적화 알고리즘 정의 {5px}손실 함수와 최적화 알고리즘 정의 ﻿​Python복사criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
​nn.CrossEntropyLoss: 교차 엔트로피 손실 함수를 정의합니다.optim.Adam: Adam 최적화 알고리즘을 정의합니다. lr은 학습률을 지정합니다.은 학습률을 지정합니다.﻿​ 모델 학습모델 학습 {5px}모델 학습 ﻿​Python복사num_epochs = 10
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)
for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
for i, (inputs, labels) in enumerate(trainloader):
        inputs, labels = inputs.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
if i % 100 == 99: # 매 100 미니배치마다 출력
print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 100:.3f}')
            running_loss = 0.0
print('Finished Training')
​model.train(): 모델을 학습 모드로 전환합니다.optimizer.zero_grad(): 이전 단계에서 계산된 기울기를 초기화합니다.loss.backward(): 역전파를 통해 기울기를 계산합니다.optimizer.step(): 계산된 기울기를 바탕으로 가중치를 업데이트합니다. 모델 평가모델 평가 {5px}모델 평가 ﻿​Python복사model.eval()
correct = 0
total = 0
with torch.no_grad():
for inputs, labels in testloader:
        inputs, labels = inputs.to(device), labels.to(device)
        outputs = model(inputs)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')
​model.eval(): 모델을 평가 모드로 전환합니다.torch.no_grad(): 평가 단계에서는 기울기를 계산할 필요가 없으므로, 이를 비활성화하여 메모리 사용을 줄입니다.torch.max: 텐서의 최대 값을 찾습니다. torch.max(outputs.data, 1)은 각 샘플에 대해 가장 높은 확률을 가진 클래스를 반환합니다..max(outputs.data, 1)은 각 샘플에 대해 가장 높은 확률을 가진 클래스를 반환합니다.﻿​labels.size(0): 배치 크기를 반환합니다.(predicted == labels).sum().item(): 예측 값과 실제 값이 일치하는 샘플의 수를 계산합니다.2)  데이터 증강을 통한 모델 성능 향상 실습데이터셋 로드 및 전처리데이터셋 로드 및 전처리 {5px}데이터셋 로드 및 전처리 ﻿​Python복사# 데이터 증강 적용
transform_train = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomCrop(32, padding=4),
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

transform_test = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])
# CIFAR-10 데이터셋 로드
trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)

testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)
testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)
​transforms.RandomHorizontalFlip(): 이미지를 무작위로 수평 반전합니다.transforms.RandomCrop(size, padding): 이미지를 무작위로 자르고, 패딩을 추가합니다.  드롭아웃과 배치 정규화를 적용한 모델 정의첫번째 실습에서 정의한 모델을 그대로 사용합니다. 드롭아웃과 배치 정규화를 적용한 모델 정의 {5px} 드롭아웃과 배치 정규화를 적용한 모델 정의 ﻿​Python복사class CNNWithDropoutAndBatchNorm(nn.Module):
def __init__(self):
super(CNNWithDropoutAndBatchNorm, self).__init__()
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)
        self.bn1 = nn.BatchNorm2d(64)
        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.bn2 = nn.BatchNorm2d(128)
        self.fc1 = nn.Linear(128 * 56 * 56, 256)
        self.dropout = nn.Dropout(0.5)
        self.fc2 = nn.Linear(256, 10)
def forward(self, x):
        x = torch.relu(self.bn1(self.conv1(x)))
        x = torch.max_pool2d(x, 2)
        x = torch.relu(self.bn2(self.conv2(x)))
        x = torch.max_pool2d(x, 2)
        x = x.view(x.size(0), -1)
        x = torch.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
return x

model = CNNWithDropoutAndBatchNorm()
​ 손실 함수와 최적화 알고리즘 정의첫번째 실습에서 정의한 손실함수와 최적화 알고리즘을 그대로 사용합니다.손실 함수와 최적화 알고리즘 정의 {5px}손실 함수와 최적화 알고리즘 정의 ﻿​Python복사criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)
​ 모델 학습첫번째 실습에서 정의한 모델 학습 코드를 그대로 사용합니다.모델 학습 {5px}모델 학습 ﻿​Python복사num_epochs = 10
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)
for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
for i, (inputs, labels) in enumerate(trainloader):
        inputs, labels = inputs.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
if i % 100 == 99: # 매 100 미니배치마다 출력
print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 100:.3f}')
            running_loss = 0.0
print('Finished Training')
​ 모델 평가첫번째 실습에서 정의한 모델 평가 코드를 그대로 사용합니다.모델 평가 {5px}모델 평가 ﻿​Python복사model.eval()
correct = 0
total = 0
with torch.no_grad():
for inputs, labels in testloader:
        inputs, labels = inputs.to(device), labels.to(device)
        outputs = model(inputs)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
print(f'Accuracy of the network on the 10000 test images: {100 * correct / total:.2f}%')
​.
데이터 전처리: 인코딩 (Encoding)인공지능을 위한 파이썬 라이브러리/인공지능을 위한 파이썬 라이브러리 - 5주차/데이터 전처리: 인코딩 (Encoding)제작:데이터 전처리: 인코딩 (Encoding)수업 목표범주형 데이터를 수치형 데이터로 변환하는 과정에 대해서 학습합니다.목차인코딩(Encoding)이란?레이블 인코딩(Label Encoding) 원-핫 인코딩(One-Hot Encoding)차원 축소 인코딩 (Count or Frequency Encoding)순서형 인코딩 (Ordinal Encoding)임베딩(Embedding)요약인코딩(Encoding)이란?인코딩(Encoding)은 범주형 데이터(Categorical Data)를 수치형 데이터로 변환하는 과정입니다. 많은 머신러닝 모델은 수치형 데이터만 처리할 수 있기 때문에, 범주형 데이터를 인코딩하는 것이 필수적입니다.레이블 인코딩(Label Encoding) 레이블 인코딩은 범주형 데이터를 순서가 있는 숫자로 변환합니다. 각 범주에 고유한 숫자가 할당됩니다.Python복사import pandas as pd
from sklearn.preprocessing import LabelEncoder

# 예시 데이터프레임 생성
data = {'과일': ['사과', '바나나', '사과', '오렌지', '바나나']}
df = pd.DataFrame(data)
# 레이블 인코딩
label_encoder = LabelEncoder()
df['과일_인코딩'] = label_encoder.fit_transform(df['과일'])
print(df)

​결과:Plain Text복사      과일  과일_인코딩
0     사과       0
1    바나나       1
2     사과       0
3    오렌지       2
4    바나나       1

​사과는 0, 바나나는 1, 오렌지는 2로 인코딩되었습니다.레이블 인코딩의 주의점레이블 인코딩은 범주형 데이터에 순서가 있을 때 적합합니다. 순서가 없는 데이터에 사용하면, 모델이 이 값을 크기로 인식해 잘못된 결과를 초래할 수 있습니다.원-핫 인코딩(One-Hot Encoding)원-핫 인코딩은 각 범주를 이진 벡터로 변환합니다. 각 범주는 고유한 열을 가지며, 해당하는 열에는 1, 나머지 열에는 0이 할당됩니다.Python복사# 원-핫 인코딩
df_one_hot = pd.get_dummies(df['과일'], prefix='과일')
print(df_one_hot)

​결과:Plain Text복사   과일_바나나  과일_사과  과일_오렌지
0        0        1        0
1        1        0        0
2        0        1        0
3        0        0        1
4        1        0        0

​바나나, 사과, 오렌지 각각이 독립된 열로 변환되었고, 해당하는 위치에 1이 표시됩니다.원-핫 인코딩의 장점범주형 데이터에 순서가 없을 때 사용하기 좋습니다. 모델이 범주 간의 순서나 관계를 인식하지 않고 각 범주를 독립적으로 처리할 수 있습니다.차원 축소 인코딩 (Count or Frequency Encoding)차원 축소 인코딩은 범주형 데이터가 많을 때 유용합니다. 각 범주를 데이터셋 내에서의 출현 빈도로 인코딩합니다.Python복사# 빈도 기반 인코딩
df['과일_빈도'] = df['과일'].map(df['과일'].value_counts())
print(df)

​결과:Plain Text복사      과일  과일_인코딩  과일_빈도
0     사과       0       2
1    바나나       1       2
2     사과       0       2
3    오렌지       2       1
4    바나나       1       2

​사과와 바나나는 각각 2회, 오렌지는 1회 출현하여 해당 빈도로 인코딩되었습니다.주의점이 방법은 범주형 데이터의 빈도가 매우 중요한 경우에 적합하며, 범주의 고유성을 잃을 수 있으므로 주의해서 사용해야 합니다.순서형 인코딩 (Ordinal Encoding)순서형 인코딩은 순서가 있는 범주형 데이터를 그 순서에 따라 숫자로 변환하는 방식입니다.Python복사# 예시 데이터
data = {'등급': ['낮음', '중간', '높음', '중간', '높음']}
df = pd.DataFrame(data)
# 순서형 인코딩
order = {'낮음': 1, '중간': 2, '높음': 3}
df['등급_인코딩'] = df['등급'].map(order)
print(df)

​결과:Plain Text복사    등급  등급_인코딩
0  낮음       1
1  중간       2
2  높음       3
3  중간       2
4  높음       3

​낮음은 1, 중간은 2, 높음은 3으로 인코딩되었습니다.사용 예시순서형 인코딩은 만족도(낮음, 중간, 높음), 등급(A, B, C) 등 순서가 중요한 경우에 적합합니다.임베딩(Embedding)임베딩은 딥러닝에서 주로 사용되며, 범주형 데이터를 벡터 공간에 매핑하여 변환합니다. 이는 특히 고차원 범주형 데이터에 유용합니다.임베딩의 장점원-핫 인코딩보다 차원 축소와 메모리 절약 효과가 있으며, 범주 간의 내재된 관계를 학습할 수 있습니다.임베딩은 주로 텍스트 데이터에서 단어를 벡터로 변환할 때 사용되며, Keras 등의 라이브러리에서 쉽게 구현할 수 있습니다.요약요약인코딩은 범주형 데이터를 수치형 데이터로 변환하는 과정으로, 머신러닝 모델에서 데이터를 처리하기 위해 필수적입니다.레이블 인코딩은 순서가 있는 범주형 데이터에 적합하며, 원-핫 인코딩은 순서가 없는 범주형 데이터에 사용됩니다.빈도 인코딩은 범주의 출현 빈도를 사용하고, 순서형 인코딩은 순서가 중요한 범주형 데이터에 유용합니다.임베딩은 딥러닝에서 고차원 범주형 데이터를 처리할 때 주로 사용됩니다.이제 적절한 인코딩 기법을 사용하여 데이터 전처리를 완료하고, 모델 학습의 기초를 다져보세요! ​
[스파르타코딩클럽] 15강. 비지도학습 : 군집화모델 - 계층적 군집화[SCC] 바닥부터 시작하는 머신러닝/[스파르타코딩클럽] 바닥부터 시작하는 머신러닝 - 4주차 /[스파르타코딩클럽] 15강. 비지도학습 : 군집화모델 - 계층적 군집화제작:[스파르타코딩클럽] 15강. 비지도학습 : 군집화모델 - 계층적 군집화[수업 목표]비지도학습 군집화모델 중 계층적 군집화 에 대한 개념을 배우고, 데이터를 이용해 실습해 봅니다[목차]01. 계층적 군집화 개념02. 계층적 군집화 실습💡
 
 01. 계층적 군집화 개념✔️계층적 군집화란 무엇인지 알아봅시다1) 계층적 군집화 계층적 군집화란?계층적 군집화(Hierarchical Clustering)는 데이터포인트를 계층 구조로 그룹화하는 방법입니다. 데이터 포인트를 점진적으로 병합하거나 분할하여 군집을 형성합니다계층적 군집화는 주로 두가지 방식으로 나뉩니다계층적 군집화는 주로 두가지 방식으로 나뉩니다﻿
병합 군집화: 각 데이터 포인트를 개별 군집으로 시작하여, 가장 가까운 군집을 반복적으로 병합합니다.병합 군집화: 각 데이터 포인트를 개별 군집으로 시작하여, 가장 가까운 군집을 반복적으로 병합합니다.﻿​분할 군집화: 모든 데이터 포인트를 하나의 군집으로 시작하여, 반복적으로 가장 멀리 떨어진 군집을 분할합니다.분할 군집화: 모든 데이터 포인트를 하나의 군집으로 시작하여, 반복적으로 가장 멀리 떨어진 군집을 분할합니다.﻿​ 계층적 군집화의 작동 원리거리 행렬 계산: 데이터 포인트 간의 거리를 계산하여 거리 행렬을 만듭니다.군집 병합/분할: 거리 행렬을 기반으로 가장 가까운 군집을 병합하거나, 가장 멀리 떨어진 군집을 분할합니다.덴드로그램 생성: 군집화 과정을 시각화한 덴드로그램을 생성합니다.2) 병합 군집화 vs 분할 군집화 병합 군집화(Agglomerative Clustering)병합 군집화는 각 데이터 포인트를 개별 군집으로 시작하여, 가장 가까운 군집을 반복적으로 병합합니다.병합 군집화의 특징은 아래와 같습니다단순성: 구현이 비교적 간단합니다.계산 비용: 데이터 포인트 수가 많아질수록 계산 비용이 증가합니다.덴드로그램: 군집화 과정을 시각화한 덴드로그램을 생성할 수 있습니다. 분할 군집화(Divisive Clustering)분할 군집화는 모든 데이터 포인트를 하나의 군집으로 시작하여, 반복적으로 가장 멀리 떨어진 군집을 분할합니다. 분할 군집화의 주요 특징은 다음과 같습니다:상대적으로 복잡함: 병합 군집화보다 구현이 상대적으로 복잡할 수 있습니다.효율성: 큰 데이터셋에서 병합 군집화보다 효율적일 수 있습니다.덴드로그램: 군집화 과정을 시각화한 덴드로그램을 생성할 수 있습니다.02. 계층적 군집화 실습✔️Kaggle 쇼핑몰 데이터를 이용하여 계층적 군집화 실습을 진행합니다1) 쇼핑몰 데이터 데이터셋 다운로드Kaggle에서 "Mall_Customers.csv" 파일을 다운로드합니다.다운로드한 파일을 작업 디렉토리에 저장합니다. 데이터 로드 및 전처리쇼핑몰 데이터 로드 및 전처리 {5px}쇼핑몰 데이터 로드 및 전처리 ﻿​Python복사import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import AgglomerativeClustering
import scipy.cluster.hierarchy as sch

# 데이터셋 불러오기
df = pd.read_csv('Mall_Customers.csv')
# 데이터 확인
print(df.head())
# 필요한 열만 선택
X = df[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']]
# 데이터 정규화
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
​pandas.read_csv: CSV 파일을 로드하여 데이터프레임 생성’Mall_Customers.csv’: 로드할 파일의 경로입니다.’Mall_Customers.csv’: 로드할 파일의 경로입니다.﻿​pandas.DataFrame.dropna: 결측값이 있는 행을 제거합니다.pandas.DataFrame.map: 데이터 값을 다른 값으로 매핑합니다.sklearn.preprocessing.StandardScaler: 데이터의 평균을 0, 분산을 1로 스케일링fit_transform(X_train): 훈련 세트를 스케일링하고 변환합니다._transform(X_train): 훈련 세트를 스케일링하고 변환합니다.﻿​ 덴드로그램 생성덴드로그램 생성 {5px}덴드로그램 생성 ﻿​Python복사# 덴드로그램 생성
plt.figure(figsize=(10, 7))
dendrogram = sch.dendrogram(sch.linkage(X_scaled, method='ward'))
plt.title('Dendrogram')
plt.xlabel('Customers')
plt.ylabel('Euclidean distances')
plt.show()
​sklearn.cluster.KMeans: k-means 군집화 모델을 생성합니다n_clusters=k: 군집의 수를 설정합니다._clusters=k: 군집의 수를 설정합니다.﻿​random_state=42: 랜덤 시드 값으로, 결과의 재현성을 위해 사용됩니다._state=42: 랜덤 시드 값으로, 결과의 재현성을 위해 사용됩니다.﻿​fit(data_scaled): 데이터를 학습하여 군집을 형성합니다.(data_scaled): 데이터를 학습하여 군집을 형성합니다.﻿​labels_: 각 데이터 포인트가 속한 군집 레이블을 반환합니다._: 각 데이터 포인트가 속한 군집 레이블을 반환합니다.﻿​ 계층적 군집화 모델 구현덴드로그램을 통해 최적의 군집 수를 결정한 후, 계층적 군집화 모델을 구현합니다.계층적 군집화 모델 구현 {5px}계층적 군집화 모델 구현 ﻿​Python복사# 계층적 군집화 모델 생성
hc = AgglomerativeClustering(n_clusters=5, metric='euclidean', linkage='ward')
# 모델 학습 및 예측
y_hc = hc.fit_predict(X_scaled)
# 결과 시각화
plt.figure(figsize=(10, 7))
plt.scatter(X_scaled[y_hc == 0, 0], X_scaled[y_hc == 0, 1], s=100, c='red', label='Cluster 1')
plt.scatter(X_scaled[y_hc == 1, 0], X_scaled[y_hc == 1, 1], s=100, c='blue', label='Cluster 2')
plt.scatter(X_scaled[y_hc == 2, 0], X_scaled[y_hc == 2, 1], s=100, c='green', label='Cluster 3')
plt.scatter(X_scaled[y_hc == 3, 0], X_scaled[y_hc == 3, 1], s=100, c='cyan', label='Cluster 4')
plt.scatter(X_scaled[y_hc == 4, 0], X_scaled[y_hc == 4, 1], s=100, c='magenta', label='Cluster 5')
plt.title('Clusters of customers')
plt.xlabel('Age')
plt.ylabel('Annual Income (k$)')
plt.legend()
plt.show()
​matplotlib.pyplot.plot: 그래프를 그립니다.K, inertia, ’bx-’: x축, y축 데이터와 그래프 스타일을 설정합니다., inertia, ’bx-’: x축, y축 데이터와 그래프 스타일을 설정합니다.﻿​seaborn.scatterplot: 산점도를 그립니다.x=data[’Age’]: x축 데이터=data[’Age’]: x축 데이터﻿​y=data[’Annual Income (k$)’]: y축 데이터=data[’Annual Income (k$)’]: y축 데이터﻿​hue=data[’Cluster’]: 색상에 따라 군집을 구분합니다.=data[’Cluster’]: 색상에 따라 군집을 구분합니다.﻿​palette=’viridis’: 색상 팔레트를 설정합니다.=’viridis’: 색상 팔레트를 설정합니다.﻿​ 모델 평가모델 평가 {5px}모델 평가 ﻿​Python복사from sklearn.metrics import silhouette_score

# 실루엣 점수 계산
silhouette_avg = silhouette_score(X_scaled, y_hc)
print(f'Silhouette Score: {silhouette_avg}')
​.
[스파르타코딩클럽] 15. 하이퍼파라미터 튜닝[SCC] 기초가 탄탄한 딥러닝/[스파르타코딩클럽] 기초가 탄탄한 딥러닝 - 6주차/[스파르타코딩클럽] 15. 하이퍼파라미터 튜닝제작:[스파르타코딩클럽] 15. 하이퍼파라미터 튜닝[수업 목표]주요 하이퍼파라미터 종류와 자동화 튜닝방법에 대해 배워봅시다[목차]01. 하이퍼파라미터 튜닝 방법💡
 
 01. 하이퍼파라미터 튜닝 방법✔️하이퍼파라미터란 무엇인지, 주요 하이퍼파라미터엔 어떤것들이 있는지 알아보고, 자동 튜닝기법에 대해 배워봅시다.1) 주요 하이퍼파라미터와 튜닝 방법 하이퍼파라미터란?하이퍼파라미터는 모델 학습 과정에서 사용자가 설정해야 하는 값으로, 모델의 성능에 큰 영향을 미칩니다.ALT 학습률 (Learning Rate)학습률은 모델의 가중치를 업데이트하는 속도를 결정합니다.너무 크면 학습이 불안정해지고, 너무 작으면 학습이 느려집니다.일반적으로 0.1, 0.01, 0.001 등의 값을 시도해볼 수 있습니다. 배치 크기 (Batch Size)배치 크기는 한 번의 업데이트에 사용되는 데이터 샘플의 수를 결정합니다.큰 배치 크기는 학습 속도를 높이지만, 메모리 사용량이 증가합니다.일반적으로 32, 64, 128 등의 값을 시도해볼 수 있습니다. 에포크 수 (Number of Epochs)에포크 수는 전체 데이터셋을 몇 번 반복하여 학습할지를 결정합니다.너무 적으면 과소적합이 발생하고, 너무 많으면 과적합이 발생할 수 있습니다.조기 종료(Early Stopping) 기법을 사용하여 적절한 에포크 수를 결정할 수 있습니다. 모멘텀 (Momentum)모멘텀은 이전 기울기를 현재 기울기에 반영하여, 학습 속도를 높이고 진동을 줄입니다.일반적으로 0.9, 0.99 등의 값을 시도해볼 수 있습니다. 가중치 초기화 (Weight Initialization)가중치 초기화는 모델의 가중치를 초기화하는 방법을 결정합니다.일반적으로 Xavier 초기화, He 초기화 등을 사용합니다.2) 하이퍼파라미터 자동 튜닝 기법 Grid Search하이퍼파라미터의 모든 조합을 시도하여 최적의 값을 찾습니다.계산 비용이 많이 들지만, 모든 조합을 탐색할 수 있습니다. Random Search하이퍼파라미터 공간에서 무작위로 값을 선택하여 최적의 값을 찾습니다.Grid Search보다 계산 비용이 적고, 더 넓은 하이퍼파라미터 공간을 탐색할 수 있습니다. Bayesian Optimization베이지안 최적화는 이전 평가 결과를 바탕으로, 다음 평가할 하이퍼파라미터를 선택합니다.계산 비용이 적고, 효율적으로 최적의 값을 찾을 수 있습니다..
판다스 심화: 멀티 인덱스와 복합 인덱스인공지능을 위한 파이썬 라이브러리/인공지능을 위한 파이썬 라이브러리 - 6주차/판다스 심화: 멀티 인덱스와 복합 인덱스제작:판다스 심화: 멀티 인덱스와 복합 인덱스수업 목표판다스의 멀티 인덱스와 복합 인덱스에 대해서 학습합니다.다차원 데이터의 효과적인 관리 방법에 대해서 학습합니다.목차멀티 인덱스(MultiIndex)란?멀티 인덱스 생성하기멀티 인덱스 데이터 접근하기복합 인덱스(MultiIndex) 활용멀티 인덱스의 응용요약멀티 인덱스(MultiIndex)란?📚멀티 인덱스(MultiIndex)는 하나 이상의 인덱스를 사용하여 데이터프레임의 행과 열을 구조화하는 방법입니다. 이는 다차원 데이터를 보다 효율적으로 관리하고 분석할 수 있게 해줍니다.멀티 인덱스 생성하기set_index()로 멀티 인덱스 설정여러 열을 사용해 멀티 인덱스를 설정할 수 있습니다.Python복사import pandas as pd

# 예시 데이터프레임 생성
data = {
'도시': ['서울', '서울', '부산', '부산'],
'년도': [2021, 2022, 2021, 2022],
'인구수': [9700000, 9720000, 3400000, 3450000]
}
df = pd.DataFrame(data)
# '도시'와 '년도'를 멀티 인덱스로 설정
df_multi_index = df.set_index(['도시', '년도'])
print(df_multi_index)

​결과:Plain Text복사          인구수
도시   년도
서울  2021  9700000
     2022  9720000
부산  2021  3400000
     2022  3450000

​도시와 년도가 인덱스로 설정되어, 데이터가 더 구조화된 형태로 보입니다.pd.MultiIndex.from_tuples()로 멀티 인덱스 생성from_tuples()를 사용해 튜플로 구성된 멀티 인덱스를 생성할 수도 있습니다.Python복사# 멀티 인덱스를 튜플로 직접 생성
index = pd.MultiIndex.from_tuples([('서울', 2021), ('서울', 2022), ('부산', 2021), ('부산', 2022)], names=['도시', '년도'])
# 데이터프레임에 적용
df_multi_index = pd.DataFrame({'인구수': [9700000, 9720000, 3400000, 3450000]}, index=index)
print(df_multi_index)

​결과:Plain Text복사          인구수
도시   년도
서울  2021  9700000
     2022  9720000
부산  2021  3400000
     2022  3450000

​멀티 인덱스 데이터 접근하기loc[]을 사용한 데이터 접근loc[]을 사용해 멀티 인덱스에서 특정 데이터를 선택할 수 있습니다.Python복사# 특정 인덱스의 데이터 선택
print(df_multi_index.loc['서울'])

​결과:Plain Text복사        인구수
년도
2021  9700000
2022  9720000

​하위 레벨까지 지정하여 데이터를 더 구체적으로 접근할 수도 있습니다.Python복사# '서울'의 2021년 데이터 선택
print(df_multi_index.loc[('서울', 2021)])

​결과:Plain Text복사인구수    9700000
Name: (서울, 2021), dtype: int64

​슬라이싱(Slicing)으로 데이터 접근멀티 인덱스는 슬라이싱을 통해 특정 구간의 데이터를 쉽게 선택할 수 있습니다.Python복사# 부산의 모든 데이터를 선택
df_multi_index = df_multi_index.sort_index()
print(df_multi_index.loc['부산':'부산'])

​결과:Plain Text복사          인구수
도시   년도
부산  2021  3400000
     2022  3450000

​xs()를 사용한 멀티 인덱스 교차 선택xs()는 특정 레벨에서 데이터를 선택하거나, 레벨을 넘어서 데이터를 선택할 때 유용합니다.Python복사# '도시' 레벨에서 '서울'의 데이터를 선택
print(df_multi_index.xs('서울', level='도시'))

​결과:Plain Text복사        인구수
년도
2021  9700000
2022  9720000

​복합 인덱스(MultiIndex) 활용인덱스 정렬 및 정렬된 상태 확인멀티 인덱스를 사용한 데이터프레임은 정렬된 상태로 관리하는 것이 일반적입니다. sort_index()로 인덱스를 정렬할 수 있습니다.Python복사# 멀티 인덱스 정렬
df_sorted = df_multi_index.sort_index()
print(df_sorted)

​unstack()과 stack()으로 인덱스 변환unstack()은 멀티 인덱스를 열로 변환합니다. 반대로 stack()은 열을 인덱스로 변환합니다.Python복사# 멀티 인덱스를 열로 변환 (unstack)
df_unstacked = df_multi_index.unstack(level='년도')
print(df_unstacked)

​결과:Plain Text복사         인구수
년도      2021      2022
도시
서울   9700000  9720000
부산   3400000  3450000

​*stack()**을 사용하면, 다시 인덱스로 변환할 수 있습니다.Python복사# 다시 인덱스로 변환 (stack)
df_stacked = df_unstacked.stack()
print(df_stacked)

​멀티 인덱스의 응용그룹화(Grouping)와 함께 사용📚멀티 인덱스는 그룹화와 함께 사용할 때 더 강력해집니다. 그룹화한 데이터를 멀티 인덱스로 변환하여 복잡한 분석을 수행할 수 있습니다.Python복사# 데이터프레임 생성
data = {
'도시': ['서울', '서울', '부산', '부산', '서울', '부산'],
'년도': [2021, 2022, 2021, 2022, 2021, 2022],
'인구수': [9700000, 9720000, 3400000, 3450000, 9800000, 3500000],
'소득': [60000, 62000, 45000, 46000, 63000, 47000]
}
df = pd.DataFrame(data)
# '도시'와 '년도'를 기준으로 그룹화하여 평균 계산
grouped_df = df.groupby(['도시', '년도']).mean()
print(grouped_df)

​결과:Plain Text복사             인구수      소득
도시  년도
부산  2021  3400000  45000
     2022  3450000  46500
서울  2021  9750000  61500
     2022  9720000  62000

​요약📚요약멀티 인덱스(MultiIndex)는 하나 이상의 인덱스를 사용하여 다차원 데이터를 효율적으로 관리하는 방법입니다.멀티 인덱스는 set_index()나 from_tuples()를 사용해 생성할 수 있으며, loc[], xs(), 슬라이싱 등을 통해 데이터를 선택할 수 있습니다.복합 인덱스를 사용하면 데이터를 정렬하거나, unstack()과 stack()을 통해 인덱스를 변환하는 작업이 용이합니다.멀티 인덱스는 그룹화와 같은 복잡한 데이터 분석 작업에도 유용하게 사용됩니다.이제 멀티 인덱스를 활용하여 더 복잡하고 깊이 있는 데이터 분석을 진행해보세요! ​
[스파르타코딩클럽] 16강. 비지도학습 : 군집화모델 - DBSCAN[SCC] 바닥부터 시작하는 머신러닝/[스파르타코딩클럽] 바닥부터 시작하는 머신러닝 - 4주차 /[스파르타코딩클럽] 16강. 비지도학습 : 군집화모델 - DBSCAN제작:[스파르타코딩클럽] 16강. 비지도학습 : 군집화모델 - DBSCAN[수업 목표]비지도학습 군집화모델 중 DBSCAN 에 대한 개념을 배우고, 데이터를 이용해 실습해 봅니다[목차]01. DBSCAN 개념02. DBSCAN 실습
 
 01. DBSCAN 개념DBSCAN이 무엇인지 알아봅시다1) DBSCAN DBSCAN이란?DBSCAN(Density-Based Spatial Clustering of Applications with Noise)은 밀도 기반 군집화 알고리즘입니다DBSCAN은 데이터 밀도가 높은 영역을 군집으로 간주하고, 밀도가 낮은 영역은 노이즈로 처리합니다ALT주요 매개변수주요 매개변수﻿
eps: 두 데이터 포인트가 같은 군집에 속하기 위해 가져야 하는 최대 거리입니다.: 두 데이터 포인트가 같은 군집에 속하기 위해 가져야 하는 최대 거리입니다.﻿​min_samples: 한 군집을 형성하기 위해 필요한 최소 데이터 포인트 수입니다_samples: 한 군집을 형성하기 위해 필요한 최소 데이터 포인트 수입니다﻿​ DBSCAN의 작동 원리임의의 데이터 포인트를 선택합니다.선택한 데이터 포인트의 eps 반경 내에 있는 모든 데이터 포인트를 찾습니다.eps 반경 내의 데이터수 ≥ min_samples : 해당 데이터 포인트를 중심으로 새로운 군집 형성.eps 반경 내의 데이터수 < min_samples : 해당 데이터 포인트를 노이즈로 간주군집에 속한 데이터 포인트에 대해 2~4 단계를 반복합니다.모든 데이터 포인트가 처리될 때까지 이 과정을 반복합니다. DBSCAN의 장점비구형 군집 탐지: DBSCAN은 비구형 군집을 탐지할 수 있습니다.노이즈 처리: DBSCAN은 노이즈를 효과적으로 처리할 수 있습니다.군집 수 자동 결정: DBSCAN은 군집 수를 사전에 지정할 필요가 없습니다.02. DBSCAN 실습Kaggle 쇼핑몰 데이터를 이용하여 DBSCAN 실습을 진행합니다1) 쇼핑몰 데이터 데이터 로드 쇼핑몰 데이터 로드 {5px}쇼핑몰 데이터 로드 ﻿​Python복사import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import AgglomerativeClustering
import scipy.cluster.hierarchy as sch

# 데이터셋 불러오기
df = pd.read_csv('Mall_Customers.csv')
# 데이터 확인
print(df.head())
# 필요한 열만 선택
X = df[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']]
# 데이터 정규화
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
​ DBSCAN 수행Scikit-learn의 DBSCAN을 사용하여 DBSCAN 군집화를 수행합니다.DBSCAN수행 {5px}DBSCAN수행 ﻿​Python복사from sklearn.cluster import DBSCAN
import matplotlib.pyplot as plt
import seaborn as sns

# DBSCAN 모델 생성
dbscan = DBSCAN(eps=5, min_samples=5)
# 모델 학습 및 예측
df['Cluster'] = dbscan.fit_predict(X)
# 군집화 결과 시각화
plt.figure(figsize=(10, 7))
sns.scatterplot(x='Annual Income (k$)', y='Spending Score (1-100)', hue='Cluster', data=df, palette='viridis')
plt.title('DBSCAN Clustering of Mall Customers')
plt.show()
​ 파라미터 튜닝DBSCAN의 성능은 eps와 min_samples 파라미터에 크게 의존합니다. 적절한 파라미터를 찾기 위해 여러 값을 시도해볼 수 있습니다.파라미터 튜닝 {5px}파라미터 튜닝 ﻿​Python복사# 다양한 eps와 min_samples 값 시도
eps_values = [3, 5, 7, 10]
min_samples_values = [3, 5, 7, 10]
for eps in eps_values:
for min_samples in min_samples_values:
        dbscan = DBSCAN(eps=eps, min_samples=min_samples)
        df['Cluster'] = dbscan.fit_predict(X)
        
        plt.figure(figsize=(10, 7))
        sns.scatterplot(x='Annual Income (k$)', y='Spending Score (1-100)', hue='Cluster', data=df, palette='viridis')
        plt.title(f'DBSCAN Clustering (eps={eps}, min_samples={min_samples})')
        plt.show()
​.
[스파르타코딩클럽] 16. 모델 평가와 검증 및 Pytorch 문법 정리[SCC] 기초가 탄탄한 딥러닝/[스파르타코딩클럽] 기초가 탄탄한 딥러닝 - 6주차/[스파르타코딩클럽] 16. 모델 평가와 검증 및 Pytorch 문법 정리제작:[스파르타코딩클럽] 16. 모델 평가와 검증 및 Pytorch 문법 정리[수업 목표]교차검증이 무엇인지, 왜 필요한지 알아봅시다[목차]01. 교차검증 02. Pytorch 문법 정리 💡
 
 01. 교차검증 ✔️교차검증이 무엇인지, 왜 필요한지 알아보고 교차검증의 한 종류인 K-Fold 교차검증에 대해 배워봅시다1) 교차검증 개념 및 필요성 교차검증 이란?교차 검증(Cross-Validation)은 모델의 일반화 성능을 평가하기 위해 데이터를 여러 번 나누어 학습과 검증을 반복하는 방법입니다교차 검증은 모델이 과적합(overfitting)되지 않고, 새로운 데이터에 대해 잘 일반화되는지 평가하는 데 유용합니다.ALT - 데이터를 여러 개의 폴드(fold)로 나누고, 각 폴드에 대해 학습과 검증을 반복합니다. - 데이터를 여러 개의 폴드(fold)로 나누고, 각 폴드에 대해 학습과 검증을 반복합니다.﻿
 - 각 폴드가 한 번씩 검증 데이터로 사용되며, 나머지 폴드는 학습 데이터로 사용됩니다.{
- 각 폴드가 한 번씩 검증 데이터로 사용되며, 나머지 폴드는 학습 데이터로 사용됩니다.} - 각 폴드가 한 번씩 검증 데이터로 사용되며, 나머지 폴드는 학습 데이터로 사용됩니다.﻿
 - 모든 폴드에 대한 검증 결과를 평균하여 모델의 성능을 평가합니다.{
- 모든 폴드에 대한 검증 결과를 평균하여 모델의 성능을 평가합니다.} - 모든 폴드에 대한 검증 결과를 평균하여 모델의 성능을 평가합니다.﻿​ 교차검증의 필요성과적합 방지: 교차 검증은 모델이 특정 데이터셋에 과적합되지 않도록 도와줍니다.일반화 성능 평가: 교차 검증은 모델의 일반화 성능을 더 정확하게 평가할 수 있습니다.데이터 효율성: 교차 검증은 데이터를 최대한 활용하여 모델을 평가할 수 있습니다.2) K-Fold 교차 검증 K-Fold 교차 검증의 원리데이터를 K개의 폴드로 나눕니다.각 폴드가 한 번씩 검증 데이터로 사용되며, 나머지 K-1개의 폴드는 학습 데이터로 사용됩니다.K번의 학습과 검증을 반복하여, 각 폴드에 대한 검증 결과를 평균하여 모델의 성능을 평가합니다. 적용 방법데이터를 K개의 폴드로 나누고, 각 폴드에 대해 학습과 검증을 수행합니다.각 폴드에 대한 검증 결과를 저장하고, 최종적으로 평균하여 모델의 성능을 평가합니다.02. Pytorch 문법 정리 ✔️한번 문법을 살펴볼까요?1) Pytorch PytorchPyTorch는 딥러닝 프레임워크로, 유연성과 사용 편의성을 제공하여 연구와 개발에서 널리 사용되고 있습니다. PyTorch의 주요 API를 기법별, 모델별, 기능별로 정리하겠습니다.PyTorch의 주요 API 정리1. 모델 구축 및 학습기본 모델 구축torch.nn.Module: 모든 신경망 모델의 기본 클래스입니다.Python복사import torch.nn as nn

class MyModel(nn.Module):
def __init__(self):
super(MyModel, self).__init__()
        self.layer1 = nn.Linear(10, 20)
def forward(self, x):
        x = self.layer1(x)
return x

​손실 함수torch.nn.CrossEntropyLoss: 분류 문제에 주로 사용됩니다.Python복사loss_fn = nn.CrossEntropyLoss()

​torch.nn.MSELoss: 회귀 문제에 주로 사용됩니다.Python복사loss_fn = nn.MSELoss()

​최적화 알고리즘torch.optim.SGD: 확률적 경사 하강법 최적화 알고리즘입니다.Python복사optimizer = torch.optim.SGD(model.parameters(), lr=0.01)

​torch.optim.Adam: Adam 최적화 알고리즘입니다.Python복사optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

​2. 데이터 로드 및 전처리데이터셋 및 데이터로더torch.utils.data.Dataset: 사용자 정의 데이터셋을 만들기 위한 기본 클래스입니다.Python복사from torch.utils.data import Dataset

class MyDataset(Dataset):
def __init__(self, data, targets):
        self.data = data
        self.targets = targets

    def __len__(self):
return len(self.data)
def __getitem__(self, idx):
return self.data[idx], self.targets[idx]

​torch.utils.data.DataLoader: 미니 배치 학습을 위한 데이터 로더입니다.Python복사from torch.utils.data import DataLoader

dataset = MyDataset(data, targets)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)

​데이터 변환torchvision.transforms: 이미지 데이터 변환을 위한 유틸리티입니다.Python복사from torchvision import transforms

transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])

​3. GPU 사용GPU 설정 및 텐서 이동모델을 GPU로 이동Python복사device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

​텐서를 GPU로 이동Python복사inputs, targets = inputs.to(device), targets.to(device)

​4. 모델 기법별 API합성곱 신경망 (CNN)torch.nn.Conv2d: 2D 합성곱 레이어입니다.Python복사conv_layer = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)

​순환 신경망 (RNN)torch.nn.RNN: 기본 순환 신경망 레이어입니다.Python복사rnn_layer = nn.RNN(input_size=10, hidden_size=20, num_layers=2, batch_first=True)

​torch.nn.LSTM: LSTM 레이어입니다.Python복사lstm_layer = nn.LSTM(input_size=10, hidden_size=20, num_layers=2, batch_first=True)

​torch.nn.GRU: GRU 레이어입니다.Python복사gru_layer = nn.GRU(input_size=10, hidden_size=20, num_layers=2, batch_first=True)

​트랜스포머 (Transformer)torch.nn.Transformer: 트랜스포머 모델입니다.Python복사transformer_model = nn.Transformer(nhead=8, num_encoder_layers=6)

​torch.nn.TransformerEncoderLayer: 트랜스포머 인코더 레이어입니다.Python복사encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)

​5. 유틸리티 함수저장 및 로드모델 저장Python복사torch.save(model.state_dict(), 'model.pth')

​모델 로드Python복사model.load_state_dict(torch.load('model.pth'))
model.eval()

​학습 및 평가 모드 설정모델을 학습 모드로 설정Python복사model.train()

​모델을 평가 모드로 설정Python복사model.eval()

​.
판다스 심화: 데이터프레임 구조화 다시하기와 크기 조정하기인공지능을 위한 파이썬 라이브러리/인공지능을 위한 파이썬 라이브러리 - 6주차/판다스 심화: 데이터프레임 구조화 다시하기와 크기 조정하기제작:판다스 심화: 데이터프레임 구조화 다시하기와 크기 조정하기수업 목표판다스에서 데이터를 원하는 형태로 재구성하고 크기를 조정하는 방법에 대해 학습합니다.목차데이터프레임 구조화 다시하기데이터프레임 크기 조정하기요약데이터프레임 구조화 다시하기데이터프레임의 구조를 재조정하여, 데이터를 원하는 형태로 변형할 수 있습니다. 주요 작업으로는 피벗(pivot), 변경(melt), 스택(stack)과 언스택(unstack)이 있습니다.pivot()을 사용한 피벗 테이블 생성pivot() 함수는 열 데이터를 행 또는 열로 이동시켜 새로운 데이터프레임을 만듭니다. 이는 데이터를 재구성하고 분석하는 데 매우 유용합니다.Python복사import pandas as pd

# 예시 데이터프레임 생성
data = {
'날짜': ['2023-01-01', '2023-01-02', '2023-01-01', '2023-01-02'],
'도시': ['서울', '서울', '부산', '부산'],
'온도': [2, 3, 6, 7],
'습도': [55, 60, 80, 85]
}
df = pd.DataFrame(data)
# '도시'를 기준으로 '날짜'를 인덱스로, '온도'를 값으로 하는 피벗 테이블 생성
pivot_df = df.pivot(index='날짜', columns='도시', values='온도')
print(pivot_df)

​결과:Plain Text복사도시        부산  서울
날짜
2023-01-01  6  2
2023-01-02  7  3

​날짜를 인덱스로, 도시를 열로 하여 각 온도 값을 피벗 테이블로 변환했습니다.melt()를 사용한 데이터 구조 해체melt() 함수는 피벗된 데이터를 다시 긴 형식(long format)으로 변환할 때 사용됩니다. 이는 여러 열을 하나의 열로 통합하는 데 유용합니다.Python복사# 데이터프레임 구조 해체 (melt)
melted_df = pd.melt(df, id_vars=['날짜', '도시'], value_vars=['온도', '습도'])
print(melted_df)

​결과:Plain Text복사          날짜  도시  variable  value
0  2023-01-01  서울     온도      2
1  2023-01-02  서울     온도      3
2  2023-01-01  부산     온도      6
3  2023-01-02  부산     온도      7
4  2023-01-01  서울     습도     55
5  2023-01-02  서울     습도     60
6  2023-01-01  부산     습도     80
7  2023-01-02  부산     습도     85

​온도와 습도 열이 하나의 열(variable)로 통합되어 melt()되었습니다.stack()과 unstack()을 사용한 데이터 변환stack()은 열 데이터를 인덱스의 하위 레벨로 이동시키고, unstack()은 그 반대 작업을 수행합니다. 이는 멀티 인덱스 데이터프레임에서 유용합니다.Python복사# '도시' 레벨을 인덱스로 스택(stack)
stacked_df = pivot_df.stack()
print(stacked_df)

​결과:Plain Text복사날짜          도시
2023-01-01  부산    6
            서울    2
2023-01-02  부산    7
            서울    3
dtype: int64

​도시가 인덱스로 변환되었습니다.Python복사# 다시 언스택(unstack)하여 원래 구조로 복원
unstacked_df = stacked_df.unstack()
print(unstacked_df)

​결과는 원래의 피벗 테이블 형태로 돌아옵니다.데이터프레임 크기 조정하기데이터프레임의 크기를 조정하는 방법에는 행과 열의 추가, 삭제, 데이터 병합 등이 있습니다. 이를 통해 데이터를 분석하기에 적합한 형태로 조정할 수 있습니다.행과 열 추가하기새로운 열을 추가할 때는 새로운 데이터를 할당하면 됩니다. 예를 들어, df['새로운 열'] = 값의 형태로 추가할 수 있습니다.Python복사# 새로운 열 추가
df['날씨'] = ['맑음', '흐림', '맑음', '흐림']
print(df)

​결과:Plain Text복사          날짜  도시  온도  습도  날씨
0  2023-01-01  서울    2   55  맑음
1  2023-01-02  서울    3   60  흐림
2  2023-01-01  부산    6   80  맑음
3  2023-01-02  부산    7   85  흐림

​새로운 열(날씨)이 추가되었습니다.행과 열 삭제하기행과 열 삭제는 drop() 함수를 사용합니다. axis=0은 행을, axis=1은 열을 삭제합니다.Python복사# '습도' 열 삭제
df_dropped = df.drop(columns=['습도'])
print(df_dropped)

​결과:Plain Text복사          날짜  도시  온도  날씨
0  2023-01-01  서울    2  맑음
1  2023-01-02  서울    3  흐림
2  2023-01-01  부산    6  맑음
3  2023-01-02  부산    7  흐림

​습도 열이 삭제되었습니다.Python복사# 특정 행 삭제 (예: 첫 번째 행)
df_dropped_row = df.drop(index=0)
print(df_dropped_row)

​결과:Plain Text복사          날짜  도시  온도  습도  날씨
1  2023-01-02  서울    3   60  흐림
2  2023-01-01  부산    6   80  맑음
3  2023-01-02  부산    7   85  흐림

​첫 번째 행이 삭제되었습니다.데이터 병합하기데이터프레임을 병합할 때는 concat()과 merge()를 사용합니다. 이들은 데이터를 추가하거나 병합하는 데 사용됩니다.Python복사# 새로운 데이터프레임 생성
data2 = {
'날짜': ['2023-01-03', '2023-01-04'],
'도시': ['서울', '부산'],
'온도': [5, 8],
'습도': [70, 75],
'날씨': ['맑음', '흐림']
}
df2 = pd.DataFrame(data2)
# 행을 기준으로 데이터프레임 병합 (concat)
merged_df = pd.concat([df, df2], ignore_index=True)
print(merged_df)

​결과:Plain Text복사          날짜  도시  온도  습도  날씨
0  2023-01-01  서울    2   55  맑음
1  2023-01-02  서울    3   60  흐림
2  2023-01-01  부산    6   80  맑음
3  2023-01-02  부산    7   85  흐림
4  2023-01-03  서울    5   70  맑음
5  2023-01-04  부산    8   75  흐림

​새로운 데이터가 병합되어 데이터프레임의 크기가 확장되었습니다.요약요약데이터프레임 구조화 다시하기에는 pivot(), melt(), stack(), unstack() 등이 사용되며, 이를 통해 데이터를 다양한 방식으로 변형하고 재구성할 수 있습니다.크기 조정에는 새로운 행과 열을 추가하거나, drop()을 사용해 삭제하는 방법이 있으며, concat()과 merge()를 통해 데이터를 병합할 수 있습니다.이러한 기능들은 데이터를 더 잘 분석할 수 있도록 적절한 형태로 변환하고 조정하는 데 필수적입니다.이제 판다스를 활용하여 데이터프레임을 원하는 형태로 구조화하고 크기를 조정하여 분석을 더욱 효과적으로 진행해보세요! ​
[스파르타코딩클럽] 17강. 비지도학습 : 차원축소 - PCA [SCC] 바닥부터 시작하는 머신러닝/[스파르타코딩클럽] 바닥부터 시작하는 머신러닝 - 4주차 /[스파르타코딩클럽] 17강. 비지도학습 : 차원축소 - PCA 제작:[스파르타코딩클럽] 17강. 비지도학습 : 차원축소 - PCA [수업 목표]비지도학습 차원축소 중 PCA 에 대한 개념을 배우고, 데이터를 이용해 실습해 봅니다[목차]01. PCA 개념02. PCA 실습
 
 01. PCA 개념PCA가 무엇인지 알아봅시다1) PCA PCA란?PCA(Principal Component Analysis, 주성분 분석)는 고차원 데이터를 저차원으로 변환하는 차원 축소 기법입니다PCA는 데이터의 분산을 최대한 보존하면서, 데이터의 주요 특징을 추출해 저차원 공간으로 변환합니다데이터의 시각화, 노이즈 제거, 계산 효율성 향상 등의 이점을 얻을 수 있습니다.ALT PCA의 작동 원리데이터 표준화: 각 특성의 평균을 0, 분산을 1로 맞춥니다.공분산 행렬 계산: 데이터의 공분산 행렬을 계산합니다.고유값 및 고유벡터 계산: 공분산 행렬의 고유값과 고유벡터를 계산합니다.주성분 선택: 고유값이 큰 순서대로 고유벡터를 정렬하여 주성분을 선택합니다.데이터 변환: 선택된 주성분을 사용하여 데이터를 저차원 공간으로 변환합니다.2) 공분산 행렬 및 주성분 선택 공분산 행렬공분산 행렬은 데이터의 각 특성 간의 공분산을 나타내는 행렬입니다. 공분산 행렬을 통해 데이터의 분산과 특성 간의 상관관계를 파악할 수 있습니다. 주성분 선택고유값이 큰 순서대로 고유벡터를 정렬하여 주성분을 선택합니다.고유값이 클수록 해당 주성분이 데이터의 분산을 더 많이 설명합니다.일반적으로, 전체 분산의 95% 이상을 설명하는 주성분을 선택합니다.02. PCA 실습MNIST 데이터셋을 사용한 PCA 실습을 진행합니다1) MNIST 데이터셋 데이터 로드 MNIST 데이터 로드 {5px}MNIST 데이터 로드 ﻿​Python복사from sklearn.datasets import fetch_openml
import pandas as pd

# MNIST 데이터셋 불러오기
mnist = fetch_openml('mnist_784', version=1)
# 데이터와 레이블 분리
X = mnist.data
y = mnist.target

# 데이터 프레임의 첫 5행 출력
print(X.head())
print(y.head())
​ 데이터 표준화PCA를 수행하기 전에 데이터를 표준화합니다.데이터 표준화 {5px}데이터 표준화 ﻿​Python복사from sklearn.preprocessing import StandardScaler

# 데이터 표준화
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
​ PCA 수행Scikit-learn의 PCA를 사용하여 PCA를 수행합니다.PCA수행 {5px}PCA수행 ﻿​Python복사from sklearn.decomposition import PCA

# PCA 모델 생성
pca = PCA(n_components=0.95) # 전체 분산의 95%를 설명하는 주성분 선택
# PCA 학습 및 변환
X_pca = pca.fit_transform(X_scaled)
# 변환된 데이터의 크기 확인
print(X_pca.shape)
​ 주성분 확인선택된 주성분의 수와 각 주성분이 설명하는 분산 비율을 확인합니다.주성분 확인 {5px}주성분 확인 ﻿​Python복사# 선택된 주성분의 수
print(f'선택된 주성분의 수: {pca.n_components_}')
# 각 주성분이 설명하는 분산 비율
print(f'각 주성분이 설명하는 분산 비율: {pca.explained_variance_ratio_}')
# 누적 분산 비율
print(f'누적 분산 비율: {pca.explained_variance_ratio_.cumsum()}')
​ PCA 결과 시각화PCA 결과를 2차원 또는 3차원으로 시각화합니다.PCA 결과 시각화 {5px}PCA 결과 시각화 ﻿​Python복사import matplotlib.pyplot as plt
import seaborn as sns

# 2차원 시각화
plt.figure(figsize=(10, 7))
sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=y, palette='viridis', legend=None)
plt.title('PCA of MNIST Dataset (2D)')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.show()

​.
[스파르타코딩클럽] 18강. 비지도학습 : 차원축소 - t-SNE[SCC] 바닥부터 시작하는 머신러닝/[스파르타코딩클럽] 바닥부터 시작하는 머신러닝 - 4주차 /[스파르타코딩클럽] 18강. 비지도학습 : 차원축소 - t-SNE제작:[스파르타코딩클럽] 18강. 비지도학습 : 차원축소 - t-SNE[수업 목표]비지도학습 차원축소 중 t-SNE 에 대한 개념을 배우고, 데이터를 이용해 실습해 봅니다[목차]01. t-SNE 개념02. t-SNE 실습💡
 
 01. t-SNE 개념✔️t-SNE가 무엇인지 알아봅시다1) t-SNE t-SNE란?t-SNE(t-Distributed Stochastic Neighbor Embedding)는 고차원 데이터를 저차원으로 변환하여 시각화하는 차원 축소 기법입니다.데이터 포인트 간의 유사성을 보존하면서, 고차원 데이터를 2차원 또는 3차원 공간으로 변환합니다데이터의 구조와 패턴을 시각적으로 이해할 수 있습니다. t-SNE의 작동 원리고차원 공간에서의 유사성 계산: 고차원 데이터 포인트 간의 유사성을 확률 분포로 계산합니다.저차원 공간에서의 유사성 계산: 저차원 데이터 포인트 간의 유사성을 t-분포를 사용하여 계산합니다.KL 발산 최소화: 고차원 공간과 저차원 공간 간의 유사성 분포 차이를 KL 발산(Kullback-Leibler divergence)을 통해 최소화합니다.반복적 최적화: 저차원 공간에서의 데이터 포인트 위치를 반복적으로 조정하여 최적의 시각화를 얻습니다. t-SNE의 장점비선형 구조 탐지: t-SNE는 비선형 구조를 효과적으로 탐지할 수 있습니다.클러스터 시각화: t-SNE는 데이터의 클러스터를 명확하게 시각화할 수 있습니다.고차원 데이터 시각화: t-SNE는 고차원 데이터를 2차원 또는 3차원으로 변환하여 시각화할 수 있습니다.02. t-SNE 실습✔️MNIST 데이터셋을 사용한 t-SNE 실습을 진행합니다1) MNIST 데이터셋 데이터 로드 MNIST 데이터 로드 {5px}MNIST 데이터 로드 ﻿​Python복사from sklearn.datasets import fetch_openml
import pandas as pd

# MNIST 데이터셋 불러오기
mnist = fetch_openml('mnist_784', version=1)
# 데이터와 레이블 분리
X = mnist.data
y = mnist.target

# 데이터 프레임의 첫 5행 출력
print(X.head())
print(y.head())
​ 데이터 표준화데이터 표준화 {5px}데이터 표준화 ﻿​Python복사from sklearn.preprocessing import StandardScaler

# 데이터 표준화
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
​ t-SNE 수행Scikit-learn의 TSNE를 사용하여 t-SNE를 수행합니다.t-SNE수행 {5px}t-SNE수행 ﻿​Python복사from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import seaborn as sns

# t-SNE 모델 생성
tsne = TSNE(n_components=2, random_state=42)
# t-SNE 학습 및 변환
X_tsne = tsne.fit_transform(X_scaled)
# 변환된 데이터의 크기 확인
print(X_tsne.shape)
​ t-SNE 결과 시각화t-SNE 결과를 2차원으로 시각화합니다.t-SNE 결과 시각화 {5px}t-SNE 결과 시각화 ﻿​Python복사# 2차원 시각화
plt.figure(figsize=(10, 7))
sns.scatterplot(x=X_tsne[:, 0], y=X_tsne[:, 1], hue=y, palette='viridis', legend=None)
plt.title('t-SNE of MNIST Dataset (2D)')
plt.xlabel('t-SNE Component 1')
plt.ylabel('t-SNE Component 2')
plt.show()
​.t-SNE는 데이터 포인트 간의 유사성을 보존하면서, 고차원 데이터를 2차원 또는 3차원 공간으로 변환합니다
[스파르타코딩클럽] 19강. 비지도학습 : 차원축소 - LDA[SCC] 바닥부터 시작하는 머신러닝/[스파르타코딩클럽] 바닥부터 시작하는 머신러닝 - 4주차 /[스파르타코딩클럽] 19강. 비지도학습 : 차원축소 - LDA제작:[스파르타코딩클럽] 19강. 비지도학습 : 차원축소 - LDA[수업 목표]비지도학습 차원축소 중 LDA에 대한 개념을 배우고, 데이터를 이용해 실습해 봅니다[목차]01. LDA 개념02. LDA실습💡
 
 01. LDA 개념✔️LDA가 무엇인지 알아봅시다1) LDA LDA란?LDA(Linear Discriminant Analysis, 선형 판별 분석)는 차원 축소와 분류를 동시에 수행합니다.LDA는 데이터의 클래스 간 분산을 최대화하고, 클래스 내 분산을 최소화하는 방향으로 데이터를 변환합니다데이터의 분류 성능을 향상시키고, 저차원 공간에서 데이터의 구조를 시각화할 수 있습니다.ALT LDA의 작동 원리클래스별 평균 계산: 각 클래스의 평균 벡터를 계산합니다.클래스 내 분산 행렬 계산: 각 클래스 내 데이터 포인트의 분산을 계산해 클래스 내 분산 행렬을 만듭니다.클래스 간 분산 행렬 계산: 클래스 간 평균 벡터의 분산을 계산하여 클래스 간 분산 행렬을 만듭니다.고유값 및 고유벡터 계산: 클래스 내 분산 행렬의 역행렬과 클래스 간 분산 행렬의 곱의 고유값과 고유벡터를 계산합니다.선형 판별 축 선택: 고유값이 큰 순서대로 고유벡터를 정렬하여 선형 판별 축을 선택합니다.데이터 변환: 선택된 선형 판별 축을 사용하여 데이터를 저차원 공간으로 변환합니다. 선형 판별 축 선택고유값이 큰 순서대로 고유벡터를 정렬하여 선형 판별 축을 선택합니다고유값이 클수록 해당 선형 판별 축이 클래스 간 분산을 더 많이 설명합니다일반적으로, 클래스의 수 - 1 만큼의 선형 판별 축을 선택합니다02. LDA실습✔️MNIST 데이터셋을 사용한 LDA 실습을 진행합니다1) MNIST 데이터셋 데이터 로드 MNIST 데이터 로드 {5px}MNIST 데이터 로드 ﻿​Python복사from sklearn.datasets import fetch_openml
import pandas as pd

# MNIST 데이터셋 불러오기
mnist = fetch_openml('mnist_784', version=1)
# 데이터와 레이블 분리
X = mnist.data
y = mnist.target

# 데이터 프레임의 첫 5행 출력
print(X.head())
print(y.head())
​ 데이터 표준화데이터 표준화 {5px}데이터 표준화 ﻿​Python복사from sklearn.preprocessing import StandardScaler

# 데이터 표준화
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
​ LDA 수행Scikit-learn의 LinearDiscriminantAnalysis를 사용하여 LDA를 수행합니다.LDA 수행 {5px}LDA 수행 ﻿​Python복사from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

# LDA 모델 생성
lda = LinearDiscriminantAnalysis(n_components=9) # 클래스의 수 - 1 만큼의 선형 판별 축 선택
# LDA 학습 및 변환
X_lda = lda.fit_transform(X_scaled, y)
# 변환된 데이터의 크기 확인
print(X_lda.shape)
​ LDA 결과 시각화LDA 결과를 2차원 또는 3차원으로 시각화합니다.LDA 결과 시각화 {5px}LDA 결과 시각화 ﻿​Python복사import matplotlib.pyplot as plt
import seaborn as sns

# 2차원 시각화
plt.figure(figsize=(10, 7))
sns.scatterplot(x=X_lda[:, 0], y=X_lda[:, 1], hue=y, palette='viridis', legend=None)
plt.title('LDA of MNIST Dataset (2D)')
plt.xlabel('LDA Component 1')
plt.ylabel('LDA Component 2')
plt.show()

​.
[스파르타코딩클럽] 20강. 앙상블 학습 - 배깅과 부스팅[SCC] 바닥부터 시작하는 머신러닝/[스파르타코딩클럽] 바닥부터 시작하는 머신러닝 - 5주차/[스파르타코딩클럽] 20강. 앙상블 학습 - 배깅과 부스팅제작:[스파르타코딩클럽] 20강. 앙상블 학습 - 배깅과 부스팅[수업 목표]앙상블 학습이란 무엇인지 알아보고, 앙상블 학습의 배깅과 부스팅에 대해 배워봅시다[목차]01. 앙상블 학습02.배깅과 부스팅 모델 구현 및 평가
 
 01. 앙상블 학습앙상블 학습이란 무엇인지 배우고,  앙상블 학습의 기법인 배깅과 부스팅에 대해 알아 봅시다1) 앙상블 학습 앙상블 학습이란?앙상블 학습(Ensemble Learning)은 여러 개의 학습 모델을 결합하여 하나의 강력한 모델을 만드는 기법입니다앙상블 학습은 개별 모델의 예측을 결합함으로써, 단일 모델보다 더 높은 예측 성능과 일반화 능력을 얻을 수 있습니다.앙상블 학습의 주요 기법으로는 배깅(Bagging)과 부스팅(Boosting)이 있습니다.ALT2) 배깅 : 다수결 원리 배깅이란?배깅(Bootstrap Aggregating)은 여러 개의 학습 모델을 병렬로 학습시키고, 그 예측 결과를 평균 또는 다수결로 결합하는 앙상블 기법입니다.배깅은 데이터의 샘플링 과정에서 부트스트래핑(bootstrap) 기법을 사용하여, 원본 데이터셋에서 중복을 허용한 무작위 샘플을 생성합니다. 각 모델은 서로 다른 데이터 샘플을 학습하게 되어, 모델 간의 상관성을 줄이고 예측 성능을 향상시킵니다. 배깅의 장점과적합 감소: 여러 모델의 예측을 결합함으로써 과적합을 줄일 수 있습니다.안정성 향상: 데이터의 변동에 덜 민감해집니다.병렬 처리 가능: 각 모델을 독립적으로 학습시킬 수 있어 병렬 처리가 가능합니다.3) 부스팅: 약한 학습기를 결합한 강한 학습기 부스팅이란?부스팅(Boosting)은 여러 개의 약한 학습기(weak learner)를 순차적으로 학습시키고, 그 예측 결과를 결합하여 강한 학습기(strong learner)를 만드는 앙상블 기법입니다부스팅은 이전 모델이 잘못 예측한 데이터 포인트에 가중치를 부여하여, 다음 모델이 이를 더 잘 학습하도록 합니다.  부스팅의 장점높은 예측 성능: 약한 학습기를 결합하여 높은 예측 성능을 얻을 수 있습니다.과적합 방지: 모델의 복잡도를 조절하여 과적합을 방지할 수 있습니다.순차적 학습: 이전 모델의 오류를 보완하는 방식으로 학습이 진행됩니다.02.배깅과 부스팅 모델 구현 및 평가유방암 데이터셋을 이용하여 배깅과 부스팅모델을 구현하고 평가하는 실습을 합니다1) 유방암 데이터셋 데이터 로드 유방암 데이터셋 로드 {5px}유방암 데이터셋 로드 ﻿​Python복사from sklearn.ensemble import BaggingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# 유방암 데이터 로드
cancer_data = load_breast_cancer()
X, y = cancer_data.data, cancer_data.target

# 데이터 분할
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# 데이터 스케일링
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
​ 배깅 모델 구현 및 평가Scikit-learn의 BaggingRegressor를 사용하여 배깅 모델을 구현하고 평가합니다.배깅 모델 구현 및 평가 {5px}배깅 모델 구현 및 평가 ﻿​Python복사
# 배깅 모델 생성
bagging_model = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=100, random_state=42)
# 모델 학습
bagging_model.fit(X_train_scaled, y_train)
# 예측
y_pred_bagging = bagging_model.predict(X_test_scaled)
# 평가
mse_bagging = mean_squared_error(y_test, y_pred_bagging)
print(f'배깅 모델의 MSE: {mse_bagging}')
​ 부스팅 모델 구현 및 평가Scikit-learn의 GradientBoostingRegressor를 사용하여 부스팅 모델을 구현하고 평가합니다.부스팅 모델 구현 및 평가{5px}부스팅 모델 구현 및 평가﻿​Python복사from sklearn.ensemble import GradientBoostingRegressor

# 부스팅 모델 생성
boosting_model = GradientBoostingRegressor(n_estimators=100, random_state=42)
# 모델 학습
boosting_model.fit(X_train_scaled, y_train)
# 예측
y_pred_boosting = boosting_model.predict(X_test_scaled)
# 평가
mse_boosting = mean_squared_error(y_test, y_pred_boosting)
print(f'부스팅 모델의 MSE: {mse_boosting}')

​.
[스파르타코딩클럽] 21강. 앙상블 학습 - 랜덤 포레스트[SCC] 바닥부터 시작하는 머신러닝/[스파르타코딩클럽] 바닥부터 시작하는 머신러닝 - 5주차/[스파르타코딩클럽] 21강. 앙상블 학습 - 랜덤 포레스트제작:[스파르타코딩클럽] 21강. 앙상블 학습 - 랜덤 포레스트[수업 목표]앙상블 학습의 랜덤포레스트에 대한 개념을 배우고, 데이터를 이용해 실습해 봅니다[목차]01. 랜덤 포레스트02.랜덤 포레스트 모델 구현 및 평가💡
 
 01. 랜덤 포레스트✔️앙상블 기법 중 하나인 랜덤 포레스트에 대해 배워봅시다1) 랜덤 포레스트 랜덤 포레스트란?랜덤 포레스트(Random Forest)는 배깅(Bagging) 기법을 기반으로 한 앙상블 학습 모델입니다.여러 개의 결정 트리(Decision Tree)를 학습시키고, 그 예측 결과를 결합하여 최종 예측을 수행합니다. 각 트리가 독립적으로 학습되기 때문에, 과적합을 방지하고 예측 성능을 향상시킬 수 있습니다.ALT 랜덤 포레스트의 구조랜덤 포레스트는 여러 개의 결정 트리로 구성됩니다.각 결정 트리는 데이터의 무작위 샘플을 사용하여 학습되며, 트리의 예측 결과를 평균 또는 다수결로 결합하여 최종 예측을 수행합니다. 랜덤 포레스트의 원리부트스트랩 샘플링: 원본 데이터셋에서 중복을 허용한 무작위 샘플을 생성합니다.결정 트리 학습: 각 부트스트랩 샘플을 사용하여 결정 트리를 학습시킵니다. 이때, 각 노드에서 무작위로 선택된 특성의 일부만을 사용하여 분할을 수행합니다.예측 결합: 모든 결정 트리의 예측 결과를 결합하여 최종 예측을 수행합니다. 회귀 문제에서는 평균을 사용하고, 분류 문제에서는 다수결을 사용합니다. 무작위성랜덤 포레스트는 두 가지 무작위성을 도입하여 모델의 다양성을 증가시키고, 과적합을 방지합니다:데이터 샘플링의 무작위성: 각 결정 트리는 원본 데이터셋에서 무작위로 샘플링된 데이터로 학습됩니다.특성 선택의 무작위성: 각 노드에서 분할을 수행할 때, 무작위로 선택된 특성의 일부만을 사용합니다.이러한 무작위성은 모델의 상관성을 줄이고, 예측 성능을 향상시킵니다.02.랜덤 포레스트 모델 구현 및 평가✔️유방암 데이터셋을 이용하여 랜덤포레스트를 구현하고 평가하는 실습을 합니다1)  유방암  데이터셋 데이터 로드 자동차 수요예측 데이터셋 로드 {5px}자동차 수요예측 데이터셋 로드 ﻿​Python복사from sklearn.ensemble import BaggingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# 유방암 데이터 로드
cancer_data = load_breast_cancer()
X, y = cancer_data.data, cancer_data.target

# 데이터 분할
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# 데이터 스케일링
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
​ 랜덤 포레스트 모델 구현 및 평가Scikit-learn의 RandomForestRegressor를 사용하여 랜덤 포레스트 모델을 구현하고 평가합니다.랜덤 포레스트 모델 구현 및 평가 {5px}랜덤 포레스트 모델 구현 및 평가 ﻿​Python복사from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

# 랜덤 포레스트 모델 생성
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
# 모델 학습
rf_model.fit(X_train_scaled, y_train)
# 예측
y_pred_rf = rf_model.predict(X_test_scaled)
# 평가
mse_rf = mean_squared_error(y_test, y_pred_rf)
print(f'랜덤 포레스트 모델의 MSE: {mse_rf}')
​ 중요 특성 확인중요 특성 확인 {5px}중요 특성 확인 ﻿​Python복사import matplotlib.pyplot as plt
import seaborn as sns

# 특성 중요도 추출
feature_importances = rf_model.feature_importances_

# 특성 중요도를 데이터프레임으로 변환
feature_importances_df = pd.DataFrame({
'Feature': X.columns,
'Importance': feature_importances
})
# 중요도 순으로 정렬
feature_importances_df = feature_importances_df.sort_values(by='Importance', ascending=False)
# 특성 중요도 시각화
plt.figure(figsize=(10, 7))
sns.barplot(x='Importance', y='Feature', data=feature_importances_df)
plt.title('Feature Importances in Random Forest')
plt.show()
​.
[스파르타코딩클럽] 22강. 앙상블 학습 - 그래디언트 부스팅 머신 (GBM) [SCC] 바닥부터 시작하는 머신러닝/[스파르타코딩클럽] 바닥부터 시작하는 머신러닝 - 5주차/[스파르타코딩클럽] 22강. 앙상블 학습 - 그래디언트 부스팅 머신 (GBM) 제작:[스파르타코딩클럽] 22강. 앙상블 학습 - 그래디언트 부스팅 머신 (GBM) [수업 목표]앙상블 학습의 그래디언트 부스팅머신에 대해서 배우고 실습해 봅시다[목차]01. 그래디언트 부스팅 머신02.GBM 실습💡
 
 01. 그래디언트 부스팅 머신✔️앙상블 기법 중 하나인 그래디언트 부스팅 머신에 대해 배워봅시다1) 그래디언트 부스팅 머신 그래디언트 부스팅 머신이란?그래디언트 부스팅 머신(Gradient Boosting Machine, GBM)은 여러 개의 약한 학습기(weak learner)를 순차적으로 학습시키고, 그 예측 결과를 결합하여 강한 학습기(strong learner)를 만드는 앙상블 기법입니다.GBM은 이전 모델이 잘못 예측한 데이터 포인트에 가중치를 부여하여, 다음 모델이 이를 더 잘 학습하도록 합니다.각 트리가 독립적으로 학습되기 때문에, 과적합을 방지하고 예측 성능을 향상시킬 수 있습니다. GBM의 구조GBM은 여러 개의 결정 트리(Decision Tree)로 구성됩니다.각 결정 트리는 이전 트리의 예측 오류를 보완하는 방식으로 학습됩니다. GBM은 각 트리의 예측 결과를 가중합하여 최종 예측을 수행합니다. GBM의 원리초기 모델 학습: 첫 번째 결정 트리를 학습시켜 초기 모델을 만듭니다.잔여 오차 계산: 초기 모델의 예측 결과와 실제 값 간의 잔여 오차를 계산합니다.잔여 오차 학습: 잔여 오차를 예측하는 새로운 결정 트리를 학습시킵니다.모델 업데이트: 새로운 결정 트리를 기존 모델에 추가하여 모델을 업데이트합니다.반복: 잔여 오차가 충분히 작아질 때까지 2~4 단계를 반복합니다. 그래디언트 부스팅의 단계적 학습GBM은 단계적으로 학습을 진행하여, 이전 모델의 오류를 보완하는 방식으로 예측 성능을 향상시킵니다.각 단계에서 학습된 모델은 이전 모델의 잔여 오차를 줄이는 데 집중합니다02.GBM 실습✔️유방암 데이터셋을 이용하여 GBM을 구현하고 평가하는 실습을 합니다1) 유방암  데이터셋 데이터 로드 자동차 수요예측 데이터셋 로드 {5px}자동차 수요예측 데이터셋 로드 ﻿​Python복사from sklearn.ensemble import BaggingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# 유방암 데이터 로드
cancer_data = load_breast_cancer()
X, y = cancer_data.data, cancer_data.target

# 데이터 분할
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# 데이터 스케일링
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
​ GBM 모델 구현 및 평가Scikit-learn의 GradientBoostingRegressor를 사용하여 GBM을 구현하고 평가합니다.GBM 모델 구현 및 평가 {5px}GBM 모델 구현 및 평가 ﻿​Python복사from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_squared_error

# GBM 모델 생성
gbm_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
# 모델 학습
gbm_model.fit(X_train_scaled, y_train)
# 예측
y_pred_gbm = gbm_model.predict(X_test_scaled)
# 평가
mse_gbm = mean_squared_error(y_test, y_pred_gbm)
print(f'GBM 모델의 MSE: {mse_gbm}')
​.
[스파르타코딩클럽] 23강. 앙상블 학습 - XGBoost[SCC] 바닥부터 시작하는 머신러닝/[스파르타코딩클럽] 바닥부터 시작하는 머신러닝 - 5주차/[스파르타코딩클럽] 23강. 앙상블 학습 - XGBoost제작:[스파르타코딩클럽] 23강. 앙상블 학습 - XGBoost[수업 목표]앙상블 학습의 XGBoost에 대해서 배우고 실습해 봅시다[목차]01. XGBoost02.XGBoost 실습💡
 
 01. XGBoost✔️앙상블 기법 중 하나인 XGBoost에 대해 배워봅시다1) XGBoost XGBoost란?XGBoost(eXtreme Gradient Boosting)는 그래디언트 부스팅 알고리즘을 기반으로 한 고성능 앙상블 학습 기법입니다.XGBoost는 효율성, 유연성, 이식성을 목표로 설계되었으며, 다양한 머신러닝 경진대회에서 우수한 성능을 보여주고 있습니다.병렬 처리, 조기 종료, 정규화 등의 기능을 통해 성능을 극대화합니다. XGBoost의 구조XGBoost는 여러 개의 결정 트리(Decision Tree)로 구성됩니다. 각 결정 트리는 이전 트리의 예측 오류를 보완하는 방식으로 학습됩니다.XGBoost는 각 트리의 예측 결과를 가중합하여 최종 예측을 수행합니다. XGBoost의 원리초기 모델 학습: 첫 번째 결정 트리를 학습시켜 초기 모델을 만듭니다.잔여 오차 계산: 초기 모델의 예측 결과와 실제 값 간의 잔여 오차를 계산합니다.잔여 오차 학습: 잔여 오차를 예측하는 새로운 결정 트리를 학습시킵니다.모델 업데이트: 새로운 결정 트리를 기존 모델에 추가하여 모델을 업데이트합니다.반복: 잔여 오차가 충분히 작아질 때까지 2~4 단계를 반복합니다. XGBoost의 장점병렬 처리: 트리의 분할을 병렬로 수행하여 학습 속도를 향상시킵니다.조기 종료: 검증 데이터셋의 성능이 향상되지 않으면 학습을 조기에 종료하여 과적합을 방지합니다.정규화: L1 및 L2 정규화를 통해 모델의 복잡도를 조절하고 과적합을 방지합니다.유연성: 다양한 손실 함수와 평가 지표를 지원하여 다양한 문제에 적용할 수 있습니다.02.XGBoost 실습✔️자동차 수요예측 데이터셋을 이용하여 XGBoost을 구현하고 평가하는 실습을 합니다1)  유방암  데이터셋 데이터 로드 자동차 수요예측 데이터셋 로드 {5px}자동차 수요예측 데이터셋 로드 ﻿​Python복사from sklearn.ensemble import BaggingRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import mean_squared_error
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# 유방암 데이터 로드
cancer_data = load_breast_cancer()
X, y = cancer_data.data, cancer_data.target

# 데이터 분할
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# 데이터 스케일링
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
​ XGBoost 모델 구현 및 평가XGBoost 라이브러리를 사용하여 XGBoost 모델을 구현하고 평가합니다.XGBoost 모델 구현 및 평가 {5px}XGBoost 모델 구현 및 평가 ﻿​Python복사import xgboost as xgb
from sklearn.metrics import mean_squared_error

# XGBoost 모델 생성
xgb_model = xgb.XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)
# 모델 학습
xgb_model.fit(X_train_scaled, y_train)
# 예측
y_pred_xgb = xgb_model.predict(X_test_scaled)
# 평가
mse_xgb = mean_squared_error(y_test, y_pred_xgb)
print(f'XGBoost 모델의 MSE: {mse_xgb}')
​.
