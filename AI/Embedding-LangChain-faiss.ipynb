{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 문장 변환 임베딩\n",
    "# pip install sentence-transformers\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "model = SentenceTransformer('intfloat/multilingual-e5-large')\n",
    "\n",
    "sentences = [\n",
    "    \"참새는 짹짹하고 웁니다.\",\n",
    "    \"LangChain과 Faiss를 활용한 예시입니다.\",\n",
    "    \"자연어 처리를 위한 임베딩 모델 사용법을 배워봅시다.\",\n",
    "    \"유사한 문장을 검색하는 방법을 살펴보겠습니다.\",\n",
    "    \"강좌를 수강하시는 수강생 여러분 감사합니다!\"\n",
    "]\n",
    "\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "print(embeddings.shape)\n",
    "print(embeddings[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. LangChain 을 활용한 OpenAI 모델 사용\n",
    "# pip install langchain langchain-openai faiss-cpu\n",
    "\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = getpass(\"OpenAI API key 입력: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "안녕하세요! 무엇을 도와드릴까요? 궁금한 점이나 필요한 정보가 있으면 말씀해 주세요.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# 모델 초기화\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "#모델에 메세지 전달\n",
    "response = model.invoke([HumanMessage(content=\"안녕하세요, 무엇을 도와드릴까요?\")])\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='Translate the following sentence from English to French:', additional_kwargs={}, response_metadata={}), HumanMessage(content='How are you?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# 시스템 메시지 설정\n",
    "system_template = \"Translate the following sentence from English to {language}:\"\n",
    "\n",
    "# 사용자 텍스트 입력\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_template),\n",
    "    (\"user\", \"{text}\")\n",
    "])\n",
    "\n",
    "# 프롬프트 생성\n",
    "result = prompt_template.invoke({\"language\": \"French\", \"text\": \"How are you?\"})\n",
    "print(result.to_messages())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Dónde está la biblioteca?\n"
     ]
    }
   ],
   "source": [
    "# LangChain Expression Language 로 체인 연결\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 응답을 파싱하는 파서 초기화\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# 템플릿, 모델, 파서를 체인으로 연결\n",
    "chain = prompt_template | model | parser\n",
    "\n",
    "# 체인 실행\n",
    "response = chain.invoke({\"language\": \"Spanish\", \"text\": \"Where is the library?\"})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* 내일 날씨는 맑고 따뜻할 예정입니다. [{'source': 'news'}]\n",
      "* 주식 시장이 경기 침체 우려로 하락 중입니다. [{'source': 'news'}]\n",
      "* [SIM=0.159] LangChain을 사용해 프로젝트를 구축하고 있습니다! [{'source': 'tweet'}]\n"
     ]
    }
   ],
   "source": [
    "# 3. FAISS를 활용한 벡터 데이터베이스 구성 및 쿼리\n",
    "\n",
    "# Step 1: OpenAI 임베딩 모델로 벡터 임베딩 생성\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# OpenAI 임베딩 모델 초기화\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "# Step 2: FAISS 인덱스 초기화\n",
    "import faiss\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "\n",
    "# FAISS 인덱스 생성\n",
    "index = faiss.IndexFlatL2(len(embeddings.embed_query(\"hello world\")))\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={}\n",
    ")\n",
    "\n",
    "# Step 3: 벡터 데이터베이스에 문서 추가\n",
    "from langchain_core.documents import Document\n",
    "from uuid import uuid4\n",
    "\n",
    "# 문서 생성\n",
    "documents = [\n",
    "    Document(page_content=\"LangChain을 사용해 프로젝트를 구축하고 있습니다!\", metadata={\"source\": \"tweet\"}),\n",
    "    Document(page_content=\"내일 날씨는 맑고 따뜻할 예정입니다.\", metadata={\"source\": \"news\"}),\n",
    "    Document(page_content=\"오늘 아침에는 팬케이크와 계란을 먹었어요.\", metadata={\"source\": \"personal\"}),\n",
    "    Document(page_content=\"주식 시장이 경기 침체 우려로 하락 중입니다.\", metadata={\"source\": \"news\"}),\n",
    "]\n",
    "\n",
    "# 고유 ID 생성 및 문서 추가\n",
    "uuids = [str(uuid4()) for _ in range(len(documents))]\n",
    "vector_store.add_documents(documents=documents, ids=uuids)\n",
    "\n",
    "\n",
    "# Step 4: 벡터 데이터베이스 쿼리\n",
    "# 기본 유사성 검색\n",
    "results = vector_store.similarity_search(\"내일 날씨는 어떨까요?\", k=2, filter={\"source\": \"news\"})\n",
    "for res in results:\n",
    "    print(f\"* {res.page_content} [{res.metadata}]\")\n",
    "\n",
    "# 점수와 함께 유사성 검색\n",
    "results_with_scores = vector_store.similarity_search_with_score(\"LangChain에 대해 이야기해주세요.\", k=2, filter={\"source\": \"tweet\"})\n",
    "for res, score in results_with_scores:\n",
    "    print(f\"* [SIM={score:.3f}] {res.page_content} [{res.metadata}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug Output: 강사이름은?\n",
      "Debug Output: {'context': [Document(metadata={'source': 'news'}, page_content='내일 날씨는 맑고 따뜻할 예정입니다.')], 'question': '강사이름은?'}\n",
      "Final Response:\n",
      "주어진 문맥에서는 강사 이름에 대한 정보가 없습니다.\n"
     ]
    }
   ],
   "source": [
    "# 3. RAG 체인에 FAISS 통합\n",
    "\n",
    "# Step 1: Retriever로 변환\n",
    "retriever = vector_store.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 1})\n",
    "\n",
    "# Step 2: RAG 체인 생성\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# 프롬프트 템플릿 정의\n",
    "contextual_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the question using only the following context.\"),\n",
    "    (\"user\", \"Context: {context}\\\\n\\\\nQuestion: {question}\")\n",
    "])\n",
    "\n",
    "\n",
    "class DebugPassThrough(RunnablePassthrough):\n",
    "    def invoke(self, *args, **kwargs):\n",
    "        output = super().invoke(*args, **kwargs)\n",
    "        print(\"Debug Output:\", output)\n",
    "        return output\n",
    "# 문서 리스트를 텍스트로 변환하는 단계 추가\n",
    "class ContextToText(RunnablePassthrough):\n",
    "    def invoke(self, inputs, config=None, **kwargs):  # config 인수 추가\n",
    "        # context의 각 문서를 문자열로 결합\n",
    "        context_text = \"\\n\".join([doc.page_content for doc in inputs[\"context\"]])\n",
    "        return {\"context\": context_text, \"question\": inputs[\"question\"]}\n",
    "\n",
    "# RAG 체인에서 각 단계마다 DebugPassThrough 추가\n",
    "rag_chain_debug = {\n",
    "    \"context\": retriever,                    # 컨텍스트를 가져오는 retriever\n",
    "    \"question\": DebugPassThrough()        # 사용자 질문이 그대로 전달되는지 확인하는 passthrough\n",
    "}  | DebugPassThrough() | ContextToText()|   contextual_prompt | model\n",
    "\n",
    "# 질문 실행 및 각 단계 출력 확인\n",
    "response = rag_chain_debug.invoke(\"강사이름은?\")\n",
    "print(\"Final Response:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAISS 인덱스의 저장 및 로드\n",
    "# 인덱스 저장\n",
    "vector_store.save_local(\"faiss_index\")\n",
    "\n",
    "# 저장된 인덱스 로드\n",
    "new_vector_store = FAISS.load_local(\"faiss_index\", embeddings)\n",
    "\n",
    "# FAISS 데이터베이스 병합\n",
    "db1 = FAISS.from_texts([\"문서 1 내용\"], embeddings)\n",
    "db2 = FAISS.from_texts([\"문서 2 내용\"], embeddings)\n",
    "\n",
    "# 병합\n",
    "db1.merge_from(db2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv1014",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
