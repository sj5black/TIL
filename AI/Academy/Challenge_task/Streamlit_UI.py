from langchain.vectorstores import Chroma
from langchain.embeddings.openai import OpenAIEmbeddings
import streamlit as st
# from RAG_comparing_Task import AI_custom_model  # í•¨ìˆ˜ ê°€ì ¸ì˜¤ê¸°

# Streamlit UI êµ¬ì„±
st.set_page_config(page_title="AI Chatbot", page_icon="ğŸ¤–", layout="wide")

st.title("AI Chatbot")
st.write("GPTì™€ ëŒ€í™”í•˜ê³ , ìš”ì•½ ë° ì›ë³¸ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ì„ ë¹„êµí•˜ì„¸ìš”.")

# ëŒ€í™” ìƒíƒœ ê´€ë¦¬
if "messages" not in st.session_state:
    st.session_state["messages"] = []

# ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ
try:
    vectorstore_summary = Chroma(
        persist_directory="./chroma_summary_store",
        embedding_function=OpenAIEmbeddings()
    )
    vectorstore_original = Chroma(
        persist_directory="./chroma_original_store",
        embedding_function=OpenAIEmbeddings()
    )

    retriever_s = vectorstore_summary.as_retriever()
    retriever_o = vectorstore_original.as_retriever()

except Exception as e:
    print(f"ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from pprint import pprint

llm = ChatOpenAI(model="gpt-4o-mini")

# ì±—ë´‡ ëŒ€í™” ì‹œì‘
def AI_custom_model(query : str):
    
    # ë¦¬íŠ¸ë¦¬ë²„ë¥¼ ì°¸ì¡°í•˜ëŠ” ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt = ChatPromptTemplate.from_messages([
        ("system", "Answer the question briefly, based on the following context. \n\n {context}")
    ])

    # RAG ì²´ì¸ ì„ ì–¸ (ì›ë³¸/ìš”ì•½)
    rag_chain_s = (
            {"context": retriever_s}
            | prompt
            | llm
            | StrOutputParser()
    )

    rag_chain_o = (
            {"context": retriever_o}
            | prompt
            | llm
            | StrOutputParser()
    )

    if query == "exit" : return "ëŒ€í™”ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤."

    # GPT ë‹µë³€ ì¶œë ¥
    answer_o = rag_chain_o.invoke(query)
    print(" - GPT Answer based on original - ")
    pprint(answer_o)
    answer_s = rag_chain_s.invoke(query)
    print(" - GPT Answer based on summary - ")
    pprint(answer_s)
    print("\n")

    answer = f" - GPT Answer based on original - \n{answer_o}\n\n - GPT Answer based on summary - \n{answer_s}"
    return answer

# ì‚¬ìš©ì ì…ë ¥
user_input = st.text_input("ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”:", key="user_input")
if st.button("ì „ì†¡") and user_input:
    # GPT ì‘ë‹µ ìƒì„±
    with st.spinner("AI ì‘ë‹µ ìƒì„± ì¤‘..."):
        try:
            result = AI_custom_model(user_input)  # í•¨ìˆ˜ í˜¸ì¶œ
            st.session_state["messages"].append({"role": "user", "content": user_input})
            st.session_state["messages"].append({"role": "bot", "content": result})
        except Exception as e:
            st.error(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# ëŒ€í™” ê¸°ë¡ í‘œì‹œ
for message in st.session_state["messages"]:
    if message["role"] == "user":
        st.text_area("ğŸ‘¤ ì‚¬ìš©ì:", value=message["content"], height=100)
    elif message["role"] == "bot":
        st.text_area("ğŸ¤– AI:", value=message["content"], height=230)

