{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\sj5bl/.cache\\torch\\hub\\huggingface_pytorch-transformers_main\n",
      "Using cache found in C:\\Users\\sj5bl/.cache\\torch\\hub\\huggingface_pytorch-transformers_main\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Once upon a time, the world was a place of great beauty and great danger. The world was a place of great danger, and the world was a place of great danger. The world was a place of great danger, and the world was a'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1 generate sentence with Transformer Model\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "model = Transformer(d_model=512, nhead=8, num_encoder_layers=6, num_decoder_layers=6)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     optimizer.zero_grad()\n",
    "#     output = model(src, tgt)\n",
    "#     loss = criterion(output, tgt_labels)\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "\"\"\"학습에 너무 오랜 시간이 걸리므로, 아래 명령어로 사전학습된 모델 호출\"\"\"\n",
    "# pip install sentencepiece sacremoses\n",
    "\n",
    "model = torch.hub.load('huggingface/pytorch-transformers', 'modelForCausalLM', 'gpt2')\n",
    "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'gpt2')\n",
    "\n",
    "input_text = \"Once upon a time\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "output = model.generate(input_ids, max_length = 50, num_return_sequences = 1)\n",
    "tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'I have a cat because his mother died of leukemia, then I came home from work and we were seeing pictures of his body on a wall,\" he said. \"We took a look at his body and when we looked at us his body would start moving. Our neighbor said \\'how can this happen?\\'\"\\n\\nPolice and firefighters brought an 11-year-old girl to hospital, but her remains were pronounced dead from her injuries.\\n\\nA 17-year-old girl has been charged with'},\n",
       " {'generated_text': 'I have a cat I\\'ve run away from, and she\\'s still here. All kinds of things happen...But it\\'s nice to know she\\'s getting on with how she is. I have no fear of him being here anymore.\"\\n\\nI asked the boy what he was saying, \"He\\'s so beautiful. He\\'s so quiet, he\\'s very quiet. You just don\\'t know if he can hear you.\" I asked how much I can expect him to get. (laughs)\\n'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2 generate sentence with GPT 2\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from transformers import pipeline\n",
    "\n",
    "func = pipeline(\"text-generation\", model = \"gpt2\")\n",
    "result = func(\"I have a cat\", max_length = 100, num_return_sequences = 2)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998200535774231}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3 sentiment analysis (감정어 분석)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "func = pipeline(\"sentiment-analysis\")\n",
    "result = func(\"I'm fine.\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['the', 'quick', 'brown', 'fox', 'jump', 'over', 'the', 'lazy', 'dog'], ['love', 'playing', 'with', 'my', 'pet', 'dog'], ['the', 'dog', 'barks', 'at', 'the', 'stranger'], ['the', 'cat', 'sleeps', 'on', 'the', 'sofa']]\n",
      "-0.16885965776110257\n"
     ]
    }
   ],
   "source": [
    "#4 gensim model을 활용한 단어간 유사도 계산\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from scipy.spatial.distance import cosine\n",
    "import numpy as np\n",
    "\n",
    "sentences = [\n",
    "    \"The quick brown fox jump over the lazy dog\",\n",
    "    \"I love playing with my pet dog\",\n",
    "    \"The dog barks at the stranger\",\n",
    "    \"The cat sleeps on the sofa\",\n",
    "]\n",
    "\n",
    "func = [simple_preprocess(i) for i in sentences]\n",
    "print(func)\n",
    "\n",
    "model = Word2Vec(sentences = func, vector_size=5, window =5, min_count=1, sg = 0)\n",
    "# window : 단어를 예측할 때 고려할 주변 단어들의 범위를 지정. window=5 라면 현재 단어를 기준으로 좌우 5개의 단어를 문맥으로 고려하여 예측\n",
    "# min_count : 전체 문장 리스트에서 등장 빈도가 해당값 미만인 단어는 학습에서 제외\n",
    "# sg: 모델 유형 (0: CBOW, 1: Skip-gram, 기본 0)\n",
    "\n",
    "dog = model.wv['dog']\n",
    "cat = model.wv['cat']\n",
    "\n",
    "# cosine 유사도 계산\n",
    "sim = 1 - cosine(dog,cat)\n",
    "print(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8146521854087398\n"
     ]
    }
   ],
   "source": [
    "#5 BERT 기반으로 문장 간 유사도 판별\n",
    "\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "sentences = [\n",
    "    \"The quick brown fox jump over the lazy dog\",\n",
    "    \"I love playing with my pet dog\",\n",
    "    \"The dog barks at the stranger\",\n",
    "    \"The cat sleeps on the sofa\",\n",
    "    \"You don't love me.\",\n",
    "    \"You hate me, actually.\",\n",
    "]\n",
    "\n",
    "# return_tensors 종류\n",
    "# 'pt': PyTorch 텐서로 반환. PyTorch를 사용하는 모델에 입력할 때 필요\n",
    "# 'tf': TensorFlow 텐서로 반환. TensorFlow를 사용하는 모델에 입력할 때 필요\n",
    "# 'np': NumPy 배열로 반환. 모델이나 연산에서 NumPy 배열을 사용할 때 필요\n",
    "# None: 텐서나 배열 형식으로 변환하지 않고 그냥 파이썬 리스트 또는 딕셔너리로 반환\n",
    "\n",
    "input1 = tokenizer(sentences[4], return_tensors='pt')\n",
    "input2 = tokenizer(sentences[5], return_tensors='pt')\n",
    "\n",
    "with torch.no_grad():\n",
    "    output1 = model(**input1)\n",
    "    output2 = model(**input2)\n",
    "\n",
    "embedding1 = output1.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "embedding2 = output2.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "# output.last_hidden_state.mean(dim=1): 모델의 마지막 히든 상태에서 각 문장의 단어 벡터를 평균하여 문장 벡터 반환\n",
    "# squeeze(): 결과 텐서에서 불필요한 차원을 제거하여, 문장 임베딩이 (batch_size, hidden_size) 형태가 되도록 유도\n",
    "# cpu(): 만약 GPU에서 계산된 텐서라면 이를 CPU로 옮김\n",
    "# numpy(): 최종적으로 PyTorch 텐서를 Numpy 배열로 변환\n",
    "\n",
    "# cosine 유사도 계산\n",
    "sim = 1 - cosine(embedding1, embedding2)\n",
    "print(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I want to apologize.'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#6 언어 번역 모델 (성능 X, 참조용)\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# NLLB-200 모델과 tokenizer 로드\n",
    "model_name = \"facebook/nllb-200-distilled-600M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "#번역할 문장\n",
    "sentence = \"사과 먹고 싶다.\"\n",
    "\n",
    "#입력할 문장을 토큰화\n",
    "encoded_sentence = tokenizer(sentence, return_tensors='pt')\n",
    "\n",
    "# 번역 대상 언어 설정\n",
    "tokenizer.src_lang = \"Hangul\"\n",
    "# tokenizer.tgt_lang = \"Latin\"  # 영어로 번역\n",
    "\n",
    "#번역 수행\n",
    "func = model.generate(encoded_sentence.input_ids, forced_bos_token_id=tokenizer.convert_tokens_to_ids(\"eng_Latn\"), max_length=30)\n",
    "\n",
    "#번역 결과 디코딩\n",
    "result = tokenizer.decode(func[0], skip_special_tokens=True)\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv1014",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
